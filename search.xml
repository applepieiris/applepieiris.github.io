<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Adaptation Tuning of LLMs</title>
    <url>/2023/05/19/Adaptation-Tuning-of-LLMs/</url>
    <content><![CDATA[<p>让LLM适配specific的下游任务，两条线：1. 在prompt engineering上下功夫
2. Fine-tune LLM.
其实这两条线并不分家，中间也有一些技术是有overlap的。prompt
engineering并不只是手动设计prompt让LLM返回更好的结果，使得其在下游任务中得以使用，一些研究并不想自己手动设计prompt，那就产生了很多自动产生prompt的方式。刘鹏飞博士的review文章<a href="http://arxiv.org/abs/2107.13586">Pre-train, Prompt, and Predict: A
Systematic Survey of Prompting Methods in Natural Language
Processing</a>将这些技术统一到一个体系里来，分类方式也比较清晰：</p>
<figure>
<img src="/2023/05/19/Adaptation-Tuning-of-LLMs/image-20230524100148539.png" alt="characteristics of different tuning strategies">
<figcaption aria-hidden="true">characteristics of different tuning
strategies</figcaption>
</figure>
<h1 id="full-fine-tunepromptless-finetune">Full Fine-tune（Promptless
Finetune）</h1>
<p>Bert就是一个典型的应用，将模型在一个很大的语料库上pretrain之后，再在一些任务的数据集上对模型参数进行调整。注意这里模型的所有参数都会进行调整。</p>
<figure>
<img src="/2023/05/19/Adaptation-Tuning-of-LLMs/image-20230519142832464.png" alt="image-20230519142832464">
<figcaption aria-hidden="true">image-20230519142832464</figcaption>
</figure>
<p>对所有模型参数调整就带来很多问题：</p>
<ul>
<li>要维护每一个task上的模型，有一些模型的参数量都是亿级别的，这对存储是一个考验</li>
<li>finetune所有参数就需要数据集达到一定的数量级，这在特定领域不一定是可以达到的；如果没有很多数据，有可能finetune完之后还会引起perfomance的下降或者过拟合。</li>
<li>计算资源的限制</li>
</ul>
<h1 id="more-efficient-ways-of-tuning">More Efficient Ways of
Tuning</h1>
<p>或许有更合适的tuning方式，less overfitting and more efficient
finetuning and inference</p>
<h2 id="prefix-tuning">Prefix-Tuning</h2>
<p><a href="https://arxiv.org/pdf/2101.00190.pdf">Prefix-Tuning:
Optimizing Continuous Prompts for Generation</a></p>
<figure>
<img src="/2023/05/19/Adaptation-Tuning-of-LLMs/image-20230519150912298.png" alt="prefix Tuning">
<figcaption aria-hidden="true">prefix Tuning</figcaption>
</figure>
<p>一开始理解prefix
tuning其实是从“如果不调整所有参数，那么是不是可以调整部分参数”来思考这个模型的。但是看了原文之后会发现作者的思考路径其实有点不太一样。paper中说</p>
<blockquote>
<p>Prefix-tuning draws inspiration from prompting, allowing subsequent
tokens to attend to this prefix as if it were “virtual tokens”</p>
</blockquote>
<p>lilian的<a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#automatic-prompt-design">博客</a>对于这个也解释的蛮有意思：</p>
<blockquote>
<p>Prompt is a sequence of prefix tokens that increase the probability
of getting desired output given input. Therefore we can treat them as
trainable parameters and <a href="https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#smart-prompt-design">optimize
them directly</a> on the embedding space via gradient descent, such as
<strong>AutoPrompt</strong> (<a href="https://arxiv.org/abs/2010.15980">Shin et al., 2020</a>,
<strong>Prefix-Tuning</strong> (<a href="https://arxiv.org/abs/2101.00190">Li &amp; Liang (2021)</a>),
<strong>P-tuning</strong> (<a href="https://arxiv.org/abs/2103.10385">Liu et al. 2021</a>) and
<strong>Prompt-Tuning</strong> (<a href="https://arxiv.org/abs/2104.08691">Lester et al. 2021</a>). <a href="https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#smart-prompt-design">This
section in my “Controllable Neural Text Generation” post</a> has a good
coverage of them. The trend from AutoPrompt to Prompt-Tuning is that the
setup gets gradually simplified.</p>
</blockquote>
<p>也就是既然我们发现in-context
learning是可以促进大语言模型解决特定问题（因为我们让LLM以更高的概率输出我们想要的结果了），那么是不是可以可以把这一部分信息编码进模型参数里，从而在特定地数据集上单独训练这些参数。</p>
<p>所以研究者也想了一些办法如何以最小的成本为特定的任务增加一些参数，fix住预训练模型的大部分参数，而去finetune给每一个任务增加的那一部分参数。其中adapter-tuning就是一种。</p>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Anaconda使用</title>
    <url>/2021/11/05/Anaconda%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>Anaconda 安装包可以到 <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/" class="uri">https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/</a>
下载。</p>
<p>TUNA 还提供了 Anaconda
仓库与第三方源（conda-forge、msys2、pytorch等，查看完整列表）的镜像，各系统都可以通过修改用户目录下的
.condarc 文件。Windows 用户无法直接创建名为 .condarc 的文件，可先执行
conda config --set show_channel_urls yes 生成该文件之后再修改。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: true</span><br><span class="line">channel_alias: https://mirrors.tuna.tsinghua.edu.cn/anaconda</span><br><span class="line">default_channels:</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line">  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br></pre></td></tr></table></figure>
<p>一般表示 conda
应用程序的配置文件，在用户的家目录（windows：C:\users\username\，linux：/home/username/）。但对于.condarc配置文件，是一种可选的（optional）运行期配置文件，其默认情况下是不存在的，但当用户第一次运行
conda config命令时，将会在用户的家目录创建该文件。</p>
<h3 id="换国内源">换国内源</h3>
<ul>
<li>查看现在的源地址：<code>conda config --show-sources</code></li>
<li>设置搜索时显示通道地址
<code>conda config --set show_channel_urls yes</code></li>
<li>添加镜像源
<code>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</code></li>
</ul>
<h1 id="虚拟环境">虚拟环境</h1>
<p>常用命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda list 查看安装了那些包</span><br><span class="line"></span><br><span class="line">conda env list或者conda info -e 查看当前存在哪些虚拟环境</span><br><span class="line"></span><br><span class="line">conda update conda 更新当前conda</span><br><span class="line"></span><br><span class="line">conda create -n env_name python=3.6 创建虚拟环境</span><br><span class="line"></span><br><span class="line">activate env_name 激活某虚拟环境</span><br><span class="line"></span><br><span class="line">python --version 检查当前python版本</span><br><span class="line"></span><br><span class="line">conda install -n env_name [package] 安装某个包至某一个虚拟环境下</span><br><span class="line"></span><br><span class="line">deactivate 关闭当前虚拟环境</span><br><span class="line"></span><br><span class="line">conda remove -n env_name --all 删除某虚拟环境</span><br><span class="line"></span><br><span class="line">conda remove package_name 删除某个包</span><br><span class="line">pip unistall package_name</span><br><span class="line"></span><br><span class="line">conda env export &gt; requirements.yaml 将当前环境下安装的包保存为yaml文件</span><br><span class="line">conda env update -f=/path/requirements.yaml</span><br><span class="line"></span><br><span class="line">(如果不要conda，用pip的时候导出的是txt文件)</span><br><span class="line">pip freeze &gt; requirements.txt</span><br><span class="line">pip install -r /path/requirements.txt</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>其他技术</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention and Transformer model</title>
    <url>/2022/10/27/Attention-and-Transformer-model/</url>
    <content><![CDATA[<p>斯坦福cs231n最新的课程中包含了attention的模型讲解，但是很可惜我们现在只能看到17年的老课程，在youtube上可以找到，课程主页是<a href="http://cs231n.stanford.edu/schedule.html">cs231n</a>。可以在课程主页中下载对应的slides和查看推荐的blog，都是学习attention机制的好教材。另外我在学习cs231n课程过程中，也参考了吴恩达对于sequence
model的讲解，它课程中也涉及到了attention机制，课后作业也包含了简单的attention机制的实现，可以作为辅助理解来看。这篇博客权当自己学习attention以及由此创造的attention系列模型比如transformer的记录。cs231n推荐的<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">博客</a>内容也是很通俗易懂，英文不好的同学有中文翻译可以参考。</p>
<h1 id="general-attention-model">general attention model</h1>
<p>RNN有多种类型的网络：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221027100649410.png" alt="image-20221027100649410">
<figcaption aria-hidden="true">image-20221027100649410</figcaption>
</figure>
<p>对于"many to
many"类型的网络，有可能输入的长度不等于输出的长度，在机器翻译的任务中很常见。这种网络也叫Sequence
to
Sequence，首先该网络会经由encoder对输入进行编码，然后再有decoder进行sequence的生成。但是这种网络在长句子中表现很差，如果输入句子的长度很长，encoder网络就很难记忆住所有信息，从而在decoder中翻译出准确的词语。由此，需要用到attention
model。从计算角度来说就是encoder每次都会产生一个固定长度的vector，这对于长句子来说fixed
length的向量很难记住很早之前的信息：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221027104026997.png" alt="image-20221027104026997">
<figcaption aria-hidden="true">image-20221027104026997</figcaption>
</figure>
<p>那为了解决一个fixed
length的vector很难记忆前序信息的缺陷，所以诞生了attention
机制！具体的就是在decoder阶段的每一个时间步利用都产生不同的context，这个context产生的过程就是attention计算的过程。主要思想是在产生y之前做一个attention的权重计算，这个权重指的是在计算某个时间步的y值时，我们应该对输入句子的每一个词给予多少关注，给予的关注多，权重就大。所以这里我们会基于initial
decoder state（previous hidden state of the (post-attention)
LSTM）和encoder网络的输出值计算权重，计算过程采用dense
layer.这些权重值的和是1。</p>
<p>对于很长输入的句子，encoder不再是输出一个固定的context。如下：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221027103845891.png" alt="image-20221027103845891">
<figcaption aria-hidden="true">image-20221027103845891</figcaption>
</figure>
<p>context的详细计算如下：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221027101307861.png" alt="吴恩达课程attention">
<figcaption aria-hidden="true">吴恩达课程attention</figcaption>
</figure>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221027101324129.png" alt="吴恩达课程attention">
<figcaption aria-hidden="true">吴恩达课程attention</figcaption>
</figure>
<p>上面两张图是吴恩达在深度学习专项课程中的讲解细节，在cs231n的图中可以看出来它将dense
layer的输出单独列出来了，也就是下图中alignment
scores（<code>e</code>），在e的基础上再计算attention
weights（<code>a</code>），所有的attention weights总和为1 <img src="/2022/10/27/Attention-and-Transformer-model/image-20221206141252321.png" alt="cs231n_attention"></p>
<p>但是总体来说两个的讲解方式是一致的，cs231n这门课为什么把其中的<code>e</code>单独拎出来也是有它的用意，主要为了后面讲解self-attention。这里我琢磨了好一会儿才明白。还有一点值得提一下，吴恩达的图里那个与hidden
state 一同输入进dense
layer的repeateVector不要搞混淆，其实这里每次在decoder的一个时间步计算context时用到的s都不一样，比如在上面的里，计算<code>c1</code>用的是encoder的最后一个hidden
state，而计算<code>C2</code>的时候我们需要用decoder的第一个时间步的hidden
state来计算：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221206142430931-16703078727111.png" alt="decoder第二个时间步">
<figcaption aria-hidden="true">decoder第二个时间步</figcaption>
</figure>
<p>这种attention计算机制如果用到image
caption上面如何做呢？获取图片的特征我们使用CNN来抓取，得到的feature
map用来当作rnn中的hidden state计算：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221206145117196-16703094790462.png" alt="image caption">
<figcaption aria-hidden="true">image caption</figcaption>
</figure>
<p>理解以上的原理很重要，再把上面这个图改写一下，将h变成decoder的query：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221206145317028-16703095989033.png" alt="query">
<figcaption aria-hidden="true">query</figcaption>
</figure>
<p>以上都是简单的attention机制，随后的transformer模型真正的将attention机制推广开来，见transformer的<a href="https://arxiv.org/abs/1706.03762">paper</a></p>
<h1 id="attention-is-all-you-need-2017">Attention is all you need
2017</h1>
<p>在transformer的<a href="https://arxiv.org/abs/1706.03762">paper</a>中，作者首先介绍本文：主流的sequence
tranduction模型主要基于复杂的RNN或者CNN模型，它们包含encoder和decoder两部分，其中表现最好的模型在encoder和decoder之间增加了attention
mechanism。本文提出了一个新的简单的网络结构名叫<em>transformer</em>，也是完全基于attention机制，"dispensing
with recurrence and convolutions entirely"!
根本无需循环和卷积！了不起的Network~</p>
<p>在阅读这篇文章之前需要提前了解我在另外一篇博客 Attention and
transformer
model中的知识，在translation领域我们的科学家们是如何从RNN循环神经网络过渡到CNN，然后最终是transformer的天下的状态。技术经过了一轮轮的迭代，每一种基础模型架构提出后，会不断的有文章提出新的改进，文章千千万，不可能全部读完，就精读一些经典文章就好，Vaswani这篇文章是NMT领域必读paper，文章不长，加上参考文献才12页，介绍部分非常简单，导致这篇文章的入门门槛很高（个人感觉）。我一开始先读的这篇文章，发现啃不下去，又去找了很多资料来看，其中对我非常帮助的有很多：</p>
<ul>
<li><a href="http://jalammar.github.io/illustrated-transformer/">非常通俗易懂的blog</a>
有中文版本的翻译</li>
<li><a href="http://arxiv.org/abs/1912.02047">Neural Machine
Translation: A Review and Survey</a>
虽然这篇paper很长，90+页。前六章可以作为参照，不多25页左右，写的非常好</li>
<li><a href="http://cs231n.stanford.edu/slides/2022/lecture_11_ruohan.pdf">stanford
cs231n课程的ppt</a>
斯坦福这个课程真的很棒，youtube上可以找到17年的视频，17年的课程中没有attention的内容，所以就姑且看看ppt吧，希望斯坦福有朝一日能将最新的课程分享出来，也算是做贡献了</li>
<li>cs231n推荐的阅读<a href="https://lilianweng.github.io/posts/2018-06-24-attention/">博客</a>
非常全面的整理，强烈建议食用.
这位作者也附上了自己的transformer实现，在它参考的那些github实现里，哈佛大学的<a href="http://nlp.seas.harvard.edu/2018/04/01/attention.html">pytorch实现</a>也值得借鉴。</li>
</ul>
<p>Transformer这篇文章有几个主要的创新点：</p>
<ol type="1">
<li>使用self-attention机制，并首次提出使用multi-head attention</li>
</ol>
<p>该机制作用是在编码当前word的时候，这个self-attention就会告诉我们编码这个词语我们应该放多少注意力在这个句子中其他的词语身上，说白了其实就是计算当前词语和其他词语的关系。这也是CNN用于解决NMT问题时用不同width的kernel来扫input
metric的原因。</p>
<p>multi-head的意思是我使用多个不同的self-attention
layer来处理我们的输入，直观感觉是训练的参数更多了，模型的表现力自然要好一点。</p>
<ol start="2" type="1">
<li>Positional embeddings</li>
</ol>
<p>前一个创新点解决了dependence的问题，那如何解决位置的问题呢？也就是我这个词在编码的时候或者解码的时候应该放置在句子的哪个位置上。文章就用pisitional
embedding来解决这个问题。这个positional embedding和input
embedding拥有相同的shape，所以两者可以直接相加。transformer这篇文章提供了两种encoding方式：</p>
<p>1） sunusoidal positional encoding</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230302133247664.png" alt="image-20230302133247664">
<figcaption aria-hidden="true">image-20230302133247664</figcaption>
</figure>
<p>其中，pos=1,...,L(L是input句子的长度)，i是某一个PE中的一个维度，取值范围是1到dmodel。python实现为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span>(<span class="params">length, depth</span>):</span></span><br><span class="line">  depth = depth/<span class="number">2</span></span><br><span class="line"></span><br><span class="line">  positions = np.arange(length)[:, np.newaxis]     <span class="comment"># (seq, 1)</span></span><br><span class="line">  depths = np.arange(depth)[np.newaxis, :]/depth   <span class="comment"># (1, depth)</span></span><br><span class="line"></span><br><span class="line">  angle_rates = <span class="number">1</span> / (<span class="number">10000</span>**depths)         <span class="comment"># (1, depth)</span></span><br><span class="line">  angle_rads = positions * angle_rates      <span class="comment"># (pos, depth)</span></span><br><span class="line"></span><br><span class="line">  pos_encoding = np.concatenate(</span><br><span class="line">      [np.sin(angle_rads), np.cos(angle_rads)],</span><br><span class="line">      axis=-<span class="number">1</span>) </span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> tf.cast(pos_encoding, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">pos_encoding = positional_encoding(length=<span class="number">2048</span>, depth=<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the shape.</span></span><br><span class="line"><span class="built_in">print</span>(pos_encoding.shape) <span class="comment"># (2014,512)</span></span><br></pre></td></tr></table></figure>
<p>2） learned positional encoding</p>
<p>整体上看，这篇文章提出的transformer模型在做translation的任务时，架构是这样的：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230301133249379.png" alt="image-20230301133249379">
<figcaption aria-hidden="true">image-20230301133249379</figcaption>
</figure>
<p>其中encoders部分包含了6个encoders的block，decoders部分也包含了6个decoders的block，将encoders的每一个block拆开来看，有两个sub
layer：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230301133424302.png" alt="image-20230301133424302">
<figcaption aria-hidden="true">image-20230301133424302</figcaption>
</figure>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230301133440152.png" alt="image-20230301133440152">
<figcaption aria-hidden="true">image-20230301133440152</figcaption>
</figure>
<p>其中decoder部分的block比encoder部分的block多了一个sub
layer，其中self-attention和encoder-decoder attention都是multi-head
attention layer，只不过decoder部分的第一个multi-head attention
layer是一个masked multi-head
attention，为了防止未来的信息泄露给当下（prevent positions from
attending to the future）.</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230301133608484.png" alt="image-20230301133608484">
<figcaption aria-hidden="true">image-20230301133608484</figcaption>
</figure>
<p>在transformer模型中，作者还使用了residual
connection，所以在encoder的每一个block中，数据的flow是:</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230307162830473-16781777124571.png" alt="transformer架构">
<figcaption aria-hidden="true">transformer架构</figcaption>
</figure>
<p>其中self-attention中涉及的运算details是：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230301134258180.png" alt="image-20230301134258180">
<figcaption aria-hidden="true">image-20230301134258180</figcaption>
</figure>
<p>可以发现其中涉及的运算都是矩阵的点乘，并没有RNN中那种时间步的概念，所以所有运算都是可以parallelizable，这就能使得模型的推理和训练更加的efficient。并且！Transformers也可以抓住distant的依赖，而不是像rnn那样对于长依赖并不是很擅长，因为它前面的信息如果像传递到很后面的单词推理上，需要经历很多时间步的计算，而transformer在推理每一个单词的时候都可以access到input句子中的每一个单词（毕竟我们的Z中包含了每一个单词跟其他单词的关系)。</p>
<p>其中positional encoding现在可以简单的理解成在我们编码的word
embedding上我们又加了一个positional
encoding，维度和我们的embedding一模一样。</p>
<blockquote>
<p>在tensorflow中有一个layer是<code>MultiHeadAttention</code>,如果我们想实现transformer里的这个self-attention，那就是query，key，value其实都是由input
vector计算来的。</p>
</blockquote>
<p>以上的理论计算看起来可能会有点模糊，可以同步参照<a href="https://lilianweng.github.io/posts/2018-06-24-attention/">博客</a>
参考 <a href="http://jalammar.github.io/illustrated-transformer/">illustrated
transformer</a>介绍的详细细节，基于tensorflow框架实现的<a href="https://github.com/lilianweng/transformer-tensorflow/blob/master/transformer.py">transformer</a>来帮助自己理解transformer模型。</p>
<h2 id="encoder部分">encoder部分</h2>
<p>encoder的每一个block由两个sub-layer组成，中间穿插resnet
connection。</p>
<p><img src="/2022/10/27/Attention-and-Transformer-model/transformer_encoder_block.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multihead_attention</span>(<span class="params">self, query, memory=<span class="literal">None</span>, mask=<span class="literal">None</span>, scope=<span class="string">&#x27;attn&#x27;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            query (tf.tensor): of shape (batch, q_size, d_model)</span></span><br><span class="line"><span class="string">            memory (tf.tensor): of shape (batch, m_size, d_model)</span></span><br><span class="line"><span class="string">            mask (tf.tensor): shape (batch, q_size, k_size)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:h</span></span><br><span class="line"><span class="string">            a tensor of shape (bs, q_size, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> memory <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment"># 如果memory是None，那么就是一个典型的self-attention layer</span></span><br><span class="line">            memory = query</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># Linear project to d_model dimension: [batch, q_size/k_size, d_model]</span></span><br><span class="line">            Q = tf.layers.dense(query, self.d_model, activation=tf.nn.relu)</span><br><span class="line">            K = tf.layers.dense(memory, self.d_model, activation=tf.nn.relu)</span><br><span class="line">            V = tf.layers.dense(memory, self.d_model, activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Split the matrix to multiple heads and then concatenate to have a larger</span></span><br><span class="line">            <span class="comment"># batch size: [h*batch, q_size/k_size, d_model/num_heads]</span></span><br><span class="line">            Q_split = tf.concat(tf.split(Q, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            K_split = tf.concat(tf.split(K, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            V_split = tf.concat(tf.split(V, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            mask_split = tf.tile(mask, [self.h, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Apply scaled dot product attention</span></span><br><span class="line">            out = self.scaled_dot_product_attention(Q_split, K_split, V_split, mask=mask_split)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Merge the multi-head back to the original shape</span></span><br><span class="line">            out = tf.concat(tf.split(out, self.h, axis=<span class="number">0</span>), axis=<span class="number">2</span>)  <span class="comment"># [bs, q_size, d_model]</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># The final linear layer and dropout.</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_model)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dropout(out, rate=self.drop_rate, training=self._is_training)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_forwad</span>(<span class="params">self, inp, scope=<span class="string">&#x27;ff&#x27;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Position-wise fully connected feed-forward network, applied to each position</span></span><br><span class="line"><span class="string">        separately and identically. It can be implemented as (linear + ReLU + linear) or</span></span><br><span class="line"><span class="string">        (conv1d + ReLU + conv1d).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            inp (tf.tensor): shape [batch, length, d_model]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        out = inp</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_ff, activation=tf.nn.relu)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dropout(out, rate=self.drop_rate, training=self._is_training)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_model, activation=None)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># by default, use_bias=True</span></span><br><span class="line">            out = tf.layers.conv1d(out, filters=self.d_ff, kernel_size=<span class="number">1</span>, activation=tf.nn.relu)</span><br><span class="line">            out = tf.layers.conv1d(out, filters=self.d_model, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out  </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encoder_layer</span>(<span class="params">self, inp, input_mask, scope</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            inp: tf.tensor of shape (batch, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            input_mask: tf.tensor of shape (batch, seq_len, seq_len)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        out = inp</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># One multi-head attention + one feed-forword</span></span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(out, mask=input_mask))</span><br><span class="line">            out = self.layer_norm(out + self.feed_forwad(out))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h2 id="decoder部分">decoder部分</h2>
<p><img src="/2022/10/27/Attention-and-Transformer-model/encoder-decoder.png"></p>
<p>在decoder部分，我们可以看到每一个decoder
block的输入有两个：<em>整个encoder部分的输出</em>以及<em>上一个decoder
block的输出（第一个decoder
block是词向量的输入）</em>，而encoder部分的输出是接到每一个decoder
block的第二个sublayer的。正如刚刚提到了，decoder部分的每一个block跟encoder部分的block有一个不一样的地方，那就是多了一个sublayer：
encoder-decoder
attention。至于encoder部分和decoder部分是如何connect的，</p>
<blockquote>
<p>The encoder start by processing the input sequence. The output of the
top encoder is then transformed into a set of attention vectors K and V.
These are to be used by each decoder in its “encoder-decoder attention”
layer which helps the decoder focus on appropriate places in the input
sequence</p>
</blockquote>
<p>也就是我们得到了encoder部分top layer（最后一个encoder
layer）的输出之后，我们将输出转化成K和V.
我们可以看到在<code>multihead_attention</code>里，memory是<code>enc_out</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder_layer</span>(<span class="params">self, target, enc_out, input_mask, target_mask, scope</span>):</span></span><br><span class="line">        out = target</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(</span><br><span class="line">                out, mask=target_mask, scope=<span class="string">&#x27;self_attn&#x27;</span>))</span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(</span><br><span class="line">                out, memory=enc_out, mask=input_mask)) <span class="comment"># 将encoder部分的输出结果作为输入</span></span><br><span class="line">            out = self.layer_norm(out + self.feed_forwad(out))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder</span>(<span class="params">self, target, enc_out, input_mask, target_mask, scope=<span class="string">&#x27;decoder&#x27;</span></span>):</span></span><br><span class="line">        out = target</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_enc_layers):</span><br><span class="line">                out = self.decoder_layer(out, enc_out, input_mask, target_mask, <span class="string">f&#x27;dec_<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>以上实现的transformer其实我觉得还是有一点点复杂，毕竟在tensorflow2.0+版本中已经有了官方实现好的<code>layers.MultiHeadAttention</code>可以使用，应该可以大大简化我们实现步骤，特别是上面的<code>def multihead_attention(self, query, memory=None, mask=None, scope='attn'):</code>。从刚刚的实现里我们可以发现，除了decoder部分每一个block的第二个sublayer的attention计算有一点不一样之外，其他的attention计算都是一模一样的。我在github上找了不少用TF2.0实现的transformer（最标准的也是Attention
is all you
need的模型），发现很多都都写得一般般，最终发现还是tensorflow官方文档写的<a href="https://www.tensorflow.org/text/tutorials/transformer#add_and_normalize">tutotial</a>写的最好.</p>
<p>现在对照tensorflow的tutorial以及上面transformer的计算过程，拆解一下官方给的代码。</p>
<p>首先定义一个baseAttention类，然后在此基础上我们再定义encoder和decoder中的attention：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseAttention</span>(<span class="params">tf.keras.layers.Layer</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)</span><br><span class="line">    self.layernorm = tf.keras.layers.LayerNormalization()</span><br><span class="line">    self.add = tf.keras.layers.Add()</span><br></pre></td></tr></table></figure>
<p>那么针对encoder结果输入到decoder的cross attention
layer怎么处理呢？这时候我们使用MultiHeadAttention时就需要将target
sequence x当作是query，将encoder输出当作是context
sequence也就是key/value。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossAttention</span>(<span class="params">BaseAttention</span>):</span> <span class="comment"># encoder结果输入到decoder的层</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x, context</span>):</span> <span class="comment"># 这里的x是target sequence,context是encoder的输出结果</span></span><br><span class="line">    attn_output, attn_scores = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        key=context,</span><br><span class="line">        value=context,</span><br><span class="line">        return_attention_scores=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Cache the attention scores for plotting later.</span></span><br><span class="line">    self.last_attn_scores = attn_scores</span><br><span class="line"></span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>然后我们再定义global attention，global
attention就是没有任何特殊操作的（比如上面的attention计算它有特别的context），而在transformor中更多的是self-attention，也就是我们传递给MultiHeadAttention的query,key,value都是同一个值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GlobalSelfAttention</span>(<span class="params">BaseAttention</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    attn_output = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        value=x,</span><br><span class="line">        key=x)</span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>最后我们定义causal self attention
layer，这个是在decoder的每一个block的第一个sublayer：self-attention
layer.其实这个layer是和global attention
layer差不多的，但还是有一点微小的差别。为什么呢？因为我们在decoder阶段，我们是一个词语一个词语的预测的，这其实包含了一层因果关系，我们在预测一个词语的时候，我们应该已知它前面一个词语是什么，RNN中的hidden
state传递到下一个时间步就是这个因果关系的传递。那么如果我们使用刚刚我们实现的global
attention layer来实现这个self
attention，并没有包含这个因果关系，不仅如此，如果我们使用常规的self
attention的计算，将target
sequence全部当作输入输入到decoder中的第一个block中，会有未来的数据提前被当前时刻看到的风险，所以在Transformer这篇文章中，作者提出使用mask的技术来避免这个问题。</p>
<p>在tensorflow中实现很简单，就只需要给MultiHeadAttention传递一个use_causal_mask
= True的参数即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CausalSelfAttention</span>(<span class="params">BaseAttention</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    attn_output = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        value=x,</span><br><span class="line">        key=x,</span><br><span class="line">        use_causal_mask = <span class="literal">True</span>) <span class="comment"># The causal mask ensures that each location only has access to the locations that come before it</span></span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这样就可以保证先前的sequence并不依赖于之后的elements。这里我本来有一个疑问是，这样一来这个causal
layer并不能实现bi-rnn的能力？但后来一想并不是，因为双向的RNN的后向是指后面的词语先输入，其实就是从后往前输入，这样就可以知道一个sequence当前词语依赖于后面的词语的权重。</p>
<h2 id="why-transformer">why transformer?</h2>
<p>这里想补充一个东西，在从encoder-decoder过渡到transformer的时候，我一直的疑问是为什么要用transformer呢？为什么Effective
Approaches to Attention-based Neural Machine
Translation这篇paper介绍的方法就渐渐不被人所用了呢？一开始我去看了下transformer的原文，发现paper介绍的非常简单，所以就去找了下博客，找到一篇解释为什么transformer比LSTM快的<a href="https://voidful.medium.com/why-transformer-faster-then-lstm-on-generation-c3f30977d747">博客</a></p>
<p>文章说在传统的也就是paper：Neural Machine Translation by Jointly
Learning to Align and
Translate中介绍的用LSTM的encoder-decoder架构来做机器翻译的问题，一个问题在于：在RNN模型中，我们在计算当前时间步时，需要使用到前一个时间步的hidden_state，这就造成一个问题是无法并行训练，你必须要等到前面的东西都输完了你才能计算当前时间步的结果。transformer就可以解决这个问题，它完全摒弃了RNN的结构，基本上是一个FCNN。首先我们都知道输入到模型中来的是一个序列，序列中的每一个单词我们都转化为了词向量。如果是传统的RNN模型，这时候就要一个vector一个vector的往RNN里输入了，但transformer不是，它是将这个embedding变幻成了三个向量空间，也就是我们后面看到的Q,K,V.</p>
<p>后面又去谷歌了一番，找到一个<a href="https://ai.stackexchange.com/questions/20075/why-does-the-transformer-do-better-than-rnn-and-lstm-in-long-range-context-depen">stack</a>上的问题，也有人有这个疑问，被采取的回答总结起来就是：</p>
<ol type="1">
<li>transformer避免了recursion，从而可以方便并行运算（减少了训练时间），这个说法其实跟上面博客一个意思</li>
<li>同时transformer在长句子的dependency上提高了performance</li>
</ol>
<blockquote>
<ul>
<li><strong>Non sequential</strong>: sentences are processed as a whole
rather than word by word.</li>
<li><strong>Self Attention</strong>: this is the newly introduced 'unit'
used to compute similarity scores between words in a sentence.</li>
<li><strong>Positional embeddings</strong>: another innovation
introduced to replace recurrence. The idea is to use fixed or learned
weights which encode information related to a specific position of a
token in a sentence.</li>
</ul>
</blockquote>
<p>transformer并没有long
dependency的问题，为什么？我们知道transformer的做法是将整个sequence作为一个整体输入到模型。也就是它在预测当前时间步的word时并不依赖于上一个时间步的状态，模型看到的是整个序列，也许有人会质疑双向RNN不是也可以解决这个问题吗，但是这个作者说双向RNN仍然不能彻底解决长句子的依赖问题。</p>
<p>其实在机器翻译领域，为了解决长句子的依赖问题，CNN曾经也被广泛用于解决这个问题，不仅如此，CNN还有共享参数的优点，也就是可以在GPU上并行计算。如何用CNN处理句子可以参考<a href="Convolutional%20Neural%20Networks%20for%20Sentence%20Classification">paper</a>,CNN解决依赖问题是用不同宽度的kernel去学习依赖，比如width=2就学习两个词之间的依赖关系，width=3就学习三个词之间的依赖，但长句子的依赖很可能会有很多组合，所以就需要使用到很多不同宽度的kernel，这是不现实的。虽然现在CNN不怎么用来解决S2S的问题，但我觉得它是RNN结构的模型过度到transformer的一个中间桥梁，同时也可以帮助我们理解attention
is all your need这篇文章。感兴趣的可以读一下<a href="https://arxiv.org/abs/1705.03122">Convolutional Sequence to
Sequence Learning 2017</a></p>
<h2 id="drawbacks-and-variants-of-transformer">drawbacks and variants of
transformer</h2>
<p>transformer的一个很大的问题是它的计算量是随着sequence长度的增加而指数型增加的，从矩阵运算我们就可以发现每一个单词都和句子中的其他单词做了attention
score的计算。所以自从17年transformer模型出来之后，很多用于改进transformer计算效率的小改动paper出了不少，详见<a href="http://arxiv.org/abs/2102.11972">Do Transformer Modifications
Transfer Across Implementations and Applications?</a>.
但这篇文章的作者发现：</p>
<blockquote>
<p>Surprisingly, we find that most modifications do not meaningfully
improve performance.</p>
</blockquote>
<p>也就是说这些文章的改动更多的依赖于实现细节，并没有对tansformer的性能有本质上的提高。</p>
<p>另外，对于positional encoding也有不少researcher做了功课，比如relative
linear postion attention,dependency syntax-based position等。</p>
<h2 id="tensorflow-api实现补充介绍">tensorflow API实现补充介绍</h2>
<h3 id="tf.keras.layers.multiheadattention">tf.keras.layers.MultiHeadAttention</h3>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention">doc</a></p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230309150541287.png" alt="image-20230309150541287">
<figcaption aria-hidden="true">image-20230309150541287</figcaption>
</figure>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230309150600806.png" alt="image-20230309150600806">
<figcaption aria-hidden="true">image-20230309150600806</figcaption>
</figure>
<p>注意，return的结果包含两个，其中attention_output的shape的第二维是和target
sequence的长度是一致的，并且E是和query的最后一维是一致的。</p>
<p>https://dl.acm.org/doi/10.5555/3305381.3305510)</p>
<h1 id="attention-family">Attention Family</h1>
<p>这个章节整理于<a href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/">blog</a>，这个作者之前写了一篇介绍attention的文章，后面在2023年一月的时候又更新了两篇博客，详细介绍了从2020年以来出现的新的Transformer
models。权当自己学习记录一些我还需要补充的知识。</p>
<blockquote>
<p>The <strong>Transformer</strong> (which will be referred to as
“vanilla Transformer” to distinguish it from other enhanced versions; <a href="https://arxiv.org/abs/1706.03762">Vaswani, et al., 2017</a>) model
has an encoder-decoder architecture, as commonly used in many <a href="https://lilianweng.github.io/posts/2018-06-24-attention/#born-for-translation">NMT</a>
models. Later simplified Transformer was shown to achieve great
performance in language modeling tasks, like in encoder-only <a href="https://lilianweng.github.io/posts/2019-01-31-lm/#bert">BERT</a>
or decoder-only <a href="https://lilianweng.github.io/posts/2019-01-31-lm/#openai-gpt">GPT</a>.</p>
</blockquote>
<h1 id="pretraining">pretraining</h1>
<p>pretraining的技术在NLP领域取得了空前的发展，比如我们熟知的GPT系列模型以及Bert模型。自从transformer应运而生之后，pretrain的技术就发展开来。</p>
<p>model pretrain的方式有三种：decoders,encoders,encoder-decoders:</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230315113701127.png" alt="pretrain for three types of architectures">
<figcaption aria-hidden="true">pretrain for three types of
architectures</figcaption>
</figure>
<hr>
<p>上图中第一个将transformer中decoders拿来做pre-train的模型是GPT，paper是：<a href="https://openai.com/research/language-unsupervised">Improving
Language Understanding by Generative Pre-Training</a></p>
<p>GPT模型分为两个阶段，第一个阶段是unsupervised
learning，第二个是supervised learning。首先第一阶段是一个典型的language
modeling模型，目标函数是：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230315141236557.png" alt="image-20230315141236557">
<figcaption aria-hidden="true">image-20230315141236557</figcaption>
</figure>
<p>其中k是context窗口的大小，也就是我们预测一个单词的时候，只看它前k个单词，在前k个单词的基础上使得出现当前单词的概率最大。至于这个模型是什么？其实就是我们熟知的transformer
decoder部分，包含multi-head
self-attention(注意这里是masked的attention，原因我们只看前k个单词，并不看后面的部分)和feed-forward：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230315141542314.png" alt="Transformer used in GPT paper">
<figcaption aria-hidden="true">Transformer used in GPT
paper</figcaption>
</figure>
<p>我们取最上层的decoder的输出，然后使用softmax来预测p(u)。以上的decoder模型在一个很大的语料库上进行训练之后，我们预训练部分就做完了，通过这一步我们拥有了一个decoder，它的作用是当我们每次输入一个sentence，它会告诉我们紧接着的那个单词是什么。</p>
<p>接着第二部分是我们的监督学习，原文说监督学习部分需要在不同的task上进行fine-tune：</p>
<blockquote>
<p>First, we use a language modeling objective on the unlabeled data to
learn the initial parameters of a neural network model. Subsequently, we
adapt these parameters to a target task using the corresponding
supervised objective.</p>
</blockquote>
<hr>
<p>第二种用于pre-train的模型架构是encoders。它和用decoder来做非监督学习不一样的地方在于，摘录自博客<a href="https://medium.com/dataseries/openai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4">OpenAI
GPT: Generative Pre-Training for Language Understanding</a></p>
<p>Open AI GPT uses a <strong>Transformer Decoder</strong> architecture
as opposed to <a href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af">BERT’s</a>
Transformer Encoder architecture. I have already covered the difference
between the Transformer Encoder and Decoder in <a href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af">this</a>
post; however, it is as follows:</p>
<ul>
<li><strong>The Transformer Encoder</strong> is essentially a
Bidirectional Self-Attentive Model, that uses all the tokens in a
sequence to attend each token in that sequence</li>
</ul>
<blockquote>
<p>i.e. for a given word, the attention is computed using all the words
in the sentence and not just the words preceding the given word in one
of the left-to-right or right-to-left traversal order.</p>
</blockquote>
<ul>
<li>While the <strong>Transformer Decoder</strong>, is a Unidirectional
Self-Attentive Model, that uses only the tokens preceding a given token
in the sequence to attend that token</li>
</ul>
<blockquote>
<p>i.e. for a given word, the attention is computed using only the words
preceding the given word in that sentence according to the traversal
order, left-to-right or right-to-left.</p>
</blockquote>
<p>— from <a href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af">BERT:
Pre-Training of Transformers for Language Understanding</a></p>
<p>Thus, <strong>GPT gets its auto-regressive nature from this
directionality provided by the Transformer Decoder</strong> as it uses
just the previous tokens from the sequence to predict the next
token.</p>
<p>在这里典型的代表就是<a href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af">Bert</a>.</p>
<hr>
<p>参考读物：</p>
<ul>
<li><a href="https://medium.com/dataseries/openai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4">OpenAI
GPT: Generative Pre-Training for Language Understanding</a></li>
<li><a href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af">Bert's
Transformer Encoder architecture</a></li>
<li></li>
</ul>
<h1 id="bertpre-training-of-deep-bidirectional-transformers-for-language-understanding-2019">Bert：Pre-training
of Deep Bidirectional Transformers for Language Understanding 2019</h1>
<p>这篇文章出现在openai的GPT模型之前，前身是ELMo。</p>
<p>有两种方式将pre-trained language
representations用于下游任务，第一种是feature-based，第二种是fine-tine，其中第一种代表是ELMo，将pre-trained
representations用于附加的features输入到下游任务中；第二种是得到pre-trained的representation之后，将模型接入下游任务，然后同时fine-tune所有的参数。</p>
<p>作者在finetune模型时，采取11个下游任务：</p>
<ol type="1">
<li>GLUE general language understanding evalation</li>
</ol>
<p>它的输入主要是sentence
pair。比如MNLI数据集就是区分输入的两个句子，后者跟前者是entailment，contradiction还是中立的关系。另外GLUE
benchmark中还包含其他的几个数据集：QQP（比较两个question是不是语义上近似的），QNLI（问答数据集，sentence中是否包含能够回答question的answer）等。</p>
<p>Bert在处理此类任务的时候，不是单独对两个句子分别编码的，而是将两个句子拼接在一起，共同输入给self-attention，并且在两个句子中间加了一个标志[SEP]。在fine-tune的时候，直接将输出的hidden
states的中的第一个token的向量输出到全连接层中接softmax分类器。</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230321101507418.png" alt="sentence pair classification tasks">
<figcaption aria-hidden="true">sentence pair classification
tasks</figcaption>
</figure>
<ol start="2" type="1">
<li>SQuAD v1.1 stanford question answering dataset</li>
</ol>
<p>数据集的结构是一个question，一个passage，这个passage里面包含answer。该任务是预测answer在passage中的哪儿。在fine-tune的过程中引入了start和end两个vector，毕竟我们想知道这个answer在passage的哪个位置：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230321102053181.png" alt="SQuAD v1.1">
<figcaption aria-hidden="true">SQuAD v1.1</figcaption>
</figure>
<p>这里做分类的时候不是用的全连接层，而是使用的dot product：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230321102257845.png" alt="image-20230321102257845">
<figcaption aria-hidden="true">image-20230321102257845</figcaption>
</figure>
<p>其中，S是start vector，i是指位置i的token
word，Pi指位置i的word成为start的概率。</p>
<p>作者还在SQuAD
v2.0上做了实验，这个数据集和1.1不一样的地方在于数据集中包含不存在answer的情况。</p>
<ol start="3" type="1">
<li>SWAG situation with adversatial generations</li>
</ol>
<hr>
<p>在阅读本篇文章的过程中一个有一个疑问就是，其实bert的思想并不陌生，有点像之前的word2vec，那为什么现在大家不用预训练的word2vec来解决问题了呢？在Bert原文的第二章节，作者解答了这个问题：</p>
<blockquote>
<p>The advantage of these approaches is that few parameters need to be
learned from scratch.</p>
</blockquote>
<p>同样在cs224n的课件上我们也可以看到为什么大家转而从静态的词向量投向了GPT，Bert：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230321093156801.png" alt="pretrained word embeddings">
<figcaption aria-hidden="true">pretrained word embeddings</figcaption>
</figure>
<p>上面这张图说明的是以往的用预训练的词向量用于模型的方式，可以看到大多数的模型参数是随机初始化的，只是模型的输入部分是我们预训练的词向量。</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230321093348933.png" alt="pretrain whole models">
<figcaption aria-hidden="true">pretrain whole models</figcaption>
</figure>
<p>但是在model
nlp内，我们可以看到所有的参数在初始化的时候都是使用的预训练好的参数。这一类模型可以在预训练的过程中学习到如何表示一整个句子。</p>
<h1 id="improving-language-understanding-by-generative-pre-training-2018">Improving
Language Understanding by Generative Pre-Training 2018</h1>
<p>GPT stands for "Generative pretrained transformer" or "generative
pre-trained"</p>
<p>two main reasons for leveraging more than word-level information from
unlabeled text data</p>
<ol type="1">
<li>unclear about what type of optimization objectives are most
effective at learning text representations that are useful for transfer
缺乏很好的优化函数</li>
<li>Second, there is no consensus on the most effective way to transfer
these learned representations to the target task
如何更有效的将pre-train的知识transfer到target task还没有很好的方法</li>
</ol>
<p>作者在introduction部分强调了自己提出的模型在fine-tune阶段只需要对模型架构进行微调便可以适应target
task，在知识迁移时，采用了paper： Reasoning about entailment with neural
attention，which process structured text input as a single contiguous
sequence of tokens.</p>
<p><strong>Related work</strong> : LSTM using as the pre-trained network
to capture the representation but they has lots of restrictions. This
paper use transformer networks to allow capturing longer range
linguistic structure. Further, in fine-tuning process, the model only
requires minimal changes to model architecture other than involving
substantial amount of new parameters.</p>
<h2 id="framework">Framework</h2>
<h3 id="unsupervised-pre-train-process">1. unsupervised pre-train
process</h3>
<p><img src="/2022/10/27/Attention-and-Transformer-model/unsupervised%20pre-train%20process.png"></p>
<p>以当前token的前k个单词为context，预测当前token。典型的language
modeling。</p>
<h3 id="supervised-fine-tuning">2. Supervised fine-tuning</h3>
<p>这里和Bert不一样的是，作者采用了两个目标，第一个目标是target
task的目标（比如分类），第二个目标是pre-train时候的language
modeling的目标，即预测当前词语。为什么这么设定？作者的意思是将language
modeling的目标加入模型的fine-tune阶段可以增加模型的泛化能力和加速收敛。所以整体的模型架构是：</p>
<p><img src="/2022/10/27/Attention-and-Transformer-model/whole-picture%20of%20GPT.png"></p>
<p>截至到目前，如果我们的下游任务是分类任务，模型的fine-tune是很简单的，就将decoder的输出的最后一个token的vector拿出来接一个classifier就可以了，但是如果处理QA，texttual
entailment的数据集任务，这一类的任务的输入往往是句子的pair或者question，answer的组合。这时候作者想到一个办法是，将他们都当成一个连续的sequence，并在其中增加了随机初始化的vectors比如start/END.
同时作者指出之前有一些论文在这个方面也做了不少research，但都是“re-introduce
a significant amount of task-specific customization and does not use
transfer learning for these additional architectural components”.
那么具体是如何对模型的输入进行小改造的，如图：</p>
<p><img src="/2022/10/27/Attention-and-Transformer-model/whole%20architectures.png"></p>
<p>其中对于QA的task，作者的处理有一丢丢特殊，我们拿到的数据是一系列context，question和一系列answers，作者将document
context，question和answer拼接在一起变成一整个sequence输入到decoder中，然后计算当前question和answer的匹配程度。</p>
<h2 id="experiment">Experiment</h2>
<p>dataset ： BooksCorpus dataset contains 7000 unique books</p>
<h2 id="analysis">Analysis</h2>
<ol type="1">
<li>随着decoder部分层数的增加，performance会越来越好。作者得出结论
<em>”This indicates that each layer in the pre-trained model contains
useful functionality for solving target tasks“</em></li>
<li>zero-shot</li>
<li>ablation studies 作者在fine-tune阶段将LM任务去除，其实这时候和Bert
finetune阶段是一模一样的，只有target
task的任务需要优化。作者从这个实验中得出的结论是：LM可以在NLI和QQP两个任务上帮助提高performance。同时也发现，更大的数据集会从LM任务中获利更多，但小数据集并不会获利。同时作者将模型不进行pretrain，直接在数据集上训练，发现如果没有pretrain，所有的任务的performance都会下降，以此证明pre-train确实给所有任务都带来了performance的提升</li>
</ol>
<h2 id="conclusions">Conclusions</h2>
<p>GPT的两个关键词： generative pre-training &amp; discriminative
fine-tuning</p>
<p>同时作者得出结论是：在Transformer(what models)上利用text with long
range dependencies(which dataset to use)会收获好的performance。</p>
<h2 id="本文推荐阅读材料">本文推荐阅读材料</h2>
<ul>
<li><a href="https://huggingface.co/docs/transformers/tokenizer_summary#summary-of-the-tokenizers">summary
of tokenizers</a></li>
<li><a href="https://towardsdatascience.com/word-subword-and-character-based-tokenization-know-the-difference-ea0976b64e17">Word,
Subword, and Character-Based Tokenization: Know the Difference</a></li>
<li></li>
</ul>
<h1 id="reference">Reference</h1>
<p>在读transformer论文的时候，有几个概念key，query，value三个概念一下子就抛出来了。在讲transformer的<a href="https://www.youtube.com/watch?v=OyFJWRnt_AY">lecture
video</a>里,我看到有不少评论反应讲者没有将这三个概念讲清楚。我一开始在看cs231n的lecture
ppt时也有点疑惑，老师刚说完general attention layer转到讲self-attention
layer，就直接从h变成了q，确实有点云里雾里。</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221206170135727-16703172985855.png" alt="image-20221206170135727">
<figcaption aria-hidden="true">image-20221206170135727</figcaption>
</figure>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221206170101403.png" alt="image-20221206170101403">
<figcaption aria-hidden="true">image-20221206170101403</figcaption>
</figure>
<p>可以从上面的ppt中看出，原来是仅仅有q这个变量的，这是从一开始的h演变来的，而我们可以看到为了“add
more expressivity to the layer”，所以我们在1.输入x输入到FC得到alignment
score之前又加了一个不同的FC 2. 对输入x用attention weights进行weight
sum的过程也加了一个完全不同的FC layer。而我们可以看到这里加的这两个FC
layer是为了增加模型的表现力。这两个FC
layer的输出也就是成了我们所说的key和value。后面的过程就清晰了，首先利用query和key计算attention
weights，然后用attention weights和value进行计算得到context。</p>
<blockquote>
<p>An attention layer does a fuzzy lookup like this, but it's not just
looking for the best key. It combines the <code>values</code> based on
how well the <code>query</code> matches each <code>key</code>.</p>
<p>How does that work? In an attention layer the <code>query</code>,
<code>key</code>, and <code>value</code> are each vectors. Instead of
doing a hash lookup the attention layer combines the <code>query</code>
and <code>key</code> vectors to determine how well they match, the
"attention score". The layer returns the average across all the
<code>values</code>, weighted by the "attention scores</p>
</blockquote>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title>HuggingFace Transformers</title>
    <url>/2023/08/16/HuggingFace%20Transformer/</url>
    <content><![CDATA[<p>今天新开一篇博客，用于记录自己在学习hugging
face的一些笔记。初衷是自己在研究大语言模型的时候，很多时候并不知道如何finetune预训练好的大语言模型，再加上现在很多框架都封装的蛮厉害的，比如如果想实现lora等parameter-efficient的finetune方式，hugging
face中已经写好了PEFT这个库，只需几行代码即可实现，但想知道里面的具体实现还得一层层翻阅源码。之前对于transformer这个库的掌握都不系统，今天开此专题希望能读完hugging
face官方出的关于transformer这个tutorial。</p>
<p>首先一点hugging
face是承载着一系列模型，这些模型有nlp的也有cv的,除了模型还有数据，模型的checkpoints。另外它不仅支持pytorch，还支持tensorflow，我本来是tensorflow的坚定支持者，但看到现在pytorch越发有超越tensorflow的趋势，不掌握不行了快。</p>
<p>我们知道transformer衍生出来的模型有三类，一类是gpt类型，只有decoder，一类是bert，只有encoder部分，还有一类是encoder-decoder，以bart,
T5为首的。</p>
<figure>
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg" alt="A brief chronology of Transformers models.">
<figcaption aria-hidden="true">A brief chronology of Transformers
models.</figcaption>
</figure>
<p>BERT一类的模型可以称为：Encoder models(auto-encoding models),
pre-train的方式就是通过mask掉一个句子中的一些单词，然后让模型去重建这些单词。这一类模型有BERT,RoBERTa,DistilBERT,ALBERT,ELECTRA。<strong>这一类模型更适合用于解决需要理解整个句子意思的一类task</strong>，比如sentence
classification,NER(word classification), extractive question
answering。</p>
<p>GPT一类的模型可以称为：decoder models(auto-regressive
models),pre-train的方式是预测一个句子中下一个单词是什么，它只能看到前面单词的信息，不像bert类的encoder
models可以看到左右的信息。这一类模型有GPT，GPT-2，CTRL等。<strong>这一类模型更适合用于解决文本生成一类的任务</strong>。</p>
<p>T5一类的模型可以称为：encoder-decoder models(sequence-to-sequence
models),pre-train的方式比较复杂，因模型而异。<strong>这一类模型更适合用于解决给定一个input，生成一个新的句子</strong>，比如summarization,translation,generative
question answering.</p>
<h1 id="tokenizer">tokenizer</h1>
<ul>
<li>建议用AutoTokenizer类来导入预训练模型的tokenizer</li>
<li>可以批量encode</li>
<li>集成的函数是两步骤：tokenize+convert_tokens_to_ids</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;distilbert-base-cased&quot;</span>) <span class="comment"># convenient! Defaults to Fast</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># You can pass multiple strings into the tokenizer and pad them as you need</span></span><br><span class="line">model_inputs = tokenizer([<span class="string">&quot;Hugging Face Transformers is great!&quot;</span>,</span><br><span class="line">                         <span class="string">&quot;The quick brown fox jumps over the lazy dog.&quot;</span> +\</span><br><span class="line">                         <span class="string">&quot;Then the dog got up and ran away because she didn&#x27;t like foxes.&quot;</span>,</span><br><span class="line">                         ],</span><br><span class="line">                         return_tensors=<span class="string">&quot;pt&quot;</span>,</span><br><span class="line">                         padding=<span class="literal">True</span>,</span><br><span class="line">                         truncation=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Pad token: <span class="subst">&#123;tokenizer.pad_token&#125;</span> | Pad token id: <span class="subst">&#123;tokenizer.pad_token_id&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Padding:&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># You can also decode a whole batch at once: 可以批量解码！</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Batch Decode:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.batch_decode(model_inputs.input_ids))</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Batch Decode: (no special characters)&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.batch_decode(model_inputs.input_ids, skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<h1 id="model">model</h1>
<p>一个很重要的点，不同模型可以做不同的下游任务，比如distilBERT，如果我们想用它来做句子分类的任务，在bert的基础架构上还需要有一些参数，这里叫"heads",这些heads要在你的数据集上训练，transformer提供了特定的类，这样你就不用自己写这个头了。比如DistilBert，它就有DistilBertForSequenceClassification这个类，可以加的后缀有：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">*</span><br><span class="line">*ForMaskedLM</span><br><span class="line">*ForSequenceClassification</span><br><span class="line">*ForTokenClassification</span><br><span class="line">*ForQuestionAnswering</span><br><span class="line">*ForMultipleChoice</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>✳️可以是AutoModel或者是某一个特定的预训练模型，比如DistilBert</p>
<blockquote>
<p>官方建议使用Auto的方式来调用checkpoints，这样可以随意更改传入的pretrained的模型是哪个</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForMaskedLM</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;camembert-base&quot;</span>)</span><br><span class="line">model = AutoModelForMaskedLM.from_pretrained(<span class="string">&quot;camembert-base&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="finetune-a-model-with-trainer-api">finetune a model with Trainer
API</h1>
<h2 id="pytorch">pytorch</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, DataCollatorWithPadding</span><br><span class="line"></span><br><span class="line"><span class="comment"># data processing</span></span><br><span class="line">raw_datasets = load_dataset(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_function</span>(<span class="params">example</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(example[<span class="string">&quot;sentence1&quot;</span>], example[<span class="string">&quot;sentence2&quot;</span>], truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tokenized_datasets = raw_datasets.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model training</span></span><br><span class="line">training_args = TrainingArguments(<span class="string">&quot;test-trainer&quot;</span>, evaluation_strategy=<span class="string">&quot;epoch&quot;</span>)</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_metrics</span>(<span class="params">eval_preds</span>):</span></span><br><span class="line">    metric = evaluate.load(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">    logits, labels = eval_preds</span><br><span class="line">    predictions = np.argmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> metric.compute(predictions=predictions, references=labels)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model,</span><br><span class="line">    training_args,</span><br><span class="line">    train_dataset=tokenized_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=tokenized_datasets[<span class="string">&quot;validation&quot;</span>],</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>
<h2 id="tensorflow">tensorflow</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, DataCollatorWithPadding</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">raw_datasets = load_dataset(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_function</span>(<span class="params">example</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(example[<span class="string">&quot;sentence1&quot;</span>], example[<span class="string">&quot;sentence2&quot;</span>], truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tokenized_datasets = raw_datasets.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="string">&quot;tf&quot;</span>)</span><br><span class="line"></span><br><span class="line">tf_train_dataset = tokenized_datasets[<span class="string">&quot;train&quot;</span>].to_tf_dataset(</span><br><span class="line">    columns=[<span class="string">&quot;attention_mask&quot;</span>, <span class="string">&quot;input_ids&quot;</span>, <span class="string">&quot;token_type_ids&quot;</span>],</span><br><span class="line">    label_cols=[<span class="string">&quot;labels&quot;</span>],</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    collate_fn=data_collator,</span><br><span class="line">    batch_size=<span class="number">8</span>,</span><br><span class="line">) <span class="comment"># 这里会和pytorch不一样</span></span><br><span class="line"></span><br><span class="line">tf_validation_dataset = tokenized_datasets[<span class="string">&quot;validation&quot;</span>].to_tf_dataset(</span><br><span class="line">    columns=[<span class="string">&quot;attention_mask&quot;</span>, <span class="string">&quot;input_ids&quot;</span>, <span class="string">&quot;token_type_ids&quot;</span>],</span><br><span class="line">    label_cols=[<span class="string">&quot;labels&quot;</span>],</span><br><span class="line">    shuffle=<span class="literal">False</span>,</span><br><span class="line">    collate_fn=data_collator,</span><br><span class="line">    batch_size=<span class="number">8</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers.schedules <span class="keyword">import</span> PolynomialDecay</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="comment"># The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied</span></span><br><span class="line"><span class="comment"># by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,</span></span><br><span class="line"><span class="comment"># not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.</span></span><br><span class="line">num_train_steps = <span class="built_in">len</span>(tf_train_dataset) * num_epochs</span><br><span class="line">lr_scheduler = PolynomialDecay(</span><br><span class="line">    initial_learning_rate=<span class="number">5e-5</span>, end_learning_rate=<span class="number">0.0</span>, decay_steps=num_train_steps</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line">opt = Adam(learning_rate=lr_scheduler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="number">2</span>)</span><br><span class="line">loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=opt, loss=loss, metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line"></span><br><span class="line">model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">preds = model.predict(tf_validation_dataset)[<span class="string">&quot;logits&quot;</span>]</span><br><span class="line">class_preds = np.argmax(preds, axis=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(preds.shape, class_preds.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line"></span><br><span class="line">metric = evaluate.load(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">metric.compute(predictions=class_preds, references=raw_datasets[<span class="string">&quot;validation&quot;</span>][<span class="string">&quot;label&quot;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="customized-loops">customized loops</h1>
<h2 id="pytorch-1">pytorch</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AdamW, AutoModelForSequenceClassification, get_scheduler</span><br><span class="line"></span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="number">2</span>)</span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">3e-5</span>)</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">num_training_steps = num_epochs * <span class="built_in">len</span>(train_dataloader)</span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">    num_training_steps=num_training_steps,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">progress_bar = tqdm(<span class="built_in">range</span>(num_training_steps))</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line">        loss = outputs.loss</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        progress_bar.update(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h1 id="datasets">Datasets</h1>
<h1 id="导入">导入</h1>
<p>datasets这个库越来越多人使用了，主要在于方便。支持多种数据存储类型，比如json，txt，csv，pockled
dataframes。</p>
<p>对于json类型的数据，最方便的是一行存储一个字典，也就是一个数据sample，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;&quot;a&quot;: 1, &quot;b&quot;: 2.0, &quot;c&quot;: &quot;foo&quot;, &quot;d&quot;: false&#125;</span><br><span class="line">&#123;&quot;a&quot;: 4, &quot;b&quot;: -5.5, &quot;c&quot;: null, &quot;d&quot;: true&#125;</span><br></pre></td></tr></table></figure>
<p>导入就用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from datasets import load_dataset</span><br><span class="line">dataset = load_dataset(&quot;json&quot;, data_files=&quot;my_file.json&quot;)</span><br></pre></td></tr></table></figure>
<p>如果json的组织形式是套娃格式的，只需要指定一个field参数就可以了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;&quot;version&quot;: &quot;0.1.0&quot;,</span><br><span class="line"> &quot;data&quot;: [&#123;&quot;a&quot;: 1, &quot;b&quot;: 2.0, &quot;c&quot;: &quot;foo&quot;, &quot;d&quot;: false&#125;,</span><br><span class="line">          &#123;&quot;a&quot;: 4, &quot;b&quot;: -5.5, &quot;c&quot;: null, &quot;d&quot;: true&#125;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">from datasets import load_dataset</span><br><span class="line">dataset = load_dataset(&quot;json&quot;, data_files=&quot;my_file.json&quot;, field=&quot;data&quot;)</span><br></pre></td></tr></table></figure>
<p>炒鸡方便！</p>
<p>不仅如此，它的data_files这个参数很灵活，可以接受一个字符串也就是数据集所在的路径，也可以接受一个list，也可以接受一个字典，字典的key就是你每个数据集的用处比如train，test啥的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">data_files = &#123;&quot;train&quot;: &quot;SQuAD_it-train.json&quot;, &quot;test&quot;: &quot;SQuAD_it-test.json&quot;&#125;</span><br><span class="line">squad_it_dataset = load_dataset(&quot;json&quot;, data_files=data_files, field=&quot;data&quot;)</span><br><span class="line">squad_it_dataset</span><br></pre></td></tr></table></figure>
<p>还有一个更方便的，可以接受压缩包数据，这样就不用自己手动解压缩了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">data_files = &#123;&quot;train&quot;: &quot;SQuAD_it-train.json.gz&quot;, &quot;test&quot;: &quot;SQuAD_it-test.json.gz&quot;&#125;</span><br><span class="line">squad_it_dataset = load_dataset(&quot;json&quot;, data_files=data_files, field=&quot;data&quot;)</span><br></pre></td></tr></table></figure>
<h1 id="process">process</h1>
<p>数据导入进来的，在正式输入进模型训练之前，还需要进一步的进行处理。</p>
<h1 id="推荐阅读">推荐阅读</h1>
<ul>
<li><a href="https://huggingface.co/learn/nlp-course/">hugging face NLP
course</a></li>
<li><a href="https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/ch01.html#idm45146321158640">natural
language processing for transformers</a></li>
<li></li>
</ul>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepLearning.AI-Tensorfow-Developer-Course课程整理知识点</title>
    <url>/2022/04/05/DeepLearning-AI-Tensorfow-Developer-Course%E8%AF%BE%E7%A8%8B%E6%95%B4%E7%90%86%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
    <content><![CDATA[<p>该课程地址<a href="https://www.coursera.org/professional-certificates/tensorflow-in-practice" class="uri">https://www.coursera.org/professional-certificates/tensorflow-in-practice</a></p>
<p>出品方是
deeplearning.ai,老师是google的，这门专项课程共分为4个单独的课程，每一个课程分为4周的课程，讲者
Laurence Moroney也有一本配套书籍 AI and Machine Learning for Coders
出版了，英文版无中文版，原版书籍可以去Oreilly官方网站找到。所有的代码文件可以在课程教材中找到link。</p>
<p>本博客只记录本人在学习过程中我还不清楚的知识点，不是全部课程的整理。本人是计算机背景，AI方向，建议想学这门课程的小伙伴先去熟练掌握python，其他需要的知识背景还包括：机器学习，深度学习的理论知识（这两点可以先学完Coursera上吴恩达的Machine
Learning，Deep
Learning两个专项课程）。既然说到这里了，还有个自己的小建议，在学习这些课程的时候，最好的方式是一步一步跟着课程来，包括课后练习和课程问答题，会特别！加深自己对于理论知识的掌握。光看视频是不够的！</p>
<p>再有掌握理论知识和会自己熟练写出来中间还有很大的Gap，这也是我会开始学习这门面向coders的课程的原因之一，虽然现在官方API文档写的也很全面，但是这位老师多讲了很多理解这些函数的方式，也给了我们去读Tensorflow源码的兴趣。</p>
<h1 id="课程1-introduction-to-tensorflow-for-artificial-intelligence-machine-learning-and-deep-learning">课程1：
Introduction to TensorFlow for Artificial Intelligence, Machine
Learning, and Deep Learning</h1>
<p>课程一的内容相对简单。对我而言可以学习的是在Horse和人的分类任务中（week4），上传自己从网上load图片让model预测结果的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!wget https://storage.googleapis.com/tensorflow-<span class="number">1</span>-public/course2/week3/horse-<span class="keyword">or</span>-human.<span class="built_in">zip</span></span><br><span class="line">!wget https://storage.googleapis.com/tensorflow-<span class="number">1</span>-public/course2/week3/validation-horse-<span class="keyword">or</span>-human.<span class="built_in">zip</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 运行以下代码之前先将数据集的训练集和验证集下载到本地</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="comment"># Unzip training set</span></span><br><span class="line">local_zip = <span class="string">&#x27;./horse-or-human.zip&#x27;</span></span><br><span class="line">zip_ref = zipfile.ZipFile(local_zip, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">zip_ref.extractall(<span class="string">&#x27;./horse-or-human&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Unzip validation set</span></span><br><span class="line">local_zip = <span class="string">&#x27;./validation-horse-or-human.zip&#x27;</span></span><br><span class="line">zip_ref = zipfile.ZipFile(local_zip, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">zip_ref.extractall(<span class="string">&#x27;./validation-horse-or-human&#x27;</span>)</span><br><span class="line">zip_ref.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># model construction</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line"></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    <span class="comment"># Note the input shape is the desired size of the image 300x300 with 3 bytes color</span></span><br><span class="line">    <span class="comment"># This is the first convolution</span></span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">16</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">300</span>, <span class="number">300</span>, <span class="number">3</span>)),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">    <span class="comment"># The second convolution</span></span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># The third convolution</span></span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># The fourth convolution</span></span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># The fifth convolution</span></span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># Flatten the results to feed into a DNN</span></span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    <span class="comment"># 512 neuron hidden layer</span></span><br><span class="line">    tf.keras.layers.Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    <span class="comment"># Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class (&#x27;horses&#x27;) and 1 for the other (&#x27;humans&#x27;)</span></span><br><span class="line">    tf.keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>) <span class="comment"># 这里是二分类，所以只有一个神经元作为输出，激活函数用sigmoid</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, <span class="comment"># 二分类用binary_crossentropy</span></span><br><span class="line">              optimizer=RMSprop(learning_rate=<span class="number">0.001</span>),</span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line"><span class="comment"># All images will be rescaled by 1./255</span></span><br><span class="line">train_datagen = ImageDataGenerator(rescale=<span class="number">1</span>/<span class="number">255</span>) <span class="comment"># 这里用Tensorflow写好的DataGenerator针对数据集自动产生训练集，注意这里传入的文件夹是./horse-or-human,该文件内有两个文件夹，一个是horse一个是human，generator会用文件夹名字作为分类lable。</span></span><br><span class="line">validation_datagen = ImageDataGenerator(rescale=<span class="number">1</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Flow training images in batches of 128 using train_datagen generator</span></span><br><span class="line">train_generator = train_datagen.flow_from_directory(</span><br><span class="line">        <span class="string">&#x27;./horse-or-human/&#x27;</span>,  <span class="comment"># This is the source directory for training images</span></span><br><span class="line">        target_size=(<span class="number">300</span>, <span class="number">300</span>),  <span class="comment"># All images will be resized to 300x300</span></span><br><span class="line">        batch_size=<span class="number">128</span>,</span><br><span class="line">        <span class="comment"># Since you use binary_crossentropy loss, you need binary labels</span></span><br><span class="line">        class_mode=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Flow validation images in batches of 128 using validation_datagen generator</span></span><br><span class="line">validation_generator = validation_datagen.flow_from_directory(</span><br><span class="line">        <span class="string">&#x27;./validation-horse-or-human/&#x27;</span>,  <span class="comment"># This is the source directory for validation images</span></span><br><span class="line">        target_size=(<span class="number">300</span>, <span class="number">300</span>),  <span class="comment"># All images will be resized to 300x300</span></span><br><span class="line">        batch_size=<span class="number">32</span>,</span><br><span class="line">        <span class="comment"># Since you use binary_crossentropy loss, you need binary labels</span></span><br><span class="line">        class_mode=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit(</span><br><span class="line">      train_generator,</span><br><span class="line">      steps_per_epoch=<span class="number">8</span>,  <span class="comment"># 这里8是1024（train图片的个数）/ train_batch_size(128) = 8</span></span><br><span class="line">      epochs=<span class="number">15</span>,</span><br><span class="line">      verbose=<span class="number">1</span>,</span><br><span class="line">      validation_data = validation_generator,</span><br><span class="line">      validation_steps=<span class="number">8</span>) <span class="comment"># 这里也是val_size（256） / val_batch_size（32）</span></span><br></pre></td></tr></table></figure>
<p>以上模型训练完之后，我们再拿网上下载的图片来看看模型效果如何：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">images = os.listdir(<span class="string">&quot;/tmp/images&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(images)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> images:</span><br><span class="line"> <span class="comment"># predicting images</span></span><br><span class="line"> path = <span class="string">&#x27;/tmp/images/&#x27;</span> + i</span><br><span class="line"> img = image.load_img(path, target_size=(<span class="number">300</span>, <span class="number">300</span>)) <span class="comment"># 这里将图片resize成我们模型需要的大小(300,300)</span></span><br><span class="line"> x = image.img_to_array(img) <span class="comment"># 将img转化成array,x shape (300,300,3)</span></span><br><span class="line"> x /= <span class="number">255</span></span><br><span class="line"> x = np.expand_dims(x, axis=<span class="number">0</span>) <span class="comment"># 这一步是为了扩展为shape (1,300,300,3),因为模型接受的输入是(sample_size,width,height,depth)</span></span><br><span class="line"></span><br><span class="line"> images = np.vstack([x])</span><br><span class="line"> classes = model.predict(images, batch_size=<span class="number">10</span>)</span><br><span class="line"> <span class="built_in">print</span>(classes[<span class="number">0</span>])</span><br><span class="line"> <span class="keyword">if</span> classes[<span class="number">0</span>]&gt;<span class="number">0.5</span>:</span><br><span class="line">   <span class="built_in">print</span>(i + <span class="string">&quot; is a human&quot;</span>)</span><br><span class="line"> <span class="keyword">else</span>:</span><br><span class="line">   <span class="built_in">print</span>(i + <span class="string">&quot; is a horse&quot;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>补充一个在colab中如何将数据下载到本地：<a href="https://blog.csdn.net/jizhidexiaoming/article/details/108302794" class="uri">https://blog.csdn.net/jizhidexiaoming/article/details/108302794</a></p>
</blockquote>
<p>课程1的Week4 的作业是关于笑脸和哭脸的分类，数据集在这个地址 ："<a href="https://storage.googleapis.com/laurencemoroney-blog.appspot.com/happy-or-sad.zip" class="uri">https://storage.googleapis.com/laurencemoroney-blog.appspot.com/happy-or-sad.zip</a>"。github中的作业没有附带这个数据集，需要的读者可以自行用<code>wget</code>在<code>colab</code>中下载，然后将其存储在<code>data</code>文件夹中。</p>
<h2 id="总结">总结</h2>
<p>第一门课总体来说还是很简单的，为了学更多的东西，建议小伙伴深入阅读每一行代码，然后依次去官方API文档里看还有那些参数我们可以传入<code>fit</code>,
<code>complie</code>函数。课程内容是很少的，但如果花时间去钻研还是能学到不少东西。</p>
<p>我在第一门课上总共花的时间是3天时间，加上看视频的时间。</p>
<h1 id="课程2-convolutional-neural-networks-in-tensorflow">课程2:
Convolutional Neural Networks in TensorFlow</h1>
<p>第一周的作业是<code>kaggle</code>上<code>cats</code>和<code>dogs</code>的分类任务
<a href="https://www.kaggle.com/c/dogs-vs-cats" class="uri">https://www.kaggle.com/c/dogs-vs-cats</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"><span class="keyword">from</span> shutil <span class="keyword">import</span> copyfile</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载并解压数据集</span></span><br><span class="line">!wget --no-check-certificate \</span><br><span class="line">    <span class="string">&quot;https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip&quot;</span> \</span><br><span class="line">    -O <span class="string">&quot;/tmp/cats-and-dogs.zip&quot;</span></span><br><span class="line"></span><br><span class="line">local_zip = <span class="string">&#x27;/tmp/cats-and-dogs.zip&#x27;</span></span><br><span class="line">zip_ref   = zipfile.ZipFile(local_zip, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">zip_ref.extractall(<span class="string">&#x27;/tmp&#x27;</span>)</span><br><span class="line">zip_ref.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据DataGenerator需要的格式创建数据集</span></span><br><span class="line"><span class="comment"># Define root directory</span></span><br><span class="line">root_dir = <span class="string">&#x27;/tmp/cats-v-dogs&#x27;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># Empty directory to prevent FileExistsError is the function is run several times</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(root_dir):</span><br><span class="line">  shutil.rmtree(root_dir)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_train_test_dirs</span>(<span class="params">root_path</span>):</span></span><br><span class="line">  <span class="comment"># Use os.makedirs to create your directories with intermediate subdirectories</span></span><br><span class="line">  os.makedirs(os.path.join(root_dir,<span class="string">&quot;training&quot;</span>,<span class="string">&quot;cats&quot;</span>))</span><br><span class="line">  os.makedirs(os.path.join(root_dir,<span class="string">&quot;training&quot;</span>,<span class="string">&quot;dogs&quot;</span>))</span><br><span class="line">  os.makedirs(os.path.join(root_dir,<span class="string">&quot;testing&quot;</span>,<span class="string">&quot;cats&quot;</span>))</span><br><span class="line">  os.makedirs(os.path.join(root_dir,<span class="string">&quot;testing&quot;</span>,<span class="string">&#x27;dogs&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  create_train_test_dirs(root_path=root_dir)</span><br><span class="line"><span class="keyword">except</span> FileExistsError:</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;You should not be seeing this since the upper directory is removed beforehand&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据分割比例分割出训练集和测试集，并将源文件夹中的图片拷贝至相应的文件夹下</span></span><br><span class="line"><span class="comment"># split_data 该函数也需要完成图片的验证：图片length=0，就省略不纳入train或者test</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_data</span>(<span class="params">SOURCE, TRAINING, TESTING, SPLIT_SIZE</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  SOURCE : 例如：PetImages/Cat</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  not_null_files = []</span><br><span class="line">  <span class="keyword">for</span> files <span class="keyword">in</span> os.listdir(SOURCE):</span><br><span class="line">      file_path = os.path.join(SOURCE,files)</span><br><span class="line">      <span class="keyword">if</span> os.path.getsize(file_path) &gt; <span class="number">0</span> :</span><br><span class="line">         not_null_files.append(files)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">         <span class="built_in">print</span>(files + <span class="string">&quot;is zero length, so ignoring&quot;</span>)</span><br><span class="line">  </span><br><span class="line">  count = <span class="built_in">len</span>(not_null_files)</span><br><span class="line">  trains = <span class="built_in">int</span>(count * SPLIT_SIZE)</span><br><span class="line">  tests = count * SPLIT_SIZE</span><br><span class="line"></span><br><span class="line">  shuffle_set = random.sample(not_null_files,count)</span><br><span class="line">  train_set = shuffle_set[<span class="number">0</span>:trains]</span><br><span class="line">  test_set = shuffle_set[trains:]</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 将文件拷贝至指定文件夹</span></span><br><span class="line">  <span class="keyword">for</span> train_file <span class="keyword">in</span> train_set :</span><br><span class="line">      shutil.copyfile(os.path.join(SOURCE,train_file),os.path.join(TRAINING,train_file))</span><br><span class="line">  <span class="keyword">for</span> test_file <span class="keyword">in</span> test_set :</span><br><span class="line">      shutil.copyfile(os.path.join(SOURCE,test_file),os.path.join(TESTING,test_file))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CAT_SOURCE_DIR = <span class="string">&quot;/tmp/PetImages/Cat/&quot;</span></span><br><span class="line">DOG_SOURCE_DIR = <span class="string">&quot;/tmp/PetImages/Dog/&quot;</span></span><br><span class="line"></span><br><span class="line">TRAINING_DIR = <span class="string">&quot;/tmp/cats-v-dogs/training/&quot;</span></span><br><span class="line">TESTING_DIR = <span class="string">&quot;/tmp/cats-v-dogs/testing/&quot;</span></span><br><span class="line"></span><br><span class="line">TRAINING_CATS_DIR = os.path.join(TRAINING_DIR, <span class="string">&quot;cats/&quot;</span>)</span><br><span class="line">TESTING_CATS_DIR = os.path.join(TESTING_DIR, <span class="string">&quot;cats/&quot;</span>)</span><br><span class="line"></span><br><span class="line">TRAINING_DOGS_DIR = os.path.join(TRAINING_DIR, <span class="string">&quot;dogs/&quot;</span>)</span><br><span class="line">TESTING_DOGS_DIR = os.path.join(TESTING_DIR, <span class="string">&quot;dogs/&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Empty directories in case you run this cell multiple times</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(os.listdir(TRAINING_CATS_DIR)) &gt; <span class="number">0</span>:</span><br><span class="line">  <span class="keyword">for</span> file <span class="keyword">in</span> os.scandir(TRAINING_CATS_DIR):</span><br><span class="line">    os.remove(file.path)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(os.listdir(TRAINING_DOGS_DIR)) &gt; <span class="number">0</span>:</span><br><span class="line">  <span class="keyword">for</span> file <span class="keyword">in</span> os.scandir(TRAINING_DOGS_DIR):</span><br><span class="line">    os.remove(file.path)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(os.listdir(TESTING_CATS_DIR)) &gt; <span class="number">0</span>:</span><br><span class="line">  <span class="keyword">for</span> file <span class="keyword">in</span> os.scandir(TESTING_CATS_DIR):</span><br><span class="line">    os.remove(file.path)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(os.listdir(TESTING_DOGS_DIR)) &gt; <span class="number">0</span>:</span><br><span class="line">  <span class="keyword">for</span> file <span class="keyword">in</span> os.scandir(TESTING_DOGS_DIR):</span><br><span class="line">    os.remove(file.path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define proportion of images used for training</span></span><br><span class="line">split_size = <span class="number">.9</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> Messages about zero length images should be printed out</span></span><br><span class="line">split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)</span><br><span class="line">split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型部分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_generators</span>(<span class="params">TRAINING_DIR, VALIDATION_DIR</span>):</span></span><br><span class="line">  <span class="comment"># Instantiate the ImageDataGenerator class (don&#x27;t forget to set the rescale argument)</span></span><br><span class="line">  train_datagen = ImageDataGenerator(rescale=<span class="number">1</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Pass in the appropiate arguments to the flow_from_directory method</span></span><br><span class="line">  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,</span><br><span class="line">                                                      batch_size=<span class="number">128</span>,</span><br><span class="line">                                                      class_mode=<span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">                                                      target_size=(<span class="number">150</span>, <span class="number">150</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Instantiate the ImageDataGenerator class (don&#x27;t forget to set the rescale argument)</span></span><br><span class="line">  validation_datagen = ImageDataGenerator(rescale=<span class="number">1</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Pass in the appropiate arguments to the flow_from_directory method</span></span><br><span class="line">  validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,</span><br><span class="line">                                                                batch_size=<span class="number">32</span>,</span><br><span class="line">                                                                class_mode=<span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">                                                                target_size=(<span class="number">150</span>, <span class="number">150</span>))</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">return</span> train_generator, validation_generator</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span>():</span></span><br><span class="line">  <span class="comment"># USE AT LEAST 3 CONVOLUTION LAYERS</span></span><br><span class="line">  model = tf.keras.models.Sequential([ </span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">16</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>)),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>),</span><br><span class="line">  ])</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  model.<span class="built_in">compile</span>(optimizer=tf.keras.optimizers.RMSprop(learning_rate=<span class="number">0.001</span>),</span><br><span class="line">                loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>,</span><br><span class="line">                metrics=[<span class="string">&#x27;accuracy&#x27;</span>]) </span><br><span class="line">  <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_generator, validation_generator = train_val_generators(TRAINING_DIR, TESTING_DIR)</span><br><span class="line">model = create_model()</span><br><span class="line">history = model.fit(train_generator,</span><br><span class="line">                    epochs=<span class="number">15</span>,</span><br><span class="line">                    verbose=<span class="number">1</span>,</span><br><span class="line">                    validation_data=validation_generator)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据history返回的结果画图</span></span><br><span class="line">acc=history.history[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">val_acc=history.history[<span class="string">&#x27;val_accuracy&#x27;</span>]</span><br><span class="line">loss=history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">val_loss=history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">epochs=<span class="built_in">range</span>(<span class="built_in">len</span>(acc)) <span class="comment"># Get number of epochs</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#------------------------------------------------</span></span><br><span class="line"><span class="comment"># Plot training and validation accuracy per epoch</span></span><br><span class="line"><span class="comment">#------------------------------------------------</span></span><br><span class="line">plt.plot(epochs, acc, <span class="string">&#x27;r&#x27;</span>, <span class="string">&quot;Training Accuracy&quot;</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">&#x27;b&#x27;</span>, <span class="string">&quot;Validation Accuracy&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and validation accuracy&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#------------------------------------------------</span></span><br><span class="line"><span class="comment"># Plot training and validation loss per epoch</span></span><br><span class="line"><span class="comment">#------------------------------------------------</span></span><br><span class="line">plt.plot(epochs, loss, <span class="string">&#x27;r&#x27;</span>, <span class="string">&quot;Training Loss&quot;</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">&#x27;b&#x27;</span>, <span class="string">&quot;Validation Loss&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存history至pkl文件，并在colab中将其下载下来</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_history</span>():</span></span><br><span class="line">  <span class="keyword">import</span> pickle</span><br><span class="line">  <span class="keyword">from</span> google.colab <span class="keyword">import</span> files</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;history.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(history.history, f)</span><br><span class="line"></span><br><span class="line">  files.download(<span class="string">&#x27;history.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line">download_history()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="data-augmentation">Data Augmentation</h2>
<p>主要为了解决<code>overfitting</code>的情况(<code>train set</code>上<code>accuracy</code>很高，但是<code>val set</code>上<code>accuracy</code>低。两者相差较大)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_datagen = ImageDataGenerator(</span><br><span class="line">      rotation_range=<span class="number">40</span>,</span><br><span class="line">      width_shift_range=<span class="number">0.2</span>,</span><br><span class="line">      height_shift_range=<span class="number">0.2</span>,</span><br><span class="line">      shear_range=<span class="number">0.2</span>,</span><br><span class="line">      zoom_range=<span class="number">0.2</span>,</span><br><span class="line">      horizontal_flip=<span class="literal">True</span>,</span><br><span class="line">      fill_mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>rotation_range</code> is a value in degrees (0–180) within
which to randomly rotate pictures. 旋转角度</li>
<li><code>width_shift</code> and <code>height_shift</code> are ranges
(as a fraction of total width or height) within which to randomly
translate pictures vertically or horizontally. 主体平移</li>
<li><code>shear_range</code> is for randomly applying shearing
transformations. 剪切</li>
<li><code>zoom_range</code> is for randomly zooming inside pictures.
放大缩小</li>
<li><code>horizontal_flip</code> is for randomly flipping half of the
images horizontally. This is relevant when there are no assumptions of
horizontal assymmetry (e.g. real-world pictures). 是否镜像</li>
<li><code>fill_mode</code> is the strategy used for filling in newly
created pixels, which can appear after a rotation or a width/height
shift. 用什么值去填充旋转或者平移之后缺失的pixel的值</li>
</ul>
<p>完整代码是，仅仅是在初始化<code>ImageDataGenerator</code>时有一点改变：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create new model</span></span><br><span class="line">model_for_aug = create_model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># This code has changed. Now instead of the ImageGenerator just rescaling</span></span><br><span class="line"><span class="comment"># the image, we also rotate and do other operations</span></span><br><span class="line">train_datagen = ImageDataGenerator(</span><br><span class="line">      rescale=<span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">      rotation_range=<span class="number">40</span>,</span><br><span class="line">      width_shift_range=<span class="number">0.2</span>,</span><br><span class="line">      height_shift_range=<span class="number">0.2</span>,</span><br><span class="line">      shear_range=<span class="number">0.2</span>,</span><br><span class="line">      zoom_range=<span class="number">0.2</span>,</span><br><span class="line">      horizontal_flip=<span class="literal">True</span>,</span><br><span class="line">      fill_mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line"></span><br><span class="line">test_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Flow training images in batches of 20 using train_datagen generator</span></span><br><span class="line">train_generator = train_datagen.flow_from_directory(</span><br><span class="line">        train_dir,  <span class="comment"># This is the source directory for training images</span></span><br><span class="line">        target_size=(<span class="number">150</span>, <span class="number">150</span>),  <span class="comment"># All images will be resized to 150x150</span></span><br><span class="line">        batch_size=<span class="number">20</span>,</span><br><span class="line">        <span class="comment"># Since we use binary_crossentropy loss, we need binary labels</span></span><br><span class="line">        class_mode=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Flow validation images in batches of 20 using test_datagen generator</span></span><br><span class="line">validation_generator = test_datagen.flow_from_directory(</span><br><span class="line">        validation_dir,</span><br><span class="line">        target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">        batch_size=<span class="number">20</span>,</span><br><span class="line">        class_mode=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the new model</span></span><br><span class="line">history_with_aug = model_for_aug.fit(</span><br><span class="line">      train_generator,</span><br><span class="line">      steps_per_epoch=<span class="number">100</span>,  <span class="comment"># 2000 images = batch_size * steps</span></span><br><span class="line">      epochs=EPOCHS,</span><br><span class="line">      validation_data=validation_generator,</span><br><span class="line">      validation_steps=<span class="number">50</span>,  <span class="comment"># 1000 images = batch_size * steps</span></span><br><span class="line">      verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>这里贴一下作者写的画history的函数，可复用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_loss_acc</span>(<span class="params">history</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;Plots the training and validation loss and accuracy from a history object&#x27;&#x27;&#x27;</span></span><br><span class="line">  acc = history.history[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">  val_acc = history.history[<span class="string">&#x27;val_accuracy&#x27;</span>]</span><br><span class="line">  loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">  val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">  epochs = <span class="built_in">range</span>(<span class="built_in">len</span>(acc))</span><br><span class="line"></span><br><span class="line">  plt.plot(epochs, acc, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;Training accuracy&#x27;</span>)</span><br><span class="line">  plt.plot(epochs, val_acc, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;Validation accuracy&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&#x27;Training and validation accuracy&#x27;</span>)</span><br><span class="line"></span><br><span class="line">  plt.figure()</span><br><span class="line"></span><br><span class="line">  plt.plot(epochs, loss, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">  plt.plot(epochs, val_loss, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&#x27;Training and validation loss&#x27;</span>)</span><br><span class="line">  plt.legend()</span><br><span class="line"></span><br><span class="line">  plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="transfer-learning-迁移学习">transfer learning 迁移学习</h2>
<p>基本思想:
将别人训练好的模型的前几层的参数拿过来组装到你的模型前面，然后用你的训练集重新训练后面的<code>dense layers</code>的参数.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先下载Inception的pre-trained models，.h5文件</span></span><br><span class="line">!wget --no-check-certificate \</span><br><span class="line">    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \</span><br><span class="line">    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5</span><br><span class="line">   </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.applications.inception_v3 <span class="keyword">import</span> InceptionV3</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Model</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the weights file you downloaded into a variable</span></span><br><span class="line">local_weights_file = <span class="string">&#x27;/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the base model.</span></span><br><span class="line"><span class="comment"># Set the input shape and remove the dense layers.</span></span><br><span class="line">pre_trained_model = InceptionV3(input_shape = (<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>), </span><br><span class="line">                                include_top = <span class="literal">False</span>, <span class="comment"># 将最后一层dense去掉</span></span><br><span class="line">                                weights = <span class="literal">None</span>) <span class="comment"># 不需要默认的参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pre-trained weights you downloaded.</span></span><br><span class="line">pre_trained_model.load_weights(local_weights_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Freeze the weights of the layers. 将Inception网络中的参数都lock住，不让我们的训练集在上面fit</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> pre_trained_model.layers:</span><br><span class="line">  layer.trainable = <span class="literal">False</span></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment"># Choose `mixed_7` as the last layer of your base model</span></span><br><span class="line">last_layer = pre_trained_model.get_layer(<span class="string">&#x27;mixed7&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;last layer output shape: &#x27;</span>, last_layer.output_shape)</span><br><span class="line">last_output = last_layer.output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Flatten the output layer to 1 dimension 将Inception中mixed7层作为我们模型的输入层</span></span><br><span class="line">x = layers.Flatten()(last_output)</span><br><span class="line"><span class="comment"># Add a fully connected layer with 1,024 hidden units and ReLU activation</span></span><br><span class="line">x = layers.Dense(<span class="number">1024</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(x)</span><br><span class="line"><span class="comment"># Add a dropout rate of 0.2</span></span><br><span class="line">x = layers.Dropout(<span class="number">0.2</span>)(x)                  </span><br><span class="line"><span class="comment"># Add a final sigmoid layer for classification</span></span><br><span class="line">x = layers.Dense  (<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)(x)           </span><br><span class="line"></span><br><span class="line"><span class="comment"># Append the dense network to the base model</span></span><br><span class="line">model = Model(pre_trained_model.<span class="built_in">input</span>, x) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the model summary. See your dense network connected at the end.</span></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the training parameters</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer = RMSprop(learning_rate=<span class="number">0.0001</span>), </span><br><span class="line">              loss = <span class="string">&#x27;binary_crossentropy&#x27;</span>, </span><br><span class="line">              metrics = [<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure>
<h1 id="课程3nlp">课程3：NLP</h1>
<h2 id="tokenizer中的num_words参数">Tokenizer中的num_words参数</h2>
<p>https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do/53512348#53512348</p>
<p><code>tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)</code>使用word_index
=
tokenizer.word_index得到的word_index的长度是和vocab_size没有关系的，vocab_size指定的是只有出现频率排列前面vocab_size的词语才会被编码，如果没有，则会被跳过忽略。</p>
<p>如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Define input sentences</span></span><br><span class="line">sentences = [</span><br><span class="line">    <span class="string">&#x27;i love my dog&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;I, love my cat&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;You love my dog!&#x27;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the Tokenizer class</span></span><br><span class="line">tokenizer = Tokenizer(num_words = <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate indices for each word in the corpus</span></span><br><span class="line">tokenizer.fit_on_texts(sentences)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the indices and print it</span></span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line"><span class="built_in">print</span>(word_index)</span><br><span class="line">&gt;&gt; &#123;<span class="string">&#x27;love&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;my&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;i&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;dog&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;cat&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;you&#x27;</span>: <span class="number">6</span>&#125;</span><br><span class="line"></span><br><span class="line">tokenizer.texts_to_sequences(sentences)</span><br><span class="line">&gt;&gt; [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">2</span>]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>用<code>LSTM</code>，<code>GRU</code>对<code>IMDB</code>数据集实现分类：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow_datasets <span class="keyword">as</span> tfds</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据</span></span><br><span class="line">imdb, info = tfds.load(<span class="string">&#x27;imdb_reviews&#x27;</span>, with_info=<span class="literal">True</span>, as_supervised=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># Get the train and test sets</span></span><br><span class="line">train_data, test_data = imdb[<span class="string">&#x27;train&#x27;</span>], imdb[<span class="string">&#x27;test&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize sentences and labels lists</span></span><br><span class="line">training_sentences = []</span><br><span class="line">training_labels = []</span><br><span class="line"></span><br><span class="line">testing_sentences = []</span><br><span class="line">testing_labels = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Loop over all training examples and save the sentences and labels</span></span><br><span class="line"><span class="keyword">for</span> s,l <span class="keyword">in</span> train_data:</span><br><span class="line">  training_sentences.append(s.numpy().decode(<span class="string">&#x27;utf8&#x27;</span>))</span><br><span class="line">  training_labels.append(l.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Loop over all test examples and save the sentences and labels</span></span><br><span class="line"><span class="keyword">for</span> s,l <span class="keyword">in</span> test_data:</span><br><span class="line">  testing_sentences.append(s.numpy().decode(<span class="string">&#x27;utf8&#x27;</span>))</span><br><span class="line">  testing_labels.append(l.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert labels lists to numpy array</span></span><br><span class="line">training_labels_final = np.array(training_labels)</span><br><span class="line">testing_labels_final = np.array(testing_labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tokenize数据</span></span><br><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">vocab_size = <span class="number">10000</span></span><br><span class="line">max_length = <span class="number">120</span></span><br><span class="line">trunc_type=<span class="string">&#x27;post&#x27;</span></span><br><span class="line">oov_tok = <span class="string">&quot;&lt;OOV&gt;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the Tokenizer class</span></span><br><span class="line">tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate the word index dictionary for the training sentences</span></span><br><span class="line">tokenizer.fit_on_texts(training_sentences)</span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate and pad the training sequences</span></span><br><span class="line">sequences = tokenizer.texts_to_sequences(training_sentences)</span><br><span class="line">padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate and pad the test sequences</span></span><br><span class="line">testing_sequences = tokenizer.texts_to_sequences(testing_sentences)</span><br><span class="line">testing_padded = pad_sequences(testing_sequences,maxlen=max_length)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立模型</span></span><br><span class="line"><span class="comment"># Hyperparameters</span></span><br><span class="line">embedding_dim = <span class="number">64</span></span><br><span class="line">lstm1_dim = <span class="number">64</span></span><br><span class="line">lstm2_dim = <span class="number">32</span></span><br><span class="line">dense_dim = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the model</span></span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">    tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),</span><br><span class="line">    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm1_dim, return_sequences=<span class="literal">True</span>)),</span><br><span class="line">    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm2_dim)),</span><br><span class="line">    tf.keras.layers.Dense(dense_dim, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the model summary</span></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<h2 id="generate-next-word-产生文本">generate next word 产生文本</h2>
<p>这门课主要注意的是老师处理序列的方式，也就是产生训练<code>features</code>和<code>labels</code>的方式。其他的和前述课程一样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Initialize the Tokenizer class</span></span><br><span class="line">tokenizer = Tokenizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate the word index dictionary</span></span><br><span class="line">tokenizer.fit_on_texts(corpus)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the total words. You add 1 for the index `0` which is just the padding token.</span></span><br><span class="line">total_words = <span class="built_in">len</span>(tokenizer.word_index) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;word index dictionary: <span class="subst">&#123;tokenizer.word_index&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;total words: <span class="subst">&#123;total_words&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the sequences list</span></span><br><span class="line">input_sequences = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Loop over every line</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> corpus:</span><br><span class="line"></span><br><span class="line">	<span class="comment"># Tokenize the current line 这里将每一个句子变成token</span></span><br><span class="line">	token_list = tokenizer.texts_to_sequences([line])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">	<span class="comment"># Loop over the line several times to generate the subphrases</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(token_list)): <span class="comment"># 该循环是为了产生n_gram的训练集</span></span><br><span class="line">		</span><br><span class="line">		<span class="comment"># Generate the subphrase</span></span><br><span class="line">		n_gram_sequence = token_list[:i+<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Append the subphrase to the sequences list</span></span><br><span class="line">		input_sequences.append(n_gram_sequence)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the length of the longest line</span></span><br><span class="line">max_sequence_len = <span class="built_in">max</span>([<span class="built_in">len</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> input_sequences])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pad all sequences</span></span><br><span class="line">input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding=<span class="string">&#x27;pre&#x27;</span>)) <span class="comment"># 将所有句子都处理成一样长，这里用&quot;pre&quot;的padding模式是为了后面取lables（最后一个单词）的方便。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create inputs and label by splitting the last token in the subphrases</span></span><br><span class="line">xs, labels = input_sequences[:,:-<span class="number">1</span>],input_sequences[:,-<span class="number">1</span>] <span class="comment"># 最后一个词语作为labels，也就是我们要预测的东西，依据的输入就是它前面的单词</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert the label into one-hot arrays</span></span><br><span class="line">ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyperparameters</span></span><br><span class="line">embedding_dim = <span class="number">100</span></span><br><span class="line">lstm_units = <span class="number">150</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the model</span></span><br><span class="line">model = Sequential([</span><br><span class="line">          Embedding(total_words, embedding_dim, input_length=max_sequence_len-<span class="number">1</span>), <span class="comment"># 这里一定要记得-1,是因为我们取了sequence中最后一个单词作为labels</span></span><br><span class="line">          Bidirectional(LSTM(lstm_units)),</span><br><span class="line">          Dense(total_words, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use categorical crossentropy because this is a multi-class problem</span></span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, </span><br><span class="line">    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), </span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the model summary</span></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line">history = model.fit(xs, ys, epochs=epochs)</span><br></pre></td></tr></table></figure>
<p>基于字符集合的<code>generate text</code>可以参考 <a href="https://www.tensorflow.org/text/tutorials/text_generation" class="uri">https://www.tensorflow.org/text/tutorials/text_generation</a></p>
<h1 id="课程4-序列">课程4 序列</h1>
<p>选取<code>train,val,test</code>集合的标准，要选取包含整个序列的特征。<code>fixed-partitioning or Roll-foward partitioning</code>。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>instruction-following language models</title>
    <url>/2023/04/14/Instruction-Finetuned-LLM/</url>
    <content><![CDATA[<p>nlp领域很多新出现的名词或者火热的研究方向，没有一个统一的标准。我在接触这些新的概念的时候往往会很糊涂，需要找大量的文献来看，然后捋清楚模型或者技术路线的发展脉络。instructed
LM，它是需要对pre-trained
LLM进行finetune的，在这之前也有一种技术叫做prompt
engineering，它是一种给大模型指令输入的手段，通过调整给大模型的输入，从而使得大模型能够返回更好的输出，解决我们的问题。也有更好的解释引用自<a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/">blog</a></p>
<blockquote>
<p><strong>Prompt Engineering</strong>, also known as <strong>In-Context
Prompting</strong>, refers to methods for how to communicate with LLM to
steer its behavior for desired outcomes <em>without</em> updating the
model weights. It is an empirical science and the effect of prompt
engineering methods can vary a lot among models, thus requiring heavy
experimentation and heuristics</p>
</blockquote>
<p>prompt engineering得益于LLM拥有zero-shot learning和few-shot
learning的两种prompt 模型的方法的发展。它更多的来源于经验。</p>
<p>prompt engineering领域也出现了非常多的文章，就正如<a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/">blog</a>里的观点一样，我同样觉得有一些文章只需要很少的文字就能讲明白它提出的方法是什么，但还是花了很多的篇幅，一个通用的benchmark才是我们需要的，现在有的只是一些零零碎碎的方法论。prompt
engineering不是我的关注重点，它受制于很多因素的影响，比如如果你使用的是GPT-3模型来开展你的任务或者搭建你的application，你可能会因为输入过多的文字而超出limit，而且GPT可是按照字符数收费的，所以可能会比较贵。</p>
<p>那么除了使用prompt
engineering的方式来让LLM输出能让我们满意的结果，另外一种方式是fine-tune整个LLM，直接让它在特定的数据集上调整参数（整体调整或者局部调整，比如Lora，prefix-tuning）或者使用增强学习训练一个打分模型，这也属于fine-tune的一个大分支。</p>
<p>2013年的综述文章<a href="http://arxiv.org/abs/2303.18223">A Survey of
Large Language Models</a> 在第五章介绍了详细的adaptation tuning of
LLMs的方法，也就是我一个pretrain好的LLM，如何让它在不同的任务上得到更好的泛化能力，这时候就要tuning
LLM。作者介绍其中有两种方法，一个是instruction Tuning，第二个是alignment
tuning。后者就是利用增强学习让模型从人类的反馈中去改进自己生成的文本，InstructGPT采用了这种方法。第一种会稍微复杂一点，但原理很简单，就是创造一系列的instruction和问答对，让LLM在这些新instruction上重新finetune，loss为sequence-to-sequence的loss。</p>
<blockquote>
<p>[My personal spicy take]
这里这篇综述我觉得写的不完整，有点误导读者。这篇综述第五章只介绍了adaptation
tuning模型中的两种，但在instruction
tuning出现之前，还有不少技术能够帮助我们“further adapt LLM according to
specific goals”. 不仅如此，这篇综述也没有很好的解释instruction
tuning为什么就能帮助我们在不同任务上有了performance的提高。所以我就想写一篇博客来记录如果我们拥有了一个pretrained的大模型，我们可以有什么样的做法来使得大模型在特定的任务上为我们所用。详见另一篇博客“Adaptation
Tuning of LLMs”</p>
</blockquote>
<p>在接触羊驼模型后，我一直有一个疑问，为什么instruction
finetuned模型performance有了提高，或者说它在什么样的任务上有了提高？这个问题一直困扰我，直到我看到了google家的<a href="http://arxiv.org/abs/2109.01652">Finetuned Language Models Are
Zero-Shot Learners</a>.instruction
tuning这种finetune方式的提出是为了<strong>improve zero-shot performance
on unseen tasks</strong>，具体一点就是在一些任务上比如阅读归纳，question
answering和语言推理上，研究者发现GPT3的zero-shot learning比few-shot
能力差很多，作者说一个潜在的原因是因为如果没有一些context给到模型的话，模型在面对跟pretrain时候数据相差很大的prompt时候会很困难，说直白点，就是没有例子给它参考了，就不会做题了。instruction
tuning这种方式就提供了一种非常简单的方式，它在好多个task上finetune这个模型，这里每一个task的数据组织形式跟原来不一样了，现在被组织成了(instruction,[input],output)的形式。finetune完之后的模型在unseen
task上做evaluation，研究者发现被instruction
finetune之后的模型比原来的模型在同一任务上的zero-shot能力大大提升：</p>
<figure>
<img src="/2023/04/14/Instruction-Finetuned-LLM/image-20230522182724638-16848907566251.png" alt="performance">
<figcaption aria-hidden="true">performance</figcaption>
</figure>
<h1 id="instruction-tuning">instruction tuning</h1>
<p>想要做到instruction tuning有两个前提条件：1. 你有一个pretrained的模型
2.
有很多instructions。首先第一个条件可以看看市面上有哪些模型是已经开源了，参考<a href="http://arxiv.org/abs/2303.18223">A Survey of Large Language
Models</a>3.1的整理，2023年斯坦福的羊驼模型是基于meta的LLaMA，所以目前github上出现了很多用LLaMA为LLM，在上面做instruction
tuning工作的。</p>
<figure>
<img src="/2023/04/14/Instruction-Finetuned-LLM/image-20230418093127008.png" alt="LLMs">
<figcaption aria-hidden="true">LLMs</figcaption>
</figure>
<p>那第一个问题解决了，起码我们有开源的LLM可以load到本地来使用，感谢facebook的开源。第二个问题如何产生很多的instructions，斯坦福的<a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">羊驼模型Alpaca</a>采用的是下面文章介绍的方法，省时省力，花费上不超过600美金。当然也有其他的一些产生instruction的方法，详细可以参考<a href="http://arxiv.org/abs/2303.18223">A Survey of Large Language
Models</a>
，其中作者介绍了一系列可以从现有数据集生成instruction的方法，这些方法应该也是低成本快速产生instruction的方法。</p>
<hr>
<p><a href="http://arxiv.org/abs/2212.10560">Self-instruct: Aligning
Language Model with Self Generated Instructions</a>
这篇文章介绍了一种self generated
instructions的方法，简单说就是让LLM自己生成人类的问题的答案，然后将这些instructions
重新来fine-tune我们的LLM。这样做的一个前提条件是：1. Large
“instruction-tuned” language models (finetuned to respond to
instructions) have demonstrated a remarkable ability to generalize
zero-shot to new tasks. 2. 产生instruction
data非常的耗时，原来都是采用Human written的方式。具体步骤是：</p>
<figure>
<img src="/2023/04/14/Instruction-Finetuned-LLM/image-20230414131301078.png" alt="self-instruct whole picture">
<figcaption aria-hidden="true">self-instruct whole picture</figcaption>
</figure>
<p>作者首先使用175个手工写的instructions作为seed
set，利用这175个instructions用LLM再次生成更多的instructions，将这些instructions再次输入到LLM中我们就得到了很多input-output
pair。这些input-output pair将会用来做instruction tuning.
作者使用的LLM是GPT-3. 最终得到了52k个instructions，以及82k个input-output
pair。</p>
<h2 id="instruction-generation">Instruction generation</h2>
<p>用bootstrap的方式，以人工产生的instruction为基础，用GPT来自己生成更多的"new
and novel"instruction。</p>
<hr>
<p>自Alpaca之后，国内的一些团队也仿照斯坦福的这种模型，做了一些自己的LLM，例如https://github.com/LC1332/Chinese-alpaca-lora，instruction来自用GPT翻译的斯坦福产生的52k的instruction的数据，它基于的模型<a href="https://github.com/tloen/alpaca-lora">aplaca-lora</a>,lora的全称是Low-rank
adaptation，作者说自己"reproducing the <a href="https://github.com/tatsu-lab/stanford_alpaca">Stanford Alpaca</a>
results using <a href="https://arxiv.org/pdf/2106.09685.pdf">low-rank
adaptation (LoRA)</a>."，并且训练好的instructed
model提供的文本质量可以和text-davinci-003(GPT-3)媲美。不太了解这个LoRA，有兴趣的可以读原文：https://arxiv.org/pdf/2106.09685.pdf。</p>
<p>看了Alpaca的blog，我发现斯坦福在evaluation阶段是将alpaca的结果和gpt3来进行比较的，由此也引发了我的思考，就是我们如何去衡量一个LLM的performance。刚上文的review的第七章很好的解答了我的疑惑，包括一系列的基本评测任务以及高级的评测任务。当然作者在7.3也给出了一些公开的全面的benchmarks，而且是用的比较多的，其中有MMLU，BIG-bench，HELM，这些benchmark内都包含了很多个任务，可以综合评测一个LLM的performance。</p>
<h1 id="stanford-alpaca">stanford alpaca</h1>
<p>这是2023年斯坦福开源的一款基于meta的LLaMA的大语言模型,名字叫<a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">羊驼</a>，只有7个billion的参数。属于instruction
tuning的一个标杆。里面用了两个比较新的技术，第一个是上文提到的self-instruct，就是让GPT或者市面上的LLM在我们人工产生的种子instruction上去产生一系列更多的instruction，包括配套每一个instruction的input和output。斯坦福将这部分用GPT-3.5(text-davinci-003)产生的instruction数据慷慨开源，见<a href="https://github.com/tatsu-lab/stanford_alpaca">github</a>。不仅如此斯坦福还给出了产生这些instructions的代码，可谓是非常nice了，方便大家上手学习。</p>
<p>我比较关注用这些instructions数据如何finetune大模型LLaMA的过程，这里权当自己复现以及阅读斯坦福代码时候的记录。首先我本来是想在meta的LLaMA的7B开源模型上做实验，但发现想获取meta的weights需要提前申请，详细可参考huggingface的transformer页面。</p>
<p>斯坦福的代码仓库可以在<a href="https://github.com/tatsu-lab/stanford_alpaca">github</a>找到。</p>
<h1 id="reference">reference</h1>
<ol type="1">
<li><a href="http://arxiv.org/abs/2212.10560">Self-instruct: Aligning
Language Model with Self Generated Instructions</a></li>
<li><a href="http://arxiv.org/abs/2210.11416">Scaling
Instruction-Finetuned Language Models</a></li>
<li><a href="http://arxiv.org/abs/2109.01652">Finetuned Language Models
Are Zero-Shot Learners</a></li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>LLMs</tag>
      </tags>
  </entry>
  <entry>
    <title>LCP 40. 心算挑战</title>
    <url>/2021/11/10/LCP-40-%E5%BF%83%E7%AE%97%E6%8C%91%E6%88%98/</url>
    <content><![CDATA[<h1 id="题目">题目：</h1>
<p>「力扣挑战赛」心算项目的挑战比赛中，要求选手从 N 张卡牌中选出 cnt
张卡牌，若这 cnt 张卡牌数字总和为偶数，则选手成绩「有效」且得分为 cnt
张卡牌数字总和。 给定数组 cards 和 cnt，其中 cards[i] 表示第 i
张卡牌上的数字。
请帮参赛选手计算最大的有效得分。若不存在获取有效得分的卡牌方案，则返回
0。</p>
<p>示例 1：</p>
<p>输入：cards = [1,2,8,9], cnt = 3</p>
<p>输出：18</p>
<p>解释：选择数字为 1、8、9 的这三张卡牌，此时可获得最大的有效得分
1+8+9=18。</p>
<p>示例 2：</p>
<p>输入：cards = [3,3,1], cnt = 1</p>
<p>输出：0</p>
<p>解释：不存在获取有效得分的卡牌方案。</p>
<p>提示：</p>
<p>1 &lt;= cnt &lt;= cards.length &lt;= 10^5 1 &lt;= cards[i] &lt;=
1000</p>
<p>来源：力扣（LeetCode）
链接：https://leetcode-cn.com/problems/uOAnQW</p>
<h1 id="思路">思路</h1>
<p>排序先取最大的cnt个数，如果它们的和是偶数直接输出，不然就找一个已取的最小的奇数换成剩下未取的最大的偶数，或者找一个已取的最小的偶数换成剩下未取的最大奇数</p>
<h1 id="解答">解答</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxmiumScore</span>(<span class="params">self, cards: <span class="type">List</span>[<span class="built_in">int</span>], cnt: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span></span><br><span class="line">        cards.sort()</span><br><span class="line">        length = <span class="built_in">len</span>(cards)</span><br><span class="line">        start = length-cnt</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">sum</span>(cards[start:]) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">sum</span>(cards[start:])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sums = <span class="built_in">sum</span>(cards[start:])</span><br><span class="line">            left_min_ou,left_min_ji,left_max_ou,left_max_ji = self.find(cards[:start].copy())</span><br><span class="line">            right_min_ou,right_min_ji,right_max_ou,right_max_ji = self.find(cards[start:].copy())</span><br><span class="line">            tmp1,tmp2 = -<span class="number">1</span>,-<span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> right_min_ji != -<span class="number">1</span> <span class="keyword">and</span> left_max_ou!= -<span class="number">1</span>:</span><br><span class="line">                tmp1 = sums - right_min_ji + left_max_ou</span><br><span class="line">            <span class="keyword">if</span> right_min_ou != -<span class="number">1</span> <span class="keyword">and</span> left_max_ji!= -<span class="number">1</span> :</span><br><span class="line">                tmp2 = sums - right_min_ou + left_max_ji</span><br><span class="line">            <span class="keyword">if</span> tmp1!= -<span class="number">1</span> <span class="keyword">or</span> tmp2!= -<span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">max</span>(tmp1,tmp2)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span> </span><br><span class="line">            </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find</span>(<span class="params">self, li</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(li) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span></span><br><span class="line">        li.sort()</span><br><span class="line">        min_ou, min_ji, max_ou, max_ji = -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> li:</span><br><span class="line">            <span class="keyword">if</span> num % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> min_ou == -<span class="number">1</span>:</span><br><span class="line">                min_ou = num</span><br><span class="line">            <span class="keyword">elif</span> num % <span class="number">2</span> != <span class="number">0</span> <span class="keyword">and</span> min_ji == -<span class="number">1</span>:</span><br><span class="line">                min_ji = num</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> <span class="built_in">reversed</span>(li):</span><br><span class="line">            <span class="keyword">if</span> num % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> max_ou == -<span class="number">1</span>:</span><br><span class="line">                max_ou = num</span><br><span class="line">            <span class="keyword">elif</span> num % <span class="number">2</span> != <span class="number">0</span> <span class="keyword">and</span> max_ji == -<span class="number">1</span>:</span><br><span class="line">                max_ji = num</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> min_ou, min_ji, max_ou, max_ji</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM大模型推理加速</title>
    <url>/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/</url>
    <content><![CDATA[<p>最近框架vLLM在LLaMA-13B以及7B模型上的推理速度相比较Tranformers有了质的提升。之前写过一篇大模型量化技术的文章，量化技术算是大模型出来初期大家使用的普遍比较多的方法之一，这里强调一点，我这里所说的模型加速是指在推理阶段我们让模型运行的更快，而且返回的结果要和原来的大参数模型差不多。这里重点强调的原因是我在看一些资料的时候发现有不少博客分享的是在模型训练阶段或者finetune阶段如何让模型训练的更快，这里就涉及到efficient
finetuning的技术（p-tuning,
prefix-tuning等），我这篇博客只关注模型训练完成之后如何在推理阶段让它更快，在同样时间内处理更多sequence（吞吐量througout），显存占用更低。大模型推理加速技术为什么这么受关注还是因为想在一些消费级别显卡上部署一个大模型为用户所用，而不是仅仅停留在实验室阶段。</p>
<blockquote>
<p>我在看这个topic下的文章的时候，发现往往一些方法提出来有一些是减少了显存占用，有一些是提高了吞吐量（跟减少latency一回事），所以具体在实现时应用哪个办法加速你的模型推理还要根据实际情况去对比分析，或者你每个方法都尝试一下也行。当然又一些方法集成地很好，比如量化模型中的GPQT已经集成进transformers库，用起来很方便。如果碰到一些很复杂的，比如prune“剪枝”就有点难以快速验证。</p>
</blockquote>
<p><a href="https://vllm.ai/">vLLM</a>
暂时还没有文章发出来，我在谷歌搜寻有没有review介绍大模型加速文章的时候也没找到很新的文章，不过找到了一篇微软在23年发布的<a href="http://arxiv.org/abs/2304.04487">LLMA</a>,
我本来觉得思想类似，但是后来仔细看了下文章发现并不是一回事，我觉得文章标题有点误导人，起的太大了，本质上文章其实就是发现了decoding时候生成的句子和之前的句子有一部分的文字重叠，所以作者考虑这部分重叠内容其实不需要让模型再去decoding了，那就想了个办法，在decoding的时候把前面一步的结果保存下来，比较当前步骤和前一步骤token的差距，差距小的就不再进行计算了</p>
<blockquote>
<p>一点不成熟的想法：这个文章思路可取，但创造力有限。</p>
</blockquote>
<p>不过它在introduction章节介绍了四种比较通用的加速方法：quantization,
pruning, compression and distillation，同样的分类也可以在<a href="https://arxiv.org/pdf/2205.07877.pdf">A Comprehensive Survey on
Model Quantization for Deep Neural Networks</a>文章中找到，
不过两者介绍的有一点点的不同：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230828093646830.png" alt="image-20230828093646830">
<figcaption aria-hidden="true">image-20230828093646830</figcaption>
</figure>
<p><a href="https://arxiv.org/pdf/2205.07877.pdf">Survey</a>将四种技术统一包含在了模型压缩里。我觉得review里这种分类比较合理，因为微软这篇文章compression引用的文章是</p>
<p><a href="https://arxiv.org/abs/2002.02925">Bert-of-theseus:
Compressing bert by progressive module replacing</a>,
compression应该是一种统称。题外话，根据LLMA文章的意思，它提出的这种帮助reduce
the serving
cost的方式不属于上述任意一类，它认为以transformer为基础的生成模型，推理阶段主要消耗的时间瓶颈在autoregressive
decoding。这里贴原文便于理解</p>
<blockquote>
<p>While there are general methodologies that help reduce the serving
cost of LLMs such as quantization(Dettmers &amp; Zettlemoyer, 2023),
pruning (Frantar &amp; Alistarh, 2023), compression (Xu et al., 2020)
and distillation (Wang et al., 2020), <strong>the inference efficiency
bottleneck of these transformer-based generative models (e.g., GPT) is
mainly associated with autoregressive decoding: at test time, output
tokens must be decoded (sequentially) one by one, which poses
significant challenges for the LLMs to be deployed at
scale.</strong></p>
</blockquote>
<h1 id="quatization-量化">Quatization 量化</h1>
<h3 id="参考文献">参考文献</h3>
<ul>
<li><a href="https://arxiv.org/pdf/2205.07877.pdf">A Comprehensive
Survey on Model Quantization for Deep Neural Networks</a></li>
<li>https://huggingface.co/docs/transformers/main_classes/quantization#autogptq-integration
huggingface的document，其中放在最上面的就是<a href="https://arxiv.org/pdf/2210.17323.pdf">GPTQ</a></li>
</ul>
<h2 id="gptq">GPTQ</h2>
<p>读者可以自行阅读GPTQ的原文来了解它具体是如何做的，我喜欢找一些其他的文章来看别的作者是如何介绍自己的同行作品的，比如下面的这篇文章<a href="https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf">SmoothQuant:
Accurate and Efficient Post-Training Quantization for Large Language
Models</a> 的第六章节related
work里这样比较自己的smoothQuant和其他的量化模型方法的：</p>
<blockquote>
<p>GPTQ (Frantar et al., 2022) applies quantization only to weights but
not activations.
GPTQ这种方法只对weights做了量化，并没有对激活值做量化（我个人认为虽然这是事实，但有点硬凹的意思，因为对activations做量化映射并不会加速很多）</p>
<p>LLM.int8() uses mixed int8/fp16 decomposition to address the
activation outliers. However, such implementation leads to large latency
overhead, which can be even slower than FP16 inference.
意思是LLM.int8()这种方法只是减少了显存占用，并没有减少推理延迟，说白了就是慢，runtime没提高</p>
</blockquote>
<h1 id="pruning">pruning</h1>
<h1 id="low-rank-approximation">low-Rank Approximation</h1>
<h1 id="knowledge-distillation">Knowledge distillation</h1>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Part-3-Model Combination</title>
    <url>/2021/12/06/Part-3-Model-Combination/</url>
    <content><![CDATA[<h1 id="方差variance-偏差bias">方差Variance &amp; 偏差bias</h1>
<p>方差衡量的是train的过程中的loss和dev set
loss之间的差距，bias衡量的是train set loss和真实human
loss之间的差距。</p>
<p>减小bias：
用更复杂的模型，用更适合的优化算法RMSprop、Momentum，超参数搜索，Boosting，Stacking</p>
<p>减小variance: 用更简单的模型，正则化L1/L2,drop
out，用更多的数据，Bagging，Stacking</p>
<figure>
<img src="/2021/12/06/Part-3-Model-Combination/image-20211206151138567.png" alt="image-20211206151138567">
<figcaption aria-hidden="true">image-20211206151138567</figcaption>
</figure>
<h2 id="bagging---bootstrap-aggragrating">Bagging - Bootstrap
AGGragrating</h2>
<ul>
<li>同时训练n个模型，最终取平均作为结果</li>
<li>每一个模型在m个样本中训练，m个样本是对训练数据bootstrap采样来的（有放回的采样）</li>
</ul>
]]></content>
      <categories>
        <category>李沐-实用机器学习(学习笔记)</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM评测/Evaluation</title>
    <url>/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/</url>
    <content><![CDATA[<p>最近在关注模型performance评估的问题，打算在这个主题上做一个整理，也是受到很多博客和文章的启发写这篇文章，所以就将所有推荐阅读的文章放在前面，感兴趣的小伙伴可以拓展阅读。</p>
<ol type="1">
<li>老刘说NLP 公众号中8.10发的一篇文章《如何让自己的大模型榜单评分更高》
这篇文章有点借鉴了hugging face的<a href="https://huggingface.co/blog/zh/evaluating-mmlu-leaderboard">Open
LLM 排行榜近况</a></li>
<li>https://mp.weixin.qq.com/s/WiF3yRU5MZS7ulLTP8FIpw</li>
</ol>
<p>首先说一下这个<a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">LLM榜单</a>,
有四个benchmark，其中上面的博客就是重点讲了为什么同样一个模型比如LLaMA在MMLU上评测的结果会不如llama文章中提的效果，trick就在作者使用MMLU这个benchmark的方式有很大不同，这里来看看MMLU这个benchmark。</p>
<h1 id="mmlu-benchmark">MMLU benchmark</h1>
<p>首先看一下这个数据集到底是什么数据集，长什么样子，先给出文章中的定义：</p>
<blockquote>
<p><strong>MMLU</strong> (<strong>Massive Multitask Language
Understanding</strong>) is a new benchmark designed to measure knowledge
acquired during pretraining by evaluating models exclusively in
zero-shot and few-shot settings.</p>
</blockquote>
<p>这个评测集合里包含了57个学科，也就是57个task。原始的数据集长这样，里面的每个问题包含四个可能选项，且每个问题只有一个正确答案。：</p>
<figure>
<img src="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/image-20230816145305589.png" alt="image-20230816145305589">
<figcaption aria-hidden="true">image-20230816145305589</figcaption>
</figure>
<p>可以看到基本上就是question，answer的组织。注意这里看到原始数据的时候我还有点没看明白，作者的readme中也没写，还是对beginner有点不友好，第一列表示question，第二到第四列表示四个选项，最后一列是答案。所以可以看到原作者在<a href="https://github.com/hendrycks/test/blob/master/evaluate.py">evaluation</a>的代码中这样处理的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">choices = [<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>, <span class="string">&quot;C&quot;</span>, <span class="string">&quot;D&quot;</span>] <span class="comment"># 首先定义选项</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval</span>(<span class="params">args, subject, engine, dev_df, test_df</span>):</span></span><br><span class="line">    cors = []</span><br><span class="line">    all_probs = []</span><br><span class="line">    answers = choices[:test_df.shape[<span class="number">1</span>]-<span class="number">2</span>] <span class="comment"># 对于每一个csv文件读取进来后取answers</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">label = test_df.iloc[i, test_df.shape[<span class="number">1</span>]-<span class="number">1</span>] <span class="comment"># label这里其实是取得最后一列，也就是答案</span></span><br></pre></td></tr></table></figure>
<p>但这个评测数据集在用来评测LLM的过程中衍生出了很多版本，基本是prompt的变化：</p>
<figure>
<img src="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/640.png" alt="MMLU的不同实现">
<figcaption aria-hidden="true">MMLU的不同实现</figcaption>
</figure>
<p>同样的问答对，比如上面的选择题，Harness没有指令，并且衍生的两个版本也就是helm和harness版本还加了Question这个前缀，harness在选线之前还加了Choices。就这么一点差距，就导致同一个llm的出来的分数不一样：</p>
<figure>
<img src="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/LLM-01-bis-01.png" alt="LLM在不同MMLU实现上的评分">
<figcaption aria-hidden="true">LLM在不同MMLU实现上的评分</figcaption>
</figure>
<blockquote>
<p>关于如何使用这个benchmark，参考<a href="https://github.com/hendrycks/test/blob/master/evaluate.py">MMLU原始实现</a>，作者写的是用chatgpt来产生答案，prompt为：<code>prompt = "The following are multiple choice questions (with answers) about &#123;&#125;.\n\n".format(format_subject(subject))</code></p>
</blockquote>
<p>这三种实现方式不仅prompt的形式不同，也就是上面提到的。并且它在计算F1score的时候的机制也不同。</p>
<ol type="1">
<li>原始实现</li>
</ol>
<p>在原始实现中的评估的代码是这样写的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ans <span class="keyword">in</span> answers:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        lprobs.append(c[<span class="string">&quot;choices&quot;</span>][<span class="number">0</span>][<span class="string">&quot;logprobs&quot;</span>][<span class="string">&quot;top_logprobs&quot;</span>][-<span class="number">1</span>][<span class="string">&quot; &#123;&#125;&quot;</span>.<span class="built_in">format</span>(ans)]) <span class="comment"># c是chatgpt的回答</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Warning: &#123;&#125; not found. Artificially adding log prob of -100.&quot;</span>.<span class="built_in">format</span>(ans))</span><br><span class="line">    lprobs.append(-<span class="number">100</span>)</span><br><span class="line">    pred = &#123;<span class="number">0</span>: <span class="string">&quot;A&quot;</span>, <span class="number">1</span>: <span class="string">&quot;B&quot;</span>, <span class="number">2</span>: <span class="string">&quot;C&quot;</span>, <span class="number">3</span>: <span class="string">&quot;D&quot;</span>&#125;[np.argmax(lprobs)]</span><br><span class="line">    probs = softmax(np.array(lprobs))</span><br><span class="line"></span><br><span class="line">    cor = pred == label</span><br><span class="line">    cors.append(cor)</span><br><span class="line">    all_probs.append(probs)</span><br></pre></td></tr></table></figure>
<p>该方法在评估的时候，仅仅比较了模型对四个选项字母的预测概率，哪个选项的概率高就选哪个，即便是在极端情况下四个选项的概率值都很低的情况下也会选择某个选项，但其实模型有时候会回答很多不相关的东西（都是很高的概率的token），所以这种方式有点”放水“，整体评估出来的分数会偏高。</p>
<ol start="2" type="1">
<li><a href="https://github.com/stanford-crfm/helm">HELM实现</a></li>
</ol>
<p>HELM实现是根据模型预测的下一个输出词元的概率来选择输出文本，并将生成的文本与正确答案的文本进行对比。这种方式有效避免了如果模型的答案中出现概率高的token不是选项中的任意一个，那么就会判为错误答案。</p>
<p>看了helm的代码仓库，着实有点丰富。内容很多我都没有找到在哪个文件里做的evaluation的计算，只知道了读取csv的地方。有好心的小伙伴可以私信我告诉我在哪里。</p>
<ol start="3" type="1">
<li>harness实现</li>
</ol>
<p>这是hugging
face的llm榜单所用的实现。它不再是只是统计选项，而是连同选项字母以及后面的答案一起被考虑进来，计算的是整个序列的概率（获取每个词元的概率
(与上面其他实现一样)
并求它们的联合概率），那么很容易一些长文本的联合概率会比短文本的联合概率大，所以作者说可以在联合概率的基础上在做一个归一化，也就是用对数联合概率/
token数。</p>
<figure>
<img src="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/image-20230817100952429.png" alt="MMLU三种实现对于模型输出的总结">
<figcaption aria-hidden="true">MMLU三种实现对于模型输出的总结</figcaption>
</figure>
<p>例如实现如下，基于GPT2计算句子联合概率的一个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [</span><br><span class="line">    <span class="string">&quot;A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN)&quot;</span>,</span><br><span class="line">    <span class="string">&quot;The term MLP is used ambiguously, sometimes loosely to any feedforward ANN, sometimes strictly to refer to networks composed of multiple layers of perceptrons (with threshold activation); see § Terminology&quot;</span>,</span><br><span class="line">    <span class="string">&#x27;Multilayer perceptrons are sometimes colloquially referred to as &quot;vanilla&quot; neural networks, especially when they have a single hidden layer.[1]&#x27;</span>,</span><br><span class="line">    <span class="string">&quot;An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function.&quot;</span>,</span><br><span class="line">]</span><br><span class="line">model = transformers.GPT2LMHeadModel.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line">tok = transformers.GPT2Tokenizer.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line">tgs = []</span><br><span class="line"><span class="keyword">for</span> dat <span class="keyword">in</span> data:</span><br><span class="line">    random.seed(dat)</span><br><span class="line">    <span class="comment"># print(model(tok.encode(dat, return_tensors=&quot;pt&quot;))[0][0])</span></span><br><span class="line">    toks = tok.encode(dat, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">    ind = random.randrange(<span class="built_in">len</span>(toks[<span class="number">0</span>]) - <span class="number">1</span>)</span><br><span class="line">    logits = F.log_softmax(model(toks)[<span class="number">0</span>], dim=-<span class="number">1</span>)[:, :-<span class="number">1</span>]  <span class="comment"># [batch, seq, vocab]</span></span><br><span class="line">    res = torch.gather(logits, <span class="number">2</span>, toks[:, <span class="number">1</span>:].unsqueeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    tgs.append(<span class="built_in">float</span>(res[ind:].<span class="built_in">sum</span>()))</span><br></pre></td></tr></table></figure>
<p>在“老刘说NLP”的博客中也提到了一点，就是上面的方式都是开源模型，所以很容易就能得到每一个token的预测概率，所以返回结果可以拆的这么细致来分析。如果是闭源模型只返回response的话，这时候就需要用正则的方式来抽取回答内容里的选项，比如<a href="https://arxiv.org/abs/2308.04813">CEVAL</a>的测试方案：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_cot_answer</span>(<span class="params">self, line, gen_ans</span>):</span></span><br><span class="line">    m = re.findall(<span class="string">r&#x27;所以答案是(.+?)。&#x27;</span>, gen_ans, re.M)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(m) &gt; <span class="number">0</span> <span class="keyword">and</span> m[-<span class="number">1</span>] <span class="keyword">in</span> self.choices:</span><br><span class="line">        <span class="keyword">return</span> m[-<span class="number">1</span>], <span class="literal">True</span></span><br><span class="line">    answer_patterns = [</span><br><span class="line">        <span class="string">r&#x27;([ABCD])是正确的&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;选项([ABCD])正确&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;答案为([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;答案是([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;答案([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;选择([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;答案：([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;选择答案([ABCD])&#x27;</span></span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># RE extraction</span></span><br><span class="line">    <span class="keyword">for</span> answer_pattern <span class="keyword">in</span> answer_patterns:</span><br><span class="line">        m = re.search(answer_pattern, gen_ans, re.M)</span><br><span class="line">        <span class="keyword">if</span> m:</span><br><span class="line">            answer = m.group(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> answer, <span class="literal">False</span></span><br><span class="line">        <span class="comment"># only containing one choice-character</span></span><br><span class="line">        m = re.findall(<span class="string">r&#x27;[ABCD]&#x27;</span>, gen_ans, re.M)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(m) == <span class="number">1</span>:</span><br><span class="line">            answer = m[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">return</span> answer, <span class="literal">False</span></span><br><span class="line">        answer_word_counter = <span class="number">0</span></span><br><span class="line">        <span class="comment"># only containing one choice-context</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> self.choices:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(line[<span class="string">f&#x27;<span class="subst">&#123;c&#125;</span>&#x27;</span>]) <span class="keyword">in</span> gen_ans:</span><br><span class="line">                answer = c</span><br><span class="line">                answer_word_counter += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> answer_word_counter == <span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">return</span> answer, <span class="literal">False</span></span><br><span class="line">                <span class="keyword">return</span> <span class="string">&#x27;-&#x27;</span>, <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>对CLEVA评测平台感兴趣的可以看原文paper或者参考<a href="https://mp.weixin.qq.com/s/WiF3yRU5MZS7ulLTP8FIpw">文章</a>。原文说CLEVA是专门为评估中文语言模型而设计的平台。</p>
</blockquote>
<h1 id="section"></h1>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Python3.7: error while loading shared libraries: libpython3.7m.so.1.0</title>
    <url>/2021/12/29/Python3-7-error-while-loading-shared-libraries-libpython3-7m-so-1-0/</url>
    <content><![CDATA[<p>在ubuntu上安装了其他版本的python之后遇到如下报错：</p>
<p>参考[https://stackoverflow.com/questions/58649177/python3-7-error-while-loading-shared-libraries-libpython3-7m-so-1-0]</p>
<p>对于我的这种情况，先查看<code>libpython3.7m.so.1.0</code>这个文件在哪里？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">locate libpython3.7m.so.1.0</span><br></pre></td></tr></table></figure>
<p>上面的命令如果返回无，要先更新下系统内文件系统的index字典</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">updatedb</span><br></pre></td></tr></table></figure>
<p>找出libpython3.7m.so.1.0在哪一个lib文件夹内，然后将该lib路径加入搜索路径，可以通过：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=/lib:/usr/lib:/usr/local/lib</span><br></pre></td></tr></table></figure>
<p>以上方式只对该session起作用，如果reboot了系统就会失效，想要永久的方式:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ldconfig /usr/local/lib </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>QA(question answering)</title>
    <url>/2023/05/09/QA-question-answering/</url>
    <content><![CDATA[<h1 id="qa是什么">QA是什么？</h1>
<p>直观上理解，QA就是用户给出一个question，系统（模型）给出一个answer。它和reading
comprehension还是不一样的，在<a href="Latent%20Retrieval%20for%20Weakly%20Supervised%20Open%20Domain%20Question%20Answering">paper</a>中对于这两个问题做了归纳：</p>
<figure>
<img src="/2023/05/09/QA-question-answering/image-20230508115516457.png" alt="Comparison of assumptions">
<figcaption aria-hidden="true">Comparison of assumptions</figcaption>
</figure>
<p>reading
comprehension问题，我们的任务是从一个paragraph中找出answer：comprehend a
passage of text and answer questions about its content (P,Q)-&gt;
A，也就是我们事先不仅有question，还有一段paragraph，我们只需要找出answer在这个paragrapg中的位置（span），但是对于open-domain的QA来说，我们事先是不知道anwer在哪儿的，比如上图中unsupervised
QA。那么对于这一类open-domaiin的问题如何解决？最常见的就是retriver-reader架构，就是我们先从一大堆语料库中挑选出我们的paragraph，然后对于这一些paragraph，我们用reading
comprehension的技术再来定位answer在哪里。上面这篇paper就是应用的这种思想，提出了ORQA，这是19年的文章，想法很典型。</p>
<figure>
<img src="/2023/05/09/QA-question-answering/image-20230508120439778.png" alt="ORQA">
<figcaption aria-hidden="true">ORQA</figcaption>
</figure>
<p>上文提到的斯坦福的数据集SQuAD是使用的最广泛的reading
comprehension数据集，它的组成形式是（passage,
question,answer），每一个answer都是passgae里面的一个segment。</p>
<figure>
<img src="/2023/05/09/QA-question-answering/image-20230508121850697.png" alt="SQuAD example">
<figcaption aria-hidden="true">SQuAD example</figcaption>
</figure>
<p>这就有它的局限性，因为有一些问题的回答是不会在passage中找到的，所以它不能用于open-domain的task，它完全是一个监督性任务的数据集，目前在这个数据集上最好的模型的表现已经超越了人类，可以说是"almost
solved"。</p>
<h1 id="reading-comprehension">Reading Comprehension</h1>
<p>首先我们可以先从比较简单的问题开始解决，reading
comprehension可以说是QA的一个子问题。第一步搞明白问题定义：</p>
<figure>
<img src="/2023/05/09/QA-question-answering/image-20230508122338691.png" alt="problem formulation">
<figcaption aria-hidden="true">problem formulation</figcaption>
</figure>
<p><strong>2016年-2018年，主要用于解决RC的方法是LSTM+attention的模型架构，这些模型主要有15年的attentive
reader、16，17年的stanford attentive
reader、17年的match-LSTM、17年的BiDAF、17年的dynamic coattention
network、17年的DrQA、17年的R-Net、17年的ReadoNet。2019年开始，主要是18年Bert出来之后，大家普遍开始采用finetune
BERT来解决这个问题，值得一提的是在BERT的原文paper中下游任务也使用了SQuAD来做了实验，作者用于预测start和end的方式也设计的很精巧。</strong></p>
<h2 id="stanford-attentive-reader">stanford attentive reader</h2>
<p>这篇文章在logistic
regression的基础上（SQuAD数据集）有了巨大的进步，算是18年之前lstm+attention架构的集大成者。思想很简单，但里面很多细节，比如在编码passage的时候用了好多来源的vector进行拼接，然后再输入斤lstm中。但整体架构和机器翻译领域的发展历程是一样的，在transformer没有出现之前，大家都在lstm上做了很多创新，attention加入计算是其中一种，DrQA将多个来源的向量拼接也是一种，感兴趣的可以读一下原文：Reading
Wikipedia to Answer Open-Domain
Questions。作者是cs224n的QA这门课的讲师，也是一位华人小姐姐。其实我在看cs224n
QA这一讲的ppt时感觉斯坦福的这门课着重讲解了stanford attentive
reader这一系列的模型，不知道有没有私心。</p>
<figure>
<img src="/2023/05/09/QA-question-answering/image-20230508150255263.png" alt="DrQA">
<figcaption aria-hidden="true">DrQA</figcaption>
</figure>
<p>这之后还出现了bidirectional
attention的，例如BiDAF，就是不仅计算了query2Context的attention
weights，也计算了Context2Query的attention
weights，将这两者和context原来的lstm值再输入进一个双层LSTM进行start，end的预测（很复杂就是了），虽然也不知道为啥这么做就会有一个不错的performance，模型架构越来越复杂，卷死了。</p>
<p>但不管怎么说，在transformer被大家普遍使用之前，rnn+attention的这种模式是最好的解决办法了，俗称SOAT。</p>
<h2 id="bert用作reading-comprehension">Bert用作reading
comprehension</h2>
<p>自从18年开始，在reading
comprehension这个任务上，谷歌出了bert，成就了新的历史，一下子F1从79.4（DrQA）升级到了91.8，人类在SQuAD上的表现才91.2，所以算是超越人类表现了。</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language
Understanding
这篇文章的4.2节详细介绍了预训练完成之后的bert如何在SQuAD这个下游数据集上进行finetune的。</p>
<figure>
<img src="/2023/05/09/QA-question-answering/image-20230508170709211.png" alt="Bert QA">
<figcaption aria-hidden="true">Bert QA</figcaption>
</figure>
<p>上面这幅图更好的解释了Bert在finetune的时候如何做RC这个任务的。首先我们将question和paragraph拼接在一起，中间使用[SEP]连接。然后将这一串字符串整个输入进预训练好的Bert。对应的bert输出了同样长度的一连串向量，这时我们只取paragrapgh那一部分对应的向量们。第二步加了两个独特的向量，这两个向量是下游任务新增的，需要通过finetune时更新参数，分别是start
S和end
E向量。为了得到paragrapgh中每一个token它时span-start的概率，采用如下公式：</p>
<figure>
<img src="/2023/05/09/QA-question-answering/image-20230508171252656.png" alt="image-20230508171252656">
<figcaption aria-hidden="true">image-20230508171252656</figcaption>
</figure>
<p>同样对于每一个token成为泡影span-end的概率，也对E采用同样的计算方式。到这一步我们就得到了paragrapgh中每一个token它们成为span-start和span-end的概率。第三步如何通过这些概率值得出我们的answer所在的span？</p>
<p>作者的做法是对每一个candidate span都计算一个分数： <span class="math display">\[
S*T_i+E*T_j
\]</span>
选取得分最大的那个span作为预测值。计算loss时有一点不太一样，刚刚说的是predict，loss的计算是用的负log
<span class="math display">\[
L = - logP_(start^s) - logP_(end^e)
\]</span>
这里可能会有点迷惑。其实训练数据拿来时，对于paragraph中每一个token我们都会有一个label表示它是否是start还是end，是就是1，不是就是0.那么我们在做预测的时候，上面的第三步骤已经得到了每一个token的成为start还是end的概率，那么这时候我们就能用交叉墒来计算loss了。注意这里我们用的是softmax，所以我们应该是计算softmax的loss。另外这里只会计算到真正的start和end的那两个token上的loss，因为其他token的groudtruth
label都是0。</p>
<h2 id="spanbert">SpanBERT</h2>
<p>这篇是在Bert基础上改进的，主要改进点在于improve了Bert在pretrain时候的两个task：1.
mask 2. next sentence prediction。同样是chen的文章：(Joshi &amp; Chen et
al., 2020): SpanBERT: Improving Pre-training by Representing and
Predicting Spans</p>
<blockquote>
<p>To finish</p>
</blockquote>
<p>SpanBert在谷歌的bert基础上又将performance提高了许多。</p>
<h1 id="open-domain-question-answering">open-domain question
answering</h1>
<p>这个问题在Speech and language
processing的第三版的14章节进行了详细介绍，它也可以叫做是<em>information-retrieval(IR)
based
QA</em>。它首先要做的是从很大的语料库中搜索出相关的passage，然后第二步运动reading
comprehension的算法从这些passage中找出answer（spans of
text）。从一堆语料库里找到相关的passage，这个过程称之为information
retrival。</p>
<p>IR能想到的最最简单的做法就是将query和语料库中的每一个passage都编码成一个向量，然后计算这些向量之间的相似度，也就是score，分数越高的就是跟query越相似，那么就可以得出语料库中和query最相关的那些passage了。那么将query和passage们编码成向量有很多种方法，最简单的方法就是TFIDF，还有tfidf的变种BM25。这些方法现在已经不再采用，更多的是用Bert，俗称dense
vectors，这是和tfidf这种稀疏向量相对应的叫法。</p>
<p>具体做法是：</p>
<figure>
<img src="/2023/05/09/QA-question-answering/image-20230509133739252.png" alt="ORQA">
<figcaption aria-hidden="true">ORQA</figcaption>
</figure>
<p>分别用两个不同的bert分别编码query和passage们，然后将query的向量[CLS]和passage的向量[CLS]点积，这个点积的结果就作为query和passage的相似度得分。上面这张图片是将retriver和reader一起训练的，当然也可以单独用query和answer训练retriver</p>
<h1 id="用生成模型来做qa">用生成模型来做QA</h1>
<p>由于LLM的兴起，大家开始发现用生成模型来做QA更能回答复杂的问题。比如现在的GPT系列模型。它完全摈弃了抽取信息和从passage中寻找answer所在位置的环节，有点黑科技，就好像模型将所有的知识都记到脑子里去了。但也带来了新的问题，用户没有办法知道模型是从哪里找到的答案。</p>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Sigmoid和Softmax使用区别（Tensorflow）</title>
    <url>/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/</url>
    <content><![CDATA[<p>一直都对这两个函数的一些概念理不清楚，今天就整理一下，结合吴恩达老师和李沐给出的Tensorflow的coding过程，一并整理厘清概念和这两者的区别。</p>
<h1 id="sigmoid">Sigmoid</h1>
<p>Sigmoid通常用于逻辑回归Logistic
Regression的二分类中，output出一个概率值（比如：预测一张图片是一只猫的概率）。它的公式为：
<span class="math display">\[
\frac{1}{1+e^{-z}}
\]</span> 图示为:</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/Sigmoid_function_01.png"></p>
<p>特点：y的值是介于[0,1]之间的，而x是负无穷到正无穷的。x=0时，y=0.5。</p>
<p>LR的网络结构为:</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117112204836.png"></p>
<p>在将Sigmoid用于LR任务中时，模型的输入是一个特征向量X，这时的loss
function使用：</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117104751440.png"></p>
<p>这里<strong>不会</strong>使用MSE，原因是：</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117104923278.png"></p>
<p>loss
function是在单个训练样本中定义的，它衡量的是算法在单个训练样本中表现如何，为了衡量算法在全部训练样本上的表现，需要定义代价函数（cost
function），在LR中代价函数即为对m个样本的损失函数求和再平均：</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117105155751.png"></p>
<p>虽然有这么多计算步骤，但是在tensorflow中，我们并不需要自己计算sigmoid后的值a，然后将a和y放到J函数中计算中整体的cost，只需要一个函数就可以帮助我们实现。</p>
<p><code>tf.nn.sigmoid_cross_entropy_with_logits(logits=z,labels=y)</code></p>
<blockquote>
<p>注意：上述的z是before the final sigmoid
activation的值，也就是还没有传入Sigmoid前，只是经过线性计算后的值。</p>
</blockquote>
<h1 id="softmax">Softmax</h1>
<p>softmax函数是对sigmoid的推广，用于处理<strong>多分类</strong>的问题。公式：</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117111137512.png"></p>
<p><strong>它的输入是一个向量，输出也是一个向量</strong>。不同于Sigmoid的输入（实数），输出（介于0,1之间的概率值）。Sigmoid的输出是一个向量，其中
向量中的每一个元素的范围都在(0,1)之间，它能将一个含任意<em>实数</em>的K维向量压缩到另一个K维向量中。</p>
<p>它在线性模型中的应用方式为：对接最后一层，输出一个向量。</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117111549950.png"></p>
<p>它的loss function是:</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117113151953.png"></p>
<p>其中q为输出向量y的维度。</p>
<p>那么它的cost function J应该是将整个训练集的损失总和:
通常叫做<strong>cross-entropy loss交叉熵损失函数</strong></p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117113353952.png"></p>
<p>在tensorflow中该过程只需要一个函数来实现:</p>
<p><code>tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))</code></p>
<blockquote>
<ul>
<li>其中logits也是传入softmax激活函数之前的结果，也就是经过线性计算之后的Z</li>
<li>logits和labels也必须是相同的shape （num of examples,
num_classes）</li>
</ul>
</blockquote>
<p>用tensorflow的完整实现即为:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span>(<span class="params">Z3, Y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)</span></span><br><span class="line"><span class="string">    Y -- &quot;true&quot; labels vector placeholder, same shape as Z3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - Tensor of the cost function</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>    </span><br><span class="line">    <span class="comment"># to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)</span></span><br><span class="line">    logits = tf.transpose(Z3)</span><br><span class="line">    labels = tf.transpose(Y)  </span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment"># Do the training loop</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line"></span><br><span class="line">        epoch_cost = <span class="number">0.</span>                       <span class="comment"># Defines a cost related to an epoch</span></span><br><span class="line">        num_minibatches = <span class="built_in">int</span>(m / minibatch_size) <span class="comment"># number of minibatches of size minibatch_size in the train set</span></span><br><span class="line">        seed = seed + <span class="number">1</span></span><br><span class="line">        minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Select a minibatch</span></span><br><span class="line">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line"></span><br><span class="line">            <span class="comment"># IMPORTANT: The line that runs the graph on a minibatch.</span></span><br><span class="line">            <span class="comment"># Run the session to execute the &quot;optimizer&quot; and the &quot;cost&quot;, the feedict should contain a minibatch for (X,Y).</span></span><br><span class="line">            _ , minibatch_cost = sess.run([optimizer, cost], feed_dict=&#123;X: minibatch_X, Y: minibatch_Y&#125;)</span><br><span class="line"></span><br><span class="line">            epoch_cost += minibatch_cost / num_minibatches</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Print the cost every epoch</span></span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="literal">True</span> <span class="keyword">and</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span> (<span class="string">&quot;Cost after epoch %i: %f&quot;</span> % (epoch, epoch_cost))</span><br><span class="line">                <span class="keyword">if</span> print_cost == <span class="literal">True</span> <span class="keyword">and</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                    costs.append(epoch_cost)    </span><br></pre></td></tr></table></figure>
<p>以上的实现是将里面的计算步骤都展示出来的实现，也就是我们得到Z之后再进一步的得到loss。在李沐的教程中，用tensorflow实现softmax回归使用了更高级的API来实现。</p>
<h2 id="tensorflow-keras模块对softmax更简洁的实现">tensorflow
keras模块对softmax更简洁的实现</h2>
<p>softmax回归的输出层是一个全连接层。因此，为了实现我们的模型，我们只需在<code>Sequential</code>中添加一个带有10个输出的全连接层。同样，在这里，<code>Sequential</code>并不是必要的，但我们可能会形成这种习惯。因为在实现深度模型时，<code>Sequential</code>将无处不在。我们仍然以均值0和标准差0.01随机初始化权重。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = tf.keras.models.Sequential()</span><br><span class="line">net.add(tf.keras.layers.Flatten(input_shape=(<span class="number">28</span>, <span class="number">28</span>)))</span><br><span class="line">weight_initializer = tf.keras.initializers.RandomNormal(mean=<span class="number">0.0</span>, stddev=<span class="number">0.01</span>)</span><br><span class="line">net.add(tf.keras.layers.Dense(<span class="number">10</span>, kernel_initializer=weight_initializer))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>以上实现的输入是28*28大小的灰度图片，分类类别数为10。</p>
</blockquote>
<p>在这里，我们使用学习率为0.1的小批量随机梯度下降作为优化算法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)</span><br><span class="line">trainer = tf.keras.optimizers.SGD(learning_rate=.1)</span><br><span class="line">num_epochs = 10</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>R-CNN vs SPP vs Fast R-CNN vs Faster R-CNN</title>
    <url>/2022/11/02/R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/</url>
    <content><![CDATA[<p>最近在object
detection任务上读这几篇文章，见识到神仙打架。一开始我只是关注image
segmentation的任务，其中instance
segmentation任务中Mask-RCNN是其中比较火的一个model，所以就把跟这个模型相关的几个模型都找出来看了看。这里想记录下这几天看这几篇论文的心得体会，如果有写的不正确的地方，欢迎批评指正。</p>
<p>其实去仔细看这几篇论文很有意思，梳理一下时间线就是：</p>
<ol type="1">
<li><p><strong>2014</strong>年Girshick提出了RCNN，用于解决accurate
object detection 和 semantic
segmentation。该模型有一个drawbacks是每次一张图片输入进来，需要产生~2000个region
proposals，这些region的大小都是不一致的，但我们对图片进行分类的下游网络都是需要fixed
size的图片，那怎么办呢？作者提出使用wraped方法，具体可以参考作者的论文。总之最终我们输入到SVM也就是分类器的region图片大小都是一致的。</p></li>
<li><p>为了解决每次输入网络的图片大小怎么样才能变成fixed
size的vector，<strong>2015</strong>年he kaiming提出了SPP（spatial
pyramid pooling），跟前者RCNN不一样的地方在于：1) 将region
proposal的方法用在了图片输入cnn网络得到的feature map上，2)从feature
map选择出来的region
proposal不还是不一样大小么？作者没有使用wrap的方法，而是提出了一个SPP
layer,这个layer可以接受任何大小的图片，最终都会转化成一个fixed
size的向量，这样就可以轻松输入进SVM或者Dense layer进行分类了。</p></li>
<li><p>收到SPP的启发，Girshick在<strong>2015</strong>年提出了Fast-RCNN，将SPP
layer重新替换成ROI Pooling，经过ROI pooling，输出的并不是SPP
layer输出的金字塔式的向量了，而是只有一个。 <a href="https://analyticsindiamag.com/r-cnn-vs-fast-r-cnn-vs-faster-r-cnn-a-comparative-guide/">参考博客</a></p></li>
<li><p>经过前一轮的battle，虽然各自的模型都提出了自己的独特方法，但是无论是SPP
Net还是Fast-RCNN都没有提出在选择ROI(region of
interest)的方法。<strong>2016</strong>年He
Kaiming再次强势入场，提出了产生region
proposal的方法，它使用了一个单独的CNN网络来获取region
proposal.得到了这些proposal之后再将他们传递给Roi Pooling
layer，后面的过程和fast RCNN一致。 这篇Faster-RCNN的方法作者中有he
kaiming和Girshick，这里致敬下sun jian，感谢为computer
vision领域贡献的灵感和创造。</p></li>
</ol>
<h1 id="mask-rcnn">Mask-RCNN</h1>
<p>Mask-RCNN是Region-based CNN系列中的一个算法，用于解决instance
segmentation的问题，instance segmentation的难点在于我们不仅要做object
detection，而且需要将object的准确轮廓给识别出来，同时做出分类这是什么object。在<a href="https://arxiv.org/abs/1703.06870">Mask-RCNN</a></p>
<p>中，related
work一章节对RCNN这一系列的模型做了准确概括，建议大家读原文：</p>
<blockquote>
<p>The Region-based CNN (R-CNN) approach [13] to bounding-box object
detection is to attend to a manageable number of candidate object
regions [42, 20] and evaluate convolutional networks [25, 24]
independently on each RoI. R-CNN was extended [18, 12] to allow
attending to RoIs on feature maps using RoIPool, leading to fast speed
and better accuracy. Faster R-CNN [36] advanced this stream by learning
the attention mechanism with a Region Proposal Network (RPN). Faster
R-CNN is flexible and robust to many follow-up improvements (e.g., [38,
27, 21]), and is the current leading framework in several
benchmarks.</p>
</blockquote>
<p>在Mask-RCNN的文章中提出了一种新的ROIAlign
Layer，主要是为了解决Faster-Rcnn的网络中ROI pooling
layer的问题。在此补充下ROI pooling是怎么将不同size的ROI（region of
interest）都变成fixed-size的feature map的：</p>
<figure>
<img src="/2022/11/02/R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/image-20221128165901678-16696259446601.png" alt="ROI Pooling Layer">
<figcaption aria-hidden="true">ROI Pooling Layer</figcaption>
</figure>
<p>上图是将5*4大小的ROI变成了2✖2大小的feature
map。这种方式带来的影响就是可能在提取的extracted
features和ROI之间造成misalignments。但是这种misalignments并不会在faster-rcnn中对分类造成很大的影响，但如果要用这个ROI做segmentation的话就可能会造成巨大的影响，因此作者提出了一个ROIAlign
Layer。</p>
<blockquote>
<p>如果有小伙伴想对照code看这篇paper，可以参考<a href="https://github.com/matterport/Mask_RCNN/blob/3deaec5d902d16e1daf56b62d5971d428dc920bc/mrcnn/model.py#L486">tensorflow实现</a>。如果你对pytorch更熟悉可以参考paper中给出的官方github地址的实现。有一篇博客详细介绍了tensorlfow的实现，参见<a href="https://blog.paperspace.com/mask-r-cnn-in-tensorflow-2-0/">blog</a></p>
</blockquote>
<p>在MaskRCNN中使用了faster-rcnn中提出的RPN来产生ROI，然后才使用上面提到的ROI
Algin。RPN的具体细节参见fatser-rcnn原文和本博客的<a href="#RPN(Region%20Proposal%20Network)">第二章节</a></p>
<h1 id="rpnregion-proposal-network">RPN(Region Proposal Network)</h1>
<p>RPN
是在faster-rcnn中提出来的网络，主要是为了解决在rcnn和fast-rcnn两个前置模型中产生ROI的耗时问题，之前产生ROI主要是依靠Selected
Search。在读mask-rcnn的paper时发现这个网络的细节不甚了解，这里补充记录一下。感兴趣的朋友可以阅读<a href="http://arxiv.org/abs/1506.01497">paper</a>的第3.2节。</p>
<p>RPN在output长方体的ROI的同时，也会给每一个ROI产生一个Objectness
score。看原文的时候作者是以sliding
window的方式来讲解的，一开始看的有点懵。但其实就是卷积层的计算过程的拆解，我们先按照作者的思路来看RPN做了什么。</p>
<figure>
<img src="/2022/11/02/R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/image-20221129160354668-16697090363711.png" alt="faster-Rcnn">
<figcaption aria-hidden="true">faster-Rcnn</figcaption>
</figure>
<p>RPN的输入是经过一系列卷积层之后的feature
map，在这个map上，我们在上面再做一些运算：</p>
<figure>
<img src="/2022/11/02/R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/image-20221129160504387-16697091055822.png" alt="RPN network">
<figcaption aria-hidden="true">RPN network</figcaption>
</figure>
<p>针对一个window（n✖n），RPN做的就是将这个window映射到一个低维度的feature上，图上是256维的向量，然后我们再接两个dense
layer，一个用于预测box，一个用于做分类。k的意思是在每一个sliding
window上我们都维护了k个anchor，这个概念和yolo里一致。所以在每一个sliding-window上我们都可以得出来4k个box和2k个分类结果（是否有object的概率），这个anchor
box是和sliding-window的中心点绑定的，所以如果RPN的输入是一个W×H的feature
map，那么我们就会有W×H×k个anchors。</p>
<p>那么我们知道RPN是怎么计算的了，然后在训练阶段还有一些tricks。作者对每一个anchor都赋予了一个class
label，赋予positive的anchor为 1) anchor和 groud truth的box有最高的IOU 2)
如果anchor与groud
truth的box的IOU大于0.7。这两种anchor都会被赋予positive的标签，也就是代表它里面有object。对于哪些和任何groud
truth
box的IOU都小于0.3的anchor，赋予negtive的标签。在为RPN产生训练数据时，对于所有的anchors都有一个class
label，也就是它里面是否包含object。对于box的训练数据的处理有一点不一样的地方。可以参考<a href="https://github.com/matterport/Mask_RCNN/blob/3deaec5d902d16e1daf56b62d5971d428dc920bc/mrcnn/model.py#L486">tensorflow实现</a>，在<code>mrcnn/model.py</code>的<code>build_rpn_target</code>函数中：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># RPN Match: 1 = positive anchor, -1 = negative anchor, 0 = neutral</span></span><br><span class="line">rpn_match = np.zeros([anchors.shape[<span class="number">0</span>]], dtype=np.int32)</span><br><span class="line"><span class="comment"># RPN bounding boxes: [max anchors per image, (dy, dx, log(dh), log(dw))]</span></span><br><span class="line">rpn_bbox = np.zeros((config.RPN_TRAIN_ANCHORS_PER_IMAGE, <span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<p>rpn_bbox的数量是提前设定好的，也就是不是对每一个anchor都会有一个box来对应，对于那些标记为positive的anchor
box才会有bbox的target。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Generate RPN trainig targets</span></span><br><span class="line"><span class="comment"># target_rpn_match is 1 for positive anchors, -1 for negative anchors</span></span><br><span class="line"><span class="comment"># and 0 for neutral anchors.</span></span><br><span class="line">target_rpn_match, target_rpn_bbox = modellib.build_rpn_targets(</span><br><span class="line">    image.shape, model.anchors, gt_class_id, gt_bbox, model.config)</span><br><span class="line">log(<span class="string">&quot;target_rpn_match&quot;</span>, target_rpn_match)</span><br><span class="line">log(<span class="string">&quot;target_rpn_bbox&quot;</span>, target_rpn_bbox)</span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">target_rpn_match         shape: (65472,)              min:   -1.00000  max:    1.00000</span><br><span class="line">target_rpn_bbox          shape: (256, 4)              min:   -5.19860  max:    2.59641</span><br></pre></td></tr></table></figure>
<p>从上面可以看出，在全局变量设置中设置的是每一张图片最多有256个anchors，所以产生的rpn_bbox
shape就是(256,4),而用于RPN训练的class
label是全部的anchors的分类label。进一步的我们可以查看其中一张图片的anchors：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">positive_anchor_ix = np.where(target_rpn_match[:] == <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">negative_anchor_ix = np.where(target_rpn_match[:] == -<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">neutral_anchor_ix = np.where(target_rpn_match[:] == <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">positive_anchors = model.anchors[positive_anchor_ix]</span><br><span class="line">negative_anchors = model.anchors[negative_anchor_ix]</span><br><span class="line">neutral_anchors = model.anchors[neutral_anchor_ix]</span><br><span class="line">log(<span class="string">&quot;positive_anchors&quot;</span>, positive_anchors)</span><br><span class="line">log(<span class="string">&quot;negative_anchors&quot;</span>, negative_anchors)</span><br><span class="line">log(<span class="string">&quot;neutral anchors&quot;</span>, neutral_anchors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply refinement deltas to positive anchors</span></span><br><span class="line">refined_anchors = utils.apply_box_deltas(</span><br><span class="line">    positive_anchors,</span><br><span class="line">    target_rpn_bbox[:positive_anchors.shape[<span class="number">0</span>]] * model.config.RPN_BBOX_STD_DEV)</span><br><span class="line">log(<span class="string">&quot;refined_anchors&quot;</span>, refined_anchors, )</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">positive_anchors         shape: (14, 4)               min:    5.49033  max:  973.25483</span><br><span class="line">negative_anchors         shape: (242, 4)              min:  -22.62742  max: 1038.62742</span><br><span class="line">neutral anchors          shape: (65216, 4)            min: -362.03867  max: 1258.03867</span><br><span class="line">refined_anchors          shape: (14, 4)               min:    0.00000  max: 1023.99994</span><br></pre></td></tr></table></figure>
<p>即便是有65472个anchors，但其实正负anchor所占比重很小，大多数是neutral的anchor。其中对于positive的anchor，我们拥有在他们的bbox上refine过的box。</p>
<figure>
<img src="/2022/11/02/R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/positive_refined_box.png" alt="positive anchor">
<figcaption aria-hidden="true">positive anchor</figcaption>
</figure>
<p>上图中虚线画出来的是positive
anchor，实线框出来的是在这些anchor上refine过的box。</p>
<p>在训练阶段，计算loss时，对于regression loss，模型只会计算postive
anchors的regression
loss，也就是只计算那些被打上positive标签的anchor预测的box与groud-truth
box的回归loss。如果一个anchor，它的class
label在anatation阶段就是negtive的，模型并不会将它预测出来的box和ground-truth
box进行比较，他们的loss不会被计算进总的Loss。</p>
<p>关于如何训练的问题，在Faster-Rcnn的文章中给出了三种训练的算法，第一种也是文中采用的方法就是：1.
alternating training :
先把RPN单独训练，然后使用RPN产生的proposals去训练fast r-cnn.
然后将fine-tune过的RPN作为初始参数，然后再去产生proposal，再去训练fast
rcnn. 具体来说就是4步：</p>
<ol type="1">
<li>单独train RPN ： 用ImageNet pre-trained 参数做初始化，然后在region
proposal这个task上fine tune</li>
<li>利用第一步产生的proposal训练Fast-Rcnn，也就是<a href="R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/image-20221129160354668-16697090363711.png">架构图</a>中的最上面一部分，该网络也会使用ImageNet
pre-trained
参数做初始化。注意一直到这一步，两个网络都没有share任何卷积layers</li>
<li>第三步我们使用detector的network去初始化RPN，同时fix住最下面的卷积层，也就是两个网络共享的那些卷积层。这一步骤单独fine-tune
RPN的layers。</li>
<li>最后一步，fine-tune Fast-RCNN的unique的layers。</li>
</ol>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>TF2中的custom layer&amp;model&amp;training</title>
    <url>/2023/02/13/TF2%E4%B8%AD%E7%9A%84custom-layer-model-training/</url>
    <content><![CDATA[<p>在上Coursera上关于<a href="https://www.coursera.org/specializations/tensorflow-advanced-techniques">Tensorflow的高级用法课程</a>时，老师简略介绍了custom
layer和custom
model的用法，但后来看到其实课程覆盖的内容比较简单，除了介绍了__init__和call两个可override的function外没有介绍其他的。偶然看到一篇博客详细介绍了在tensorflow中如何使用sub
classing来搭建模型，写的非常好，这里贴上<a href="https://towardsdatascience.com/model-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e">链接</a></p>
<p>我们知道在tensorflow中有三种搭建模型的方式： 1) sequential API
也就是想创建一个Sequential实例，然后通过add的方式把一个layer加到模型中去，如：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># declare input shape </span></span><br><span class="line">seq_model = tf.keras.Sequential()</span><br><span class="line">seq_model.add(tf.keras.Input(shape=imput_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block 1</span></span><br><span class="line">seq_model.add(tf.keras.layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, strides=<span class="number">2</span>, activation=<span class="string">&quot;relu&quot;</span>))</span><br><span class="line">seq_model.add(tf.keras.layers.MaxPooling2D(<span class="number">3</span>))</span><br><span class="line">seq_model.add(tf.keras.layers.BatchNormalization())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block 2</span></span><br><span class="line">seq_model.add(tf.keras.layers.Conv2D(<span class="number">64</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>))</span><br><span class="line">seq_model.add(tf.keras.layers.BatchNormalization())</span><br><span class="line">seq_model.add(tf.keras.layers.Dropout(<span class="number">0.3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now that we apply global max pooling.</span></span><br><span class="line">seq_model.add(tf.keras.layers.GlobalMaxPooling2D())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Finally, we add a classification layer.</span></span><br><span class="line">seq_model.add(tf.keras.layers.Dense(output_dim))</span><br></pre></td></tr></table></figure>
sequential的方式在researcher中用的不多，随着模型变得越来越复杂，可以看到tensorflow的application模块实现的官方模型代码中，已经见不到这种形式了。
2) Functional API 正如其名，就是用函数调用的方式来搭建模型：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># declare input shape </span></span><br><span class="line"><span class="built_in">input</span> = tf.keras.Input(shape=(imput_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block 1</span></span><br><span class="line">x = tf.keras.layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, strides=<span class="number">2</span>, activation=<span class="string">&quot;relu&quot;</span>)(<span class="built_in">input</span>)</span><br><span class="line">x = tf.keras.layers.MaxPooling2D(<span class="number">3</span>)(x)</span><br><span class="line">x = tf.keras.layers.BatchNormalization()(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block 2</span></span><br><span class="line">x = tf.keras.layers.Conv2D(<span class="number">64</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">x = tf.keras.layers.BatchNormalization()(x)</span><br><span class="line">x = tf.keras.layers.Dropout(<span class="number">0.3</span>)(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now that we apply global max pooling.</span></span><br><span class="line">gap = tf.keras.layers.GlobalMaxPooling2D()(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Finally, we add a classification layer.</span></span><br><span class="line">output = tf.keras.layers.Dense(output_dim)(gap)</span><br><span class="line"></span><br><span class="line"><span class="comment"># bind all</span></span><br><span class="line">func_model = tf.keras.Model(<span class="built_in">input</span>, output)</span><br></pre></td></tr></table></figure>
注意：这种方式最终要使用<code>tf.keras.Model()</code>来将inputs和outputs接起来。</p>
<ol start="3" type="1">
<li>Model sub-classing API 第三种方式是现在用的最多的方式。
之前我没理解layer和model两种调用方式的区别，我觉得就是一系列运算，我们把输入输进来，return
output结果的一个过程。但如果一个类它是Layer的子类，它比model的子类多了一个功能，它有state属性，也就是我们熟悉的weights。比如Dense
layer，我们知道它做了线性运算+激活函数，其中的weights就是我们assign给每一个feature的权重，但其实我们并不只是想要这一类别的运算，比如下面的：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleQuadratic</span>(<span class="params">Layer</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, units=<span class="number">32</span>, activation=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;Initializes the class and sets up the internal variables&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        <span class="built_in">super</span>(SimpleQuadratic, self).__init__()</span><br><span class="line">        self.units = units</span><br><span class="line">        self.activation = tf.keras.activations.get(activation)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, input_shape</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;Create the state of the layer (weights)&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># a and b should be initialized with random normal, c (or the bias) with zeros.</span></span><br><span class="line">        <span class="comment"># remember to set these as trainable.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        a_init = tf.random_normal_initializer()</span><br><span class="line">        b_init = tf.random_normal_initializer()</span><br><span class="line">        c_init = tf.zeros_initializer()</span><br><span class="line">        </span><br><span class="line">        self.a = tf.Variable(name = <span class="string">&quot;kernel&quot;</span>, initial_value = a_init(shape= (input_shape[-<span class="number">1</span>], self.units), </span><br><span class="line">                                                                    dtype= <span class="string">&quot;float32&quot;</span>), trainable = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        self.b = tf.Variable(name = <span class="string">&quot;kernel&quot;</span>, initial_value = b_init(shape= (input_shape[-<span class="number">1</span>], self.units), </span><br><span class="line">                                                                    dtype= <span class="string">&quot;float32&quot;</span>), trainable = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        self.c = tf.Variable(name = <span class="string">&quot;bias&quot;</span>, initial_value = c_init(shape= (self.units,), </span><br><span class="line">                                                                    dtype= <span class="string">&quot;float32&quot;</span>), trainable = <span class="literal">True</span>)</span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span> </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;Defines the computation from inputs to outputs&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        result = tf.matmul(tf.math.square(inputs), self.a) + tf.matmul(inputs, self.b) + self.c</span><br><span class="line">        <span class="keyword">return</span> self.activation(result)</span><br></pre></td></tr></table></figure>
上面的代码将inputs平方之后和a做乘积，之后再加上inputs和b的乘积，最终返回的是和。这样的运算是tf.keras.layer中没有的。这个时候我们自己customize
layer就很方便。还有一个很方便的地方在于很多模型其实是按模块来的，模块内部的layer很类似。这个时候我们就可以把这些模型内的layer包起来变成一个layer的子类（Module），再定义完这些module之后我们使用Model把这些module再包起来，这就是我们最终的model。这时候我们就可以看到Model和Layer子类的区别了，虽然两者都可以实现输入进来之后实现一系列运算返回运算结果，但后者可以实现更灵活的运算，而前者往往是在把每一个模块定义好之后最终定义我们训练模型的类。
&gt; In general, we use the Layer class to define the inner computation
blocks and will use the Model class to define the outer model,
practically the object that we will train. ---粘贴自博客</li>
</ol>
<blockquote>
<p>You can treat any model as if it were a layer by invoking it on an
<code>Input</code> or on the output of another layer. By calling a model
you aren't just reusing the architecture of the model, you're also
reusing its weights</p>
</blockquote>
<p>同样值得注意的是，model的子类也可以像layer那样使用functional
API来调用，比如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">encoder_input = keras.Input(shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>), name=<span class="string">&quot;original_img&quot;</span>)</span><br><span class="line">x = layers.Conv2D(<span class="number">16</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(encoder_input)</span><br><span class="line">x = layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">x = layers.MaxPooling2D(<span class="number">3</span>)(x)</span><br><span class="line">x = layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">x = layers.Conv2D(<span class="number">16</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">encoder_output = layers.GlobalMaxPooling2D()(x)</span><br><span class="line"></span><br><span class="line">encoder = keras.Model(encoder_input, encoder_output, name=<span class="string">&quot;encoder&quot;</span>)</span><br><span class="line">encoder.summary()</span><br><span class="line"></span><br><span class="line">decoder_input = keras.Input(shape=(<span class="number">16</span>,), name=<span class="string">&quot;encoded_img&quot;</span>)</span><br><span class="line">x = layers.Reshape((<span class="number">4</span>, <span class="number">4</span>, <span class="number">1</span>))(decoder_input)</span><br><span class="line">x = layers.Conv2DTranspose(<span class="number">16</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">x = layers.Conv2DTranspose(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">x = layers.UpSampling2D(<span class="number">3</span>)(x)</span><br><span class="line">x = layers.Conv2DTranspose(<span class="number">16</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">decoder_output = layers.Conv2DTranspose(<span class="number">1</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line"></span><br><span class="line">decoder = keras.Model(decoder_input, decoder_output, name=<span class="string">&quot;decoder&quot;</span>)</span><br><span class="line">decoder.summary()</span><br><span class="line"></span><br><span class="line">autoencoder_input = keras.Input(shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>), name=<span class="string">&quot;img&quot;</span>)</span><br><span class="line">encoded_img = encoder(autoencoder_input)</span><br><span class="line">decoded_img = decoder(encoded_img)</span><br><span class="line">autoencoder = keras.Model(autoencoder_input, decoded_img, name=<span class="string">&quot;autoencoder&quot;</span>)</span><br><span class="line">autoencoder.summary()</span><br></pre></td></tr></table></figure>
<p>我们以sub-classing的方式定义的model是没有办法调用summary来看模型架构的，作者也给出了解决方案：<a href="https://github.com/tensorflow/tensorflow/issues/31647#issuecomment-692586409">github
comments</a></p>
<p>方法就是在Model的子类中添加build_graph方法： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_graph</span>(<span class="params">self, raw_shape</span>):</span></span><br><span class="line">        x = tf.keras.layers.Input(shape=raw_shape)</span><br><span class="line">        <span class="keyword">return</span> Model(inputs=[x], outputs=self.call(x))</span><br></pre></td></tr></table></figure>
这样我们就可以正常调用summary() <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cm.build_graph(raw_input).summary()</span><br><span class="line"><span class="comment"># 不仅如此还能使用tf.keras.utils.plot_model来生成png</span></span><br><span class="line">tf.keras.utils.plot_model(</span><br><span class="line">    model.build_graph(raw_input),                      <span class="comment"># here is the trick (for now)</span></span><br><span class="line">    to_file=<span class="string">&#x27;model.png&#x27;</span>, dpi=<span class="number">96</span>,              <span class="comment"># saving  </span></span><br><span class="line">    show_shapes=<span class="literal">True</span>, show_layer_names=<span class="literal">True</span>,  <span class="comment"># show shapes and layer name</span></span><br><span class="line">    expand_nested=<span class="literal">False</span>                       <span class="comment"># will show nested block</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>作者同样推荐了一篇博客讲tensorflow中保存模型的各种方式：<a href="https://colab.research.google.com/drive/172D4jishSgE3N7AO6U2OKAA_0wNnrMOq#scrollTo=vnTvqAgfspGJ">博客地址</a>.非常推荐阅读</p>
<p>总结一下就是：</p>
<ol type="1">
<li>对于Functional
API创建的模型，最好的保存模型和导入模型的方式是：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(<span class="string">&#x27;path_to_my_model.h5&#x27;</span>)</span><br><span class="line"><span class="keyword">del</span> model</span><br><span class="line">model = keras.models.load_model(<span class="string">&#x27;path_to_my_model.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>以上方式会将模型的架构，weights以及训练过程中的设定（也就是model.compile()）的内容全部保存。</p>
<ol start="2" type="1">
<li>对于sub class创建的模型，推荐的方式是用save_weights</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save_weights(<span class="string">&#x27;path_to_my_weights&#x27;</span>, save_format=<span class="string">&#x27;tf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>如果想要加载weights，必须要知道原来用sub
class建立模型的code。不仅如此，还需要用原来的code先build起模型，让模型知道输入tensor的shape以及dtype，如果没有build这一步程序将会报错。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_model = MiniInception()</span><br><span class="line">new_model.build((<span class="literal">None</span>, x_train.shape[<span class="number">1</span>:])) <span class="comment"># or .build((x_train.shape))</span></span><br><span class="line">new_model.load_weights(<span class="string">&#x27;net.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="tf.function">tf.function</h1>
<p>在我们定义custum training
过程中时我们经常会用到这个装饰器@tf.function</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span>(<span class="params">step, x, y</span>):</span></span><br><span class="line">   <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">   input: x, y &lt;- typically batches </span></span><br><span class="line"><span class="string">   input: step &lt;- batch step</span></span><br><span class="line"><span class="string">   return: loss value</span></span><br><span class="line"><span class="string">   &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># start the scope of gradient </span></span><br><span class="line">   <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">      logits = model(x, training=<span class="literal">True</span>) <span class="comment"># forward pass</span></span><br><span class="line">      train_loss_value = loss_fn(y, logits) <span class="comment"># compute loss </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute gradient </span></span><br><span class="line">   grads = tape.gradient(train_loss_value, model.trainable_weights)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update weights</span></span><br><span class="line">   optimizer.apply_gradients(<span class="built_in">zip</span>(grads, model.trainable_weights))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update metrics</span></span><br><span class="line">   train_acc_metric.update_state(y, logits)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># write training loss and accuracy to the tensorboard</span></span><br><span class="line">   <span class="keyword">with</span> train_writer.as_default():</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;loss&#x27;</span>, train_loss_value, step=step)</span><br><span class="line">        tf.summary.scalar(</span><br><span class="line">            <span class="string">&#x27;accuracy&#x27;</span>, train_acc_metric.result(), step=step</span><br><span class="line">        ) </span><br><span class="line">   <span class="keyword">return</span> train_loss_value</span><br></pre></td></tr></table></figure>
<p>先看如果一个函数不加这个装饰器会如何：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Traced with&quot;</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    f(<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">f(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Traced with 2</span><br><span class="line">Traced with 2</span><br><span class="line">Traced with 2</span><br><span class="line">Traced with 2</span><br><span class="line">Traced with 2</span><br><span class="line">Traced with 3</span><br></pre></td></tr></table></figure>
<p>加上装饰器：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Traced with&quot;</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    f(<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">f(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Traced <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Traced <span class="keyword">with</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p>可以看到第二种加了装饰器的方式，即便是循环了5遍，我们仍然只有一行打印了2.</p>
<p>如果我们在上面的代码中print之前加上一行：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Traced with&quot;</span>, x)</span><br><span class="line">    <span class="comment"># add tf.print</span></span><br><span class="line">    tf.<span class="built_in">print</span>(<span class="string">&quot;Executed with&quot;</span>, x)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    f(<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">f(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>程序的输出就变成了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Traced <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Traced <span class="keyword">with</span> <span class="number">3</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p>可以看到<code>tf.print</code>就可以正常按<code>loop</code>运行。注意一点:
被<code>tf.function</code>装饰的函数只能包含<code>operations</code>而不能定义<code>variable</code>比如<code>tf.Variable()</code></p>
]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-Advanced-Techniques-Specialization专项课程总结</title>
    <url>/2022/08/30/TensorFlow-Advanced-Techniques-Specialization%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="course-1-custom-modelslayers-and-loss-functions">Course 1:
Custom Models,Layers and loss Functions</h1>
<h2 id="多输出-multi-output">多输出 multi output</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Define model layers.</span></span><br><span class="line">input_layer = Input(shape=(<span class="built_in">len</span>(train.columns),))</span><br><span class="line">first_dense = Dense(units=<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(input_layer)</span><br><span class="line">second_dense = Dense(units=<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(first_dense)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Y1 output will be fed directly from the second dense</span></span><br><span class="line">y1_output = Dense(units=<span class="string">&#x27;1&#x27;</span>, name=<span class="string">&#x27;y1_output&#x27;</span>)(second_dense)</span><br><span class="line">third_dense = Dense(units=<span class="string">&#x27;64&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(second_dense)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Y2 output will come via the third dense</span></span><br><span class="line">y2_output = Dense(units=<span class="string">&#x27;1&#x27;</span>, name=<span class="string">&#x27;y2_output&#x27;</span>)(third_dense)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the model with the input layer and a list of output layers</span></span><br><span class="line">model = Model(inputs=input_layer, outputs=[y1_output, y2_output])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify the optimizer, and compile the model with loss functions for both outputs</span></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=optimizer,</span><br><span class="line">              loss=&#123;<span class="string">&#x27;y1_output&#x27;</span>: <span class="string">&#x27;mse&#x27;</span>, <span class="string">&#x27;y2_output&#x27;</span>: <span class="string">&#x27;mse&#x27;</span>&#125;,</span><br><span class="line">              metrics=&#123;<span class="string">&#x27;y1_output&#x27;</span>: tf.keras.metrics.RootMeanSquaredError(),</span><br><span class="line">                       <span class="string">&#x27;y2_output&#x27;</span>: tf.keras.metrics.RootMeanSquaredError()&#125;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>注意在这种多输出的模型架构下，train_y是一个元组set</p>
<h2 id="自定义loss-function">自定义loss function</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">huber_loss</span>(<span class="params">y_true , y_pred</span>):</span> <span class="comment"># y_true在前，y_pred在后</span></span><br><span class="line">    thresold = <span class="number">1</span></span><br><span class="line">    error = y_true - y_pred</span><br><span class="line">    return_type = tf.<span class="built_in">abs</span>(error) &lt;= thresold</span><br><span class="line">    r1 = <span class="number">0.5</span> * tf.square(error)</span><br><span class="line">    r2 = thresold * (tf.<span class="built_in">abs</span>(error) - (<span class="number">0.5</span>*thresold))</span><br><span class="line">    <span class="keyword">return</span> tf.where(return_type , r1 , r2)</span><br><span class="line"></span><br><span class="line">model_huber_loss = tf.keras.models.Model(inputs=<span class="built_in">input</span> , outputs=output_layer)</span><br><span class="line">model_huber_loss.<span class="built_in">compile</span>(optimizer=<span class="string">&quot;sgd&quot;</span> , loss=huber_loss)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>如果loss function有需要传递别的除了y_ture,y_pred参数： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">huber_loss_wrapper</span>(<span class="params">thresold</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">huber_loss</span>(<span class="params">y_true , y_pred</span>):</span></span><br><span class="line">        error = y_true - y_pred</span><br><span class="line">        return_type = tf.<span class="built_in">abs</span>(error) &lt;= thresold</span><br><span class="line">        r1 = <span class="number">0.5</span> * tf.square(error)</span><br><span class="line">        r2 = thresold * (tf.<span class="built_in">abs</span>(error) - (<span class="number">0.5</span>*thresold))</span><br><span class="line">        <span class="keyword">return</span> tf.where(return_type , r1 , r2)</span><br><span class="line">    <span class="keyword">return</span> huber_loss</span><br><span class="line">model_huber_loss_wrapper = tf.keras.models.Model(inputs=<span class="built_in">input</span> , outputs=output_layer)</span><br><span class="line">model_huber_loss_wrapper.<span class="built_in">compile</span>(optimizer=<span class="string">&quot;sgd&quot;</span> , loss=huber_loss_wrapper(thresold=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
也可以将loss写成tensorflow.keras.losses的继承类： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense , Input</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.losses <span class="keyword">import</span> Loss</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Huber</span>(<span class="params">Loss</span>):</span></span><br><span class="line">    thresold = <span class="number">1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self , thresold</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.thresold = thresold</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self , y_true , y_pred</span>):</span></span><br><span class="line">        error = y_true - y_pred</span><br><span class="line">        return_type = tf.<span class="built_in">abs</span>(error) &lt;= self.thresold</span><br><span class="line">        r1 = <span class="number">0.5</span> * tf.square(error)</span><br><span class="line">        r2 = self.thresold * (tf.<span class="built_in">abs</span>(error) - (<span class="number">0.5</span>*self.thresold))</span><br><span class="line">        <span class="keyword">return</span> tf.where(return_type , r1 , r2)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = Input(shape=(<span class="number">1</span>,) , name=<span class="string">&quot;input_layer&quot;</span>)</span><br><span class="line">output_layer = Dense(<span class="number">1</span> , name=<span class="string">&quot;output_layer&quot;</span>)(<span class="built_in">input</span>)</span><br><span class="line">model_huber_loss_class = tf.keras.models.Model(inputs=<span class="built_in">input</span> , outputs=output_layer)</span><br><span class="line"></span><br><span class="line">model_huber_loss_class.<span class="built_in">compile</span>(optimizer=<span class="string">&quot;sgd&quot;</span> , loss=Huber(thresold=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">model_huber_loss_class.fit(xs,ys,epochs=<span class="number">500</span>,verbose=<span class="number">0</span>)</span><br><span class="line">model_huber_loss_class.predict([[<span class="number">10.0</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h2 id="custom-layers-自定义层">custom layers 自定义层</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.nn <span class="keyword">import</span> softmax , relu</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDenseLayerwithActivation</span>(<span class="params">Layer</span>):</span> <span class="comment"># 可以传递units,activation</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self , units = <span class="number">32</span> ,activation = <span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyDenseLayerwithActivation , self).__init__()</span><br><span class="line">        self.units = units</span><br><span class="line">        self.activation = activation</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self , input_shape</span>):</span></span><br><span class="line">        w_init = tf.random_normal_initializer()</span><br><span class="line">        b_init = tf.zeros_initializer()</span><br><span class="line"></span><br><span class="line">        self.w = tf.Variable(initial_value=w_init(shape=(input_shape[-<span class="number">1</span>] , self.units) , dtype=<span class="string">&quot;float32&quot;</span>) , trainable=<span class="literal">True</span> , name=<span class="string">&quot;kernal&quot;</span>)</span><br><span class="line">        self.b = tf.Variable(initial_value=b_init(shape=(self.units , ) , dtype=<span class="string">&quot;float32&quot;</span>) , trainable=<span class="literal">True</span> , name=<span class="string">&quot;bias&quot;</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self , inputs</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.activation( tf.matmul(inputs , self.w) + self.b )</span><br><span class="line"></span><br><span class="line">model_simpledense_activation = Sequential([</span><br><span class="line">    Flatten(input_shape=(<span class="number">28</span>,<span class="number">28</span>)),</span><br><span class="line">    MyDenseLayerwithActivation(units = <span class="number">128</span> , activation=relu),</span><br><span class="line">    MyDenseLayerwithActivation(<span class="number">10</span> , activation = softmax)</span><br><span class="line">])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="lambda-layer">lambda layer</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_lambda = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(input_shape=(<span class="number">28</span>,<span class="number">28</span>)),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">128</span>),</span><br><span class="line">    tf.keras.layers.Lambda(<span class="keyword">lambda</span> x : tf.<span class="built_in">abs</span>(x)),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span> , activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h2 id="自定义model">自定义model</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense , Input , concatenate</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.nn <span class="keyword">import</span> relu </span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.utils.vis_utils <span class="keyword">import</span> plot_model</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyOwnModel</span>(<span class="params">Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,units = <span class="number">30</span> , activation = <span class="string">&quot;relu&quot;</span> , **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden1 = Dense(units , activation=activation , name=<span class="string">&quot;hidden1&quot;</span>)</span><br><span class="line">        self.hidden2 = Dense(units , activation=activation , name=<span class="string">&quot;hidden2&quot;</span>)</span><br><span class="line">        self.main_output = Dense(<span class="number">1</span>)</span><br><span class="line">        self.aux_output = Dense(<span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self , inputs</span>):</span></span><br><span class="line">        input_l , input_r = inputs</span><br><span class="line">        hidden1 = self.hidden1(input_r)</span><br><span class="line">        hidden2 = self.hidden2(hidden1)</span><br><span class="line">        concat = concatenate([input_l , hidden2])</span><br><span class="line">        main_output = self.main_output(concat)</span><br><span class="line">        aux_output  = self.aux_output(hidden2)</span><br><span class="line">        <span class="keyword">return</span> main_output , aux_output</span><br><span class="line"></span><br><span class="line">model = MyOwnModel()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="callbacks">callbacks</h2>
<h3 id="使用tensorboard">使用tensorboard</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> TensorBoard</span><br><span class="line">model = build_model(dense_units=<span class="number">256</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;sgd&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, </span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">  </span><br><span class="line">logdir = os.path.join(<span class="string">&quot;logs&quot;</span>, datetime.datetime.now().strftime(<span class="string">&quot;%Y%m%d-%H%M%S&quot;</span>))</span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir)</span><br><span class="line"></span><br><span class="line">model.fit(train_batches, </span><br><span class="line">          epochs=<span class="number">10</span>, </span><br><span class="line">          validation_data=validation_batches, </span><br><span class="line">          callbacks=[tensorboard_callback])</span><br></pre></td></tr></table></figure>
<p>模型训练完之后使用<code>tensorboard --logdir logs</code>来查看模型训练过程中的loss和acc
。
上面这种方式不太适合在服务器调试model，一般采取checkpoint的方式以一定的时间间隔保存模型参数。
### 使用ModelCheckpoint</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> EarlyStopping, LearningRateScheduler, ModelCheckpoint, CSVLogger, ReduceLROnPlateau</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="comment"># method 1:</span></span><br><span class="line">model = build_model(dense_units=<span class="number">256</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;sgd&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, </span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">  </span><br><span class="line">model.fit(train_batches, </span><br><span class="line">          epochs=<span class="number">5</span>, </span><br><span class="line">          validation_data=validation_batches, </span><br><span class="line">          verbose=<span class="number">2</span>,</span><br><span class="line">          callbacks=[ModelCheckpoint(<span class="string">&#x27;weights.&#123;epoch:02d&#125;-&#123;val_loss:.2f&#125;.h5&#x27;</span>, verbose=<span class="number">1</span>), <span class="comment"># 这样调用会保存多个文件，每个epoch结束时更新文件</span></span><br><span class="line">          ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># method2:</span></span><br><span class="line">model = build_model(dense_units=<span class="number">256</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;sgd&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, </span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">  </span><br><span class="line">model.fit(train_batches, </span><br><span class="line">          epochs=<span class="number">2</span>, </span><br><span class="line">          validation_data=validation_batches, </span><br><span class="line">          verbose=<span class="number">2</span>,</span><br><span class="line">          callbacks=[ModelCheckpoint(<span class="string">&#x27;./saved_model&#x27;</span>, verbose=<span class="number">1</span>) <span class="comment"># 如果传入的是一个文件夹，会在文件夹内创建多个文件</span></span><br><span class="line">          ])</span><br></pre></td></tr></table></figure>
<h3 id="earlystoppingcsvloggerlearningrateschedulerreducelronplateau">EarlyStopping,CSVLogger,LearningRateScheduler,ReduceLROnPlateau</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#EarlyStopping</span></span><br><span class="line">model = build_model(dense_units=<span class="number">256</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;sgd&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, </span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">  </span><br><span class="line">model.fit(train_batches, </span><br><span class="line">          epochs=<span class="number">50</span>, </span><br><span class="line">          validation_data=validation_batches, </span><br><span class="line">          verbose=<span class="number">2</span>,</span><br><span class="line">          callbacks=[EarlyStopping(</span><br><span class="line">              patience=<span class="number">3</span>,</span><br><span class="line">              min_delta=<span class="number">0.05</span>,</span><br><span class="line">              baseline=<span class="number">0.8</span>,</span><br><span class="line">              mode=<span class="string">&#x27;min&#x27;</span>,</span><br><span class="line">              monitor=<span class="string">&#x27;val_loss&#x27;</span>,</span><br><span class="line">              restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">              verbose=<span class="number">1</span>)</span><br><span class="line">          ])</span><br><span class="line"></span><br><span class="line"><span class="comment">#CSVLogger</span></span><br><span class="line">model = build_model(dense_units=<span class="number">256</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;sgd&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, </span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">  </span><br><span class="line">csv_file = <span class="string">&#x27;training.csv&#x27;</span></span><br><span class="line"></span><br><span class="line">model.fit(train_batches, </span><br><span class="line">          epochs=<span class="number">5</span>, </span><br><span class="line">          validation_data=validation_batches, </span><br><span class="line">          callbacks=[CSVLogger(csv_file)</span><br><span class="line">          ])</span><br><span class="line"></span><br><span class="line"><span class="comment">#LearningRateScheduler</span></span><br><span class="line">model = build_model(dense_units=<span class="number">256</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;sgd&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, </span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step_decay</span>(<span class="params">epoch</span>):</span></span><br><span class="line">	initial_lr = <span class="number">0.01</span></span><br><span class="line">	drop = <span class="number">0.5</span></span><br><span class="line">	epochs_drop = <span class="number">1</span></span><br><span class="line">	lr = initial_lr * math.<span class="built_in">pow</span>(drop, math.floor((<span class="number">1</span>+epoch)/epochs_drop))</span><br><span class="line">	<span class="keyword">return</span> lr</span><br><span class="line"></span><br><span class="line">model.fit(train_batches, </span><br><span class="line">          epochs=<span class="number">5</span>, </span><br><span class="line">          validation_data=validation_batches, </span><br><span class="line">          callbacks=[LearningRateScheduler(step_decay, verbose=<span class="number">1</span>),</span><br><span class="line">                    TensorBoard(log_dir=<span class="string">&#x27;./log_dir&#x27;</span>)]) <span class="comment"># 以上介绍的callbacks可以传递多个item</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#ReduceLROnPlateau</span></span><br><span class="line">model = build_model(dense_units=<span class="number">256</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;sgd&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, </span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">  </span><br><span class="line">model.fit(train_batches, </span><br><span class="line">          epochs=<span class="number">50</span>, </span><br><span class="line">          validation_data=validation_batches, </span><br><span class="line">          callbacks=[ReduceLROnPlateau(monitor=<span class="string">&#x27;val_loss&#x27;</span>, </span><br><span class="line">                                       factor=<span class="number">0.2</span>, verbose=<span class="number">1</span>,</span><br><span class="line">                                       patience=<span class="number">1</span>, min_lr=<span class="number">0.001</span>),</span><br><span class="line">                     TensorBoard(log_dir=<span class="string">&#x27;./log_dir&#x27;</span>)])</span><br></pre></td></tr></table></figure>
<h3 id="自定义callbacks">自定义callbacks</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DetectOverfittingCallback</span>(<span class="params">tf.keras.callbacks.Callback</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, threshold=<span class="number">0.7</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DetectOverfittingCallback, self).__init__()</span><br><span class="line">        self.threshold = threshold</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span>(<span class="params">self, epoch, logs=<span class="literal">None</span></span>):</span></span><br><span class="line">        ratio = logs[<span class="string">&quot;val_loss&quot;</span>] / logs[<span class="string">&quot;loss&quot;</span>]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Epoch: &#123;&#125;, Val/Train loss ratio: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(epoch, ratio))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ratio &gt; self.threshold:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Stopping training...&quot;</span>)</span><br><span class="line">            self.model.stop_training = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">model = get_model()</span><br><span class="line">_ = model.fit(x_train, y_train,</span><br><span class="line">              validation_data=(x_test, y_test),</span><br><span class="line">              batch_size=<span class="number">64</span>,</span><br><span class="line">              epochs=<span class="number">3</span>,</span><br><span class="line">              verbose=<span class="number">0</span>,</span><br><span class="line">              callbacks=[DetectOverfittingCallback()])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在tf.keras.callbacks.Callback类中还可以重写的函数还有：</p>
<h4 id="on_traintestpredict_beginself-logsnone"><code>on_(train|test|predict)_begin(self, logs=None)</code></h4>
<p>Called at the beginning of
<code>fit</code>/<code>evaluate</code>/<code>predict</code>.</p>
<h4 id="on_traintestpredict_endself-logsnone"><code>on_(train|test|predict)_end(self, logs=None)</code></h4>
<p>Called at the end of
<code>fit</code>/<code>evaluate</code>/<code>predict</code>.</p>
<h4 id="on_traintestpredict_batch_beginself-batch-logsnone"><code>on_(train|test|predict)_batch_begin(self, batch, logs=None)</code></h4>
<p>Called right before processing a batch during
training/testing/predicting. Within this method, <code>logs</code> is a
dict with <code>batch</code> and <code>size</code> available keys,
representing the current batch number and the size of the batch.</p>
<p>logs是一个字典，keys=[batch,size]，分别表示batch number和batch
size</p>
<h4 id="on_traintestpredict_batch_endself-batch-logsnone"><code>on_(train|test|predict)_batch_end(self, batch, logs=None)</code></h4>
<p>Called at the end of training/testing/predicting a batch. Within this
method, <code>logs</code> is a dict containing the stateful metrics
result.</p>
<p>这里的Logs字典中含有metrics结果</p>
<h3 id="training-specific-methods">Training specific methods</h3>
<p>在训练阶段有一些特别的函数：</p>
<h4 id="on_epoch_beginself-epoch-logsnone"><code>on_epoch_begin(self, epoch, logs=None)</code></h4>
<p>Called at the beginning of an epoch during training.</p>
<h4 id="on_epoch_endself-epoch-logsnone"><code>on_epoch_end(self, epoch, logs=None)</code></h4>
<p>Called at the end of an epoch during training.</p>
<p>可以在自定义callback类中定义display结果的函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Visualization utilities</span></span><br><span class="line">plt.rc(<span class="string">&#x27;font&#x27;</span>, size=<span class="number">20</span>)</span><br><span class="line">plt.rc(<span class="string">&#x27;figure&#x27;</span>, figsize=(<span class="number">15</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_digits</span>(<span class="params">inputs, outputs, ground_truth, epoch, n=<span class="number">10</span></span>):</span></span><br><span class="line">    plt.clf() <span class="comment"># clear the current figure</span></span><br><span class="line"></span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.grid(<span class="literal">None</span>)</span><br><span class="line">    inputs = np.reshape(inputs, [n, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">    inputs = np.swapaxes(inputs, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    inputs = np.reshape(inputs, [<span class="number">28</span>, <span class="number">28</span>*n])</span><br><span class="line">    plt.imshow(inputs)</span><br><span class="line">    plt.xticks([<span class="number">28</span>*x+<span class="number">14</span> <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(n)], outputs)</span><br><span class="line">    <span class="keyword">for</span> i,t <span class="keyword">in</span> <span class="built_in">enumerate</span>(plt.gca().xaxis.get_ticklabels()):</span><br><span class="line">        <span class="keyword">if</span> outputs[i] == ground_truth[i]: </span><br><span class="line">            t.set_color(<span class="string">&#x27;green&#x27;</span>) </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            t.set_color(<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    plt.grid(<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">GIF_PATH = <span class="string">&#x27;./animation.gif&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VisCallback</span>(<span class="params">tf.keras.callbacks.Callback</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, inputs, ground_truth, display_freq=<span class="number">10</span>, n_samples=<span class="number">10</span></span>):</span></span><br><span class="line">        self.inputs = inputs</span><br><span class="line">        self.ground_truth = ground_truth</span><br><span class="line">        self.images = []</span><br><span class="line">        self.display_freq = display_freq</span><br><span class="line">        self.n_samples = n_samples</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span>(<span class="params">self, epoch, logs=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># Randomly sample data</span></span><br><span class="line">        indexes = np.random.choice(<span class="built_in">len</span>(self.inputs), size=self.n_samples)</span><br><span class="line">        X_test, y_test = self.inputs[indexes], self.ground_truth[indexes]</span><br><span class="line">        predictions = np.argmax(self.model.predict(X_test), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Plot the digits</span></span><br><span class="line">        display_digits(X_test, predictions, y_test, epoch, n=self.display_freq)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save the figure</span></span><br><span class="line">        buf = io.BytesIO()</span><br><span class="line">        plt.savefig(buf, <span class="built_in">format</span>=<span class="string">&#x27;png&#x27;</span>)</span><br><span class="line">        buf.seek(<span class="number">0</span>)</span><br><span class="line">        image = Image.<span class="built_in">open</span>(buf)</span><br><span class="line">        self.images.append(np.array(image))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Display the digits every &#x27;display_freq&#x27; number of epochs</span></span><br><span class="line">        <span class="keyword">if</span> epoch % self.display_freq == <span class="number">0</span>:</span><br><span class="line">            plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_train_end</span>(<span class="params">self, logs=<span class="literal">None</span></span>):</span></span><br><span class="line">        imageio.mimsave(GIF_PATH, self.images, fps=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h1 id="course-2-custom-and-distributed-training-with-tensorflow">Course
2 : Custom and Distributed Training with Tensorflow</h1>
<h2 id="导数的计算">导数的计算</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape_2:</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape_1:</span><br><span class="line">        y = x * x * x</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The first gradient calculation should occur at leaset</span></span><br><span class="line">    <span class="comment"># within the outer with block</span></span><br><span class="line">    dy_dx = tape_1.gradient(y, x)</span><br><span class="line">d2y_dx2 = tape_2.gradient(dy_dx, x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dy_dx)</span><br><span class="line"><span class="built_in">print</span>(d2y_dx2)</span><br></pre></td></tr></table></figure>
<h2 id="在fashion-mnist-数据集上写specific的训练过程">在fashion mnist
数据集上写specific的训练过程</h2>
<p>这种训练方式不采用model.fit()的方式去训练模型，而是采用自己写传递导数的方式。增加了灵活度，方便后续进行分布式训练。</p>
<figure>
<img src="/2022/08/30/TensorFlow-Advanced-Techniques-Specialization%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/Users\320150117\AppData\Roaming\Typora\typora-user-images\image-20220830150121062.png" alt="image-20220830150121062">
<figcaption aria-hidden="true">image-20220830150121062</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.losses <span class="keyword">import</span> SparseCategoricalCrossentropy</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.metrics <span class="keyword">import</span> SparseCategoricalAccuracy</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">base_model</span>():</span></span><br><span class="line">    <span class="built_in">input</span>  = Input(shape=(<span class="number">28</span>*<span class="number">28</span> , ) , name = <span class="string">&quot;input_layer&quot;</span>)</span><br><span class="line">    x = Dense(<span class="number">64</span> , activation=<span class="string">&#x27;relu&#x27;</span> , name = <span class="string">&quot;dense1&quot;</span>)(<span class="built_in">input</span>)</span><br><span class="line">    x = Dense(<span class="number">64</span> , activation=<span class="string">&#x27;relu&#x27;</span> , name = <span class="string">&quot;dense2&quot;</span>)(x)</span><br><span class="line">    output = Dense(<span class="number">10</span> , activation=<span class="string">&#x27;softmax&#x27;</span> , name=<span class="string">&quot;output_layer&quot;</span>)(x)</span><br><span class="line">    model = Model(inputs = <span class="built_in">input</span> , outputs = output)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_optimizer</span>(<span class="params">model , x , y_true</span>):</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        logits = model(x)</span><br><span class="line">        loss_val = loss_obj(y_true=y_true , y_pred=logits)</span><br><span class="line">    grad = tape.gradient(loss_val , model.trainable_weights) <span class="comment"># model有6个可以train的weights</span></span><br><span class="line">    optimizer_obj.apply_gradients(<span class="built_in">zip</span>(grad , model.trainable_weights))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> logits , loss_val</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_one_epoch</span>(<span class="params">model , train_data</span>):</span> <span class="comment"># 这里输入的是整个dataset</span></span><br><span class="line">    losses = []</span><br><span class="line">    pbar = tqdm(total=<span class="built_in">len</span>(<span class="built_in">list</span>(<span class="built_in">enumerate</span>(train_data))), position=<span class="number">0</span>, leave=<span class="literal">True</span>, bar_format=<span class="string">&#x27;&#123;l_bar&#125;&#123;bar&#125;| &#123;n_fmt&#125;/&#123;total_fmt&#125; &#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> batch_no , (data , label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data): <span class="comment"># 这个for循环循环的是batch，每次输入模型中训练的是一个batch_size的数据</span></span><br><span class="line">        y_pred , loss = run_optimizer(model , data , label)</span><br><span class="line">        losses.append(loss)</span><br><span class="line">        train_acc_matrix(label , y_pred)</span><br><span class="line">        pbar.set_description(<span class="string">&quot;Training loss for step %s: %.4f&quot;</span> % (<span class="built_in">int</span>(batch_no), <span class="built_in">float</span>(loss)))</span><br><span class="line">        pbar.update()</span><br><span class="line">    <span class="keyword">return</span> losses <span class="comment"># 返回的是一个list，每一个item是一个batch的loss</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perform_validation</span>(<span class="params">model , test_data</span>):</span></span><br><span class="line">    losses = []</span><br><span class="line">    <span class="keyword">for</span> (data , label) <span class="keyword">in</span> test_data:</span><br><span class="line">        y_pred = model(data)</span><br><span class="line">        loss = loss_obj(label , y_pred)</span><br><span class="line">        losses.append(loss)</span><br><span class="line">        val_acc_matrix(label , y_pred)</span><br><span class="line">    <span class="keyword">return</span> losses</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">train , test , epochs = <span class="number">10</span></span>):</span></span><br><span class="line">    model = base_model()</span><br><span class="line"></span><br><span class="line">    history = &#123;&#125;</span><br><span class="line">    history[<span class="string">&#x27;train_loss&#x27;</span>] = []</span><br><span class="line">    history[<span class="string">&#x27;val_loss&#x27;</span>] = []</span><br><span class="line"></span><br><span class="line">    history[<span class="string">&#x27;train_acc&#x27;</span>] = []</span><br><span class="line">    history[<span class="string">&#x27;val_acc&#x27;</span>] = []</span><br><span class="line"></span><br><span class="line">    val_epoch_loss   = []</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Start of epoch %d&#x27;</span> % (epoch,))</span><br><span class="line"></span><br><span class="line">        train_losses = train_one_epoch(model , train_data=train)</span><br><span class="line">        train_acc    = train_acc_matrix.result()</span><br><span class="line">        history[<span class="string">&#x27;train_acc&#x27;</span>].append(train_acc.numpy())</span><br><span class="line">        train_acc_matrix.reset_states()</span><br><span class="line"></span><br><span class="line">        val_losses   = perform_validation(model , test_data=test)</span><br><span class="line">        val_acc      = val_acc_matrix.result()</span><br><span class="line">        history[<span class="string">&#x27;val_acc&#x27;</span>].append(val_acc.numpy())</span><br><span class="line">        val_acc_matrix.reset_states()</span><br><span class="line"></span><br><span class="line">        history[<span class="string">&#x27;train_loss&#x27;</span>].append(np.mean(train_losses)) <span class="comment"># 这里求解的是所有batch的loss总和求平均</span></span><br><span class="line">        history[<span class="string">&#x27;val_loss&#x27;</span>].append(np.mean(val_losses))</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\n Epoch %s: Train loss: %.4f  Validation Loss: %.4f,\</span></span><br><span class="line"><span class="string">         Train Accuracy: %.4f, Validation Accuracy %.4f&#x27;</span> % (epoch, <span class="built_in">float</span>(np.mean(train_losses)), <span class="built_in">float</span>(np.mean(val_losses)),</span><br><span class="line">                                                            <span class="built_in">float</span>(train_acc), <span class="built_in">float</span>(val_acc)))</span><br><span class="line"></span><br><span class="line">    history[<span class="string">&#x27;model&#x27;</span>] = model</span><br><span class="line">    <span class="keyword">return</span> history</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loss_obj       = SparseCategoricalCrossentropy()</span><br><span class="line">optimizer_obj  = Adam()</span><br><span class="line"></span><br><span class="line">train_acc_matrix = SparseCategoricalAccuracy()</span><br><span class="line">val_acc_matrix   = SparseCategoricalAccuracy()</span><br><span class="line"></span><br><span class="line">history = train(train_data , test_data)</span><br></pre></td></tr></table></figure>
<h2 id="分布式训练">分布式训练</h2>
<h3 id="mirrored-strategy">Mirrored strategy</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">strategy = tf.distribute.MirroredStrategy() </span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Number of devices: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(strategy.num_replicas_in_sync))</span><br><span class="line"></span><br><span class="line">BATCH_SIZE_PER_REPLICA = <span class="number">64</span></span><br><span class="line"><span class="comment"># Use for Mirrored Strategy</span></span><br><span class="line">BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up the train and eval data set</span></span><br><span class="line">train_dataset = mnist_train.<span class="built_in">map</span>(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)</span><br><span class="line">eval_dataset = mnist_test.<span class="built_in">map</span>(scale).batch(BATCH_SIZE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use for Mirrored Strategy -- comment out `with strategy.scope():` and deindent for no strategy</span></span><br><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">    model = tf.keras.Sequential([</span><br><span class="line">      tf.keras.layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)),</span><br><span class="line">      tf.keras.layers.MaxPooling2D(),</span><br><span class="line">      tf.keras.layers.Flatten(),</span><br><span class="line">      tf.keras.layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">      tf.keras.layers.Dense(<span class="number">10</span>)</span><br><span class="line">    ])</span><br><span class="line">model.<span class="built_in">compile</span>(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">                optimizer=tf.keras.optimizers.Adam(),</span><br><span class="line">                metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>这种方式会使用该机器的所有gpu,如果想指定使用的GPU可以使用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gpus = tf.config.experimental.list_logical_devices(<span class="string">&quot;GPU&quot;</span>)</span><br><span class="line"><span class="comment"># gpus = tf.config.list_physical_devices(&#x27;GPU&#x27;)</span></span><br><span class="line">strategy = tf.distribute.MirroredStrategy([gpu.name <span class="keyword">for</span> gpu <span class="keyword">in</span> gpus])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Running on multiple GPUs &#x27;</span>, [gpu.name <span class="keyword">for</span> gpu <span class="keyword">in</span> gpus])</span><br></pre></td></tr></table></figure>
<h1 id="course-3-advanced-computer-vision-with-tensorflow">Course 3:
Advanced Computer Vision with Tensorflow</h1>
<h2 id="object-localization">Object Localization</h2>
<p>week1介绍了transfer
learning和利用minist手写体数据集做的一个简单的物体定位+分类的模型。object
localization和object
detection的区别在于后者需要识别出image中所有物体的Box并能实现分类，而前者只是需要检测出最主要的那个object并分类。可参考博客https://towardsdatascience.com/object-localization-in-overfeat-5bb2f7328b62。</p>
<figure>
<img src="https://miro.medium.com/max/1400/1*LOjfqvJ0zDuSSq443Z9SNA.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="object-detection">Object Detection</h2>
<p>如果想要开箱即用的方式，作者介绍了tensorflow的API进行调用：https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md。在该object
detection写好的API下有几个可用的utils可以使用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> object_detection.utils <span class="keyword">import</span> label_map_util</span><br><span class="line"><span class="keyword">from</span> object_detection.utils <span class="keyword">import</span> visualization_utils <span class="keyword">as</span> viz_utils</span><br><span class="line"><span class="keyword">from</span> object_detection.utils <span class="keyword">import</span> ops <span class="keyword">as</span> utils_ops <span class="comment"># 在image segmentation中有使用到mask</span></span><br><span class="line"></span><br><span class="line">PATH_TO_LABELS = <span class="string">&#x27;./models/research/object_detection/data/mscoco_label_map.pbtxt&#x27;</span></span><br><span class="line">category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS) <span class="comment"># category_index是一个字典，key为unique id，values也是一个字典：key是index，value是label_name</span></span><br><span class="line"></span><br><span class="line">viz_utils.visualize_boxes_and_labels_on_image_array(</span><br><span class="line">    image_np_with_detections[<span class="number">0</span>],</span><br><span class="line">    result[<span class="string">&#x27;detection_boxes&#x27;</span>][<span class="number">0</span>],</span><br><span class="line">    (result[<span class="string">&#x27;detection_classes&#x27;</span>][<span class="number">0</span>] + label_id_offset).astype(<span class="built_in">int</span>),</span><br><span class="line">    result[<span class="string">&#x27;detection_scores&#x27;</span>][<span class="number">0</span>],</span><br><span class="line">    category_index,</span><br><span class="line">    use_normalized_coordinates=<span class="literal">True</span></span><br><span class="line">) <span class="comment"># images,boxes,classes,scores</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>另外还介绍了在已有weights上finetune模型的方法。使用的例子是tensorflow官方出的tutorial。https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb</p>
<h3 id="补充介绍object-detection">补充介绍object detection</h3>
<p>这里我又去翻阅了一些review的文章，想了解一下在物体检测领域的其他算法。参考文章
Object Detection With Deep Learning: A Review.</p>
<figure>
<img src="/2022/08/30/TensorFlow-Advanced-Techniques-Specialization%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/Users\320150117\AppData\Roaming\Typora\typora-user-images\image-20220913152542940.png" alt="image-20220913152542940">
<figcaption aria-hidden="true">image-20220913152542940</figcaption>
</figure>
<p>该文章将物体检测领域的模型分为两个种类，第一类就是以R-CNN为代表的region
proposal方法，第二类就是遵循一个统一的模式，将其看成是一个regression/classification的问题.</p>
<ol type="1">
<li>region proposal</li>
</ol>
<p>R-CNN,
SPP-net(在R-CNN基础上进行了改进)，Fast-RCNN，Faster-RCNN，R-FCN。</p>
<ol type="1">
<li>regression/classification based</li>
</ol>
<p>MultiBox，AttentionNet，G-CNN，yolo，SSD（single shot Multibox
detector）</p>
<h2 id="image-segmentation">image segmentation</h2>
<p>除了FCN-8和Unet。作者还介绍了Mask-RCNN。</p>
<h2 id="可解释性">可解释性</h2>
<p>Class Activation Map : A class activation map is a matrix that shows
what parts of the image the model was paying attention to when deciding
what class to assign the image.</p>
<p><a href="https://github.com/alexisbcook/ResNetCAM-keras/blob/master/ResNet_CAM.py">代码地址</a></p>
<p><a href="https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/#:~:text=to%20avoid%20overfitting.-,Global%20Average%20Pooling,of%20a%20three-dimensional%20tensor.">代码博客</a></p>
<p><a href="https://towardsdatascience.com/class-activation-mapping-using-transfer-learning-of-resnet50-e8ca7cfd657e">参考博客</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> ast</span><br><span class="line"><span class="keyword">import</span> scipy   </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> cv2     </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.applications.resnet50 <span class="keyword">import</span> ResNet50, preprocess_input</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing <span class="keyword">import</span> image    </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model   </span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretrained_path_to_tensor</span>(<span class="params">img_path</span>):</span></span><br><span class="line">    <span class="comment"># loads RGB image as PIL.Image.Image type</span></span><br><span class="line">    img = image.load_img(img_path, target_size=(<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    <span class="comment"># convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)</span></span><br><span class="line">    x = image.img_to_array(img)</span><br><span class="line">    <span class="comment"># convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor</span></span><br><span class="line">    x = np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># convert RGB -&gt; BGR, subtract mean ImageNet pixel, and return 4D tensor</span></span><br><span class="line">    <span class="keyword">return</span> preprocess_input(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ResNet</span>():</span></span><br><span class="line">    <span class="comment"># define ResNet50 model</span></span><br><span class="line">    model = ResNet50(weights=<span class="string">&#x27;imagenet&#x27;</span>)</span><br><span class="line">    <span class="comment"># model.summary()</span></span><br><span class="line">    <span class="comment"># get AMP layer weights (2048,1000)</span></span><br><span class="line">    all_amp_layer_weights = model.layers[-<span class="number">1</span>].get_weights()[<span class="number">0</span>] <span class="comment"># 这里get_weights()返回的是一个list,list[0]是kernel matrix,list[1]是bias</span></span><br><span class="line">    <span class="comment"># extract wanted output</span></span><br><span class="line">    ResNet_model = Model(inputs=model.<span class="built_in">input</span>, </span><br><span class="line">        outputs=(model.layers[-<span class="number">4</span>].output, model.layers[-<span class="number">1</span>].output)) </span><br><span class="line">    <span class="keyword">return</span> ResNet_model, all_amp_layer_weights</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet_CAM</span>(<span class="params">img_path, model, all_amp_layer_weights</span>):</span></span><br><span class="line">    <span class="comment"># get filtered images from convolutional output + model prediction vector</span></span><br><span class="line">    last_conv_output, pred_vec = model.predict(pretrained_path_to_tensor(img_path))</span><br><span class="line">    <span class="comment"># change dimensions of last convolutional outpu tto 7 x 7 x 2048</span></span><br><span class="line">    last_conv_output = np.squeeze(last_conv_output) </span><br><span class="line">    <span class="comment"># get model&#x27;s prediction (number between 0 and 999, inclusive)</span></span><br><span class="line">    pred = np.argmax(pred_vec)</span><br><span class="line">    <span class="comment"># bilinear upsampling to resize each filtered image to size of original image </span></span><br><span class="line">    mat_for_mult = scipy.ndimage.zoom(last_conv_output, (<span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>), order=<span class="number">1</span>) <span class="comment"># dim: 224 x 224 x 2048</span></span><br><span class="line">    <span class="comment"># get AMP layer weights</span></span><br><span class="line">    amp_layer_weights = all_amp_layer_weights[:, pred] <span class="comment"># dim: (2048,) </span></span><br><span class="line">    <span class="comment"># get class activation map for object class that is predicted to be in the image</span></span><br><span class="line">    final_output = np.dot(mat_for_mult.reshape((<span class="number">224</span>*<span class="number">224</span>, <span class="number">2048</span>)), amp_layer_weights).reshape(<span class="number">224</span>,<span class="number">224</span>) <span class="comment"># dim: 224 x 224</span></span><br><span class="line">    <span class="comment"># return class activation map</span></span><br><span class="line">    <span class="keyword">return</span> final_output, pred</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_ResNet_CAM</span>(<span class="params">img_path, ax, model, all_amp_layer_weights</span>):</span></span><br><span class="line">    <span class="comment"># load image, convert BGR --&gt; RGB, resize image to 224 x 224,</span></span><br><span class="line">    im = cv2.resize(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB), (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    <span class="comment"># plot image</span></span><br><span class="line">    ax.imshow(im, alpha=<span class="number">0.5</span>)</span><br><span class="line">    <span class="comment"># get class activation map</span></span><br><span class="line">    CAM, pred = ResNet_CAM(img_path, model, all_amp_layer_weights)</span><br><span class="line">    <span class="comment"># plot class activation map</span></span><br><span class="line">    ax.imshow(CAM, cmap=<span class="string">&#x27;jet&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">    <span class="comment"># load the dictionary that identifies each ImageNet category to an index in the prediction vector</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;imagenet1000_clsid_to_human.txt&#x27;</span>) <span class="keyword">as</span> imagenet_classes_file:</span><br><span class="line">        imagenet_classes_dict = ast.literal_eval(imagenet_classes_file.read())</span><br><span class="line">    <span class="comment"># obtain the predicted ImageNet category</span></span><br><span class="line">    ax.set_title(imagenet_classes_dict[pred]) </span><br><span class="line"></span><br><span class="line">ResNet_model, all_amp_layer_weights = get_ResNet()</span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">CAM = plot_ResNet_CAM(img_path, ax, ResNet_model, all_amp_layer_weights)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>Saliency Maps：see where the pixels that were most impactful to the
final classification were found.
显示的是哪些pixel会对最终的分类结果造成重大影响。</p>
<p>Grad-CAM Map：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gradModel = Model(inputs=[model.inputs],outputs[model.get_layer(layer_name).output,model.output])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">   <span class="comment"># cast the image tensor to a float-32 data type, pass the</span></span><br><span class="line">   <span class="comment"># forward propagate the image through the gradient model, and grab the loss</span></span><br><span class="line">   <span class="comment"># associated with the specific class index</span></span><br><span class="line">   inputs = tf.cast(img_array, tf.float32)</span><br><span class="line">   (convOutputs, predictions) = gradModel(inputs)</span><br><span class="line">   loss = predictions[:, <span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line"><span class="comment"># use automatic differentiation to compute the gradients</span></span><br><span class="line">grads = tape.gradient(loss, convOutputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute the guided gradients</span></span><br><span class="line">castConvOutputs = tf.cast(convOutputs &gt; <span class="number">0</span>, <span class="string">&quot;float32&quot;</span>)</span><br><span class="line">castGrads = tf.cast(grads &gt; <span class="number">0</span>, <span class="string">&quot;float32&quot;</span>)</span><br><span class="line">guidedGrads = castConvOutputs * castGrads * grads</span><br><span class="line"></span><br><span class="line"><span class="comment"># the convolution and guided gradients have a batch dimension</span></span><br><span class="line"><span class="comment"># (which we don&#x27;t need) so let&#x27;s grab the volume itself and</span></span><br><span class="line"><span class="comment"># discard the batch</span></span><br><span class="line">convOutputs = convOutputs[<span class="number">0</span>]</span><br><span class="line">guidedGrads = guidedGrads[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute the average of the gradient values, and using them</span></span><br><span class="line"><span class="comment"># as weights, compute the ponderation of the filters with</span></span><br><span class="line"><span class="comment"># respect to the weights</span></span><br><span class="line">weights = tf.reduce_mean(guidedGrads, axis=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># grab the spatial dimensions of the input image and resize</span></span><br><span class="line"><span class="comment"># the output class activation map to match the input image</span></span><br><span class="line"><span class="comment"># dimensions</span></span><br><span class="line">(w, h) = (img_array.shape[<span class="number">2</span>], img_array.shape[<span class="number">1</span>])</span><br><span class="line">heatmap = cv2.resize(cam.numpy(), (w, h))</span><br><span class="line"></span><br><span class="line"><span class="comment"># normalize the heatmap such that all values lie in the range</span></span><br><span class="line"><span class="comment"># [0, 1], scale the resulting values to the range [0, 255],</span></span><br><span class="line"><span class="comment"># and then convert to an unsigned 8-bit integer</span></span><br><span class="line">numer = heatmap - np.<span class="built_in">min</span>(heatmap)</span><br><span class="line">denom = (heatmap.<span class="built_in">max</span>() - heatmap.<span class="built_in">min</span>()) + eps</span><br><span class="line">heatmap = numer / denom</span><br></pre></td></tr></table></figure>
<h1 id="course-4-gans">Course 4: GANs</h1>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensorflow笔记</title>
    <url>/2022/04/03/Tensorflow%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="tf.keras.losses">tf.keras.losses</h1>
<h2 id="categorical_crossentropy-vs-sparse_categorical_crossentropy"><code>categorical_crossentropy</code>
VS <code>sparse_categorical_crossentropy</code></h2>
<p><code>sparse_categorical_crossentropy</code>的用法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fmnist = tf.keras.datasets.fashion_mnist</span><br><span class="line"></span><br><span class="line">(training_images, training_labels), (test_images, test_labels) = fmnist.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the classification model</span></span><br><span class="line">model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), </span><br><span class="line">                                    tf.keras.layers.Dense(<span class="number">128</span>, activation=tf.nn.relu), </span><br><span class="line">                                    tf.keras.layers.Dense(<span class="number">10</span>, activation=tf.nn.softmax)])</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer = tf.optimizers.Adam(),</span><br><span class="line">              loss = <span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>,</span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">model.fit(training_images, training_labels, epochs=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">classifications = model.predict(test_images)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(classifications[<span class="number">0</span>]) <span class="comment"># [6.7404380e-06 9.1553174e-09 7.0345862e-08 1.8929207e-07 2.0586524e-06 4.4078561e-03 4.8766628e-06 2.5287211e-02 5.4279792e-07 9.7029042e-01]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>]) <span class="comment"># 9</span></span><br></pre></td></tr></table></figure>
<p>也就是<code>labels</code>是数字模式（1-9）分类类型，不是binary的模式，<code>classifications</code>是经过<code>softmax</code>激活层之后的结果，是一个<code>list</code>，每一个<code>item</code>代表的是该类的概率。这种情况我们用<code>sparse_categorical_crossentropy</code>，如果loss传入的是<code>categorical_crossentropy</code>是会报错的。</p>
<p><code>categorical_crossentropy</code>的用法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_true = [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">y_pred = [[<span class="number">0.05</span>, <span class="number">0.95</span>, <span class="number">0</span>], [<span class="number">0.1</span>, <span class="number">0.8</span>, <span class="number">0.1</span>]]</span><br><span class="line"><span class="comment"># Using &#x27;auto&#x27;/&#x27;sum_over_batch_size&#x27; reduction type.</span></span><br><span class="line">cce = tf.keras.losses.CategoricalCrossentropy()</span><br><span class="line">cce(y_true, y_pred).numpy()</span><br></pre></td></tr></table></figure>
<p>可以看出，CategoricalCrossentropy（）接受的是one-hot编码的labels，预测结果可以是softmax</p>
<blockquote>
<p>关于两者间的区别，更多的细节可以参考Tensorflow
官方文档和该博客[https://gombru.github.io/2018/05/23/cross_entropy_loss/]</p>
</blockquote>
<h1 id="callback回调函数"><code>callback()</code>回调函数</h1>
<p>参考 https://www.tensorflow.org/guide/keras/custom_callback</p>
<p>用途：可以在训练、评估或推断过程中自定义 Keras
模型行为，最常见的就是当accuracy达到某个阈值就停止训练。</p>
<p>所有回调函数都将 <code>keras.callbacks.Callback</code>
类作为子类，并重写在训练、测试和预测的各个阶段调用的一组方法。回调函数对于在训练期间了解模型的内部状态和统计信息十分有用。</p>
<p>您可以将回调函数的列表（作为关键字参数
<code>callbacks</code>）传递给以下模型方法：</p>
<ul>
<li><code>keras.Model.fit()</code></li>
<li><code>keras.Model.evaluate()</code></li>
<li><code>keras.Model.predict()</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myCallback</span>(<span class="params">tf.keras.callbacks.Callback</span>):</span> <span class="comment"># 这里是一定要继承Callback类的</span></span><br><span class="line">        <span class="comment"># Define the correct function signature for on_epoch_end</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span>(<span class="params">self, epoch, logs</span>):</span> <span class="comment"># 这里是在每一个epoch停止才会检查accuracy，如果在某个epoch中间出现了大于阈值的情况是不会停止的</span></span><br><span class="line">            <span class="keyword">if</span> logs.get(<span class="string">&#x27;accuracy&#x27;</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> logs.get(<span class="string">&#x27;accuracy&#x27;</span>) &gt; <span class="number">0.99</span>: <span class="comment"># @KEEP</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;\nReached 99% accuracy so cancelling training!&quot;</span>) </span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Stop training once the above condition is met</span></span><br><span class="line">                self.model.stop_training = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()</span><br><span class="line">x_train = x_train / <span class="number">255.0</span></span><br><span class="line">callbacks = myCallback()</span><br><span class="line">model = tf.keras.models.Sequential([ </span><br><span class="line">        tf.keras.layers.Flatten(input_shape=(<span class="number">28</span>, <span class="number">28</span>)),</span><br><span class="line">        tf.keras.layers.Dense(units=<span class="number">512</span>,activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">        tf.keras.layers.Dense(units=<span class="number">10</span>,activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">    ]) </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, </span><br><span class="line">              loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, </span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>]) </span><br><span class="line"></span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">10</span>, callbacks=[callbacks])</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="con2d-and-maxpooling2d">Con2D and MaxPooling2D</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = tf.keras.datasets.fashion_mnist</span><br><span class="line">(training_images, training_labels), (test_images, test_labels) = data.load_data()</span><br><span class="line">training_images = training_images.reshape(<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>) <span class="comment"># 这里是一定要reshape的，Con2D作为网络第一层，接受的input是一个4维的array，(sample_size,width,height,depth)</span></span><br><span class="line">training_images = training_images / <span class="number">255.0</span></span><br><span class="line">test_images = test_images.reshape(<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line">test_images = test_images / <span class="number">255.0</span></span><br><span class="line"><span class="comment"># Define the model</span></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">                                                         </span><br><span class="line">  <span class="comment"># Add convolutions and max pooling</span></span><br><span class="line">  tf.keras.layers.Conv2D(<span class="number">64</span>, kernel_size=(<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)), <span class="comment"># 这里input_shape=(width,height,depth) grey picture depth是1，RGB picture是3</span></span><br><span class="line">  tf.keras.layers.MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">  tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">  tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Add the same layers as before</span></span><br><span class="line">  tf.keras.layers.Flatten(),</span><br><span class="line">  tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">  tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the model summary</span></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use same settings</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;\nMODEL TRAINING:&#x27;</span>)</span><br><span class="line">model.fit(training_images, training_labels, epochs=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate on the test set</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;\nMODEL EVALUATION:&#x27;</span>)</span><br><span class="line">test_loss = model.evaluate(test_images, test_labels)</span><br></pre></td></tr></table></figure>
<ul>
<li>There are other approaches to pooling, such as <em>min</em> pooling,
which takes the smallest pixel value from the pool, and <em>average</em>
pooling, which takes the overall average value</li>
<li>When using this layer as the first layer in a model, provide the
keyword argument <code>input_shape</code> (tuple of integers or
<code>None</code>, does not include the sample axis), e.g.
<code>input_shape=(128, 128, 3)</code> for 128x128 RGB pictures in
<code>data_format="channels_last"</code>. You can use <code>None</code>
when a dimension has variable size</li>
</ul>
]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>cv2中bitwise_and()</title>
    <url>/2022/10/12/cv2%E4%B8%ADbitwise-and/</url>
    <content><![CDATA[<p>参考文档：</p>
<ol type="1">
<li><a href="https://docs.opencv.org/4.x/d0/d86/tutorial_py_image_arithmetics.html">opencv
tutorial</a></li>
<li><a href="https://docs.opencv.org/4.x/d2/de8/group__core__array.html#ga60b4d04b251ba5eb1392c34425497e14">opencv
官方文档</a></li>
<li><a href="https://stackoverflow.com/questions/44333605/what-does-bitwise-and-operator-exactly-do-in-opencv">stackoverflow
question</a></li>
</ol>
<p>没有多少文档能讲清楚具体是如何计算的。在stackoverflow中的这个问题，第一个高赞答案也只是选取了最普通的一种情况，也就是当image的pixel的值全部是0或者1的时候，and的运算。但是cv2中的bitwise_and()还有一个很重要的参数mask并没有讲清楚，并且当image的像素值有除了0或者1以外的其他值，这时候该如何运算？</p>
<p>官方文档中说mask是决定了哪些位置要进行运算，当mask中某个元素不为0时，我们对相应位置的像素做“与”运算，如果scr1和scr2(bitwise_and的两个参数)的值不是0或者1，会将十进制的值转化为2进制，然后对二进制再进行运算，运算完了之后又会转化为十进制(<a href="https://dsp.stackexchange.com/questions/58276/opencv-how-does-bitwise-not-work">参考</a>)
当mask相应位置的值=0时，任何操作都不会做，不仅如此，结果的图片相应位置的像素值会变成0，也就是黑色（这一点官方文档没说）.</p>
<p>所以经过bitwise_and之后的图片会看到mask中像素值为0的地方全部是黑色。这就相应的把mask中不为0的地方强调突出了。</p>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>ai and machinelearning for coders book notes</title>
    <url>/2022/04/14/ai-and-machinelearning-for-coders-book-notes/</url>
    <content><![CDATA[<p>该篇为阅读 AI and Machine Learing for Coders
的阅读笔记，相应的课程内容可以移步我另一篇博客《DeepLearning.AI-Tensorfow-Developer-Course课程整理知识点》。该书没有中文版，英文版读起来并不困难，面向的读者是有编程基础并掌握机器学习和深度学习算法理论知识的读者。此篇为本人看这本书时的阅读笔记，并不是全文翻译。主要目的是为了以后当做手册查看一些用法。</p>
<h1 id="chapter-5-nlp-intro">Chapter 5 NLP Intro</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(num_words = <span class="number">100</span>, oov_token=<span class="string">&quot;&lt;OOV&gt;&quot;</span>) <span class="comment"># 取出现频率最高的100个词</span></span><br><span class="line">tokenizer.fit_on_texts(sentences)</span><br><span class="line">word_index = tokenizer.word_index <span class="comment"># &#123;&#x27;&lt;OOV&gt;&#x27;: 1, &#x27;today&#x27;: 2, &#x27;is&#x27;: 3, &#x27;a&#x27;: 4, &#x27;sunny&#x27;: 5, &#x27;day&#x27;: 6, &#x27;rainy&#x27;: 7, &#x27;it&#x27;: 8&#125;</span></span><br><span class="line">sequences = tokenizer.texts_to_sequences(sentences) <span class="comment"># [[2, 3, 4, 1, 6], [1, 8, 1, 7, 1]]</span></span><br><span class="line"></span><br><span class="line">padded = pad_sequences(sequences, padding=<span class="string">&#x27;post&#x27;</span>, maxlen=<span class="number">6</span>,truncating=<span class="string">&#x27;post&#x27;</span>) <span class="comment"># &#x27;post&#x27;表示后补0，maxlen表示最长的sentence只能有6个词语,truncating=&#x27;post&#x27;表示如果一个句子过长，会将前面的words剔除</span></span><br></pre></td></tr></table></figure>
<p>去除stopwords和标点：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line">stopwords = [<span class="string">&quot;a&quot;</span>, ... , <span class="string">&quot;yourselves&quot;</span>]</span><br><span class="line"></span><br><span class="line">table = <span class="built_in">str</span>.maketrans(<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;&#x27;</span>, string.punctuation)</span><br><span class="line"></span><br><span class="line">imdb_sentences = []</span><br><span class="line">train_data = tfds.as_numpy(tfds.load(<span class="string">&#x27;imdb_reviews&#x27;</span>, split=<span class="string">&quot;train&quot;</span>))</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> train_data:</span><br><span class="line">    sentence = <span class="built_in">str</span>(item[<span class="string">&#x27;text&#x27;</span>].decode(<span class="string">&#x27;UTF-8&#x27;</span>).lower())</span><br><span class="line">    soup = BeautifulSoup(sentence)</span><br><span class="line">    sentence = soup.get_text()</span><br><span class="line">    sentence = sentence.replace(<span class="string">&quot;,&quot;</span>, <span class="string">&quot; , &quot;</span>) <span class="comment"># 这四行是为了解决其中有he/she的情况</span></span><br><span class="line">    sentence = sentence.replace(<span class="string">&quot;.&quot;</span>, <span class="string">&quot; . &quot;</span>)</span><br><span class="line">    sentence = sentence.replace(<span class="string">&quot;-&quot;</span>, <span class="string">&quot; - &quot;</span>)</span><br><span class="line">    sentence = sentence.replace(<span class="string">&quot;/&quot;</span>, <span class="string">&quot; / &quot;</span>)</span><br><span class="line">    words = sentence.split()</span><br><span class="line">    filtered_sentence = <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        word = word.translate(table)</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stopwords:</span><br><span class="line">            filtered_sentence = filtered_sentence + word + <span class="string">&quot; &quot;</span></span><br><span class="line">    imdb_sentences.append(filtered_sentence)</span><br><span class="line"></span><br><span class="line">tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=<span class="number">25000</span>)</span><br><span class="line">tokenizer.fit_on_texts(imdb_sentences) <span class="comment"># 这里fit_on_texts（）接受的是一个列表</span></span><br><span class="line">sequences = tokenizer.texts_to_sequences(imdb_sentences)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.word_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取&#123;token:word&#125;</span></span><br><span class="line">reverse_word_index = <span class="built_in">dict</span>(</span><br><span class="line">    [(value, key) <span class="keyword">for</span> (key, value) <span class="keyword">in</span> tokenizer.word_index.items()])</span><br><span class="line"></span><br><span class="line">decoded_review = <span class="string">&#x27; &#x27;</span>.join([reverse_word_index.get(i, <span class="string">&#x27;?&#x27;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> sequences[<span class="number">0</span>]])</span><br></pre></td></tr></table></figure>
<p>如果想用tensorflow内置的corpus编码自己的文本：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(train_data, test_data), info = tfds.load(</span><br><span class="line">    <span class="string">&#x27;imdb_reviews/subwords8k&#x27;</span>, </span><br><span class="line">    split = (tfds.Split.TRAIN, tfds.Split.TEST),</span><br><span class="line">    as_supervised=<span class="literal">True</span>,</span><br><span class="line">    with_info=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">encoder = info.features[<span class="string">&#x27;text&#x27;</span>].encoder</span><br><span class="line"></span><br><span class="line">sample_string = <span class="string">&#x27;Today is a sunny day&#x27;</span></span><br><span class="line"></span><br><span class="line">encoded_string = encoder.encode(sample_string)<span class="comment"># [6427, 4869, 9, 4, 2365, 1361, 606]</span></span><br><span class="line"></span><br><span class="line">original_string = encoder.decode(encoded_string) <span class="comment"># &#x27;Today is a sunny day&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<p>补充说明sklearn中有一个类似的class：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line">vectorizer = CountVectorizer()</span><br><span class="line">vectorizer.fit_transform(imdb_sentences).shape</span><br></pre></td></tr></table></figure>
<p>以上返回的也是一个矩阵，但是每一个array是一个统计每个词的出现次数的向量，比如是0,1,2这些数字，出现几次，那个位置就是几。和<code>keras.preprocessing.text.Tokenizer</code>还是有很大区别的。</p>
<h1 id="chapter-6.-using-word-embeddings">Chapter 6. Using word
Embeddings</h1>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>decoding strategy in NG tasks</title>
    <url>/2023/04/21/decoding-strategy-in-NG-tasks/</url>
    <content><![CDATA[<p>在Neural Language
Generation的任务中，如何在每一个时间步产生一个token称为decoding
method.最常见的decoding方法就是用softmax激活函数计算概率分布之后，将概率最大的那个token列为当前的预测值（most
likely string）。这种argmax的方式在machine
translation等non-open-ended的任务中表现还可以，但是如果是在纯粹的开放性的任务中，比如写一首诗歌。这种方式会造成重复性的输出，并且输出的句子有时候连续性也比较差。</p>
<p>这时候很多研究工作就对decoding strategic展开了研究，比如beam
search，基本原理很简单，就是从取概率排名前k个token作为当前预测值。但是这还是会有一个问题，就是当我们概率分布均衡的时候，这个方法可以，但如果概率分布不均衡，也就是softmax计算出来的概率值只有几个token的概率比较大，其他概率都非常小，比如零点几，那这个时候其实我们不太需要考虑k个单词，只需要考虑概率比较大的那些tokens就够了。</p>
<h2 id="top-pnucleus-sampling">Top-p（Nucleus Sampling）</h2>
<p>所以这时候就有人提出了Top-p(nucleus) sampling的方法<a href="http://arxiv.org/abs/1904.09751">The Curious Case of Neural Text
Degeneration</a></p>
<p>具体做法就是维护一个动态的k值，这个k值随着softmax的输出概率分布决定，</p>
<figure>
<img src="/2023/04/21/decoding-strategy-in-NG-tasks/image-20230421134555223.png" alt="image-20230421134555223">
<figcaption aria-hidden="true">image-20230421134555223</figcaption>
</figure>
<p>作者的思路就是当概率分布比较flat的时候，我们应该把sample的池子定的大一点。但是当概率分布比较陡峭，也就是上图中的第三种情况时，我们就需要把这个sample的池子变得小一点。具体是如何操作的呢？</p>
<figure>
<img src="/2023/04/21/decoding-strategy-in-NG-tasks/image-20230421134938176.png" alt="image-20230421134938176">
<figcaption aria-hidden="true">image-20230421134938176</figcaption>
</figure>
<figure>
<img src="/2023/04/21/decoding-strategy-in-NG-tasks/image-20230421134952769.png" alt="image-20230421134952769">
<figcaption aria-hidden="true">image-20230421134952769</figcaption>
</figure>
<p>看公式可能会比较复杂，具体做法就是提前预设一个阈值p，然后对于某个时间步的概率分布P，我们寻找一个最小的top-p的一个token池子，这个池子里所有的token的概率值加起来要大于p，注意这里是找一个smallest
set，也就是我们在找这个token池子时，从概率最高的token往下捋，一直到概率和大于p。找到这个池子之后，我们将原来经过softmax函数计算之后的概率分布按照上图中（3）的公式重新计算得出一个新的概率分布。最终我们得到的概率分布不属于我们之前找的token池子里的token的概率全部置为0，至于在这个token池子里的token的概率值会除以这个池子里所有token概率值的<strong>和</strong>。然后我们从这个分布中去sample我们的预测token。</p>
<h2 id="sampling-with-temperature">Sampling with Temperature</h2>
<p>Temperature这个概念在GPT中也有，不同的温度值，你会得到不同的结果。温度值越低，它会对自己的输出结果更自信，而温度值越高，它会降低模型的确信值，也就会返回更多的结果给你。<a href="https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277">Blog</a>对上面提到的<a href="http://arxiv.org/abs/1904.09751">The Curious Case of Neural Text
Degeneration</a>文章进行了解答，但我发现有一点和paper中不一样的是：</p>
<figure>
<img src="/2023/04/21/decoding-strategy-in-NG-tasks/image-20230421144452222.png" alt="image-20230421144452222">
<figcaption aria-hidden="true">image-20230421144452222</figcaption>
</figure>
<p>temperature值t并不是取值是<code>[0,1)</code>，stanford224n的课件以及博客里对t的取值是可以大于1的：</p>
<figure>
<img src="/2023/04/21/decoding-strategy-in-NG-tasks/image-20230421144619190.png" alt="image-20230421144619190">
<figcaption aria-hidden="true">image-20230421144619190</figcaption>
</figure>
<p>在博客中我们可以看到作者对于t值大于1和小于1画出的图的区别：</p>
<figure>
<img src="/2023/04/21/decoding-strategy-in-NG-tasks/image-20230421144711636.png" alt="image-20230421144711636">
<figcaption aria-hidden="true">image-20230421144711636</figcaption>
</figure>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>git常用命令总结</title>
    <url>/2022/01/13/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>github官网给出的教程挺通俗易懂的，移步[https://docs.github.com/cn/get-started/using-git]</p>
<p>本地配置ssh和github，参考官方文档[https://docs.github.com/cn/authentication/connecting-to-github-with-ssh/about-ssh]</p>
<h1 id="远程仓库使用">远程仓库使用</h1>
<p>在本地设置推送到远程仓库的用户名:[https://docs.github.com/cn/get-started/getting-started-with-git/setting-your-username-in-git]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git config --global user.name &quot;your name&quot;</span><br></pre></td></tr></table></figure>
<p>远程URL是 Git 一种指示“您的代码存储位置”的绝佳方式，您只能推送到两类
URL 地址：</p>
<ul>
<li>HTTPS URL，如 <code>https://github.com/user/repo.git</code></li>
<li>SSH URL，如 <code>git@github.com:user/repo.git</code></li>
</ul>
<p>Git 将远程 URL 与名称相关联，您的默认远程通常名为
<code>origin</code></p>
<p>创建远程仓库，并将其命名为master</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote add master &lt;REMOTE_URL&gt; </span><br></pre></td></tr></table></figure>
<p>如果后面想更改url，使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote set-url origin &lt;new_url&gt;</span><br></pre></td></tr></table></figure>
<p>查看远程仓库设置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote -v</span><br></pre></td></tr></table></figure>
<p>重命名远程仓库</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote rename origin destination</span><br></pre></td></tr></table></figure>
<p>删除远程仓库</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote rm destination</span><br></pre></td></tr></table></figure>
<p>推送提交至远程仓库，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git push &lt;REMOTENAME&gt; &lt;LOCALBRANCHNAME&gt;:&lt;REMOTEBRANCHNAME&gt; </span><br></pre></td></tr></table></figure>
<p>拉取某远程仓库距离上一次抓取之后的工作：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git fetch &lt;remote_name&gt;</span><br><span class="line">必须注意 git fetch 命令只会将数据下载到你的本地仓库——它并不会自动合并或修改你当前的工作。 当准备好时你必须手动将其合并入你的工作。</span><br></pre></td></tr></table></figure>
<p>显示某远程仓库的信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote show &lt;remote_name&gt;</span><br></pre></td></tr></table></figure>
<h1 id="忽略文件">忽略文件</h1>
<p><a href="https://docs.github.com/cn/get-started/getting-started-with-git/ignoring-files">见文档</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">touch .gitignore #该命令在项目根目录会创建一个.gitignore文件,然后往该文件中填东西</span><br></pre></td></tr></table></figure>
<p>如果有些我们不需要跟踪的文件已经提交到了暂存区，那么使用下面的命令来删除暂存区的该文件，再将该文件写入.gitignore文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git rm --cached FILENAME 该命令直接将暂存区的那个版本删除了</span><br><span class="line">或者 git restore --staged &lt;file&gt; 该命令会用暂存区的代码覆盖掉工作区的代码</span><br></pre></td></tr></table></figure>
<h1 id="在自己的project中添加别人的project">在自己的project中添加别人的project</h1>
<p><a href="https://devconnected.com/how-to-add-and-update-git-submodules/" class="uri">https://devconnected.com/how-to-add-and-update-git-submodules/</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git submodule add &lt;remote_url&gt; &lt;destination_folder&gt;</span><br><span class="line"></span><br><span class="line">git commit -m &quot;Added the submodule to the project.&quot;</span><br><span class="line"></span><br><span class="line">git push</span><br><span class="line"></span><br><span class="line">git submodule update --init --recursive # 如果想要拉取别人仓库里的submodule到本地</span><br></pre></td></tr></table></figure>
<h1 id="分支">分支</h1>
<p>创建分支：当执行git init时，默认创建名字是master的分支</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git branch &lt;branch_name&gt;</span><br></pre></td></tr></table></figure>
<p>切换到某分支:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git checkout &lt;branch_name&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>分支切换会改变你工作目录中的文件。在切换分支时，一定要注意你工作目录里的文件会被改变。
如果是切换到一个较旧的分支，你的工作目录会恢复到该分支最后一次提交时的样子。
如果 Git 不能干净利落地完成这个任务，它将禁止切换分支。</p>
</blockquote>
<p>上面两条命令可以使用一条命令搞定：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git checkout -b &lt;branch_name&gt;</span><br></pre></td></tr></table></figure>
<p>在一条分支上，比如hotfix修改一些文件提交后，需要回到master分支并将hotfix上的分支的修改合并到master</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git merge hotfix</span><br></pre></td></tr></table></figure>
<p>这是一个Fast-forward。合并完之后master会和hotfix指向同一个位置，这时可以删除hotfix这个分支：<code>git branch -d hotfix</code></p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo图片插入问题：在主页和文章详情页同时显示图片</title>
    <url>/2021/11/17/hexo%E5%9B%BE%E7%89%87%E6%8F%92%E5%85%A5%E9%97%AE%E9%A2%98%EF%BC%9A%E5%9C%A8%E4%B8%BB%E9%A1%B5%E5%92%8C%E6%96%87%E7%AB%A0%E8%AF%A6%E6%83%85%E9%A1%B5%E5%90%8C%E6%97%B6%E6%98%BE%E7%A4%BA%E5%9B%BE%E7%89%87/</url>
    <content><![CDATA[<h1 id="设置静态文件根目录">设置静态文件根目录</h1>
<p><code>_config.yml</code>中有个<code>url</code>和<code>root</code>参数
如果你部署的地址是<code>http://yoursite.com/child</code>，需要设置下面两个参数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">url: &#x27;http://yoursite.com/child&#x27; // 部署的域名</span><br><span class="line">root: &#x27;/child/&#x27; // 部署的根目录</span><br><span class="line">12</span><br></pre></td></tr></table></figure>
<h1 id="设置资源文件夹">设置资源文件夹</h1>
<p>资源（Asset）代表 source
文件夹中除了文章以外的所有文件，例如图片、CSS、JS
文件等。比方说，如果你的Hexo项目中只有少量图片，那最简单的方法就是将它们放在
<code>source/images</code> 文件夹中。然后通过类似于
<code>![](/images/image.jpg)</code> 的方法访问它们。</p>
<p>&lt; !--more--&gt;</p>
<p>文章资源文件夹
对于那些想要更有规律地提供图片和其他资源以及想要将他们的资源分布在各个文章上的人来说，Hexo也提供了更组织化的方式来管理资源。这个稍微有些复杂但是管理资源非常方便的功能可以通过<strong>将
<code>config.yml</code> 文件中的 <code>post_asset_folder</code> 选项设为
true 来打开</strong>。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">_config.yml</span><br><span class="line">post_asset_folder: true</span><br><span class="line">12</span><br></pre></td></tr></table></figure>
<p>当资源文件管理功能打开后，Hexo将会在你<strong>每一次通过
<code>hexo new [layout] &lt;title&gt;</code>
命令创建新文章时自动创建一个文件夹</strong>。这个资源文件夹将会有与这个
<code>markdown</code>
文件一样的名字。将所有与你的文章有关的资源放在这个关联文件夹中之后，你可以通过相对路径来引用它们，这样你就得到了一个更简单而且方便得多的工作流。</p>
<p>相对路径引用的标签插件 通过常规的 markdown
语法和相对路径来引用图片和其它资源可能会导致它们在存档页或者主页上显示不正确。在Hexo
2时代，社区创建了很多插件来解决这个问题。但是，随着Hexo 3
的发布，许多新的标签插件被加入到了核心代码中。这使得你可以更简单地在文章中引用你的资源。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% asset_path slug %&#125;</span><br><span class="line">&#123;% asset_img slug [title] %&#125;</span><br><span class="line">&#123;% asset_link slug [title] %&#125;</span><br><span class="line">123</span><br></pre></td></tr></table></figure>
<p>比如说：当你打开文章资源文件夹功能后，你把一个
<code>example.jpg</code>
图片放在了你的资源文件夹中，如果通过使用相对路径的常规 markdown 语法
<code>![](/example.jpg)</code> ，它将 不会
出现在首页上。（但是它会在文章中按你期待的方式工作）</p>
<p>正确的引用图片方式是使用下列的标签插件而不是 markdown ：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% asset_img example.jpg avatar %&#125;</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<p><strong>通过这种方式，图片将会同时出现在文章和主页以及归档页中。通过<code>&#123;% asset_img 图片名称 图片说明 %&#125;</code></strong></p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>image classification models总结</title>
    <url>/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>本博客旨在记录自己在了解<code>image classification</code>这个术语<code>computer vision</code>的一个子任务中常见的模型。耳熟能详的就是<code>ResNet</code>,<code>VGG</code>,<code>Inception</code>,<code>MobileNet</code>,
<code>Efficientnet</code>。每一个模型之间有什么区别，他们自身又有哪些变种，比如<code>VGG</code>，它拥有<code>VGG16,VGG19</code>等，<code>ResNet</code>又有很多，单是查看<code>Tensorflow</code>的官方文档就会发现在<code>tf.keras.applications</code>模块下，就有很多模型架构可选（也都有预训练参数）。整理这个博客的目的在于让自己对这些模型之间的差别有所了解，这样在不同的任务中才会知道使用什么样的模型架构来handle自己的数据。</p>
<p>在整理这篇博客的过程中，我也去搜了有没有<code>image classification</code>这个单任务上的<code>review</code>文章，文章都挺多的，筛选之后推荐这篇<a href="https://www.mdpi.com/2072-4292/13/22/4712">Review of Image
Classification Algorithms Based on Convolutional Neural
Networks</a>.这篇文章主要是介绍基于CNN的一些模型，共有三个章节。重点是第二章节梳理了<code>CNN-based</code>的一些模型，包括本文想要<code>cover</code>的<code>VGG</code>，<code>inception</code>，<code>resnet</code>，<code>mobilenet</code>。重点关注图像分类算法的小伙伴可以通读一下这篇文章。</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/image-20230207094857739-16757345394748.png" alt="Review of Image Classification Algorithms Based on Convolutional Neural Networks第二章节目录">
<figcaption aria-hidden="true">Review of Image Classification Algorithms
Based on Convolutional Neural Networks第二章节目录</figcaption>
</figure>
<h1 id="vgg">VGG</h1>
<p>首次提出在2014年的<a href="https://arxiv.org/abs/1409.1556">paper</a></p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/vgg16.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>上图中包含13个卷积层和3个全连接层，是<code>VGG16</code>的结构。而<code>VGG19</code>包含了16个卷积层和3个全连接层：</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/1dNYBNBDP7ZvckfOSYzHxIw.jpeg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><code>VGG</code>系列就是<code>VGG16</code>和<code>VGG19</code>，两者的区别在于19用了更多的卷积层。<code>Tensorflow</code>也提供了这两个模型的黑盒子实现供大家使用。</p>
<h1 id="resnet">ResNet</h1>
<p>首次提出在2016年的<a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">paper</a>，其中最重要的就是网络中的<code>residual block</code>:</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/1_nmPcwwnsHE-AC69ASkj9w.jpeg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>在作者的原文中我们可以发现，文章中提出的Resnet是34层，也就是ResNet34。在具体实现的时候，作者在每一个卷积操作之后（激活函数之前）加上了batch
Normalization。在<code>Module: tf.keras.applications.resnet</code><a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet">Tensorflow
applications
resnet</a>中现在只有ResNet101，152，50三个版本，其中ResNet50和ResNet34的区别在于：前者使用三个卷积一个block，后者是2个卷积一个block.
ResNet50表现更优异。ResNet101和ResNet152在一个block内使用了更多的卷积layer。</p>
<p>以上所说的都是resnet v1，后来同一个作者又发表了<a href="https://arxiv.org/pdf/1603.05027.pdf">Identity Mappings in Deep
Residual Networks</a>,提出了ResNet
v2。同样我们在tensorflow中也可以看到模块<code>Module: tf.keras.applications.resnet_v2</code>，同样的也有50，101，152三个版本的model。</p>
<p>v1和v2的区别在于：</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/image-20230206150618805-16756671800395.png" alt="ResNet v1 and ResNet v2">
<figcaption aria-hidden="true">ResNet v1 and ResNet v2</figcaption>
</figure>
<p>以上只是概念上的解释，看代码会更合适一点，其中<code>Deep Residual Learning for Image Recognition</code>文章中也给出了34，50，101，152等几个模型在实现中注意的细节：</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/image-20230206153053177-16756686549546.png" alt="image-20230206153053177">
<figcaption aria-hidden="true">image-20230206153053177</figcaption>
</figure>
<p>ResNet34 V1中，一个resnet
block是由两个卷积layer组成的，同时它和V2的一个区别就在于X进来后就先进行卷积运算，也就是上图中的weight</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras.activations <span class="keyword">import</span> relu</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers <span class="keyword">as</span> Layers</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResBlock</span>(<span class="params">Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, channels, stride=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ResBlock, self).__init__(name=<span class="string">&#x27;ResBlock&#x27;</span>)</span><br><span class="line">        self.flag = (stride != <span class="number">1</span>)</span><br><span class="line">        self.conv1 = Conv2D(channels, <span class="number">3</span>, stride, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.bn1 = BatchNormalization()</span><br><span class="line">        self.conv2 = Conv2D(channels, <span class="number">3</span>, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.bn2 = BatchNormalization()</span><br><span class="line">        self.relu = ReLU()</span><br><span class="line">        <span class="keyword">if</span> self.flag:</span><br><span class="line">            self.bn3 = BatchNormalization()</span><br><span class="line">            self.conv3 = Conv2D(channels, <span class="number">1</span>, stride)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x1 = self.conv1(x)</span><br><span class="line">        x1 = self.bn1(x1)</span><br><span class="line">        x1 = self.relu(x1)</span><br><span class="line">        x1 = self.conv2(x1)</span><br><span class="line">        x1 = self.bn2(x1)</span><br><span class="line">        <span class="keyword">if</span> self.flag:</span><br><span class="line">            x = self.conv3(x)</span><br><span class="line">            x = self.bn3(x)</span><br><span class="line">        x1 = Layers.add([x, x1])</span><br><span class="line">        x1 = self.relu(x1)</span><br><span class="line">        <span class="keyword">return</span> x1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet34</span>(<span class="params">Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ResNet34, self).__init__(name=<span class="string">&#x27;ResNet34&#x27;</span>)</span><br><span class="line">        self.conv1 = Conv2D(<span class="number">64</span>, <span class="number">7</span>, <span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.bn = BatchNormalization()</span><br><span class="line">        self.relu = ReLU()</span><br><span class="line">        self.mp1 = MaxPooling2D(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.conv2_1 = ResBlock(<span class="number">64</span>)</span><br><span class="line">        self.conv2_2 = ResBlock(<span class="number">64</span>)</span><br><span class="line">        self.conv2_3 = ResBlock(<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">        self.conv3_1 = ResBlock(<span class="number">128</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv3_2 = ResBlock(<span class="number">128</span>)</span><br><span class="line">        self.conv3_3 = ResBlock(<span class="number">128</span>)</span><br><span class="line">        self.conv3_4 = ResBlock(<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">        self.conv4_1 = ResBlock(<span class="number">256</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv4_2 = ResBlock(<span class="number">256</span>)</span><br><span class="line">        self.conv4_3 = ResBlock(<span class="number">256</span>)</span><br><span class="line">        self.conv4_4 = ResBlock(<span class="number">256</span>)</span><br><span class="line">        self.conv4_5 = ResBlock(<span class="number">256</span>)</span><br><span class="line">        self.conv4_6 = ResBlock(<span class="number">256</span>)</span><br><span class="line"></span><br><span class="line">        self.conv5_1 = ResBlock(<span class="number">512</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv5_2 = ResBlock(<span class="number">512</span>)</span><br><span class="line">        self.conv5_3 = ResBlock(<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">        self.pool = GlobalAveragePooling2D()</span><br><span class="line">        self.fc1 = Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.dp1 = Dropout(<span class="number">0.5</span>)</span><br><span class="line">        self.fc2 = Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.dp2 = Dropout(<span class="number">0.5</span>)</span><br><span class="line">        self.fc3 = Dense(<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.mp1(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2_1(x)</span><br><span class="line">        x = self.conv2_2(x)</span><br><span class="line">        x = self.conv2_3(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv3_1(x)</span><br><span class="line">        x = self.conv3_2(x)</span><br><span class="line">        x = self.conv3_3(x)</span><br><span class="line">        x = self.conv3_4(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv4_1(x)</span><br><span class="line">        x = self.conv4_2(x)</span><br><span class="line">        x = self.conv4_3(x)</span><br><span class="line">        x = self.conv4_4(x)</span><br><span class="line">        x = self.conv4_5(x)</span><br><span class="line">        x = self.conv4_6(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv5_1(x)</span><br><span class="line">        x = self.conv5_2(x)</span><br><span class="line">        x = self.conv5_3(x)</span><br><span class="line"></span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.dp1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.dp2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = ResNet34()</span><br><span class="line">model.build(input_shape=(<span class="number">1</span>, <span class="number">480</span>, <span class="number">480</span>, <span class="number">3</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<h1 id="inception">Inception</h1>
<p>已经有不少博客在科普Inception系列模型的区别<a href="https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202">A
Simple Guide to the Versions of the Inception Network</a></p>
<p>首先提出该模型的是2014年的<a href="https://arxiv.org/pdf/1409.4842v1.pdf">Going deeper with
convolutions</a> Inception
V1（GoogleNet）,然后又分别有了好几个变体：<code>Inception V2，Inception V3，Inception V4</code>，<a href="http://arxiv.org/abs/1602.07261">Inception-ResNet-v2</a>。和<code>ResNet</code>一样，<code>Inception</code>网络中一个重要的<code>module</code>是<code>Inception Module</code>：</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/image-20230206161239990-16756711610527.png" alt="Inception Module">
<figcaption aria-hidden="true">Inception Module</figcaption>
</figure>
<p>其中这些Network中被广泛使用的是<a href="https://arxiv.org/pdf/1512.00567v3.pdf">Inception_v3</a>和<a href="https://arxiv.org/pdf/1602.07261.pdf">Inception-ResNet-v2</a>.</p>
<h1 id="mobilenet">MobileNet</h1>
<blockquote>
<p>The idea behind MobileNet is to use depthwise separable convolutions
to build loghter deep neural networks. In regular convolutional layer,
the convolution kernel or filter is applied to all of the channels of
the input image, by doing weighted sum of the input pixels with the
filter and then slides to the next input pixels across the
images.MobileNet uses this regular convolution only in the first layer.
The next layers are the depthwise separable convolutions which are the
combination of the depthwise and pointwise convolution. The depthwise
convolution does the convolution on each channel separately. If the
image has three channels, therefore, the output image also has three
channels. This depthwise convolution is used to filter the input
channels. The next step is the pointwise convolution, which is similar
to the regular convolution but with a 1x1 filter. The purpose of
pointwise convolution is to merge the output channels of the depthwise
convolution to create new features. By doing so, the computational work
needed to be done is less than the regular convolutional networks.</p>
<p>引用自<a href="https://medium.com/@fransiska26/the-differences-between-inception-resnet-and-mobilenet-e97736a709b0">the-differences-between-inception-resnet-and-mobilenet</a></p>
</blockquote>
<p><code>MobileNet</code>使用了两种卷积形式，<code>depthwise</code>和<code>pointwise</code>，后者就是我们常见的卷积操作，只是使用的是1✖1的卷积核，input
image有多少个<code>channel</code>，<code>filter</code>就会延展为几个<code>channel</code>，比如输入进来的<code>channel</code>数是3，那么一个<code>3*3</code>大小的filter就会extend成3✖3✖3的一个立方体，然后这27个数分别禹输入image对应的区域做乘积之后相加取和。但是<code>depthwise</code>卷积是对每一个<code>channel</code>分别做卷积，如果输入图片有三个<code>channel</code>，那么输出的也会是三个<code>channel</code>。如图：</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/image-20230207132331415.png" alt="depthwise convolution">
<figcaption aria-hidden="true">depthwise convolution</figcaption>
</figure>
<p><code>tensorflow</code>中有<code>DepthwiseConv2D</code>这个<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D">layer</a>,它对于<code>depthwise convolution</code>的解释是：</p>
<blockquote>
<p>Depthwise convolution is a type of convolution in which each input
channel is convolved with a different kernel (called a depthwise
kernel). You can understand depthwise convolution as the first step in a
depthwise separable convolution.</p>
</blockquote>
<p><code>MobileNetV2</code>主要引进了<code>Inverted residuals</code>和<code>linear bottlenecks</code>去解决在<code>depthwise</code>卷积操作中卷积核的参数往往是0的问题</p>
<h1 id="other-topics">other topics</h1>
<p>在阅读<a href="https://www.mdpi.com/2072-4292/13/22/4712">Review of
Image Classification Algorithms Based on Convolutional Neural
Networks</a>的最后一章节时，作者不仅介绍了现在research和industry领域用的比较多的image
classification的模型，也给出了各个模型在image-net数据集上的accuracy。在总结章，作者还提出了一些结论性的发现，我觉得蛮收益的，将文章的观点整理在这里。</p>
<ol type="1">
<li>2012年到2017年主要提供了日后用于分类的basic
CNN模型架构，这期间的模型架构有2012的alexnet，2014年的vgg，2014年的inception，2015年的resnet，2017年提出了attention加cnn的架构</li>
<li>attention加入到cnn之后形成了新的模型，也因此提高了模型的performance。现在很多模型会将SE
block嵌入到模型架构中去，我查了下这个SE
block是SEnet中的一个block，squeeze and excitation block。<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf">SEnet</a>是在2017年提出的，这个知识点待补充</li>
<li>超参数的选择对于CNN网络的performance影响很大，很多的工作在着力于减少超参数的个数以及replace
them with other composite coefficients。</li>
<li>手动设计一个performance很好的网络往往需要很多effort，<a href="https://en.wikipedia.org/wiki/Neural_architecture_search">NAS
search</a> （neural architecture search）可以让这个过程变得更简单</li>
<li>想要提升模型的performance，不仅仅需要将关注力放在模型架构的设计上，data
augmentation,transfer learning,training
strategies也可以帮助我们提高模型的准确度。在transfer learning上，paper：
Large Scale Learning of General Visual Representations for Transfer
总结了一些在不同的task上如何利用transfer
learning取得很好的performance的办法。</li>
</ol>
<hr>
<p>CNN model 还面临的挑战：</p>
<ol type="1">
<li>lightweight
models比如mobileNet系列的轻量级模型往往需要牺牲accuracy来提高efficiency。未来在embedded系统上，CNN的运行效率值得去explore</li>
<li>cnn模型在semi-supervised和unsupervised上的发挥不如NLP领域。</li>
</ol>
<hr>
<p>future directions:</p>
<ol type="1">
<li>重视vision transformer.
如何将卷积和transformer有效结合起来是当前的一个热点。目前的SOTA
network是 <a href="https://arxiv.org/abs/2106.04803">CoAtNet</a>，在image
net数据集上的accuracy是90.88，确实是目前在image
net数据集上performance最高的模型架构。值得读一下，mark！</li>
<li>有一些关于CNN的传统技术可能会成为阻碍CNN发展的重要因素，诸如：activation
function的选择，dropout，batch normalization。</li>
</ol>
<h1 id="senet-2017">SENet 2017</h1>
<p>原文 <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf">SEnet</a>，是由自动驾驶公司Momenta在2017年公布的一种全新的图像识别结构，它通过对特征通道间的相关性进行建模，把重要的特征进行强化来提升准确率。这个结构是2017
ILSVR竞赛的冠军，top5的错误率达到了2.251%，比2016年的第一名还要低25%，可谓提升巨大。</p>
<h1 id="coatnet">CoAtNet</h1>
<h1 id="reference">Reference</h1>
<ol type="1">
<li><a href="https://towardsdatascience.com/architecture-comparison-of-alexnet-vggnet-resnet-inception-densenet-beb8b116866d">Architecture
comparison of AlexNet, VGGNet, ResNet, Inception, DenseNet</a></li>
<li><a href="https://medium.com/analytics-vidhya/vggnet-architecture-explained-e5c7318aa5b6">VGGNet
Architecture Explained</a></li>
<li><a href="https://viso.ai/deep-learning/resnet-residual-neural-network/">resnet-residual-neural-network
Resnet系列网络架构的区别</a></li>
</ol>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>image segmentation(图像分割)中的IOU,Dice的计算</title>
    <url>/2022/08/22/image-segmentation-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E4%B8%AD%E7%9A%84IOU-Dice%E7%9A%84%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<p>在图像分割任务中，通常需要将loss设置成dice或者IOU的值，这里总结一下他们的使用方式：</p>
<h1 id="二分类问题mask只有0或者1">二分类问题（mask只有0或者1）</h1>
<p>这种task网络的最后一层通常会加sigmoid激活函数，比如unet实现中最后一层就是一个卷积层：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv10 = Conv2D(<span class="number">1</span>, (<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">&#x27;sigmoid&#x27;</span>)(conv9)</span><br></pre></td></tr></table></figure> 因此输出的feature
map中每一个pixel的值就是0~1之间的值。</p>
<p>dice loss可以如下计算： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> keras.losses <span class="keyword">import</span> binary_crossentropy</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line">SMOOTH = <span class="number">1.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dice_coef</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    y_true_f = K.flatten(y_true)</span><br><span class="line">    y_pred_f = K.flatten(y_pred)</span><br><span class="line">    intersection = K.<span class="built_in">sum</span>(y_true_f * y_pred_f)</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">2.</span> * intersection + SMOOTH) / (K.<span class="built_in">sum</span>(y_true_f) + K.<span class="built_in">sum</span>(y_pred_f) + SMOOTH)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iou_coef</span>(<span class="params">y_true, y_pred, smooth=<span class="number">1</span></span>):</span></span><br><span class="line">    intersection = K.<span class="built_in">sum</span>(K.<span class="built_in">abs</span>(y_true * y_pred), axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">    union = K.<span class="built_in">sum</span>(y_true,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])+K.<span class="built_in">sum</span>(y_pred,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])-intersection</span><br><span class="line">    iou = K.mean((intersection + smooth) / (union + smooth), axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> iou</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bce_dice_loss</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred) <span class="comment"># 这里也可以用 + (1-dice_coef)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(Adam(learning_rate=<span class="number">1e-4</span>),</span><br><span class="line">              bce_dice_loss,</span><br><span class="line">              metrics=[binary_crossentropy, dice_coef])</span><br></pre></td></tr></table></figure></p>
<h1 id="多分类问题mask有除了0和1以外的其他值">多分类问题（mask有除了0和1以外的其他值）</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">class_wise_metrics</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Computes the class-wise IOU and Dice Score.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    y_true (tensor) - ground truth label maps</span></span><br><span class="line"><span class="string">    y_pred (tensor) - predicted label maps</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  class_wise_iou = []</span><br><span class="line">  class_wise_dice_score = []</span><br><span class="line"></span><br><span class="line">  smoothing_factor = <span class="number">0.00001</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">    intersection = np.<span class="built_in">sum</span>((y_pred == i) * (y_true == i)) <span class="comment"># 计算true positive的pixel个数</span></span><br><span class="line">    y_true_area = np.<span class="built_in">sum</span>((y_true == i)) <span class="comment"># 计算pixcel=i的像素个数</span></span><br><span class="line">    y_pred_area = np.<span class="built_in">sum</span>((y_pred == i))</span><br><span class="line">    combined_area = y_true_area + y_pred_area</span><br><span class="line">    </span><br><span class="line">    iou = (intersection) / (combined_area - intersection + smoothing_factor)</span><br><span class="line">    class_wise_iou.append(iou)</span><br><span class="line">    </span><br><span class="line">    dice_score =  <span class="number">2</span> * ((intersection) / (combined_area + smoothing_factor))</span><br><span class="line">    class_wise_dice_score.append(dice_score)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> class_wise_iou, class_wise_dice_score</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在上述计算过程中需要注意的是，这里的y_pred是要经过np.argmax()的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results = model.predict(test_dataset, steps=test_steps)</span><br><span class="line"><span class="built_in">print</span>(results.shape) <span class="comment"># (192, 64, 84, 11)</span></span><br><span class="line"></span><br><span class="line">results = np.argmax(results, axis=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">integer_slider = <span class="number">105</span> <span class="comment"># 取第105个图片</span></span><br><span class="line">iou, dice_score = class_wise_metrics(np.argmax(y_true_segments[integer_slider], axis=<span class="number">3</span>), results[integer_slider]) </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>image-segmentation图像分割中的数据读取和处理</title>
    <url>/2022/08/23/image-segmentation%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%92%8C%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<h1 id="用tensorflow.image">用tensorflow.image</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> tensorflow_io <span class="keyword">as</span> tfio</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_image</span>(<span class="params">image_path, mask=<span class="literal">False</span></span>):</span></span><br><span class="line">    image = tf.io.read_file(image_path)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> mask:</span><br><span class="line">        image = tf.image.decode_png(image, channels=<span class="number">1</span>)</span><br><span class="line">        image.set_shape([<span class="literal">None</span>, <span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])</span><br><span class="line">        image = tf.cast(image, tf.int32)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        image = tf.image.decode_png(image, channels=<span class="number">3</span>)</span><br><span class="line">        image.set_shape([<span class="literal">None</span>, <span class="literal">None</span>, <span class="number">3</span>])</span><br><span class="line">        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])</span><br><span class="line">        image = image / <span class="number">255.</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">image_list, mask_list</span>):</span></span><br><span class="line">    image = read_image(image_list)</span><br><span class="line">    mask  = read_image(mask_list, mask=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> image, mask</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_generator</span>(<span class="params">image_list, mask_list, split=<span class="string">&#x27;train&#x27;</span></span>):</span></span><br><span class="line">    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))</span><br><span class="line">    dataset = dataset.shuffle(<span class="number">8</span>*BATCH_SIZE) <span class="keyword">if</span> split == <span class="string">&#x27;train&#x27;</span> <span class="keyword">else</span> dataset </span><br><span class="line">    dataset = dataset.<span class="built_in">map</span>(load_data, num_parallel_calls=tf.data.AUTOTUNE)</span><br><span class="line">    dataset = dataset.batch(BATCH_SIZE, drop_remainder=<span class="literal">True</span>)</span><br><span class="line">    dataset = dataset.prefetch(tf.data.AUTOTUNE)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line">IMAGE_SIZE = <span class="number">128</span></span><br><span class="line">BATCH_SIZE = <span class="number">86</span></span><br><span class="line"></span><br><span class="line">train_dataset = data_generator(images, masks)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Train Dataset:&quot;</span>, train_dataset)</span><br><span class="line"></span><br><span class="line">input_data = os.path.join(root, <span class="string">&#x27;train_images&#x27;</span>)</span><br><span class="line">images = <span class="built_in">sorted</span>(</span><br><span class="line">    [</span><br><span class="line">        os.path.join(input_data, fname)</span><br><span class="line">        <span class="keyword">for</span> fname <span class="keyword">in</span> os.listdir(input_data)</span><br><span class="line">        <span class="keyword">if</span> fname.endswith(exts) <span class="keyword">and</span> <span class="keyword">not</span> fname.startswith(<span class="string">&quot;.&quot;</span>)</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">target_data = os.path.join(root, <span class="string">&#x27;train_masks&#x27;</span>)</span><br><span class="line">masks = <span class="built_in">sorted</span>(</span><br><span class="line">    [</span><br><span class="line">        os.path.join(target_data, fname)</span><br><span class="line">        <span class="keyword">for</span> fname <span class="keyword">in</span> os.listdir(target_data)</span><br><span class="line">        <span class="keyword">if</span> fname.endswith(exts) <span class="keyword">and</span> <span class="keyword">not</span> fname.startswith(<span class="string">&quot;.&quot;</span>)</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of samples:&quot;</span>, <span class="built_in">len</span>(images), <span class="built_in">len</span>(masks))</span><br><span class="line"><span class="keyword">for</span> input_path, target_path <span class="keyword">in</span> <span class="built_in">zip</span>(images[:<span class="number">10</span>], masks[:<span class="number">10</span>]):</span><br><span class="line">    <span class="built_in">print</span>(input_path[-<span class="number">32</span>:], <span class="string">&quot;|&quot;</span>, target_path[-<span class="number">31</span>:], <span class="string">&#x27;|&#x27;</span>, np.unique(cv2.imread(target_path)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以上方式传入data_generator的是image和mask所在图片路径.</p>
<h1 id="用cv2">用cv2</h1>
<p>这种方式适合图片不多的情况下使用，直接读入ndarry里存储。
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">train_img_paths = <span class="built_in">sorted</span>(glob(<span class="string">&#x27;../output/kaggle/working/train/*.jpg&#x27;</span>))[:SAMPLE]</span><br><span class="line">train_mask_paths = <span class="built_in">sorted</span>(glob(<span class="string">&#x27;../output/kaggle/working/train_masks/*.gif&#x27;</span>))[:SAMPLE]</span><br><span class="line"></span><br><span class="line">train_imgs = np.array([cv2.resize(imageio.imread(path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">                        <span class="keyword">for</span> path <span class="keyword">in</span> train_img_paths])</span><br><span class="line"></span><br><span class="line">train_masks = np.array([cv2.resize(imageio.imread(path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">                        <span class="keyword">for</span> path <span class="keyword">in</span> train_mask_paths])</span><br><span class="line"></span><br><span class="line">train_masks = train_masks.astype(np.float32)</span><br><span class="line">train_masks[train_masks&lt;=<span class="number">127</span>] = <span class="number">0.</span></span><br><span class="line">train_masks[train_masks&gt;<span class="number">127</span>] = <span class="number">1.</span></span><br><span class="line">train_masks = np.reshape(train_masks, (*train_masks.shape, <span class="number">1</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h1 id="用generator方式">用generator方式</h1>
<p>推荐用这种方式，占用内存小。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img_paths = <span class="built_in">sorted</span>(glob(<span class="string">&#x27;../output/kaggle/working/train/*.jpg&#x27;</span>))[:<span class="number">500</span>]</span><br><span class="line">mask_paths = <span class="built_in">sorted</span>(glob(<span class="string">&#x27;../output/kaggle/working/train_masks/*.gif&#x27;</span>))[:<span class="number">500</span>]</span><br><span class="line">train_img_files,val_img_files,train_mask_files,val_mask_files = train_test_split(img_paths,mask_paths,test_size=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_mask</span>(<span class="params">masks</span>):</span></span><br><span class="line">    masks[masks&lt;=<span class="number">127</span>] = <span class="number">0.</span></span><br><span class="line">    masks[masks&gt;<span class="number">127</span>] = <span class="number">1.</span></span><br><span class="line">    masks = masks.astype(np.float32)</span><br><span class="line">    masks = np.reshape(masks, (*masks.shape, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> masks</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_img_mask_gen</span>():</span></span><br><span class="line">    <span class="keyword">for</span> img_path,mask_path <span class="keyword">in</span> <span class="built_in">zip</span>(train_img_files,train_mask_files):</span><br><span class="line">        img = cv2.resize(imageio.imread(img_path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">        img = img / <span class="number">127.5</span></span><br><span class="line">        mask = cv2.resize(imageio.imread(mask_path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">        mask = process_mask(mask)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">yield</span> img, mask</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val_img_mask_gen</span>():</span></span><br><span class="line">    <span class="keyword">for</span> img_path,mask_path <span class="keyword">in</span> <span class="built_in">zip</span>(val_img_files,val_mask_files):</span><br><span class="line">        img = cv2.resize(imageio.imread(img_path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">        mask = cv2.resize(imageio.imread(mask_path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">        img = img / <span class="number">127.5</span></span><br><span class="line">        mask = process_mask(mask)</span><br><span class="line">        <span class="keyword">yield</span> img, mask</span><br><span class="line"></span><br><span class="line">train_dataset = tf.data.Dataset.from_generator(train_img_mask_gen,</span><br><span class="line">                                              output_signature=(</span><br><span class="line">                                                    tf.TensorSpec(shape=(IMG_ROWS, IMG_COLS, <span class="number">3</span>), dtype=tf.float32),</span><br><span class="line">                                                    tf.TensorSpec(shape=(IMG_ROWS, IMG_COLS, <span class="number">1</span>), dtype=tf.float32))</span><br><span class="line">                                              )</span><br><span class="line">val_dataset = tf.data.Dataset.from_generator(val_img_mask_gen,</span><br><span class="line">                                             output_signature=(</span><br><span class="line">                                                    tf.TensorSpec(shape=(IMG_ROWS, IMG_COLS, <span class="number">3</span>), dtype=tf.float32),</span><br><span class="line">                                                    tf.TensorSpec(shape=(IMG_ROWS, IMG_COLS, <span class="number">1</span>), dtype=tf.float32))</span><br><span class="line">                                            )</span><br></pre></td></tr></table></figure>
上面这种方式需要创建一个generator函数，该函数不接受参数，如果需要传入参数可以另外新建一个函数，该函数会返回一个不接受任何参数的fun().比如：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img_masks_generator_from_files</span>(<span class="params">img_files, mask_files, sample_weights=<span class="literal">None</span>, width=<span class="number">512</span>, height=<span class="number">512</span>, shuffle=<span class="literal">True</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> sample_weights:</span><br><span class="line">        sample_weights = [[<span class="number">1</span>]] * <span class="built_in">len</span>(img_files) <span class="comment"># Make sample_weights equipped with a broadcastable shape when fed to a tf.Tensor</span></span><br><span class="line">    </span><br><span class="line">    sample_weights = np.array(sample_weights)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(sample_weights.shape) == <span class="number">1</span>:</span><br><span class="line">        sample_weights = sample_weights[..., np.newaxis] <span class="comment"># Make sample_weights equipped with a broadcastable shape when fed to a tf.Tensor</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(<span class="built_in">len</span>(sample_weights.shape) == <span class="number">2</span> <span class="keyword">and</span> sample_weights.shape[-<span class="number">1</span>] == <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        indexes = np.arange(<span class="built_in">len</span>(img_files))</span><br><span class="line">        np.random.shuffle(indexes)</span><br><span class="line">        img_files = [ img_files[i] <span class="keyword">for</span> i <span class="keyword">in</span> indexes ]</span><br><span class="line">        mask_files = [ mask_files[i] <span class="keyword">for</span> i <span class="keyword">in</span> indexes ]</span><br><span class="line">        sample_weights = [ sample_weights[i] <span class="keyword">for</span> i <span class="keyword">in</span> indexes ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f</span>():</span></span><br><span class="line">        <span class="keyword">for</span> img, masks, sample_weight <span class="keyword">in</span> <span class="built_in">zip</span>(img_files, mask_files, sample_weights):</span><br><span class="line">            <span class="keyword">yield</span> img_masks_from_file(img, masks, sample_weight)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> f</span><br><span class="line"></span><br><span class="line">train_img_masks_gen = img_masks_generator_from_files(train_img_files, train_mask_files, train_sample_weights, width=W, height=H)</span><br><span class="line"></span><br><span class="line">train_img_masks_dataset = tf.data.Dataset.from_generator(</span><br><span class="line">    train_img_masks_gen, </span><br><span class="line">    output_signature=(</span><br><span class="line">        tf.TensorSpec(shape=(W, H, <span class="number">1</span>), dtype=tf.float32),</span><br><span class="line">        tf.TensorSpec(shape=(W, H, <span class="built_in">len</span>(train_mask_files[<span class="number">0</span>])), dtype=tf.int32),</span><br><span class="line">        tf.TensorSpec(shape=(<span class="number">1</span>), dtype=tf.float32)</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
上面这种方式还可以换成另一种，在tensorflow.data.Dataset初始化时直接传递args参数:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ds_train = tf.data.Dataset.from_generator(noise_generator,args=[<span class="string">&#x27;train&#x27;</span>, mode],output_types=tf.int32,output_shapes=(<span class="literal">None</span>, <span class="literal">None</span>, n_channels))</span><br></pre></td></tr></table></figure>
<p>官方文档是这样写的：</p>
<blockquote>
<p>(Optional.) A tuple of <a href="https://www.tensorflow.org/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a>
objects that will be evaluated and passed to <code>generator</code> as
NumPy-array arguments.</p>
</blockquote>
<p>关于更多<code>tf.data.Dataset.from_generator</code>的用法可以参考<a href="https://vak.ai/tensorflow/TensorFlow2.0-dataset/">博客</a>。里面有一句话解答了我的疑惑：</p>
<blockquote>
<p>we need to have a python <a href="https://www.programiz.com/python-programming/generator">generator</a>
function which generates <strong>one</strong> training pair needed for
our model.</p>
</blockquote>
<p>这就是说我们在创建<code>generator</code>这个函数的时候，函数返回值应该是一个<code>training pair</code>，也就是<code>X</code>和<code>y</code></p>
<hr>
<p>visualize图片的可视化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span>(<span class="params">**images</span>):</span> <span class="comment"># **images是(key,item)的方式，*images是item list的方式</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;PLot images in one row.&quot;&quot;&quot;</span></span><br><span class="line">    n = <span class="built_in">len</span>(images)</span><br><span class="line">    plt.figure(figsize=(<span class="number">16</span>, <span class="number">5</span>))</span><br><span class="line">    <span class="keyword">for</span> i, (name, image) <span class="keyword">in</span> <span class="built_in">enumerate</span>(images.items()):</span><br><span class="line">        plt.subplot(<span class="number">1</span>, n, i + <span class="number">1</span>)</span><br><span class="line">        plt.xticks([])</span><br><span class="line">        plt.yticks([])</span><br><span class="line">        plt.title(<span class="string">&#x27; &#x27;</span>.join(name.split(<span class="string">&#x27;_&#x27;</span>)).title())</span><br><span class="line">        plt.imshow(image)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">image, mask = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataset.take(<span class="number">1</span>))) <span class="comment"># train_dataset</span></span><br><span class="line"><span class="comment"># image,mask = list(train_dataset.take(1)) </span></span><br><span class="line"><span class="built_in">print</span>(image.shape, mask.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (img, msk) <span class="keyword">in</span> <span class="built_in">zip</span>(image[:<span class="number">5</span>], mask[:<span class="number">5</span>]):</span><br><span class="line">    <span class="built_in">print</span>(mask.numpy().<span class="built_in">min</span>(), mask.numpy().<span class="built_in">max</span>())</span><br><span class="line">    visualize(</span><br><span class="line">        image=img.numpy(),</span><br><span class="line">        gt_mask=msk.numpy(), </span><br><span class="line">    )</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以下是mask标注大于两类（0或者1）的情况下可以观察数据的方式（需要被赋予color）
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Visualization Utilities</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># there are 11 classes in the dataset: one class for each digit (0 to 9) plus the background class</span></span><br><span class="line">n_classes = <span class="number">11</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># assign a random color for each class</span></span><br><span class="line">colors = [<span class="built_in">tuple</span>(np.random.randint(<span class="number">256</span>, size=<span class="number">3</span>) / <span class="number">255.0</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fuse_with_pil</span>(<span class="params">images</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Creates a blank image and pastes input images</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    images (list of numpy arrays) - numpy array representations of the images to paste</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    PIL Image object containing the images</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">  widths = (image.shape[<span class="number">1</span>] <span class="keyword">for</span> image <span class="keyword">in</span> images)</span><br><span class="line">  heights = (image.shape[<span class="number">0</span>] <span class="keyword">for</span> image <span class="keyword">in</span> images)</span><br><span class="line">  total_width = <span class="built_in">sum</span>(widths)</span><br><span class="line">  max_height = <span class="built_in">max</span>(heights)</span><br><span class="line"></span><br><span class="line">  new_im = PIL.Image.new(<span class="string">&#x27;RGB&#x27;</span>, (total_width, max_height))</span><br><span class="line"></span><br><span class="line">  x_offset = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> im <span class="keyword">in</span> images:</span><br><span class="line">    pil_image = PIL.Image.fromarray(np.uint8(im))</span><br><span class="line">    new_im.paste(pil_image, (x_offset,<span class="number">0</span>))</span><br><span class="line">    x_offset += im.shape[<span class="number">1</span>]</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> new_im</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">give_color_to_annotation</span>(<span class="params">annotation</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Converts a 2-D annotation to a numpy array with shape (height, width, 3) where</span></span><br><span class="line"><span class="string">  the third axis represents the color channel. The label values are multiplied by</span></span><br><span class="line"><span class="string">  255 and placed in this axis to give color to the annotation</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    annotation (numpy array) - label map array</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    the annotation array with an additional color channel/axis</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  seg_img = np.zeros( (annotation.shape[<span class="number">0</span>],annotation.shape[<span class="number">1</span>], <span class="number">3</span>) ).astype(<span class="string">&#x27;float&#x27;</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">    segc = (annotation == c)</span><br><span class="line">    seg_img[:,:,<span class="number">0</span>] += segc*( colors[c][<span class="number">0</span>] * <span class="number">255.0</span>)</span><br><span class="line">    seg_img[:,:,<span class="number">1</span>] += segc*( colors[c][<span class="number">1</span>] * <span class="number">255.0</span>)</span><br><span class="line">    seg_img[:,:,<span class="number">2</span>] += segc*( colors[c][<span class="number">2</span>] * <span class="number">255.0</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> seg_img</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_annotation_and_prediction</span>(<span class="params">image, annotation, prediction, iou_list, dice_score_list</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Displays the images with the ground truth and predicted label maps. Also overlays the metrics.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    image (numpy array) -- the input image</span></span><br><span class="line"><span class="string">    annotation (numpy array) -- the ground truth label map</span></span><br><span class="line"><span class="string">    prediction (numpy array) -- the predicted label map</span></span><br><span class="line"><span class="string">    iou_list (list of floats) -- the IOU values for each class</span></span><br><span class="line"><span class="string">    dice_score_list (list of floats) -- the Dice Score for each class</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">  new_ann = np.argmax(annotation, axis=<span class="number">2</span>)</span><br><span class="line">  true_img = give_color_to_annotation(new_ann)</span><br><span class="line">  pred_img = give_color_to_annotation(prediction)</span><br><span class="line"></span><br><span class="line">  image = image + <span class="number">1</span></span><br><span class="line">  image = image * <span class="number">127.5</span></span><br><span class="line">  image = np.reshape(image, (image.shape[<span class="number">0</span>], image.shape[<span class="number">1</span>],))</span><br><span class="line">  image = np.uint8(image)</span><br><span class="line">  images = [image, np.uint8(pred_img), np.uint8(true_img)]</span><br><span class="line"></span><br><span class="line">  metrics_by_id = [(idx, iou, dice_score) <span class="keyword">for</span> idx, (iou, dice_score) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(iou_list, dice_score_list)) <span class="keyword">if</span> iou &gt; <span class="number">0.0</span> <span class="keyword">and</span> idx &lt; <span class="number">10</span>]</span><br><span class="line">  metrics_by_id.sort(key=<span class="keyword">lambda</span> tup: tup[<span class="number">1</span>], reverse=<span class="literal">True</span>)  <span class="comment"># sorts in place</span></span><br><span class="line"></span><br><span class="line">  display_string_list = [<span class="string">&quot;&#123;&#125;: IOU: &#123;&#125; Dice Score: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(idx, iou, dice_score) <span class="keyword">for</span> idx, iou, dice_score <span class="keyword">in</span> metrics_by_id]</span><br><span class="line">  display_string = <span class="string">&quot;\n&quot;</span>.join(display_string_list)</span><br><span class="line"></span><br><span class="line">  plt.figure(figsize=(<span class="number">15</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> idx, im <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">3</span>, idx+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> idx == <span class="number">1</span>:</span><br><span class="line">      plt.xlabel(display_string)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.imshow(im)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_annotation_and_image</span>(<span class="params">image, annotation</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Displays the image and its annotation side by side</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    image (numpy array) -- the input image</span></span><br><span class="line"><span class="string">    annotation (numpy array) -- the label map</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  new_ann = np.argmax(annotation, axis=<span class="number">2</span>)</span><br><span class="line">  seg_img = give_color_to_annotation(new_ann)</span><br><span class="line">  </span><br><span class="line">  image = image + <span class="number">1</span></span><br><span class="line">  image = image * <span class="number">127.5</span></span><br><span class="line">  image = np.reshape(image, (image.shape[<span class="number">0</span>], image.shape[<span class="number">1</span>],))</span><br><span class="line"></span><br><span class="line">  image = np.uint8(image)</span><br><span class="line">  images = [image, seg_img]</span><br><span class="line">  </span><br><span class="line">  images = [image, seg_img]</span><br><span class="line">  fused_img = fuse_with_pil(images)</span><br><span class="line">  plt.imshow(fused_img)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">list_show_annotation</span>(<span class="params">dataset, num_images</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Displays images and its annotations side by side</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    dataset (tf Dataset) -- batch of images and annotations</span></span><br><span class="line"><span class="string">    num_images (int) -- number of images to display</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  ds = dataset.unbatch()</span><br><span class="line"></span><br><span class="line">  plt.figure(figsize=(<span class="number">20</span>, <span class="number">15</span>))</span><br><span class="line">  plt.title(<span class="string">&quot;Images And Annotations&quot;</span>)</span><br><span class="line">  plt.subplots_adjust(bottom=<span class="number">0.1</span>, top=<span class="number">0.9</span>, hspace=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> idx, (image, annotation) <span class="keyword">in</span> <span class="built_in">enumerate</span>(ds.take(num_images)):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">5</span>, idx + <span class="number">1</span>)</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    show_annotation_and_image(image.numpy(), annotation.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># get 10 images from the training set</span></span><br><span class="line">list_show_annotation(training_dataset, <span class="number">10</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h1 id="segmentation中checkpoint的设置">segmentation中checkpoint的设置</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DisplayCallback</span>(<span class="params">tf.keras.callbacks.Callback</span>):</span> <span class="comment"># 每间隔5个spoch显示一次结果</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dataset, epoch_interval=<span class="number">5</span></span>):</span></span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        self.epoch_interval = epoch_interval</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">display</span>(<span class="params">self, display_list, extra_title=<span class="string">&#x27;&#x27;</span></span>):</span></span><br><span class="line">        plt.figure(figsize=(<span class="number">15</span>, <span class="number">15</span>))</span><br><span class="line">        title = [<span class="string">&#x27;Input Image&#x27;</span>, <span class="string">&#x27;True Mask&#x27;</span>, <span class="string">&#x27;Predicted Mask&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(display_list) &gt; <span class="built_in">len</span>(title):</span><br><span class="line">            title.append(extra_title)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(display_list)):</span><br><span class="line">            plt.subplot(<span class="number">1</span>, <span class="built_in">len</span>(display_list), i+<span class="number">1</span>)</span><br><span class="line">            plt.title(title[i])</span><br><span class="line">            plt.imshow(display_list[i])</span><br><span class="line">            plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_mask</span>(<span class="params">self, pred_mask</span>):</span></span><br><span class="line">        pred_mask = (pred_mask &gt; <span class="number">0.5</span>).astype(<span class="string">&quot;int32&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> pred_mask[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show_predictions</span>(<span class="params">self, dataset, num=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="keyword">for</span> image, mask <span class="keyword">in</span> dataset.take(num):</span><br><span class="line">            pred_mask = model.predict(image)</span><br><span class="line">            self.display([image[<span class="number">0</span>], mask[<span class="number">0</span>], self.create_mask(pred_mask)])</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span>(<span class="params">self, epoch, logs=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> epoch <span class="keyword">and</span> epoch % self.epoch_interval == <span class="number">0</span>:</span><br><span class="line">            self.show_predictions(self.dataset)</span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&#x27;\nSample Prediction after epoch &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">30</span></span><br><span class="line">model.fit(</span><br><span class="line">    train_dataset, </span><br><span class="line">    epochs=epochs, </span><br><span class="line">    callbacks=[DisplayCallback(train_dataset)]</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>也需要有常规的checkpoint <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler</span><br><span class="line"></span><br><span class="line">filepath_dice_coeff = <span class="string">&quot;_val_loss.hdf5&quot;</span> <span class="comment"># 保存成一个单独的hdf5文件</span></span><br><span class="line">checkpointer = ModelCheckpoint(filepath_dice_coeff, monitor=<span class="string">&#x27;val_loss&#x27;</span>, verbose=<span class="number">1</span>, save_best_only=<span class="literal">True</span>, mode=<span class="string">&#x27;min&#x27;</span>)<span class="comment"># val_dice_coeff</span></span><br><span class="line">lr_reducer = ReduceLROnPlateau(factor=np.sqrt(<span class="number">0.1</span>), cooldown=<span class="number">0</span>, patience=<span class="number">30</span>, min_lr=<span class="number">0.5e-6</span>)</span><br><span class="line">early_stop = EarlyStopping(monitor=<span class="string">&#x27;val_loss&#x27;</span>, patience=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">hist = seg_classi_model.fit(train_data,</span><br><span class="line">                steps_per_epoch=(train_imgs.shape[<span class="number">0</span>] + batch_size - <span class="number">1</span>) // batch_size,</span><br><span class="line">                epochs=<span class="number">300</span>,</span><br><span class="line">                callbacks=[checkpointer, lr_reducer, early_stop],</span><br><span class="line">                validation_data=val_data,</span><br><span class="line">                validation_steps=(valid_imgs.shape[<span class="number">0</span>] + batch_size - <span class="number">1</span>) // batch_size) </span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>image-segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>llm赋能的全自动Agents</title>
    <url>/2023/07/13/llm%E8%B5%8B%E8%83%BD%E7%9A%84%E5%85%A8%E8%87%AA%E5%8A%A8Agents/</url>
    <content><![CDATA[<p>这篇文章来源于liianwen的<a href="https://lilianweng.github.io/posts/2023-06-23-agent/">blog</a>,初看这篇博客时感觉太多新技术看不懂，再者今天突然看到新智元公众号发了一篇<a href="https://mp.weixin.qq.com/s/6gu_m739yOKhRBl_2rvWhg">文章</a>，乍一看特别熟悉，对比了下确实是完全照搬翻译，让人读起来一头雾水，不仅如此，跟原博客相比缺失了很多内容。</p>
<p>强烈建议先食用<a href="https://blog.salesforceairesearch.com/large-action-models/">blog</a>,
很通俗的讲解了LLM发展到现在成为Agents的原因，这里引用博客中的一句话：</p>
<blockquote>
<p>Recent months have seen the emergence of a powerful new trend in
which large language models are augmented to become “agents”—software
entities capable of performing tasks on their own, ultimately in the
service of a goal, rather than simply responding to queries from human
users</p>
</blockquote>
<p>也就是研究人员已经不满足于让LLM仅仅是根据query回答问题，更希望它能帮我们完成一些任务，成为我们工作生活的“助手”，就好像你有一个秘书一样，你让他去定一个航班，秘书可能会进行一系列的操作，比如他要考量你的时间安排，还要考虑航班的情况等等，你最终就是拿到了秘书给你的机票，但其实秘书在中间做了超级多事情。那我们现在就希望能把LLM培养成这样的角色，他不仅能接受命令还能自己做决策，然后把任务完成了。刚刚提到的博客里还讲了一个购买车的例子。</p>
<p>那我们知道我们的终极目标是要实现一个高级别的私人助理，那么实现这个目标需要哪些技术呢，这时候才到了lilian
wen的这篇博客部分。引言就是现在一些agents的雏形比如autoGPT,
GPT-engineer和BabyAGI出现了。</p>
<p>lilian的博客认为agents是以LLM作为大脑，配置三个主要的components：planing，memory和tool。Planning主要是将复杂任务拆分，不仅如此它还要负责自我反省，吸取以往错误的教训，从而能够产生更好的结果。</p>
<figure>
<img src="https://lilianweng.github.io/posts/2023-06-23-agent/agent-overview.png" alt="overview of a llm-powered autonomous agent system">
<figcaption aria-hidden="true">overview of a llm-powered autonomous
agent system</figcaption>
</figure>
<p>Memory包含短期记忆和长期记忆，前者可以理解成in-context
learning中应用的记忆，后者主要是应用外部的向量数据库或者本地知识库抽取的知识。Tool就是agent可以拥有调用各个外部API的能力，就像你的武器库一样，不同的武器适合不同的作战场景，这些API就可以弥补预训练完的模型所欠缺的能力，比如对于当下实时信息的获取。</p>
<h1 id="component-1-planning">Component 1： Planning</h1>
<p>拆解任务有两种主流办法：1. CoT chain of thought 2. Tree of
Thoughts</p>
<p>前者被讲烂了，后者是对CoT的扩展，将任务拆解成一个子任务树，然后采用宽度优先搜索或者广度优先搜索的方式去决定接下去先解决哪个子任务。</p>
<p>planning这个子模块还有一个更重要的功能就是自我反思，人都是需要从错误中进步的，大语言模型也是一样。思想有点类似于增强学习。首先讲到的是<code>ReAct</code>,
说实话lilian博客里写的这一段我没看懂，所以还是找原文<a href="https://arxiv.org/pdf/2210.03629.pdf">paper</a>来看了下。</p>
<figure>
<img src="/2023/07/13/llm%E8%B5%8B%E8%83%BD%E7%9A%84%E5%85%A8%E8%87%AA%E5%8A%A8Agents/image-20230713145032112.png" alt="comparison results">
<figcaption aria-hidden="true">comparison results</figcaption>
</figure>
<p>从上面的例子就可以理解作者提出的办法就是将thought和action结合起来了，也就是单纯的思考比如chain
of
thought并不能很好的回答问题，受制于预训练模型自己模型内存储的知识，而如果只有action呢？就是不停的去搜索，如果搜索不到正确的答案那也是白搭。其实我理解就是作者提出我们要做一个通用的人工智能，你要告诉他在行动的时候也要思考，思考清楚之后再去考虑下一步已经采取什么样的行动，同时每一次行动也会从环境中得到反馈，比如作者举的第二个例子，你去countertop（台面）的时候，你看到了苹果，面包，胡椒粉瓶子和一个花瓶，既然我们要把胡椒粉瓶子放到抽屉里，那就可以拿走胡椒粉瓶子啦！其实这也好理解，一个优秀的人其实也是要边做边思考的，所以就形成了作者提出的prompt新范式：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Thought: ...</span><br><span class="line">Action: ...</span><br><span class="line">Observation: ...</span><br><span class="line">... (Repeated many times)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://react-lm.github.io/files/diagram.png" alt="frames">
<figcaption aria-hidden="true">frames</figcaption>
</figure>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>linux命令</title>
    <url>/2022/03/15/linux%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h1 id="tar-压缩以及解压">tar 压缩以及解压</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar -czvf test.tar.gz a.txt   //压缩 a.c文件为test.tar.gz</span><br><span class="line">tar -xzvf test.tar.gz // 解压</span><br><span class="line">tar -tzvf test.tar.gz //列出压缩文件的内容</span><br></pre></td></tr></table></figure>
<h1 id="linux中环境变量">linux中环境变量</h1>
<p>参考https://www.cjavapy.com/article/2250/</p>
<h2 id="介绍">1、介绍</h2>
<p>Linux中环境变量包括系统级和用户级</p>
<p>1）系统级</p>
<p><strong>/etc/environment</strong>：系统在登录时读取的第一个文件，用于为所有进程设置环境变量。系统使用此文件时并不是执行此文件中的命令，而是根据<code>KEY=VALUE</code>模式的代码，对<code>KEY</code>赋值以<code>VALUE</code>，因此文件中如果要定义<code>PATH</code>环境变量，只需加入类似如<code>PATH=$PATH:/xxx/bin</code>的代码即可。</p>
<p><strong>/etc/profile</strong>：是系统登录时执行的第二个文件，可以用于设定针对全系统所有用户的环境变量。该文件一般是调用<code>/etc/bash.bashrc</code>文件。</p>
<p><strong>/etc/bash.bashrc</strong>：系统级的<code>bashrc</code>文件，为每一个运行bash
shell的用户执行此文件。此文件会在用户每次打开shell时执行一次。</p>
<p><strong>注意</strong>：<code>/etc/environment</code>是设置整个系统的环境，而<code>/etc/profile</code>是设置所有用户的环境，前者与登录用户无关，后者与登录用户有关。
这两个文件修改后一般都要重启系统才能生效。</p>
<p>2）用户级</p>
<p><strong>~/.profile:</strong>
是对应当前登录用户的<code>profile</code>文件，用于定制当前用户的个人工作环境。</p>
<p>每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次。默认情况下，会设置一些环境变量，执行用户的<code>.bashrc</code>文件。</p>
<p><strong>~/.bashrc</strong>:
是对应当前登录用户的bash初始化文件，当用户每次打开shell时，系统都会执行此文件一次。通常设置环境变量修改这个文件。</p>
<p>上述配置文件执行先后顺序如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/etc/enviroment `–&gt; `/etc/profile` –&gt; `~/.profile` –&gt; `/etc/bash.bashrc `–&gt; `~/.bashrc</span><br></pre></td></tr></table></figure>
<h2 id="环境变量的作用">2、环境变量的作用</h2>
<p>环境变量相当于给系统或用户应用程序设置的一些参数，具体起什么作用这当然和具体的环境变量相关。比如<code>PATH</code>，是告诉系统，当要求系统运行一个程序而没有告诉它程序所在的完整路径时，系统除了在当前目录下面寻找此程序外，还应到哪些目录下去寻找；再如tc或vc++中，<code>set include=path1;path2</code>;
是告诉编译程序到哪里去找.h类型的文件；当然不仅仅是指定什么路径，还有其它的作用的，如<code>set dircmd=/4</code>
设置一个环境变量的作用是在使用dir命令时会把<code>/4</code>作为缺省的参数添加到你的dir命令之后，就像你的每个命令都加了/4参数，它实际上是给命令解释程序<code>command</code>设置的一个环境变量，并且是给<code>dir</code>这个内部命令。</p>
<h2 id="配置环境变量的方法">3、配置环境变量的方法</h2>
<p>1）临时环境变量</p>
<p>linux下设定环境变量时，如果只是临时用一下，可以直接在shell下用<code>set</code>或<code>export</code>命令设定环境变量。但是只能在当前shell环境下可以用，切换或关闭重新进入就会失效。具体配置方法，如下，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#终端输入：</span><br><span class="line">export MYSQLPATH=/home/mysql  #MYSQLPATH设置为该路径</span><br><span class="line">#终端查看一个特定环境变量包含的内容，比如，MYSQLPATH，PATH</span><br><span class="line">echo $PATH</span><br><span class="line">echo $MYSQLPATH</span><br></pre></td></tr></table></figure>
<p>2）永久环境变量</p>
<p>设置的环境变量，需要经常使用的，而不是临时使用，把上面的设置环境变量命令写到上面提到的相应配置文件中即可，则可以每次开机或打开shell时自动设置，</p>
<p><strong>例如，</strong></p>
<p>只需要当前用户生效的环境变量：</p>
<p>终端中输入：<code>sudo vi ~/.bashrc</code>，编辑这个文件，在其末尾添加：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export MYSQLPATH=/home/mysql:$MYSQLPATH</span><br><span class="line"># path采用:来分隔,冒号左右不需要空格.</span><br><span class="line"># :$MYSQLPATH在后面新添加的优先搜索，$MYSQLPATH:在前面说明新添加的最后搜索，不加代表新路径设置为MYSQLPATH路径。</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：在终端执行，<code>source ~/.bashrc</code>
，使其立即生效，或者重启电脑即可。</p>
<p>设置所有用户生效的环境变更：</p>
<p>终端中输入：<code>sudo vi /etc/profile</code>，编辑这个文件，在其末尾添加：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export MYSQLPATH=/home/mysql:$MYSQLPATH</span><br><span class="line"># path采用:来分隔,冒号左右不需要空格.</span><br><span class="line"># :$MYSQLPATH在后面新添加的优先搜索，$MYSQLPATH:在前面说明新添加的最后搜索，不加代表新路径设置为MYSQLPATH路径。</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：在终端执行，<code>source /etc/profile</code>
，使其立即生效，或者重启电脑即可。</p>
<h1 id="查看linux版本信息">查看linux版本信息</h1>
<p><a href="https://blog.csdn.net/lu_embedded/article/details/44350445" class="uri">https://blog.csdn.net/lu_embedded/article/details/44350445</a></p>
<h1 id="查看进程">查看进程</h1>
<p><a href="https://blog.csdn.net/lechengyuyuan/article/details/16337233" class="uri">https://blog.csdn.net/lechengyuyuan/article/details/16337233</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ps -ef # 查看所有本机进程</span><br><span class="line">ps -ef |grep python # 查看python进程</span><br><span class="line"></span><br><span class="line">kill -9 pid # 杀死某个进程</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/li528405176/article/details/83379164" class="uri">https://blog.csdn.net/li528405176/article/details/83379164</a></p>
<h1 id="将程序留在后台运行">将程序留在后台运行</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nohup python -u test.py &gt; out.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>
<p>参考 <a href="https://www.jianshu.com/p/4041c4e6e1b0" class="uri">https://www.jianshu.com/p/4041c4e6e1b0</a></p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>machine learning专项课程notes</title>
    <url>/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/</url>
    <content><![CDATA[<p>本文章整理吴恩达老师在coursera上的machine
learning专项课程的知识点，另外补充记录课堂上没有cover到的一些知识，参考西瓜书以及deep
learning花书。</p>
<h1 id="class-1-supervised-regression-and-classification">class 1
supervised regression and classification</h1>
<h2 id="picking-the-appropriate-learning-rate">picking the appropriate
learning rate:</h2>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230223103443797-16771196850861.png" alt="pick value">
<figcaption aria-hidden="true">pick value</figcaption>
</figure>
<h2 id="logistic-function的loss-function">logistic function的loss
function：</h2>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230223112138369-16771225003742.png" alt="image-20230223112138369">
<figcaption aria-hidden="true">image-20230223112138369</figcaption>
</figure>
<h2 id="bias和variance">bias和variance：</h2>
<p>参考<a href="https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229">博客</a></p>
<p><strong>What is bias?</strong></p>
<p>Bias is the difference between the average prediction of our model
and the correct value which we are trying to predict. Model with high
bias pays very little attention to the training data and oversimplifies
the model. It always leads to high error on training and test data.</p>
<p><strong>What is variance?</strong></p>
<p>Variance is the variability of model prediction for a given data
point or a value which tells us spread of our data. Model with high
variance pays a lot of attention to training data and does not
generalize on the data which it hasn’t seen before. As a result, such
models perform very well on training data but has high error rates on
test data.</p>
<p>总结来说就说，一个模型的bias如果很大的话，说明该模型是underfit，并没有很好的拟合训练数据，也就是training
error比较大；而variance比较大的话，说明该模型把更多的关注力放在训练数据上，但是在它没见过的数据集上表现很差，也就是train_error小，而dev_error大，泛化能力差。</p>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230223115103788-16771242656113.png" alt="bias and variance">
<figcaption aria-hidden="true">bias and variance</figcaption>
</figure>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230223134302857.png" alt="image-20230223134302857">
<figcaption aria-hidden="true">image-20230223134302857</figcaption>
</figure>
<h1 id="class-2-advanced-learning-algorithms">class 2 advanced learning
algorithms</h1>
<h2 id="optimizer">optimizer</h2>
<h3 id="gradient-descent-with-momentum">gradient descent with
momentum</h3>
<p>动量梯度下降法，运行速度要快于标准的梯度下降。基本的想法就是计算梯度的指数加权平均数，并利用该梯度来更新。<a href="https://towardsdatascience.com/gradient-descent-with-momentum-59420f626c8f">博客</a>中阐述这个问题的方式是：</p>
<p>标准的GD只是考虑当下的learning
rate和当下的gradient，并没有考虑前面batch的gradient。这就带来几个问题：</p>
<ol type="1">
<li>在固定learning
rate下，J在一些点上变化缓慢，基本上是处于一个平原区，其实这时候我们希望它的步子要迈的大一点</li>
<li>在一些J变化比较多的地方，我们其实希望步子要小一点</li>
</ol>
<p>那么Momemtum解决了什么问题呢？它可以在J下降不动的时候，让gradient比原来的标准值要大一点，也就是我们间接的把步子迈大了。具体需要用指数加权平均数来实现，因为指数加权平均数很好的考虑了要给予多大的权重给过去的gradient，当然给予最大权重是当前的gradient。</p>
<p><img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/gd%20with%20momentum.png"></p>
<p>以上这个公式就是gradient加了动量momentum的公式，具体的v(t-1)是上一步的weight，而delta(t)是用当前batch计算的梯度。beta是常数，通常取值为0.9</p>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230227134323054-16774766052811.png" alt="image-20230227134323054">
<figcaption aria-hidden="true">image-20230227134323054</figcaption>
</figure>
<h3 id="rmsprop">RMSprop</h3>
<p>rmsprop也可以加速训练过程，而且会降低训练过程中J下降的抖动：</p>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230227141934848-16774787770602.png" alt="image-20230227141934848">
<figcaption aria-hidden="true">image-20230227141934848</figcaption>
</figure>
<h3 id="adam">Adam</h3>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230227143504471-16774797058863.png" alt="image-20230227143504471">
<figcaption aria-hidden="true">image-20230227143504471</figcaption>
</figure>
<p>可以发现Adam其实是结合了RMSprop和momentum，其中beta1缺省值是0.9，beta2的缺省值是0.999.</p>
<p>更深入一点我们还有AdamW，是为了解决adam优化器训练出来的模型比SDG训练出来的模型泛化能力差，这个泛化能力差主要是由于L2正则化也就是weight
decay并没有在Adam中很好的体现，详细可以参考AdamW的<a href="https://arxiv.org/abs/1711.05101">paper：Fixing Weight Decay
Regularization in Adam</a>和<a href="https://towardsdatascience.com/why-adamw-matters-736223f31b5d">Why
AdamW matters</a></p>
<h2 id="metric">metric</h2>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230227155504810-16774845099534.png" alt="precision and recall">
<figcaption aria-hidden="true">precision and recall</figcaption>
</figure>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230227155742925.png" alt="image-20230227155742925">
<figcaption aria-hidden="true">image-20230227155742925</figcaption>
</figure>
<p>如果我们根据sigmoid之后的概率值来进行分类，那么如果我们设定一个很大的概率值，比如说图上的0.7，也就表示说我们只在我们非常自信的时候才会预测一个人得了病，那么必然#total
predicted
positive会变小，所以precision会是一个较大的值；而如果我们采用一个比较小的概率值，比如0.3，也就表示说我们把更多的sample预测成了阳性，那么就代表，我们并不想错过任何一个病历。</p>
<p>我们拥有另一个metric叫做F1
score来衡量一个模型在precision和recall上的共同特征。</p>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230227160513093-16774851219125.png" alt="F1 score">
<figcaption aria-hidden="true">F1 score</figcaption>
</figure>
<h2 id="decision-tree">decision tree</h2>
<p>information
entropy即信息熵是度量样本集合纯度purity最常用的一种指标，决策树就是基于将每一次分类的集合往纯度越高的方向去提升，从而达到分类数据集的目的。</p>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230228101129354-16775503015086.png" alt="信息熵的数学计算">
<figcaption aria-hidden="true">信息熵的数学计算</figcaption>
</figure>
<p>信息熵越大，D的纯度越小，说明数据很混乱。</p>
<p>我们可以用信息增益来进行决策树的划分属性选择，每一次都选择使得划分的信息增益最大。</p>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230228102027466-16775508287787.png" alt="image-20230228102027466">
<figcaption aria-hidden="true">image-20230228102027466</figcaption>
</figure>
<h3 id="decision-tree-ensembles">decision tree ensembles</h3>
<p>解决问题： trees are highly sensitive to small changes of the
data</p>
<p>解决办法： combines several decision trees to produce better
predictive performance than utilizing a single decision tree</p>
<p>有两种技术可以用来实现ensemble decision trees: bagging和boosting</p>
<ol type="1">
<li>bagging(bootstrap aggregation)</li>
</ol>
<blockquote>
<p><em><strong>Bagging*</strong> (Bootstrap Aggregation) is used when
our goal is to reduce the variance of a decision tree. </em>Here idea is
to create several subsets of data from training sample chosen randomly*
*with replacement**. Now, each collection of subset data is used to
train their decision trees. As a result, we end up with an ensemble of
different models. Average of all the predictions from different trees
are used which is more robust than a single decision tree.*</p>
</blockquote>
<p><em>Random Forest</em> is an extension over bagging. It takes one
extra step where in addition to taking the random subset of data, it
also takes the random selection of features rather than using all
features to grow trees. When you have many random trees. It’s called
Random Forest</p>
<ol start="2" type="1">
<li>Boosting</li>
</ol>
<blockquote>
<p><strong>Boosting</strong> <em>is another ensemble technique to create
a collection of predictors. In this technique, learners are learned
sequentially with early learners fitting simple models to the data and
then analyzing data for errors. In other words, we fit consecutive trees
(random sample) and at every step, the goal is to solve for net error
from the prior tree.</em></p>
</blockquote>
<p>参考资料： <a href="https://towardsdatascience.com/decision-tree-ensembles-bagging-and-boosting-266a8ba60fd9">博客</a></p>
<h2 id="conclusion">conclusion</h2>
<p>when to use decision trees?</p>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230228131322629.png" alt="decision trees vs NN">
<figcaption aria-hidden="true">decision trees vs NN</figcaption>
</figure>
<h1 id="class-3-unsupervised-learning-recommenders-reinforcement-learning">class
3 unsupervised learning, recommenders, reinforcement learning</h1>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql常用命令</title>
    <url>/2022/03/17/mysql%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h1 id="select">select</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT * FROM table_name; # 选取所有内容</span><br><span class="line"></span><br><span class="line">SELECT DISTINCT column_name,column_name</span><br><span class="line">FROM table_name; # 一个列可能包含不同的值，该句可以列出不同的值</span><br><span class="line"></span><br><span class="line">SELECT column_name,column_name</span><br><span class="line">FROM table_name</span><br><span class="line">WHERE column_name operator value; # OPERATOR部分可以是: =,&gt;,&lt;,between,in,like</span><br><span class="line"># WHERE后可以跟逻辑符号 and,or,not</span><br></pre></td></tr></table></figure>
<p>mysql语句对大小写不敏感，select == SELECT</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from emp where not sal &gt; 1500;</span><br><span class="line"></span><br><span class="line">Select * from emp where comm is null;</span><br><span class="line"></span><br><span class="line">Select * from emp where sal in (5000,3000,1500);</span><br><span class="line"></span><br><span class="line">SELECT * FROM Websites WHERE alexa &gt; 15 AND (country=&#x27;CN&#x27; OR country=&#x27;USA&#x27;); # AND和OR可以结合使用</span><br></pre></td></tr></table></figure>
<p>Like模糊查询</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Select * from emp where ename like &#x27;M%&#x27;;</span><br></pre></td></tr></table></figure>
<p>查询 EMP 表中 Ename 列中有 M 的值，M 为要查询内容中的模糊信息。</p>
<ul>
<li><strong>%</strong> 表示多个字值，**_** 下划线表示一个字符；</li>
<li><strong>M%</strong> :
为能配符，正则表达式，表示的意思为模糊查询信息为 M 开头的。</li>
<li><strong>%M%</strong> : 表示查询包含M的所有内容。</li>
<li><strong>%M_</strong> : 表示查询以M在倒数第二位的所有内容。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT column_name,column_name</span><br><span class="line">FROM table_name</span><br><span class="line">ORDER BY column_name,column_name ASC|DESC; # 默认是升序</span><br><span class="line"></span><br><span class="line">INSERT INTO table_name (column1,column2,column3,...)</span><br><span class="line">VALUES (value1,value2,value3,...); # 这种方式可以向table_name表格中插入一行，可以只向某一列插入，其他自动</span><br><span class="line"></span><br><span class="line">UPDATE table_name</span><br><span class="line">SET column1=value1,column2=value2,...</span><br><span class="line">WHERE some_column=some_value; # 用于更新某条记录 </span><br><span class="line"># 比如：</span><br><span class="line">UPDATE Websites </span><br><span class="line">SET alexa=&#x27;5000&#x27;, country=&#x27;USA&#x27; </span><br><span class="line">WHERE name=&#x27;菜鸟教程&#x27;;</span><br><span class="line"></span><br><span class="line">DELETE FROM table_name</span><br><span class="line">WHERE some_column=some_value; # 删除某一行记录</span><br></pre></td></tr></table></figure>
<h1 id="高级用法">高级用法</h1>
<h2 id="join">join</h2>
<figure>
<img src="/2022/03/17/mysql%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/sql-join.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># QL INNER JOIN（简单的 JOIN）。 SQL INNER JOIN 从多个表中返回满足 JOIN 条件的所有行。</span><br><span class="line">SELECT Websites.id, Websites.name, access_log.count, access_log.date</span><br><span class="line">FROM Websites</span><br><span class="line">INNER JOIN access_log</span><br><span class="line">ON Websites.id=access_log.site_id;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>INNER JOIN</strong>：如果表中有至少一个匹配，则返回行</li>
<li><strong>LEFT
JOIN</strong>：即使右表中没有匹配，也从左表返回所有的行</li>
<li><strong>RIGHT
JOIN</strong>：即使左表中没有匹配，也从右表返回所有的行</li>
<li><strong>FULL JOIN</strong>：只要其中一个表中存在匹配，则返回行</li>
</ul>
<h2 id="union">union</h2>
<p>UNION 操作符用于合并两个或多个 SELECT 语句的结果集。</p>
<p>请注意，UNION 内部的每个 SELECT
语句必须拥有相同数量的列。列也必须拥有相似的数据类型。同时，每个 SELECT
语句中的列的顺序必须相同。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT column_name(s) FROM table1</span><br><span class="line">UNION</span><br><span class="line">SELECT column_name(s) FROM table2;</span><br></pre></td></tr></table></figure>
<p><strong>注释：</strong>默认地，UNION
操作符选取不同的值。如果允许重复的值，请使用 UNION ALL。</p>
<h3 id="sql-union-all-语法">SQL UNION ALL 语法</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT column_name(s) FROM table1</span><br><span class="line">UNION ALL</span><br><span class="line">SELECT *column_name(s) FROM table2;</span><br></pre></td></tr></table></figure>
<p><strong>注释：</strong>UNION 结果集中的列名总是等于 UNION 中第一个
SELECT 语句中的列名。</p>
<p>前者只会列出选出的值中不同的一些数据条，后者是所有的都返回。</p>
<h2 id="count">Count</h2>
<p>COUNT() 函数返回匹配指定条件的行数。</p>
<hr>
<h3 id="sql-countcolumn_name-语法">SQL COUNT(column_name) 语法</h3>
<p>COUNT(column_name) 函数返回指定列的值的数目（NULL 不计入）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT COUNT(column_name) FROM table_name;</span><br></pre></td></tr></table></figure>
<h3 id="sql-count-语法">SQL COUNT(*) 语法</h3>
<p>COUNT(*) 函数返回表中的记录数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT COUNT(*) FROM table_name;</span><br></pre></td></tr></table></figure>
<h3 id="sql-countdistinct-column_name-语法">SQL COUNT(DISTINCT
column_name) 语法</h3>
<p>COUNT(DISTINCT column_name) 函数返回指定列的不同值的数目：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT COUNT(DISTINCT column_name) FROM table_name;</span><br></pre></td></tr></table></figure>
<p><strong>注释：</strong>COUNT(DISTINCT) 适用于 ORACLE 和 Microsoft SQL
Server，但是无法用于 Microsoft Access。</p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql查看当前使用的配置文件my.cnf的方法</title>
    <url>/2021/11/05/mysql%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8D%E4%BD%BF%E7%94%A8%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6my-cnf%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h1 id="mysql-查看当前使用的配置文件my.cnf的方法">mysql
查看当前使用的配置文件my.cnf的方法</h1>
<p>my.cnf是mysql启动时加载的配置文件，一般会放在mysql的安装目录中，用户也可以放在其他目录加载。</p>
<p>安装mysql后，系统中会有多个my.cnf文件，有些是用于测试的。</p>
<p>使用locate my.cnf命令可以列出所有的my.cnf文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">命令</span><br><span class="line">locate my.cnf</span><br><span class="line"></span><br><span class="line">输出</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/include/default_my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/federated/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/ndb/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/ndb_big/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/ndb_binlog/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/ndb_rpl/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/ndb_team/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/rpl/extension/bhs/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/rpl/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/rpl_ndb/my.cnf</span><br></pre></td></tr></table></figure>
<p>当我们需要修改配置文件时，需要找到mysql启动时是加载了哪个my.cnf文件。</p>
<h3 id="查看是否使用了指定目录的my.cnf">查看是否使用了指定目录的my.cnf</h3>
<p>启动mysql后，我们查看mysql的进程，看看是否有设置使用指定目录的my.cnf文件，如果有则表示mysql启动时是加载了这个配置文件。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">命令</span><br><span class="line">ps aux|grep mysql|grep &#x27;my.cnf&#x27;</span><br><span class="line"></span><br><span class="line">输出</span><br><span class="line">fdipzone         25174   0.0  0.0  3087244    600   ??  S     4:12下午   0:01.14 /usr/local/Cellar/mysql/5.6.24/bin/mysqld --defaults-file=/usr/local/Cellar/mysql/5.6.24/my.cnf --basedir=/usr/local/Cellar/mysql/5.6.24 --datadir=/usr/local/var/mysql --plugin-dir=/usr/local/Cellar/mysql/5.6.24/lib/plugin --bind-address=127.0.0.1 --log-error=/usr/local/var/mysql/TerrydeMacBook-Air.local.err --pid-file=/usr/local/var/mysql/TerrydeMacBook-Air.local.pid</span><br><span class="line">fdipzone         25064   0.0  0.0  2452824      4   ??  S     4:12下午   0:00.03 /bin/sh /usr/local/opt/mysql/bin/mysqld_safe --defaults-file=/usr/local/Cellar/mysql/5.6.24/my.cnf --bind-address=127.0.0.1 --datadir=/usr/local/var/mysql</span><br></pre></td></tr></table></figure>
<p>可以看到/usr/local/Cellar/mysql/5.6.24/my.cnf就是mysql启动加载的配置文件。</p>
<p>如果上面的命令没有输出，表示没有设置使用指定目录的my.cnf。</p>
<h3 id="查看mysql默认读取my.cnf的目录">查看mysql默认读取my.cnf的目录</h3>
<p>如果没有设置使用指定目录的my.cnf，mysql启动时会读取安装目录根目录及默认目录下的my.cnf文件。</p>
<p>查看mysql启动时读取配置文件的默认目录。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">命令</span><br><span class="line">mysql --help|grep &#x27;my.cnf&#x27;</span><br><span class="line"></span><br><span class="line">输出</span><br><span class="line">     order of preference, my.cnf, $MYSQL_TCP_PORT,</span><br><span class="line">/etc/my.cnf /etc/mysql/my.cnf /usr/local/etc/my.cnf ~/.my.cnf</span><br></pre></td></tr></table></figure>
<p>/etc/my.cnf, /etc/mysql/my.cnf, /usr/local/etc/my.cnf, ~/.my.cnf
这些就是mysql默认会搜寻my.cnf的目录，顺序排前的优先。</p>
<h3 id="启动时没有使用配置文件">启动时没有使用配置文件</h3>
<p>如果没有设置使用指定目录my.cnf文件及默认读取目录没有my.cnf文件，表示mysql启动时并没有加载配置文件，而是使用默认配置。</p>
<p>需要修改配置，可以在mysql默认读取的目录中，创建一个my.cnf文件(例如:/etc/my.cnf)，把需要修改的配置内容写入，重启mysql后即可生效。</p>
<h1 id="mysql使用group-by查询报错select-list-is-not-in-group-by-clause-and-contains-nonaggregated-column...解决方案">mysql使用group
by查询报错SELECT list is not in GROUP BY clause and contains
nonaggregated column...解决方案</h1>
<p><strong>MySQL5.7.5后only_full_group_by成为sql_mode的默认选项之一，这可能导致一些sql语句失效。</strong>
比如在使用<strong>group by</strong>进行分组查询报错</p>
<h3 id="查看自己的sql_mode配置">查看自己的sql_mode配置</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">在sql命令行中输入select @@sql_mode;这时我们能够看到自己的sql_mode配置,其中如果有ONLY_FULL_GROUP_BY,那它就是group by查询报错的罪魁祸首了</span><br></pre></td></tr></table></figure>
<h3 id="解决办法">解决办法</h3>
<p>命令行打开mysql.cnf,默认路径为/etc/mysql/conf.d/mysql.cnf,如果找不到可以使用whereis进行查询</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sql_mode = STRICT_TRANS_TABLES, NO_ZERO_IN_DATE, NO_ZERO_DATE, ERROR_FOR_DIVISION_BY_ZERO,      NO_AUTO_CREATE_USER, NO_ENGINE_SUBSTITUTION</span><br></pre></td></tr></table></figure>
<p>保存退出重启mysql</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo service mysqld restart #对于linux是mysql</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>其他技术</tag>
      </tags>
  </entry>
  <entry>
    <title>machine translation相关论文阅读</title>
    <url>/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<p>machine translation 这个任务一般是作为language
modeling的紧接一个话题。它的前身（2010年之前）是statistical machine
translation，但自从Neural machine
translation出来之后，用statistical的方式来做translation就少了很多。有兴趣的可以了解下statistical
machine translation的<a href="https://www.cs.upc.edu/~ageno/anlp/classeMT.pdf">具体细节</a>.
本博客主要记录NMT的主要论文和研究。NMT的架构主要是encoder-decoder架构，它其实是一个很典型的seq-to-seq的模型,
关于它的定义：</p>
<blockquote>
<p>Neural Machine Translation (NMT) is a way to do Machine Translation
with a single end-to-end neural network</p>
</blockquote>
<p>它的一般架构是这样的:</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230313132157791-16786849189061.png" alt="Seq2Seq">
<figcaption aria-hidden="true">Seq2Seq</figcaption>
</figure>
<p>NMT所有的模型都基于一个统一的数学公式：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230313134416540.png" alt="数学公式">
<figcaption aria-hidden="true">数学公式</figcaption>
</figure>
<p>注意这里和statistical machine translation的公式是不一样的：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230313134658535.png" alt="statistical machine translation">
<figcaption aria-hidden="true">statistical machine
translation</figcaption>
</figure>
<p>用统计翻译模型做的时候是分别解决translation model以及language
model的问题，涉及很多特征工程的问题，很复杂。</p>
<p>在machine
translation领域，encoder-decoder架构的模型经历了好几次演变，最终才转化成加入了attention机制，模型架构的整理可以参考<a href="http://arxiv.org/abs/1912.02047">Neural Machine Translation: A
Review and
Survey</a>。文章的第五章介绍了将encoder编码为固定长度的向量的用法。其中有两种使用这个<code>C</code>的用法，1.
作为decoder的初始化state 2.
作为decoder每一个时间步的固定输入和input一起去计算hidden state：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20221214151254068.png" alt="Encoder-decoder architectures with fixed-length sentence encodings">
<figcaption aria-hidden="true">Encoder-decoder architectures with
fixed-length sentence encodings</figcaption>
</figure>
<p>这些文章从<a href="https://arxiv.org/abs/1409.3215">Sequence to
Sequence Learning with Neural Networks</a>，再到<a href="http://arxiv.org/abs/1406.1078">Learning Phrase Representations
using RNN Encoder-Decoder for Statistical Machine Translation</a>.
然后就过度到attention时代了，所以作者在这篇review中只花了很少的第五章节就结束了。第六章就开始讲attentional
encoder-decoder networks。</p>
<blockquote>
<p>The concept of attention is no longer just a technique to improve
sentence lengths in NMT. Since its introduction by Bahdanau et al.
(2015) it has become a vital part of various NMT architectures,
culminating in the Transformer architecture</p>
</blockquote>
<p>这句话是6.1的精髓，attention的概念不再是我们上文所说的那些用于初始化呀，还是用作duplicate
context。Bahdanau 2015年的这篇文章，也就是引入multi-head
attention的这篇文章彻底打破了这个convention。因为我们可以看到transformer的架构中都没有RNN的身影，有的只是attention
weights的计算。</p>
<h1 id="learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation-2014">Learning
Phrase Representations using RNN Encoder-Decoder for Statistical Machine
Translation 2014</h1>
<p>这是在机器翻译领域encoder-decoder架构，在attention
机制提出之前表现最好的RNN模型。其实模型挺简单的，encoder负责将input
sequence编码成了一个固定的向量Context，然后基于这个向量，decoder每一个时间步产生一个单词。在decoder的每一个时间步进行的运算是：</p>
<p><img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20221212164533545.png" alt="image-20221212164533545"> <code>y_t</code>是由s_t得到的。</p>
<p>同样的，这篇文章可以结合<a href="https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb">代码</a>来看，轻易理解。该代码是用pytorch实现的。这个pytorch的实现是从<a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning
with Neural Networks</a>开始讲解的，Learning Phrase Representations
using RNN Encoder-Decoder for Statistical Machine
Translation这篇文章进步在</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20221216102034819.png" alt="image-20221216102034819">
<figcaption aria-hidden="true">image-20221216102034819</figcaption>
</figure>
<p>可以看到该篇文章介绍的模型优势在于预测y的时候加入了context以及<span class="math inline">\(y_{t-1}\)</span>,而不是仅仅依赖于<span class="math inline">\(s_t\)</span></p>
<p>以上的文章都是将input
sentence编码成一个fixed-length的vector，从下面这篇2015年Bahdanau的文章开始，attention就开始用于NMT。为了解决fixed-length
vector的问题，这样我们就不必要将input
sentence的所有信息都编码到一个固定长度的向量里。</p>
<h1 id="neural-machine-translation-by-jointly-learning-to-align-and-translate-2015">Neural
Machine Translation by Jointly Learning to Align and Translate 2015</h1>
<p>从这篇文章开始，attention的机制开始使用在翻译中。</p>
<p>在Introduction章节，最重要的一句话：</p>
<blockquote>
<p>The most important distinguishing feature of this approach from the
basic encoder–decoder is that it does not attempt to encode a whole
input sentence into a single fixed-length vector. Instead, it encodes
the input sentence into a sequence of vectors and chooses a subset of
these vectors adaptively while decoding the translation</p>
</blockquote>
<p>意即跟以往那种encoder-decoder的网络来做translation的model不同，虽然提出的模型也属于encoder-decoder架构，但不是将input
sentence编码成一个固定长度的向量，而是将input
sentence编码成一系列的向量并自适应的从中选择一个小子集的向量用来做decode。</p>
<p>截至文章发表，现有做机器翻译的模型中，表现最好的模型是RNN，内units用lstm。可以称之为RNN
Encoder-Decoder。</p>
<p>还有一个发现是，这些encoder和decoder block，里面基本上是stacked
rnns结构，也就是堆了好几层rnn。这个发现可以追溯到<a href="https://arxiv.org/pdf/1703.03906.pdf">paper</a>.
该作者发现在NMT任务上，high-performing rnns are usually multi-layer,
不仅如此，对于encoder rnn，2到4层是最好的，对于decoder
rnn，4层是最好的。通常情况下，2层堆叠的RNN比一层RNN要lot better;
为了解决long dependency的问题，用lstm
cell是必要的，但这也不够，需要使用一些其他的技术，比如skip-connection，dense-connections。</p>
<hr>
<p>这里值得一提的是，虽然Bahdanau
2015年出的这篇文章很火。但是后来通过学习cs224n和观察tensorflow的文档：<a href="https://www.tensorflow.org/text/tutorials/nmt_with_attention">Neural
machine translation with attention</a>,发现<a href="https://arxiv.org/abs/1508.04025v5">luong
2015</a>的这篇文章中的架构使用的更多，它的计算公式和Bahdanau介绍的有一点点不一样，再luong的文章中我们也可以看到它自己说的和Bahdanau不一样的地方：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230313171420621.png" alt="image-20230313171420621">
<figcaption aria-hidden="true">image-20230313171420621</figcaption>
</figure>
<blockquote>
<p>Comparison to (Bahdanau et al., 2015) – While our global attention
approach is similar in spirit to the model proposed by Bahdanau et al.
(2015), there are several key differences which reflect how we have both
simplified and generalized from the original model. First, we simply use
hidden states at the top LSTM layers in both the encoder and decoder as
illustrated in Figure 2. Bahdanau et al. (2015), on the other hand, use
the concatenation of the forward and backward source hidden states in
the bi-directional encoder and target hidden states in their
non-stacking unidirectional decoder. Second, our computation path is
simpler; we go from ht → at → ct → ̃ ht then make a prediction as
detailed in Eq. (5), Eq. (6), and Figure 2. On the other hand, at any
time t, Bahdanau et al. (2015) build from the previous hidden state ht−1
→ at → ct → ht, which, in turn, goes through a deep-output and a maxout
layer before making predictions.7 Lastly, Bahdanau et al. (2015) only
experimented with one alignment function, the concat product; whereas we
show later that the other alternatives are better.</p>
</blockquote>
<p>所以关于用attention来做machine
translation的模型，我们只需要记住下面的计算过程就行，因为它也不是现在流行的machine
translation的方法（毕竟2015年的时候transformer还没出来）：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230313171823584.png" alt="attention in equations">
<figcaption aria-hidden="true">attention in equations</figcaption>
</figure>
<p>以上的模型给我们解决了标准的seq2seq的模型在做NMT任务时的一些问题：</p>
<ul>
<li>improves NMT performance</li>
<li>provides more "human-like" model: replace the fixed length vector
with dynamic vector according to the decoder hidden states</li>
<li>solves the bottleneck problem: allows decoder to look directly at
source</li>
<li>helps with the vanishing gradient problem</li>
<li>provides some interpretability</li>
</ul>
<p>注意，虽然attention机制首先是在NMT任务中提出并得到了应用，但是它并不是seq2seq的专属，你也可以将attention用在很多architectures和不同的tasks中。有一个关于attention的更general的定义是：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230314154515967.png" alt="general definition of attention">
<figcaption aria-hidden="true">general definition of
attention</figcaption>
</figure>
<p>我们有时候会说： <strong>query attends to the
values</strong>，例如在seq2seq2+attention的模型中，每一个decoder hidden
state就是query，attends to 所有的encoder hidden states(values).</p>
<h1 id="attention-is-all-you-need-2017">Attention is all you need
2017</h1>
<p>在transformer的<a href="https://arxiv.org/abs/1706.03762">paper</a>中，作者首先介绍本文：主流的sequence
tranduction模型主要基于复杂的RNN或者CNN模型，它们包含encoder和decoder两部分，其中表现最好的模型在encoder和decoder之间增加了attention
mechanism。本文提出了一个新的简单的网络结构名叫<em>transformer</em>，也是完全基于attention机制，"dispensing
with recurrence and convolutions entirely"!
根本无需循环和卷积！了不起的Network~</p>
<p>在阅读这篇文章之前需要提前了解我在另外一篇博客 Attention and
transformer
model中的知识，在translation领域我们的科学家们是如何从RNN循环神经网络过渡到CNN，然后最终是transformer的天下的状态。技术经过了一轮轮的迭代，每一种基础模型架构提出后，会不断的有文章提出新的改进，文章千千万，不可能全部读完，就精读一些经典文章就好，Vaswani这篇文章是NMT领域必读paper，文章不长，加上参考文献才12页，介绍部分非常简单，导致这篇文章的入门门槛很高（个人感觉）。我一开始先读的这篇文章，发现啃不下去，又去找了很多资料来看，其中对我非常帮助的有很多：</p>
<ul>
<li><a href="http://jalammar.github.io/illustrated-transformer/">非常通俗易懂的blog</a>
有中文版本的翻译</li>
<li><a href="http://arxiv.org/abs/1912.02047">Neural Machine
Translation: A Review and Survey</a>
虽然这篇paper很长，90+页。前六章可以作为参照，不多25页左右，写的非常好</li>
<li><a href="http://cs231n.stanford.edu/slides/2022/lecture_11_ruohan.pdf">stanford
cs231n课程的ppt</a>
斯坦福这个课程真的很棒，youtube上可以找到17年的视频，17年的课程中没有attention的内容，所以就姑且看看ppt吧，希望斯坦福有朝一日能将最新的课程分享出来，也算是做贡献了</li>
<li>cs231n推荐的阅读<a href="https://lilianweng.github.io/posts/2018-06-24-attention/">博客</a>
非常全面的整理，强烈建议食用.
这位作者也附上了自己的transformer实现，在它参考的那些github实现里，哈佛大学的<a href="http://nlp.seas.harvard.edu/2018/04/01/attention.html">pytorch实现</a>也值得借鉴。</li>
</ul>
<p>Transformer这篇文章有几个主要的创新点：</p>
<ol type="1">
<li>使用self-attention机制，并首次提出使用multi-head attention</li>
</ol>
<p>该机制作用是在编码当前word的时候，这个self-attention就会告诉我们编码这个词语我们应该放多少注意力在这个句子中其他的词语身上，说白了其实就是计算当前词语和其他词语的关系。这也是CNN用于解决NMT问题时用不同width的kernel来扫input
metric的原因。</p>
<p>multi-head的意思是我使用多个不同的self-attention
layer来处理我们的输入，直观感觉是训练的参数更多了，模型的表现力自然要好一点。</p>
<ol start="2" type="1">
<li>Positional embeddings</li>
</ol>
<p>前一个创新点解决了dependence的问题，那如何解决位置的问题呢？也就是我这个词在编码的时候或者解码的时候应该放置在句子的哪个位置上。文章就用pisitional
embedding来解决这个问题。这个positional embedding和input
embedding拥有相同的shape，所以两者可以直接相加。transformer这篇文章提供了两种encoding方式：</p>
<p>1） sunusoidal positional encoding</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230302133247664.png" alt="image-20230302133247664">
<figcaption aria-hidden="true">image-20230302133247664</figcaption>
</figure>
<p>其中，pos=1,...,L(L是input句子的长度)，i是某一个PE中的一个维度，取值范围是1到dmodel。python实现为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span>(<span class="params">length, depth</span>):</span></span><br><span class="line">  depth = depth/<span class="number">2</span></span><br><span class="line"></span><br><span class="line">  positions = np.arange(length)[:, np.newaxis]     <span class="comment"># (seq, 1)</span></span><br><span class="line">  depths = np.arange(depth)[np.newaxis, :]/depth   <span class="comment"># (1, depth)</span></span><br><span class="line"></span><br><span class="line">  angle_rates = <span class="number">1</span> / (<span class="number">10000</span>**depths)         <span class="comment"># (1, depth)</span></span><br><span class="line">  angle_rads = positions * angle_rates      <span class="comment"># (pos, depth)</span></span><br><span class="line"></span><br><span class="line">  pos_encoding = np.concatenate(</span><br><span class="line">      [np.sin(angle_rads), np.cos(angle_rads)],</span><br><span class="line">      axis=-<span class="number">1</span>) </span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> tf.cast(pos_encoding, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">pos_encoding = positional_encoding(length=<span class="number">2048</span>, depth=<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the shape.</span></span><br><span class="line"><span class="built_in">print</span>(pos_encoding.shape) <span class="comment"># (2014,512)</span></span><br></pre></td></tr></table></figure>
<p>2） learned positional encoding</p>
<p>整体上看，这篇文章提出的transformer模型在做translation的任务时，架构是这样的：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230301133249379.png" alt="image-20230301133249379">
<figcaption aria-hidden="true">image-20230301133249379</figcaption>
</figure>
<p>其中encoders部分包含了6个encoders的block，decoders部分也包含了6个decoders的block，将encoders的每一个block拆开来看，有两个sub
layer：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230301133424302.png" alt="image-20230301133424302">
<figcaption aria-hidden="true">image-20230301133424302</figcaption>
</figure>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230301133440152.png" alt="image-20230301133440152">
<figcaption aria-hidden="true">image-20230301133440152</figcaption>
</figure>
<p>其中decoder部分的block比encoder部分的block多了一个sub
layer，其中self-attention和encoder-decoder attention都是multi-head
attention layer，只不过decoder部分的第一个multi-head attention
layer是一个masked multi-head
attention，为了防止未来的信息泄露给当下（prevent positions from
attending to the future）.</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230301133608484.png" alt="image-20230301133608484">
<figcaption aria-hidden="true">image-20230301133608484</figcaption>
</figure>
<p>在transformer模型中，作者还使用了residual
connection，所以在encoder的每一个block中，数据的flow是:</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230307162830473-16781777124571.png" alt="transformer架构">
<figcaption aria-hidden="true">transformer架构</figcaption>
</figure>
<p>其中self-attention中涉及的运算details是：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230301134258180.png" alt="image-20230301134258180">
<figcaption aria-hidden="true">image-20230301134258180</figcaption>
</figure>
<p>可以发现其中涉及的运算都是矩阵的点乘，并没有RNN中那种时间步的概念，所以所有运算都是可以parallelizable，这就能使得模型的推理和训练更加的efficient。并且！Transformers也可以抓住distant的依赖，而不是像rnn那样对于长依赖并不是很擅长，因为它前面的信息如果像传递到很后面的单词推理上，需要经历很多时间步的计算，而transformer在推理每一个单词的时候都可以access到input句子中的每一个单词（毕竟我们的Z中包含了每一个单词跟其他单词的关系)。</p>
<p>其中positional encoding现在可以简单的理解成在我们编码的word
embedding上我们又加了一个positional
encoding，维度和我们的embedding一模一样。</p>
<blockquote>
<p>在tensorflow中有一个layer是<code>MultiHeadAttention</code>,如果我们想实现transformer里的这个self-attention，那就是query，key，value其实都是由input
vector计算来的。</p>
</blockquote>
<p>以上的理论计算看起来可能会有点模糊，可以同步参照<a href="https://lilianweng.github.io/posts/2018-06-24-attention/">博客</a>
参考 <a href="http://jalammar.github.io/illustrated-transformer/">illustrated
transformer</a>介绍的详细细节，基于tensorflow框架实现的<a href="https://github.com/lilianweng/transformer-tensorflow/blob/master/transformer.py">transformer</a>来帮助自己理解transformer模型。</p>
<h2 id="encoder部分">encoder部分</h2>
<p>encoder的每一个block由两个sub-layer组成，中间穿插resnet
connection。</p>
<p><img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/transformer_encoder_block.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multihead_attention</span>(<span class="params">self, query, memory=<span class="literal">None</span>, mask=<span class="literal">None</span>, scope=<span class="string">&#x27;attn&#x27;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            query (tf.tensor): of shape (batch, q_size, d_model)</span></span><br><span class="line"><span class="string">            memory (tf.tensor): of shape (batch, m_size, d_model)</span></span><br><span class="line"><span class="string">            mask (tf.tensor): shape (batch, q_size, k_size)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:h</span></span><br><span class="line"><span class="string">            a tensor of shape (bs, q_size, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> memory <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment"># 如果memory是None，那么就是一个典型的self-attention layer</span></span><br><span class="line">            memory = query</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># Linear project to d_model dimension: [batch, q_size/k_size, d_model]</span></span><br><span class="line">            Q = tf.layers.dense(query, self.d_model, activation=tf.nn.relu)</span><br><span class="line">            K = tf.layers.dense(memory, self.d_model, activation=tf.nn.relu)</span><br><span class="line">            V = tf.layers.dense(memory, self.d_model, activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Split the matrix to multiple heads and then concatenate to have a larger</span></span><br><span class="line">            <span class="comment"># batch size: [h*batch, q_size/k_size, d_model/num_heads]</span></span><br><span class="line">            Q_split = tf.concat(tf.split(Q, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            K_split = tf.concat(tf.split(K, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            V_split = tf.concat(tf.split(V, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            mask_split = tf.tile(mask, [self.h, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Apply scaled dot product attention</span></span><br><span class="line">            out = self.scaled_dot_product_attention(Q_split, K_split, V_split, mask=mask_split)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Merge the multi-head back to the original shape</span></span><br><span class="line">            out = tf.concat(tf.split(out, self.h, axis=<span class="number">0</span>), axis=<span class="number">2</span>)  <span class="comment"># [bs, q_size, d_model]</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># The final linear layer and dropout.</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_model)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dropout(out, rate=self.drop_rate, training=self._is_training)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_forwad</span>(<span class="params">self, inp, scope=<span class="string">&#x27;ff&#x27;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Position-wise fully connected feed-forward network, applied to each position</span></span><br><span class="line"><span class="string">        separately and identically. It can be implemented as (linear + ReLU + linear) or</span></span><br><span class="line"><span class="string">        (conv1d + ReLU + conv1d).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            inp (tf.tensor): shape [batch, length, d_model]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        out = inp</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_ff, activation=tf.nn.relu)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dropout(out, rate=self.drop_rate, training=self._is_training)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_model, activation=None)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># by default, use_bias=True</span></span><br><span class="line">            out = tf.layers.conv1d(out, filters=self.d_ff, kernel_size=<span class="number">1</span>, activation=tf.nn.relu)</span><br><span class="line">            out = tf.layers.conv1d(out, filters=self.d_model, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out  </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encoder_layer</span>(<span class="params">self, inp, input_mask, scope</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            inp: tf.tensor of shape (batch, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            input_mask: tf.tensor of shape (batch, seq_len, seq_len)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        out = inp</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># One multi-head attention + one feed-forword</span></span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(out, mask=input_mask))</span><br><span class="line">            out = self.layer_norm(out + self.feed_forwad(out))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h2 id="decoder部分">decoder部分</h2>
<p><img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/encoder-decoder.png"></p>
<p>在decoder部分，我们可以看到每一个decoder
block的输入有两个：<em>整个encoder部分的输出</em>以及<em>上一个decoder
block的输出（第一个decoder
block是词向量的输入）</em>，而encoder部分的输出是接到每一个decoder
block的第二个sublayer的。正如刚刚提到了，decoder部分的每一个block跟encoder部分的block有一个不一样的地方，那就是多了一个sublayer：
encoder-decoder
attention。至于encoder部分和decoder部分是如何connect的，</p>
<blockquote>
<p>The encoder start by processing the input sequence. The output of the
top encoder is then transformed into a set of attention vectors K and V.
These are to be used by each decoder in its “encoder-decoder attention”
layer which helps the decoder focus on appropriate places in the input
sequence</p>
</blockquote>
<p>也就是我们得到了encoder部分top layer（最后一个encoder
layer）的输出之后，我们将输出转化成K和V.
我们可以看到在<code>multihead_attention</code>里，memory是<code>enc_out</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder_layer</span>(<span class="params">self, target, enc_out, input_mask, target_mask, scope</span>):</span></span><br><span class="line">        out = target</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(</span><br><span class="line">                out, mask=target_mask, scope=<span class="string">&#x27;self_attn&#x27;</span>))</span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(</span><br><span class="line">                out, memory=enc_out, mask=input_mask)) <span class="comment"># 将encoder部分的输出结果作为输入</span></span><br><span class="line">            out = self.layer_norm(out + self.feed_forwad(out))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder</span>(<span class="params">self, target, enc_out, input_mask, target_mask, scope=<span class="string">&#x27;decoder&#x27;</span></span>):</span></span><br><span class="line">        out = target</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_enc_layers):</span><br><span class="line">                out = self.decoder_layer(out, enc_out, input_mask, target_mask, <span class="string">f&#x27;dec_<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>以上实现的transformer其实我觉得还是有一点点复杂，毕竟在tensorflow2.0+版本中已经有了官方实现好的<code>layers.MultiHeadAttention</code>可以使用，应该可以大大简化我们实现步骤，特别是上面的<code>def multihead_attention(self, query, memory=None, mask=None, scope='attn'):</code>。从刚刚的实现里我们可以发现，除了decoder部分每一个block的第二个sublayer的attention计算有一点不一样之外，其他的attention计算都是一模一样的。我在github上找了不少用TF2.0实现的transformer（最标准的也是Attention
is all you
need的模型），发现很多都都写得一般般，最终发现还是tensorflow官方文档写的<a href="https://www.tensorflow.org/text/tutorials/transformer#add_and_normalize">tutotial</a>写的最好.</p>
<p>现在对照tensorflow的tutorial以及上面transformer的计算过程，拆解一下官方给的代码。</p>
<p>首先定义一个baseAttention类，然后在此基础上我们再定义encoder和decoder中的attention：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseAttention</span>(<span class="params">tf.keras.layers.Layer</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)</span><br><span class="line">    self.layernorm = tf.keras.layers.LayerNormalization()</span><br><span class="line">    self.add = tf.keras.layers.Add()</span><br></pre></td></tr></table></figure>
<p>那么针对encoder结果输入到decoder的cross attention
layer怎么处理呢？这时候我们使用MultiHeadAttention时就需要将target
sequence x当作是query，将encoder输出当作是context
sequence也就是key/value。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossAttention</span>(<span class="params">BaseAttention</span>):</span> <span class="comment"># encoder结果输入到decoder的层</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x, context</span>):</span> <span class="comment"># 这里的x是target sequence,context是encoder的输出结果</span></span><br><span class="line">    attn_output, attn_scores = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        key=context,</span><br><span class="line">        value=context,</span><br><span class="line">        return_attention_scores=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Cache the attention scores for plotting later.</span></span><br><span class="line">    self.last_attn_scores = attn_scores</span><br><span class="line"></span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>然后我们再定义global attention，global
attention就是没有任何特殊操作的（比如上面的attention计算它有特别的context），而在transformor中更多的是self-attention，也就是我们传递给MultiHeadAttention的query,key,value都是同一个值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GlobalSelfAttention</span>(<span class="params">BaseAttention</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    attn_output = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        value=x,</span><br><span class="line">        key=x)</span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>最后我们定义causal self attention
layer，这个是在decoder的每一个block的第一个sublayer：self-attention
layer.其实这个layer是和global attention
layer差不多的，但还是有一点微小的差别。为什么呢？因为我们在decoder阶段，我们是一个词语一个词语的预测的，这其实包含了一层因果关系，我们在预测一个词语的时候，我们应该已知它前面一个词语是什么，RNN中的hidden
state传递到下一个时间步就是这个因果关系的传递。那么如果我们使用刚刚我们实现的global
attention layer来实现这个self
attention，并没有包含这个因果关系，不仅如此，如果我们使用常规的self
attention的计算，将target
sequence全部当作输入输入到decoder中的第一个block中，会有未来的数据提前被当前时刻看到的风险，所以在Transformer这篇文章中，作者提出使用mask的技术来避免这个问题。</p>
<p>在tensorflow中实现很简单，就只需要给MultiHeadAttention传递一个use_causal_mask
= True的参数即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CausalSelfAttention</span>(<span class="params">BaseAttention</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    attn_output = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        value=x,</span><br><span class="line">        key=x,</span><br><span class="line">        use_causal_mask = <span class="literal">True</span>) <span class="comment"># The causal mask ensures that each location only has access to the locations that come before it</span></span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这样就可以保证先前的sequence并不依赖于之后的elements。这里我本来有一个疑问是，这样一来这个causal
layer并不能实现bi-rnn的能力？但后来一想并不是，因为双向的RNN的后向是指后面的词语先输入，其实就是从后往前输入，这样就可以知道一个sequence当前词语依赖于后面的词语的权重。</p>
<h2 id="补充介绍">补充介绍</h2>
<h3 id="tf.keras.layers.multiheadattention">tf.keras.layers.MultiHeadAttention</h3>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention">doc</a></p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230309150541287.png" alt="image-20230309150541287">
<figcaption aria-hidden="true">image-20230309150541287</figcaption>
</figure>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230309150600806.png" alt="image-20230309150600806">
<figcaption aria-hidden="true">image-20230309150600806</figcaption>
</figure>
<p>注意，return的结果包含两个，其中attention_output的shape的第二维是和target
sequence的长度是一致的，并且E是和query的最后一维是一致的。</p>
<h1 id="attention-family">Attention Family</h1>
<p>这个章节整理于<a href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/">blog</a>，这个作者之前写了一篇介绍attention的文章，后面在2023年一月的时候又更新了两篇博客，详细介绍了从2020年以来出现的新的Transformer
models。权当自己学习记录一些我还需要补充的知识。</p>
<blockquote>
<p>The <strong>Transformer</strong> (which will be referred to as
“vanilla Transformer” to distinguish it from other enhanced versions; <a href="https://arxiv.org/abs/1706.03762">Vaswani, et al., 2017</a>) model
has an encoder-decoder architecture, as commonly used in many <a href="https://lilianweng.github.io/posts/2018-06-24-attention/#born-for-translation">NMT</a>
models. Later simplified Transformer was shown to achieve great
performance in language modeling tasks, like in encoder-only <a href="https://lilianweng.github.io/posts/2019-01-31-lm/#bert">BERT</a>
or decoder-only <a href="https://lilianweng.github.io/posts/2019-01-31-lm/#openai-gpt">GPT</a>.</p>
</blockquote>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>numpy基础总结</title>
    <url>/2022/03/27/numpy%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="numpy.linalg.norm">numpy.linalg.norm</h1>
<p><code>linalg.norm(x,ord=None,axis=None,keepdims=False)</code></p>
<p>其中x可以传入array，也可以是matrix。</p>
<p>ord参数针对x是array还是matrix是不一样的计算：</p>
<table>
<thead>
<tr class="header">
<th>ord</th>
<th>norm for matrices</th>
<th>norm for vectors</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>None</td>
<td>Frobenius norm</td>
<td>2-norm</td>
</tr>
<tr class="even">
<td>‘fro’</td>
<td>Frobenius norm</td>
<td>–</td>
</tr>
<tr class="odd">
<td>‘nuc’</td>
<td>nuclear norm</td>
<td>–</td>
</tr>
<tr class="even">
<td>inf</td>
<td>max(sum(abs(x), axis=1))</td>
<td>max(abs(x))</td>
</tr>
<tr class="odd">
<td>-inf</td>
<td>min(sum(abs(x), axis=1))</td>
<td>min(abs(x))</td>
</tr>
<tr class="even">
<td>0</td>
<td>–</td>
<td>sum(x != 0)</td>
</tr>
<tr class="odd">
<td>1</td>
<td>max(sum(abs(x), axis=0))</td>
<td>as below</td>
</tr>
<tr class="even">
<td>-1</td>
<td>min(sum(abs(x), axis=0))</td>
<td>as below</td>
</tr>
<tr class="odd">
<td>2</td>
<td>2-norm (largest sing. value)</td>
<td>L2范数</td>
</tr>
<tr class="even">
<td>-2</td>
<td>smallest singular value</td>
<td>as below</td>
</tr>
<tr class="odd">
<td>other</td>
<td>–</td>
<td>sum(abs(x)<strong>ord)</strong>(1./ord)</td>
</tr>
</tbody>
</table>
<p>如果传入axis这个参数，axis=0表示每一列为向量，以每一列的向量为基础计算。然后就转化为了向量运算。</p>
<p>其中keepdims如果是True，那么规范化的轴将作为尺寸1留在结果中，使用此选项，结果将针对原始
x 正确广播。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.arange(<span class="number">9</span>) - <span class="number">4</span></span><br><span class="line">b = a.reshape((<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">a</span><br><span class="line">Out[<span class="number">7</span>]: array([-<span class="number">4</span>, -<span class="number">3</span>, -<span class="number">2</span>, -<span class="number">1</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>])</span><br><span class="line">b</span><br><span class="line">Out[<span class="number">8</span>]: </span><br><span class="line">array([[-<span class="number">4</span>, -<span class="number">3</span>, -<span class="number">2</span>],</span><br><span class="line">       [-<span class="number">1</span>,  <span class="number">0</span>,  <span class="number">1</span>],</span><br><span class="line">       [ <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line">LA.norm(b,axis=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">9</span>]: array([<span class="number">5.38516481</span>, <span class="number">1.41421356</span>, <span class="number">5.38516481</span>])</span><br><span class="line">LA.norm(b,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">Out[<span class="number">10</span>]: </span><br><span class="line">array([[<span class="number">5.38516481</span>],</span><br><span class="line">       [<span class="number">1.41421356</span>],</span><br><span class="line">       [<span class="number">5.38516481</span>]])</span><br></pre></td></tr></table></figure>
<p>从以上结果看出：如果keepdims等于True，那么输出的矩阵将会是3乘以1的矩阵，输入矩阵是3乘以3。如果用Input/
norm(input)是不会报错的。</p>
]]></content>
      <categories>
        <category>numpy</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title>pandas中对于时间TimeStamp的处理</title>
    <url>/2021/12/14/pandas%E4%B8%AD%E5%AF%B9%E4%BA%8E%E6%97%B6%E9%97%B4TimeStamp%E7%9A%84%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<h1 id="pandas.to_datetime"><code>pandas.to_datetime()</code></h1>
<p><strong>pandas.to_datetime(</strong><em>arg</em><strong>,</strong>
<em>errors='raise'</em><strong>,</strong>
<em>dayfirst=False</em><strong>,</strong>
<em>yearfirst=False</em><strong>,</strong>
<em>utc=None</em><strong>,</strong>
<em>format=None</em><strong>,</strong>
<em>exact=True</em><strong>,</strong>
<em>unit=None</em><strong>,</strong>
<em>infer_datetime_format=False</em><strong>,</strong>
<em>origin='unix'</em><strong>,</strong>
<em>cache=True</em><strong>)</strong>[<a href="https://github.com/pandas-dev/pandas/blob/v1.3.5/pandas/core/tools/datetimes.py#L676-L919">source]</a><a href="https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html?highlight=to_datetime#pandas.to_datetime">¶</a></p>
<p>Parameters</p>
<ul>
<li><p><strong>arg</strong> int, float, str, datetime, list, tuple, 1-d
array, Series, DataFrame/dict-like</p>
<p>The object to convert to a datetime</p></li>
<li><p><strong>format</strong> str, default None</p></li>
</ul>
<p>The strftime to parse time, eg “<code>%d/%m/%Y</code>”, note that
“<code>%f</code>” will parse all the way up to nanoseconds.</p>
<hr>
<p>该函数可以接受一个series，可以接受一个dateFrame。如果不确定它是否可以以默认的格式去解析你的时间，format参数可以不传递。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;year&#x27;</span>: [<span class="number">2015</span>, <span class="number">2016</span>],</span><br><span class="line"><span class="meta">... </span>                   <span class="string">&#x27;month&#x27;</span>: [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line"><span class="meta">... </span>                   <span class="string">&#x27;day&#x27;</span>: [<span class="number">4</span>, <span class="number">5</span>]&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.to_datetime(df)</span><br><span class="line"><span class="number">0</span>   <span class="number">2015</span>-02-04</span><br><span class="line"><span class="number">1</span>   <span class="number">2016</span>-03-05</span><br><span class="line">dtype: datetime64[ns]</span><br></pre></td></tr></table></figure>
<p>上面的这种产生<code>datetime</code>的方式，在创建dateframe的时候，可以指定缩写或者缩写的复数形式，其他形式不接受：<code>[‘year’, ‘month’, ‘day’, ‘minute’, ‘second’, ‘ms’, ‘us’, ‘ns’])</code></p>
<h1 id="pandas.to_timedelta"><code>pandas.to_timedelta()</code></h1>
<p>timedelta是两个时间之间的差值，该函数可以帮助我们求两个<code>timestamp</code>之间的差是多少（单位可以是<code>days,hours,minutes,seconds</code>）</p>
<p>Parameters</p>
<ul>
<li><p><strong>arg</strong> str, timedelta, list-like or Series</p></li>
<li><p><strong>unit</strong> str, optional</p>
<p>Denotes the unit of the arg for numeric arg. Defaults to
<code>"ns"</code>.</p>
<p>Possible values:</p>
<ul>
<li>‘W’</li>
<li>‘D’ / ‘days’ / ‘day’</li>
<li>‘hours’ / ‘hour’ / ‘hr’ / ‘h’</li>
<li>‘m’ / ‘minute’ / ‘min’ / ‘minutes’ / ‘T’</li>
<li>‘S’ / ‘seconds’ / ‘sec’ / ‘second’</li>
<li>‘ms’ / ‘milliseconds’ / ‘millisecond’ / ‘milli’ / ‘millis’ /
‘L’</li>
<li>‘us’ / ‘microseconds’ / ‘microsecond’ / ‘micro’ / ‘micros’ /
‘U’</li>
<li>‘ns’ / ‘nanoseconds’ / ‘nano’ / ‘nanos’ / ‘nanosecond’ / ‘N’</li>
</ul></li>
</ul>
<hr>
<p>这里如果传入的是str，是不允许再传入unit参数了，不然会报错。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; pd.to_timedelta(<span class="string">&#x27;15days 2hours&#x27;</span>)</span><br><span class="line">Timedelta(<span class="string">&#x27;15 days 02:00:00&#x27;</span>)</span><br><span class="line"></span><br><span class="line">&gt;&gt; pd.to_timedelta(<span class="string">&#x27;1 days 06:05:01.00003&#x27;</span>)</span><br><span class="line">Timedelta(<span class="string">&#x27;1 days 06:05:01.000030&#x27;</span>)</span><br><span class="line"></span><br><span class="line">&gt;&gt; pd.to_timedelta(<span class="number">4</span>,unit=<span class="string">&#x27;days&#x27;</span>)</span><br><span class="line">Timedelta(<span class="string">&#x27;4 days 00:00:00&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="series.dt"><code>Series.dt()</code></h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; seconds_series = pd.Series(pd.date_range(<span class="string">&quot;2000-01-01&quot;</span>, periods=<span class="number">3</span>, freq=<span class="string">&quot;s&quot;</span>))</span><br><span class="line">seconds_series</span><br><span class="line"><span class="number">0</span>   <span class="number">2000</span>-01-01 <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line"><span class="number">1</span>   <span class="number">2000</span>-01-01 <span class="number">00</span>:<span class="number">00</span>:01</span><br><span class="line"><span class="number">2</span>   <span class="number">2000</span>-01-01 <span class="number">00</span>:<span class="number">00</span>:02</span><br><span class="line">dtype: datetime64[ns]</span><br><span class="line">&gt;&gt; seconds_series.dt.second</span><br><span class="line"><span class="number">0</span>    <span class="number">0</span></span><br><span class="line"><span class="number">1</span>    <span class="number">1</span></span><br><span class="line"><span class="number">2</span>    <span class="number">2</span></span><br><span class="line">dtype: int64</span><br></pre></td></tr></table></figure>
<p>dt是Series的一个方法，当调用dt时，Series中必须是timestamp的格式。</p>
<p>当调用完dt后可以获取时间的具体年份等信息：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">seconds_series.dt.date <span class="comment"># 2000-01-01</span></span><br><span class="line">seconds_series.dt.hour <span class="comment"># 00</span></span><br><span class="line">seconds_series.dt.quarter <span class="comment"># 返回第几季度</span></span><br><span class="line">seconds_series.dt.time <span class="comment"># 00:00:00</span></span><br><span class="line">seconds_series.dt.year <span class="comment"># 2000</span></span><br><span class="line">seconds_series.dt.month <span class="comment"># 01</span></span><br><span class="line">seconds_series.dt.day <span class="comment"># 01</span></span><br><span class="line">seconds_series.dt.weekday <span class="comment"># 返回一个0-6的数，0表示周一，6表示周日</span></span><br><span class="line">seconds_series.dt.dayname() <span class="comment"># 会返回星期的名字：Monday</span></span><br></pre></td></tr></table></figure>
<p>有的时候我们想获取某一天是全年中的第几周，这时候weekday就不管用了，此时采用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dseries.dt.isocalendar()[<span class="string">&#x27;week&#x27;</span>] </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>pandas中count(),value_counts(),unique)区别</title>
    <url>/2021/12/13/pandas%E4%B8%ADcount-value-counts-unique-%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<h1 id="count">count()</h1>
<p>不管是Series还是DateFrame，两个类都有这个方法。</p>
<p>对于DateFrame，返回的是每一列中非空的元素的个数，可以指定是以行去统计还是以列去统计。</p>
<p>对于Series，返回的是Series中非空元素的个数。</p>
<p>对于groupby，返回的是每一组中元素的个数，这里的个数不包含空字段。</p>
<h1 id="value_counts">value_counts()</h1>
<p>对于DateFrame，返回的是unique行的个数，这里的unique行指的是只要其中一个column的值不一样就算不一样的行。</p>
<p>对于Series，返回的是每一个unique元素的个数，也就是每一个unique的元素，它会统计它在Series中出现了多少次。</p>
<h1 id="unique">unique()</h1>
<p>对于Series，返回的是所有unique的元素们，如果你想统计所有不一样的元素有多少个，可以统计list中元素的个数，如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">len(df[&#x27;a&#x27;].unique().tolist())  </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>prompt engineering</title>
    <url>/2023/05/19/prompt-engineering/</url>
    <content><![CDATA[<p>在LLM兴起之后，很多工作都在如果更好的设计prompt的形式上下了很多苦功夫，设计提示词的这个学问就是prompt
engineering。我之前一直觉得这是一个静态的过程，建议刚接触的同学先学习吴恩达和openai合作制作的prompt
engineering的知道<a href="https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/">课程</a>,另外也有中国热心的网友也翻译了自己的中文版本，还加入了一些自己的思考，推荐<a href="https://github.com/LC1332/Prophet-Andrew-Ng/blob/main/content/2.%20%E6%8F%90%E7%A4%BA%E5%8E%9F%E5%88%99%20Guidelines.ipynb">骆驼</a>。</p>
<p>我是从斯坦福的羊驼模型开始接触instruction tuning的，instruction
tuning提供给了用户一种在特定数据集上低成本finetune模型的可能性，后来又接触到了斯坦福新出的paper：Demonstrate-Search-Predict:
Composing retrieval and language models for knowledge-intensive
NLP，这是一篇用信息抽取技术retrieval
models来扩充query，从而使得LM能够得到更多context的技术，这才让我了解到其实除了最基础的我们知道的那些prompting的方式，比如zero-shot,few-shot,chain-of-thought方式，还有一些方式在这些方式上进行了创新。</p>
<p>lilian weng的<a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/">博客</a>在chiin-of-thought章节之后就介绍了自动prompt
design和augmented language
models。这些对于我来说都是新东西，新技术发展的很快，我学习的过程其实也是在织自己知识结构中的这个网，从接触GPT以来写了好几篇博客，零零散散的，很多博客都是只局限于其中的技术，时不时我需要停下里思考这些技术之间的关联，从而把整个逻辑理顺。技术的发展或者一个模型一个方法的出现都是有路径可循的，我希望能把这些路径穿起来，摸清楚发展脉络。</p>
<h1 id="useful-links">Useful Links</h1>
<ul>
<li><a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#augmented-language-models">Prompt
engineering blog by Lilian Weng</a></li>
<li></li>
</ul>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>pandas遇到的问题汇总</title>
    <url>/2022/03/17/pandas%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<h1 id="join">join()</h1>
<p>Examples</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df = pd.DataFrame(&#123;&#x27;key&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;, &#x27;K3&#x27;, &#x27;K4&#x27;, &#x27;K5&#x27;],</span><br><span class="line">...                    &#x27;A&#x27;: [&#x27;A0&#x27;, &#x27;A1&#x27;, &#x27;A2&#x27;, &#x27;A3&#x27;, &#x27;A4&#x27;, &#x27;A5&#x27;]&#125;)</span><br><span class="line">&gt;&gt;&gt; df</span><br><span class="line">  key   A</span><br><span class="line">0  K0  A0</span><br><span class="line">1  K1  A1</span><br><span class="line">2  K2  A2</span><br><span class="line">3  K3  A3</span><br><span class="line">4  K4  A4</span><br><span class="line">5  K5  A5</span><br><span class="line">&gt;&gt;&gt; other = pd.DataFrame(&#123;&#x27;key&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;],</span><br><span class="line">...                       &#x27;B&#x27;: [&#x27;B0&#x27;, &#x27;B1&#x27;, &#x27;B2&#x27;]&#125;)</span><br><span class="line">&gt;&gt;&gt; other</span><br><span class="line">  key   B</span><br><span class="line">0  K0  B0</span><br><span class="line">1  K1  B1</span><br><span class="line">2  K2  B2</span><br></pre></td></tr></table></figure>
<p>Join DataFrames using their indexes.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df.join(other, lsuffix=&#x27;_caller&#x27;, rsuffix=&#x27;_other&#x27;)</span><br><span class="line">  key_caller   A key_other    B</span><br><span class="line">0         K0  A0        K0   B0</span><br><span class="line">1         K1  A1        K1   B1</span><br><span class="line">2         K2  A2        K2   B2</span><br><span class="line">3         K3  A3       NaN  NaN</span><br><span class="line">4         K4  A4       NaN  NaN</span><br><span class="line">5         K5  A5       NaN  NaN</span><br></pre></td></tr></table></figure>
<p>如果不想以现有index为基础去Join，比如上面的例子想用key这个index去Join，可以：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt; df.join(other.set_index(&#x27;key&#x27;), on=&#x27;key&#x27;)</span><br><span class="line">  key   A    B</span><br><span class="line">0  K0  A0   B0</span><br><span class="line">1  K1  A1   B1</span><br><span class="line">2  K2  A2   B2</span><br><span class="line">3  K3  A3  NaN</span><br><span class="line">4  K4  A4  NaN</span><br><span class="line">5  K5  A5  NaN</span><br></pre></td></tr></table></figure>
<h1 id="merge">merge()</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df1 = pd.DataFrame(&#123;&#x27;lkey&#x27;: [&#x27;foo&#x27;, &#x27;bar&#x27;, &#x27;baz&#x27;, &#x27;foo&#x27;],</span><br><span class="line">                    &#x27;value&#x27;: [1, 2, 3, 5]&#125;)</span><br><span class="line">df2 = pd.DataFrame(&#123;&#x27;rkey&#x27;: [&#x27;foo&#x27;, &#x27;bar&#x27;, &#x27;baz&#x27;, &#x27;foo&#x27;],</span><br><span class="line">                    &#x27;value&#x27;: [5, 6, 7, 8]&#125;)</span><br><span class="line">df1</span><br><span class="line">    lkey value</span><br><span class="line">0   foo      1</span><br><span class="line">1   bar      2</span><br><span class="line">2   baz      3</span><br><span class="line">3   foo      5</span><br><span class="line">df2</span><br><span class="line">    rkey value</span><br><span class="line">0   foo      5</span><br><span class="line">1   bar      6</span><br><span class="line">2   baz      7</span><br><span class="line">3   foo      8</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="选择多列">选择多列</h1>
<p>有时候需要选择DataFrame中的多个列组成一个新的DataFrame，这时候要用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df[[&#x27;column1&#x27;,&#x27;column2&#x27;]] # 注意这里是双层中括号！</span><br></pre></td></tr></table></figure>
<h1 id="drop-满足条件的行">drop 满足条件的行</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_clear = df.drop(df[df[<span class="string">&#x27;x&#x27;</span>]&lt;<span class="number">0.01</span>].index)</span><br><span class="line"><span class="comment"># 也可以使用多个条件</span></span><br><span class="line">df_clear = df.drop(df[(df[<span class="string">&#x27;x&#x27;</span>]&lt;<span class="number">0.01</span>) | (df[<span class="string">&#x27;x&#x27;</span>]&gt;<span class="number">10</span>)].index) <span class="comment">#删除x小于0.01或大于10的行</span></span><br></pre></td></tr></table></figure>
<h1 id="dropna-丢掉某一列中有空的行">dropna 丢掉某一列中有空的行</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.dropna(subset=[<span class="string">&#x27;column_name&#x27;</span>])</span><br></pre></td></tr></table></figure>
<h1 id="applyapplymapmap">apply，applymap，map</h1>
<h2 id="apply">apply</h2>
<p>官方解释是 apply a function along an axis of the dataframe.</p>
<p>apply内函数的是Series，也就是在Series上做函数操作。index可以是dataframe的index，也可以是dataframe的columns(axis=1)。axis=0时，apply
the function to each
column，也就是每一列为一个整体去实施函数。axis=1时，每一行作为一个整体去实施函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.apply(np.<span class="built_in">sum</span>, axis=<span class="number">0</span>)</span><br><span class="line">df.apply(np.<span class="built_in">sum</span>, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="applymap">applymap</h2>
<p>官方解释是 apply a function to a dataframe
elementwise.和apply不一样的是对dataframe中每一个元素分别作函数。</p>
<p>applymap允许实施一个函数，这个函数接受和返回的都是一个标量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.applymap(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(<span class="built_in">str</span>(x)))</span><br></pre></td></tr></table></figure>
<h2 id="map">map</h2>
<p>map调用的对象只能是Series，而上面两个方法的调用对象是Dataframe。</p>
]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>python OS 文件/目录常用方法总结</title>
    <url>/2021/12/30/python-OS-%E6%96%87%E4%BB%B6-%E7%9B%AE%E5%BD%95%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.makedirs(path[, mode]) </span><br><span class="line">递归文件夹创建函数。像mkdir(), 但创建的所有intermediate-level文件夹需要包含子文件夹。</span><br><span class="line"></span><br><span class="line">os.walk(top[, topdown=<span class="literal">True</span>[, onerror=<span class="literal">None</span>[, followlinks=<span class="literal">False</span>]]])</span><br><span class="line">输出在文件夹中的文件名通过在树中游走，向上或者向下。返回的是一个三元组(root,dirs,files),是一个生成器类型，需要用<span class="keyword">for</span>遍历读取</span><br><span class="line"></span><br><span class="line">os.chdir(path)</span><br><span class="line">改变当前工作目录</span><br><span class="line">	</span><br><span class="line">os.listdir(path)</span><br><span class="line">返回path指定的文件夹包含的文件或文件夹的名字的列表。</span><br></pre></td></tr></table></figure>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(<span class="string">&quot;.&quot;</span>, topdown=<span class="literal">False</span>):</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> files:</span><br><span class="line">        <span class="built_in">print</span>(os.path.join(root, name))</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> dirs:</span><br><span class="line">        <span class="built_in">print</span>(os.path.join(root, name))</span><br><span class="line">        </span><br><span class="line">path = <span class="string">&quot;/var/www/html/&quot;</span></span><br><span class="line">dirs = os.listdir( path )</span><br><span class="line"><span class="comment"># 输出所有文件和文件夹</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> dirs:</span><br><span class="line">   <span class="built_in">print</span> (file)</span><br></pre></td></tr></table></figure>
<p><code>os.walk</code>和<code>os.listdir</code>两个函数的区别在于前者会遍历到子文件夹中的子文件，而后者只是返回你传入的path中的文件夹名字和文件名字。</p>
<p><code>os</code>库中有一个<code>path</code>的模块，专门用于处理文件<code>path</code>相关的属性信息。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">os.path.abspath(path)</span><br><span class="line">返回绝对路径</span><br><span class="line"></span><br><span class="line">os.path.basename(path)</span><br><span class="line">返回文件名</span><br><span class="line"></span><br><span class="line">os.path.exists(path)</span><br><span class="line">如果路径 path 存在，返回 True；如果路径 path 不存在，返回 False。</span><br><span class="line"></span><br><span class="line">os.path.join(path1[, path2[, ...]])</span><br><span class="line">把目录和文件名合成一个路径</span><br><span class="line"></span><br><span class="line">os.path.split(path)</span><br><span class="line">把路径分割成 dirname 和 basename，返回一个元组</span><br><span class="line"></span><br><span class="line">os.path.splitext(path)</span><br><span class="line">分割路径，返回路径名和文件扩展名的元组</span><br><span class="line"></span><br><span class="line">os.path.walk(path, visit, arg)</span><br><span class="line">遍历path，进入每个目录都调用visit函数，visit函数必须有3个参数(arg, dirname, names)，dirname表示当前目录的目录名，names代表当前目录下的所有文件名，args则为walk的第三个参数</span><br></pre></td></tr></table></figure>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>( os.path.basename(<span class="string">&#x27;/root/runoob.txt&#x27;</span>) )   <span class="comment"># 返回文件名</span></span><br><span class="line"><span class="built_in">print</span>( os.path.dirname(<span class="string">&#x27;/root/runoob.txt&#x27;</span>) )    <span class="comment"># 返回目录路径</span></span><br><span class="line"><span class="built_in">print</span>( os.path.split(<span class="string">&#x27;/root/runoob.txt&#x27;</span>) )      <span class="comment"># 分割文件名与路径</span></span><br><span class="line"><span class="built_in">print</span>( os.path.join(<span class="string">&#x27;root&#x27;</span>,<span class="string">&#x27;test&#x27;</span>,<span class="string">&#x27;runoob.txt&#x27;</span>) )  <span class="comment"># 将目录和文件名合成一个路径</span></span><br><span class="line"><span class="built_in">print</span>( os.path.splitext(<span class="string">&#x27;/root/runoob.txt&#x27;</span>) )</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">runoob.txt</span><br><span class="line">/root</span><br><span class="line">(&#x27;/root&#x27;, &#x27;runoob.txt&#x27;)</span><br><span class="line">root/test/runoob.txt</span><br><span class="line">(&#x27;/root/run/test&#x27;, &#x27;.txt&#x27;)</span><br></pre></td></tr></table></figure>
<p>上面的split和splitext，前者分割出了文件名和路径，而后者可以分割出路径名和扩展名，如果想要获得文件的扩展名，可以用splitext，传入文件的path就可以了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">os.mknod(filename) 创建</span><br></pre></td></tr></table></figure>
<h1 id="复制文件和删除文件移动文件">复制文件和删除文件,移动文件</h1>
<p>如果是删除一个目录，可以使用以下两种方式:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="comment"># method 1</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(root_dir):</span><br><span class="line">  shutil.rmtree(root_dir) <span class="comment"># 这里不可以使用os.removedirs(),removedirs只可以删除非空的文件夹，rmdir也是</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># method 2</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(os.listdir(root_dir)) &gt; <span class="number">0</span>:</span><br><span class="line">  <span class="keyword">for</span> file <span class="keyword">in</span> os.scandir(root_dir):</span><br><span class="line">    os.remove(file.path)</span><br></pre></td></tr></table></figure>
<p>如果想复制一个文件到另外一个文件夹，参考 <a href="https://zhuanlan.zhihu.com/p/35725217" class="uri">https://zhuanlan.zhihu.com/p/35725217</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">copyfile(source_file, destination_file)</span><br></pre></td></tr></table></figure>
<p>记住这里第二个参数一定要是可写入的文件名字，而不是目录。</p>
<h2 id="移动文件">移动文件</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"> </span><br><span class="line">file_source = <span class="string">&#x27;Path/Of/Directory&#x27;</span></span><br><span class="line">file_destination = <span class="string">&#x27;Path/Of/Directory&#x27;</span></span><br><span class="line"> </span><br><span class="line">get_files = os.listdir(file_source)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> g <span class="keyword">in</span> get_files:</span><br><span class="line">    shutil.move(file_source + g, file_destination)</span><br></pre></td></tr></table></figure>
<h1 id="读写csv文件">读写csv文件</h1>
<p>第一个方式是用pandas，具体不介绍。</p>
<p>这里总结一下csv库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取csv文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filename,<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">     csv_reader = csv.reader(csvfile)</span><br><span class="line">     <span class="keyword">for</span> row <span class="keyword">in</span> csv_reader:</span><br><span class="line">        <span class="built_in">print</span>(row[<span class="number">1</span>]) <span class="comment"># 用列表的index取值</span></span><br><span class="line"><span class="comment"># 写csv文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filename,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    csv_writer = csv.writer(csvfile)</span><br><span class="line">    spamwriter.writerow([<span class="string">&#x27;Spam&#x27;</span>, <span class="string">&#x27;Lovely Spam&#x27;</span>, <span class="string">&#x27;Wonderful Spam&#x27;</span>]) <span class="comment"># writerow接受一个list，所有值都会写在一行里</span></span><br><span class="line">    spamwriter.writerows([[],[],[]]) <span class="comment"># writerows写入多行，每一行是一个列表，传进去的是列表的列表</span></span><br></pre></td></tr></table></figure>
<p>除了写入list，还可以写字典类型的数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># my data rows as dictionary objects </span></span><br><span class="line">mydict =[&#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;COE&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;9.0&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Nikhil&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;2&#x27;</span>&#125;, </span><br><span class="line">         &#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;COE&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;9.1&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Sanchit&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;2&#x27;</span>&#125;, </span><br><span class="line">         &#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;IT&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;9.3&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Aditya&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;2&#x27;</span>&#125;, </span><br><span class="line">         &#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;SE&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;9.5&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Sagar&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;1&#x27;</span>&#125;, </span><br><span class="line">         &#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;MCE&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;7.8&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Prateek&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;3&#x27;</span>&#125;, </span><br><span class="line">         &#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;EP&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;9.1&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Sahil&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;2&#x27;</span>&#125;] </span><br><span class="line">    </span><br><span class="line"><span class="comment"># field names </span></span><br><span class="line">fields = [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;branch&#x27;</span>, <span class="string">&#x27;year&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>] </span><br><span class="line">    </span><br><span class="line"><span class="comment"># name of csv file </span></span><br><span class="line">filename = <span class="string">&quot;university_records.csv&quot;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># writing to csv file </span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> csvfile: </span><br><span class="line">    <span class="comment"># creating a csv dict writer object </span></span><br><span class="line">    writer = csv.DictWriter(csvfile, fieldnames = fields) </span><br><span class="line">        </span><br><span class="line">    <span class="comment"># writing headers (field names) </span></span><br><span class="line">    writer.writeheader() </span><br><span class="line">        </span><br><span class="line">    <span class="comment"># writing data rows </span></span><br><span class="line">    writer.writerows(mydict)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>python 全角转半角</title>
    <url>/2022/04/15/python-%E5%85%A8%E8%A7%92%E8%BD%AC%E5%8D%8A%E8%A7%92/</url>
    <content><![CDATA[<p>全角与半角转换在处理汉语语料中会经常出现，这里分别说明汉字、数字、字母的unicode编码范围。以及全角与半角的转换方法。最后给出wiki上全角和半角的编码对照表。这里Python需要用Python3版本。</p>
<h3 id="汉字的判断">汉字的判断</h3>
<p>汉字的unicode编码范围 u4e00 到 u9fa5。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_chinese</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断一个unicode是否是汉字&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> uchar &gt;= <span class="string">u&#x27;\u4e00&#x27;</span> <span class="keyword">and</span> uchar&lt;=<span class="string">u&#x27;\u9fa5&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="数字0-9的判断">数字0-9的判断</h3>
<p>数字的unicode编码范围根据全角和半角，有两个不同区域，半角数字 u0030
到 u0039，全角数字 uff10 到 uff19。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_number</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断一个unicode是否是半角数字&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> uchar &gt;= <span class="string">u&#x27;\u0030&#x27;</span> <span class="keyword">and</span> uchar&lt;=<span class="string">u&#x27;\u0039&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_Qnumber</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断一个unicode是否是全角数字&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> uchar &gt;= <span class="string">u&#x27;\uff10&#x27;</span> <span class="keyword">and</span> uchar &lt;= <span class="string">u&#x27;\uff19&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="大小写字母判断">大小写字母判断</h3>
<p>字母的unicode编码根据字母大小写，以及全角和半角共有四个区域。
半角大写字母：u0041 - u005a ，半角小写字母：u0061 - u007a ；
全角大写字母：uff21 - uff3a ， 全角小写字母：uff41 - uff5a 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_alphabet</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断一个unicode是否是半角英文字母&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> (uchar &gt;= <span class="string">u&#x27;\u0041&#x27;</span> <span class="keyword">and</span> uchar &lt;= <span class="string">u&#x27;\u005a&#x27;</span>) <span class="keyword">or</span> (uchar &gt;= <span class="string">u&#x27;\u0061&#x27;</span> <span class="keyword">and</span> uchar &lt;= <span class="string">u&#x27;\u007a&#x27;</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_Qalphabet</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断一个unicode是否是全角英文字母&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> (uchar &gt;= <span class="string">u&#x27;\uff21&#x27;</span> <span class="keyword">and</span> uchar &lt;= <span class="string">u&#x27;\uff3a&#x27;</span>) <span class="keyword">or</span> (uchar &gt;= <span class="string">u&#x27;\uff41&#x27;</span> <span class="keyword">and</span> uchar &lt;= <span class="string">u&#x27;\uff5a&#x27;</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="非汉字和数字字母的判断">非汉字和数字字母的判断</h3>
<p>判断除汉字、数字0-9、字母之外的字符。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_other</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断是否非汉字，数字和英文字符&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> (is_chinese(uchar) <span class="keyword">or</span> is_number(uchar) <span class="keyword">or</span> is_alphabet(uchar)):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="全角和半角的转换">全角和半角的转换</h3>
<p>全角半角转换需要用到上面的数字、字母等判断。</p>
<ol type="1">
<li>所有半角转全角，不是半角范围直接返回，空格半角特殊单独处理，其它半角和全角对应公式：半角
= 全角 - 0xfee0</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">B2Q</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;单个字符 半角转全角&quot;&quot;&quot;</span></span><br><span class="line">    inside_code = <span class="built_in">ord</span>(uchar)</span><br><span class="line">    <span class="keyword">if</span> inside_code &lt; <span class="number">0x0020</span> <span class="keyword">or</span> inside_code &gt; <span class="number">0x7e</span>: <span class="comment"># 不是半角字符就返回原来的字符</span></span><br><span class="line">        <span class="keyword">return</span> uchar </span><br><span class="line">    <span class="keyword">if</span> inside_code == <span class="number">0x0020</span>: <span class="comment"># 除了空格其他的全角半角的公式为: 半角 = 全角 - 0xfee0</span></span><br><span class="line">        inside_code = <span class="number">0x3000</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        inside_code += <span class="number">0xfee0</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">chr</span>(inside_code)</span><br></pre></td></tr></table></figure>
<ol type="1">
<li>所有全角转半角，和前面正好相反，公式对应：全角 = 半角 + 0xfee0</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Q2B</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;单个字符 全角转半角&quot;&quot;&quot;</span></span><br><span class="line">    inside_code = <span class="built_in">ord</span>(uchar)</span><br><span class="line">    <span class="keyword">if</span> inside_code == <span class="number">0x3000</span>:</span><br><span class="line">        inside_code = <span class="number">0x0020</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        inside_code -= <span class="number">0xfee0</span></span><br><span class="line">    <span class="keyword">if</span> inside_code &lt; <span class="number">0x0020</span> <span class="keyword">or</span> inside_code &gt; <span class="number">0x7e</span>: <span class="comment">#转完之后不是半角字符返回原来的字符</span></span><br><span class="line">        <span class="keyword">return</span> uchar</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">chr</span>(inside_code)</span><br></pre></td></tr></table></figure>
<ol type="1">
<li>把整个字符串全角转半角，也可以只转部分如数字和字母</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stringQ2B</span>(<span class="params">ustring</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;把字符串全角转半角&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join([Q2B(uchar) <span class="keyword">for</span> uchar <span class="keyword">in</span> ustring])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stringpartQ2B</span>(<span class="params">ustring</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;把字符串中数字和字母全角转半角&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join([Q2B(uchar) <span class="keyword">if</span> is_Qnumber(uchar) <span class="keyword">or</span> is_Qalphabet(uchar) <span class="keyword">else</span> uchar <span class="keyword">for</span> uchar <span class="keyword">in</span> ustring])</span><br></pre></td></tr></table></figure>
<p>测试上面的全角半角转换。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">text = <span class="string">&quot;电影《２０１２》讲述了２０１２年１２月２１日的世界末日，主人公Ｊａｃｋ以及世界各国人民挣扎求生的经历，灾难面前，尽现人间百态。&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;text原文：&quot;</span>, text, sep=<span class="string">&quot;\n&quot;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">text1 = stringQ2B(text)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;全角转半角：&quot;</span>, text1, sep=<span class="string">&quot;\n&quot;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">text2 = stringpartQ2B(text)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数字字母全角转半角：&quot;</span>, text2, sep=<span class="string">&quot;\n&quot;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果如下，只转数字字母与全部转是有区别的：</p>
<figure>
<img src="https:////upload-images.jianshu.io/upload_images/17669901-55fb5ccee96dcf60.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/980/format/webp" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>全角半角转换结果</p>
<h3 id="附全角和半角编码对应表">附全角和半角编码对应表</h3>
<ol type="1">
<li>ASCII内字符的全角和半角，包括数字0-9、大小写字母、标点符号等。</li>
</ol>
<table>
<thead>
<tr class="header">
<th>ASCII</th>
<th>全角字符</th>
<th>Unicode</th>
<th>半角字符</th>
<th>Unicode</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0x20</td>
<td>"　"</td>
<td>U+3000</td>
<td>" "</td>
<td>U+0020</td>
</tr>
<tr class="even">
<td>0x21</td>
<td>！</td>
<td>U+FF01</td>
<td>!</td>
<td>U+0021</td>
</tr>
<tr class="odd">
<td>0x22</td>
<td>＂</td>
<td>U+FF02</td>
<td>"</td>
<td>U+0022</td>
</tr>
<tr class="even">
<td>0x23</td>
<td>＃</td>
<td>U+FF03</td>
<td>#</td>
<td>U+0023</td>
</tr>
<tr class="odd">
<td>0x24</td>
<td>＄</td>
<td>U+FF04</td>
<td>$</td>
<td>U+0024</td>
</tr>
<tr class="even">
<td>0x25</td>
<td>％</td>
<td>U+FF05</td>
<td>%</td>
<td>U+0025</td>
</tr>
<tr class="odd">
<td>0x26</td>
<td>＆</td>
<td>U+FF06</td>
<td>&amp;</td>
<td>U+0026</td>
</tr>
<tr class="even">
<td>0x27</td>
<td>＇</td>
<td>U+FF07</td>
<td>'</td>
<td>U+0027</td>
</tr>
<tr class="odd">
<td>0x28</td>
<td>（</td>
<td>U+FF08</td>
<td>(</td>
<td>U+0028</td>
</tr>
<tr class="even">
<td>0x29</td>
<td>）</td>
<td>U+FF09</td>
<td>)</td>
<td>U+0029</td>
</tr>
<tr class="odd">
<td>0x2A</td>
<td>＊</td>
<td>U+FF0A</td>
<td>*</td>
<td>U+002A</td>
</tr>
<tr class="even">
<td>0x2B</td>
<td>＋</td>
<td>U+FF0B</td>
<td>+</td>
<td>U+002B</td>
</tr>
<tr class="odd">
<td>0x2C</td>
<td>，</td>
<td>U+FF0C</td>
<td>,</td>
<td>U+002C</td>
</tr>
<tr class="even">
<td>0x2D</td>
<td>－</td>
<td>U+FF0D</td>
<td>-</td>
<td>U+002D</td>
</tr>
<tr class="odd">
<td>0x2E</td>
<td>．</td>
<td>U+FF0E</td>
<td>.</td>
<td>U+002E</td>
</tr>
<tr class="even">
<td>0x2F</td>
<td>／</td>
<td>U+FF0F</td>
<td>/</td>
<td>U+002F</td>
</tr>
<tr class="odd">
<td>0x30</td>
<td>０</td>
<td>U+FF10</td>
<td>0</td>
<td>U+0030</td>
</tr>
<tr class="even">
<td>0x31</td>
<td>１</td>
<td>U+FF11</td>
<td>1</td>
<td>U+0031</td>
</tr>
<tr class="odd">
<td>0x32</td>
<td>２</td>
<td>U+FF12</td>
<td>2</td>
<td>U+0032</td>
</tr>
<tr class="even">
<td>0x33</td>
<td>３</td>
<td>U+FF13</td>
<td>3</td>
<td>U+0033</td>
</tr>
<tr class="odd">
<td>0x34</td>
<td>４</td>
<td>U+FF14</td>
<td>4</td>
<td>U+0034</td>
</tr>
<tr class="even">
<td>0x35</td>
<td>５</td>
<td>U+FF15</td>
<td>5</td>
<td>U+0035</td>
</tr>
<tr class="odd">
<td>0x36</td>
<td>６</td>
<td>U+FF16</td>
<td>6</td>
<td>U+0036</td>
</tr>
<tr class="even">
<td>0x37</td>
<td>７</td>
<td>U+FF17</td>
<td>7</td>
<td>U+0037</td>
</tr>
<tr class="odd">
<td>0x38</td>
<td>８</td>
<td>U+FF18</td>
<td>8</td>
<td>U+0038</td>
</tr>
<tr class="even">
<td>0x39</td>
<td>９</td>
<td>U+FF19</td>
<td>9</td>
<td>U+0039</td>
</tr>
<tr class="odd">
<td>0x3A</td>
<td>：</td>
<td>U+FF1A</td>
<td>:</td>
<td>U+003A</td>
</tr>
<tr class="even">
<td>0x3B</td>
<td>；</td>
<td>U+FF1B</td>
<td>;</td>
<td>U+003B</td>
</tr>
<tr class="odd">
<td>0x3C</td>
<td>＜</td>
<td>U+FF1C</td>
<td>&lt;</td>
<td>U+003C</td>
</tr>
<tr class="even">
<td>0x3D</td>
<td>＝</td>
<td>U+FF1D</td>
<td>=</td>
<td>U+003D</td>
</tr>
<tr class="odd">
<td>0x3E</td>
<td>＞</td>
<td>U+FF1E</td>
<td>&gt;</td>
<td>U+003E</td>
</tr>
<tr class="even">
<td>0x3F</td>
<td>？</td>
<td>U+FF1F</td>
<td>?</td>
<td>U+003F</td>
</tr>
<tr class="odd">
<td>0x40</td>
<td>＠</td>
<td>U+FF20</td>
<td>@</td>
<td>U+0040</td>
</tr>
<tr class="even">
<td>0x41</td>
<td>Ａ</td>
<td>U+FF21</td>
<td>A</td>
<td>U+0041</td>
</tr>
<tr class="odd">
<td>0x42</td>
<td>Ｂ</td>
<td>U+FF22</td>
<td>B</td>
<td>U+0042</td>
</tr>
<tr class="even">
<td>0x43</td>
<td>Ｃ</td>
<td>U+FF23</td>
<td>C</td>
<td>U+0043</td>
</tr>
<tr class="odd">
<td>0x44</td>
<td>Ｄ</td>
<td>U+FF24</td>
<td>D</td>
<td>U+0044</td>
</tr>
<tr class="even">
<td>0x45</td>
<td>Ｅ</td>
<td>U+FF25</td>
<td>E</td>
<td>U+0045</td>
</tr>
<tr class="odd">
<td>0x46</td>
<td>Ｆ</td>
<td>U+FF26</td>
<td>F</td>
<td>U+0046</td>
</tr>
<tr class="even">
<td>0x47</td>
<td>Ｇ</td>
<td>U+FF27</td>
<td>G</td>
<td>U+0047</td>
</tr>
<tr class="odd">
<td>0x48</td>
<td>Ｈ</td>
<td>U+FF28</td>
<td>H</td>
<td>U+0048</td>
</tr>
<tr class="even">
<td>0x49</td>
<td>Ｉ</td>
<td>U+FF29</td>
<td>I</td>
<td>U+0049</td>
</tr>
<tr class="odd">
<td>0x4A</td>
<td>Ｊ</td>
<td>U+FF2A</td>
<td>J</td>
<td>U+004A</td>
</tr>
<tr class="even">
<td>0x4B</td>
<td>Ｋ</td>
<td>U+FF2B</td>
<td>K</td>
<td>U+004B</td>
</tr>
<tr class="odd">
<td>0x4C</td>
<td>Ｌ</td>
<td>U+FF2C</td>
<td>L</td>
<td>U+004C</td>
</tr>
<tr class="even">
<td>0x4D</td>
<td>Ｍ</td>
<td>U+FF2D</td>
<td>M</td>
<td>U+004D</td>
</tr>
<tr class="odd">
<td>0x4E</td>
<td>Ｎ</td>
<td>U+FF2E</td>
<td>N</td>
<td>U+004E</td>
</tr>
<tr class="even">
<td>0x4F</td>
<td>Ｏ</td>
<td>U+FF2F</td>
<td>O</td>
<td>U+004F</td>
</tr>
<tr class="odd">
<td>0x50</td>
<td>Ｐ</td>
<td>U+FF30</td>
<td>P</td>
<td>U+0050</td>
</tr>
<tr class="even">
<td>0x51</td>
<td>Ｑ</td>
<td>U+FF31</td>
<td>Q</td>
<td>U+0051</td>
</tr>
<tr class="odd">
<td>0x52</td>
<td>Ｒ</td>
<td>U+FF32</td>
<td>R</td>
<td>U+0052</td>
</tr>
<tr class="even">
<td>0x53</td>
<td>Ｓ</td>
<td>U+FF33</td>
<td>S</td>
<td>U+0053</td>
</tr>
<tr class="odd">
<td>0x54</td>
<td>Ｔ</td>
<td>U+FF34</td>
<td>T</td>
<td>U+0054</td>
</tr>
<tr class="even">
<td>0x55</td>
<td>Ｕ</td>
<td>U+FF35</td>
<td>U</td>
<td>U+0055</td>
</tr>
<tr class="odd">
<td>0x56</td>
<td>Ｖ</td>
<td>U+FF36</td>
<td>V</td>
<td>U+0056</td>
</tr>
<tr class="even">
<td>0x57</td>
<td>Ｗ</td>
<td>U+FF37</td>
<td>W</td>
<td>U+0057</td>
</tr>
<tr class="odd">
<td>0x58</td>
<td>Ｘ</td>
<td>U+FF38</td>
<td>X</td>
<td>U+0058</td>
</tr>
<tr class="even">
<td>0x59</td>
<td>Ｙ</td>
<td>U+FF39</td>
<td>Y</td>
<td>U+0059</td>
</tr>
<tr class="odd">
<td>0x5A</td>
<td>Ｚ</td>
<td>U+FF3A</td>
<td>Z</td>
<td>U+005A</td>
</tr>
<tr class="even">
<td>0x5B</td>
<td>［</td>
<td>U+FF3B</td>
<td>[</td>
<td>U+005B</td>
</tr>
<tr class="odd">
<td>0x5C</td>
<td>＼</td>
<td>U+FF3C</td>
<td>\</td>
<td>U+005C</td>
</tr>
<tr class="even">
<td>0x5D</td>
<td>］</td>
<td>U+FF3D</td>
<td>]</td>
<td>U+005D</td>
</tr>
<tr class="odd">
<td>0x5E</td>
<td>＾</td>
<td>U+FF3E</td>
<td>^</td>
<td>U+005E</td>
</tr>
<tr class="even">
<td>0x5F</td>
<td>＿</td>
<td>U+FF3F</td>
<td>_</td>
<td>U+005F</td>
</tr>
<tr class="odd">
<td>0x60</td>
<td>｀</td>
<td>U+FF40</td>
<td>`</td>
<td>U+0060</td>
</tr>
<tr class="even">
<td>0x61</td>
<td>ａ</td>
<td>U+FF41</td>
<td>a</td>
<td>U+0061</td>
</tr>
<tr class="odd">
<td>0x62</td>
<td>ｂ</td>
<td>U+FF42</td>
<td>b</td>
<td>U+0062</td>
</tr>
<tr class="even">
<td>0x63</td>
<td>ｃ</td>
<td>U+FF43</td>
<td>c</td>
<td>U+0063</td>
</tr>
<tr class="odd">
<td>0x64</td>
<td>ｄ</td>
<td>U+FF44</td>
<td>d</td>
<td>U+0064</td>
</tr>
<tr class="even">
<td>0x65</td>
<td>ｅ</td>
<td>U+FF45</td>
<td>e</td>
<td>U+0065</td>
</tr>
<tr class="odd">
<td>0x66</td>
<td>ｆ</td>
<td>U+FF46</td>
<td>f</td>
<td>U+0066</td>
</tr>
<tr class="even">
<td>0x67</td>
<td>ｇ</td>
<td>U+FF47</td>
<td>g</td>
<td>U+0067</td>
</tr>
<tr class="odd">
<td>0x68</td>
<td>ｈ</td>
<td>U+FF48</td>
<td>h</td>
<td>U+0068</td>
</tr>
<tr class="even">
<td>0x69</td>
<td>ｉ</td>
<td>U+FF49</td>
<td>i</td>
<td>U+0069</td>
</tr>
<tr class="odd">
<td>0x6A</td>
<td>ｊ</td>
<td>U+FF4A</td>
<td>j</td>
<td>U+006A</td>
</tr>
<tr class="even">
<td>0x6B</td>
<td>ｋ</td>
<td>U+FF4B</td>
<td>k</td>
<td>U+006B</td>
</tr>
<tr class="odd">
<td>0x6C</td>
<td>ｌ</td>
<td>U+FF4C</td>
<td>l</td>
<td>U+006C</td>
</tr>
<tr class="even">
<td>0x6D</td>
<td>ｍ</td>
<td>U+FF4D</td>
<td>m</td>
<td>U+006D</td>
</tr>
<tr class="odd">
<td>0x6E</td>
<td>ｎ</td>
<td>U+FF4E</td>
<td>n</td>
<td>U+006E</td>
</tr>
<tr class="even">
<td>0x6F</td>
<td>ｏ</td>
<td>U+FF4F</td>
<td>o</td>
<td>U+006F</td>
</tr>
<tr class="odd">
<td>0x70</td>
<td>ｐ</td>
<td>U+FF50</td>
<td>p</td>
<td>U+0070</td>
</tr>
<tr class="even">
<td>0x71</td>
<td>ｑ</td>
<td>U+FF51</td>
<td>q</td>
<td>U+0071</td>
</tr>
<tr class="odd">
<td>0x72</td>
<td>ｒ</td>
<td>U+FF52</td>
<td>r</td>
<td>U+0072</td>
</tr>
<tr class="even">
<td>0x73</td>
<td>ｓ</td>
<td>U+FF53</td>
<td>s</td>
<td>U+0073</td>
</tr>
<tr class="odd">
<td>0x74</td>
<td>ｔ</td>
<td>U+FF54</td>
<td>t</td>
<td>U+0074</td>
</tr>
<tr class="even">
<td>0x75</td>
<td>ｕ</td>
<td>U+FF55</td>
<td>u</td>
<td>U+0075</td>
</tr>
<tr class="odd">
<td>0x76</td>
<td>ｖ</td>
<td>U+FF56</td>
<td>v</td>
<td>U+0076</td>
</tr>
<tr class="even">
<td>0x77</td>
<td>ｗ</td>
<td>U+FF57</td>
<td>w</td>
<td>U+0077</td>
</tr>
<tr class="odd">
<td>0x78</td>
<td>ｘ</td>
<td>U+FF58</td>
<td>x</td>
<td>U+0078</td>
</tr>
<tr class="even">
<td>0x79</td>
<td>ｙ</td>
<td>U+FF59</td>
<td>y</td>
<td>U+0079</td>
</tr>
<tr class="odd">
<td>0x7A</td>
<td>ｚ</td>
<td>U+FF5A</td>
<td>z</td>
<td>U+007A</td>
</tr>
<tr class="even">
<td>0x7B</td>
<td>｛</td>
<td>U+FF5B</td>
<td>{</td>
<td>U+007B</td>
</tr>
<tr class="odd">
<td>0x7C</td>
<td>｜</td>
<td>U+FF5C</td>
<td>|</td>
<td>U+007C</td>
</tr>
<tr class="even">
<td>0x7D</td>
<td>｝</td>
<td>U+FF5D</td>
<td>}</td>
<td>U+007D</td>
</tr>
<tr class="odd">
<td>0x7E</td>
<td>～</td>
<td>U+FF5E</td>
<td>~</td>
<td>U+007E</td>
</tr>
</tbody>
</table>
<ol type="1">
<li>其它特殊字符的全角和半角</li>
</ol>
<table>
<thead>
<tr class="header">
<th>半角字符</th>
<th>Unicode</th>
<th>全角字符</th>
<th>Unicode</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>⦅</td>
<td>U+2985</td>
<td>｟</td>
<td>U+FF5F</td>
</tr>
<tr class="even">
<td>⦆</td>
<td>U+2986</td>
<td>｠</td>
<td>U+FF60</td>
</tr>
<tr class="odd">
<td>¢</td>
<td>U+00A2</td>
<td>￠</td>
<td>U+FFE0</td>
</tr>
<tr class="even">
<td>£</td>
<td>U+00A3</td>
<td>￡</td>
<td>U+FFE1</td>
</tr>
<tr class="odd">
<td>¬</td>
<td>U+00AC</td>
<td>￢</td>
<td>U+FFE2</td>
</tr>
<tr class="even">
<td>¯</td>
<td>U+00AF</td>
<td>￣</td>
<td>U+FFE3</td>
</tr>
<tr class="odd">
<td>¦</td>
<td>U+00A6</td>
<td>￤</td>
<td>U+FFE4</td>
</tr>
<tr class="even">
<td>¥</td>
<td>U+00A5</td>
<td>￥</td>
<td>U+FFE5</td>
</tr>
<tr class="odd">
<td>₩</td>
<td>U+20A9</td>
<td>￦</td>
<td>U+FFE6</td>
</tr>
<tr class="even">
<td>￨</td>
<td>U+FFE8</td>
<td>│</td>
<td>U+2502</td>
</tr>
<tr class="odd">
<td>￩</td>
<td>U+FFE9</td>
<td>←</td>
<td>U+2190</td>
</tr>
<tr class="even">
<td>￪</td>
<td>U+FFEA</td>
<td>↑</td>
<td>U+2191</td>
</tr>
<tr class="odd">
<td>￫</td>
<td>U+FFEB</td>
<td>→</td>
<td>U+2192</td>
</tr>
<tr class="even">
<td>￬</td>
<td>U+FFEC</td>
<td>↓</td>
<td>U+2193</td>
</tr>
<tr class="odd">
<td>￭</td>
<td>U+FFED</td>
<td>■</td>
<td>U+25A0</td>
</tr>
<tr class="even">
<td>￮</td>
<td>U+FFEE</td>
<td>○</td>
<td>U+25CB</td>
</tr>
</tbody>
</table>
<h3 id="参考">参考</h3>
<p>[1]. <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fzh.wikipedia.org%2Fwiki%2F%E5%85%A8%E5%BD%A2%E5%92%8C%E5%8D%8A%E5%BD%A2">https://zh.wikipedia.org/wiki/%E5%85%A8%E5%BD%A2%E5%92%8C%E5%8D%8A%E5%BD%A2</a>
[2]. <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fwww.voidcn.com%2Farticle%2Fp-njiniuhl-nu.html">http://www.voidcn.com/article/p-njiniuhl-nu.html</a>
[3]. https://www.jianshu.com/p/152e081fec1b</p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>python对Counter内容进行排序</title>
    <url>/2022/08/11/python-%E5%AF%B9Counter%E5%86%85%E5%AE%B9%E8%BF%9B%E8%A1%8C%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="对counter中的内容进行排序">对Counter中的内容进行排序</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">x = Counter(&#123;<span class="string">&#x27;a&#x27;</span>:<span class="number">5</span>, <span class="string">&#x27;b&#x27;</span>:<span class="number">3</span>, <span class="string">&#x27;c&#x27;</span>:<span class="number">7</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line">x.most_common() <span class="comment"># [(&#x27;c&#x27;, 7), (&#x27;a&#x27;, 5), (&#x27;b&#x27;, 3)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="built_in">sorted</span>(x, key=x.get, reverse=<span class="literal">True</span>) <span class="comment"># [&#x27;c&#x27;, &#x27;a&#x27;, &#x27;b&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"><span class="built_in">sorted</span>(x.items(), key=<span class="keyword">lambda</span> pair: pair[<span class="number">1</span>], reverse=<span class="literal">True</span>) <span class="comment"># [(&#x27;c&#x27;, 7), (&#x27;a&#x27;, 5), (&#x27;b&#x27;, 3)]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>一共三种方法，其中第二种方法中传入<code>key=x.get</code>最终返回的是所有的key，而其他两种方法返回的都是排完序之后的list</p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>sklearn总结</title>
    <url>/2022/03/29/sklearn%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="model">model</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier <span class="comment"># 随机森林</span></span><br><span class="line"><span class="comment"># RandomForestClassifier随机森林：n_estimators 森林中树的数量(default=100), max_features 寻找最优划分时考虑的最大数目的特征,</span></span><br><span class="line">rf_clf_model = RandomForestClassifier(n_estimators=<span class="number">150</span>, max_depth=<span class="number">7</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>
<h1 id="metric">metric</h1>
<h2 id="accuracyprecisionrcall-f1_score">accuracy,precision,rcall,
f1_score</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, f1_score,precision_score,recall_score</span><br></pre></td></tr></table></figure>
<h1 id="model_selection">model_selection</h1>
<h2 id="交叉验证-cross_val_score">交叉验证 cross_val_score</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from sklearn.model_selection import cross_val_score</span><br></pre></td></tr></table></figure>
<p>cross_val_score交叉验证
首个参数为estimator(也就是要执行fit的model),cv
折数，默认是5折验证，返回的cv长度的score</p>
<p>参考 https://blog.csdn.net/qq_36523839/article/details/80707678</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets	<span class="comment">#自带数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split,cross_val_score	<span class="comment">#划分数据 交叉验证</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier  <span class="comment">#一个简单的模型，只有K一个参数，类似K-means</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">iris = datasets.load_iris()		<span class="comment">#加载sklearn自带的数据集</span></span><br><span class="line">X = iris.data 			<span class="comment">#这是数据</span></span><br><span class="line">y = iris.target 		<span class="comment">#这是每个数据所对应的标签</span></span><br><span class="line">train_X,test_X,train_y,test_y = train_test_split(X,y,test_size=<span class="number">1</span>/<span class="number">3</span>,random_state=<span class="number">3</span>)	<span class="comment">#这里划分数据以1/3的来划分 训练集训练结果 测试集测试结果</span></span><br><span class="line">k_range = <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">31</span>)</span><br><span class="line">cv_scores = []		<span class="comment">#用来放每个模型的结果值</span></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> k_range:</span><br><span class="line">    knn = KNeighborsClassifier(n)   <span class="comment">#knn模型，这里一个超参数可以做预测，当多个超参数时需要使用另一种方法GridSearchCV</span></span><br><span class="line">    scores = cross_val_score(knn,train_X,train_y,cv=<span class="number">10</span>,scoring=<span class="string">&#x27;accuracy&#x27;</span>)  <span class="comment">#cv：选择每次测试折数  accuracy：评价指标是准确度,可以省略使用默认值，具体使用参考下面。</span></span><br><span class="line">    cv_scores.append(scores.mean())</span><br><span class="line">plt.plot(k_range,cv_scores)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;K&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)		<span class="comment">#通过图像选择最好的参数</span></span><br><span class="line">plt.show()</span><br><span class="line">best_knn = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)	<span class="comment"># 选择最优的K=3传入模型</span></span><br><span class="line">best_knn.fit(train_X,train_y)			<span class="comment">#训练模型</span></span><br><span class="line"><span class="built_in">print</span>(best_knn.score(test_X,test_y))	<span class="comment">#看看评分</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>sklearn</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow中lstm,GRU</title>
    <url>/2023/01/04/tensorflow%E4%B8%ADlstm-GRU/</url>
    <content><![CDATA[<h1 id="gru">GRU</h1>
<figure>
<img src="/2023/01/04/tensorflow%E4%B8%ADlstm-GRU/LSTM3-var-GRU.png" alt="GRU">
<figcaption aria-hidden="true">GRU</figcaption>
</figure>
<p>其实单看输出，GRU的输出是和简单的RNN一样的，都只有一个hidden_state。所以在tensorflow中它的输出其实和RNN
layer一样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">inputs = tf.random.normal([<span class="number">32</span>, <span class="number">10</span>, <span class="number">8</span>])</span><br><span class="line">gru = tf.keras.layers.GRU(<span class="number">4</span>)</span><br><span class="line">output = gru(inputs)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>其中有两个可以传递给GRU的参数，一个是return_state，一个是return_sequence。两个值都是bool类型。如果单独传递return_sequence=True，那么输出将只有一个值，也就是每一个时间步的序列：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gru = tf.keras.layers.GRU(<span class="number">4</span>, return_sequences=<span class="literal">True</span>)</span><br><span class="line">output = gru(inputs)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">10</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>如果单独传递return_state=True，那么输出将会是两个值，可以仔细看官方文档中的说明是<code>Boolean. Whether to return the last state in addition to the output. Default:</code>False.`也就是output和最后的hidden_state会一起输出，并且output会等于final_state：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gru = tf.keras.layers.GRU(<span class="number">4</span>, return_state=<span class="literal">True</span>)</span><br><span class="line">output, final_state = gru(inputs)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"><span class="built_in">print</span>(final_state.shape) <span class="comment"># output=final_state</span></span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>如果单独传递return_sequences=True，LSTM将只返回整个序列！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lstm = tf.keras.layers.LSTM(<span class="number">4</span>,return_sequences=<span class="literal">True</span>)</span><br><span class="line">inputs = tf.random.normal([<span class="number">32</span>, <span class="number">10</span>, <span class="number">8</span>])</span><br><span class="line">whole_seq_output = lstm(inputs)</span><br><span class="line"><span class="built_in">print</span>(whole_seq_output.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">10</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>那如果两个值都设置成<code>True</code>呢？这将返回两个输出，第一个输出是整个序列，第二个输出是最终的state。注意这里并没有<code>output</code>了，因为<code>output</code>其实是<code>sequence</code>中最后一个序列<code>sequence[:,-1,:]</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gru = tf.keras.layers.GRU(<span class="number">4</span>, return_sequences=<span class="literal">True</span>, return_state=<span class="literal">True</span>)</span><br><span class="line">whole_sequence_output, final_state = gru(inputs)</span><br><span class="line"><span class="built_in">print</span>(whole_sequence_output.shape)</span><br><span class="line"><span class="built_in">print</span>(final_state.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">10</span>, <span class="number">4</span>)</span><br><span class="line">(<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h1 id="lstm">LSTM</h1>
<p>轮到LSTM，因为架构上跟GRU有点区别，所以在返回结果上就多了一个carry_state.</p>
<figure>
<img src="/2023/01/04/tensorflow%E4%B8%ADlstm-GRU/LSTM3-var-peepholes.png" alt="LSTM">
<figcaption aria-hidden="true">LSTM</figcaption>
</figure>
<p>想要了解LSTM的具体计算，参考<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">博客</a></p>
<p>在tensorflow中一样有return_state和return_sequences：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs = tf.random.normal([<span class="number">32</span>, <span class="number">10</span>, <span class="number">8</span>])</span><br><span class="line">lstm = tf.keras.layers.LSTM(<span class="number">4</span>)</span><br><span class="line">output = lstm(inputs)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>如果单独传递return_state，这里和GRU不一样的地方在于lstm有两个state，一个是memory_state一个是carry_state</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lstm = tf.keras.layers.LSTM(<span class="number">4</span>,return_state=<span class="literal">True</span>)</span><br><span class="line">output, final_memory_state, final_carry_state = lstm(inputs)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"><span class="built_in">print</span>(final_memory_state.shape) <span class="comment"># final_memory_state=output</span></span><br><span class="line"><span class="built_in">print</span>(final_carry_state.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>如果同时设置True</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lstm = tf.keras.layers.LSTM(<span class="number">4</span>,return_sequences=<span class="literal">True</span>,return_state=<span class="literal">True</span>)</span><br><span class="line">whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)</span><br><span class="line"><span class="built_in">print</span>(whole_seq_output.shape)</span><br><span class="line"><span class="built_in">print</span>(final_memory_state.shape) <span class="comment"># final_memory_state=whole_seq_output[:,-1,:]</span></span><br><span class="line"><span class="built_in">print</span>(final_carry_state.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">10</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h1 id="gru-vs-lstm">GRU vs LSTM</h1>
<p>至于我们在训练模型的时候选择哪一个cell作为RNN的cell，cs224n课程给出的答案是：</p>
<blockquote>
<p>Researchers have proposed many gated RNN variants, but LSTM and GRU
are the most widely-used.</p>
<p>Rule of thumb: LSTM is a good default choice (especially if your data
has particularly long dependencies, or you have lots of training data);
Switch to GRUs for speed and fewer parameters.</p>
</blockquote>
<p>LSTM doesn’t guarantee that there is no vanishing/exploding gradient,
but it does provide an easier way for the model to learn long-distance
dependencies.</p>
<p>在2023年的今天，lstm也不再是研究者青睐的对象，最火的模型变成了Transformer：</p>
<figure>
<img src="/2023/01/04/tensorflow%E4%B8%ADlstm-GRU/image-20230313095438649.png" alt="image-20230313095438649">
<figcaption aria-hidden="true">image-20230313095438649</figcaption>
</figure>
<p>这里也贴出2022年的最新WMT的<a href="https://www.statmt.org/wmt22/pdf/2022.wmt-1.1.pdf">结果</a></p>
]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>使用PaddleVideo实现在attention_lstm模型上训练youtube-8m数据</title>
    <url>/2021/11/25/%E4%BD%BF%E7%94%A8PaddleVideo%E5%AE%9E%E7%8E%B0%E5%9C%A8attention-lstm%E6%A8%A1%E5%9E%8B%E4%B8%8A%E8%AE%AD%E7%BB%83youtube-8m%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<h1 id="安装">安装</h1>
<p>参照
https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/install.md</p>
<p>注意这里创建虚拟环境的时候要使用python=3.7，因为3.7之后的版本，paddleVideo只支持Linux系统。</p>
<p>其他要求： <code>cuda &gt;= 10.1</code>,
<code>cuDNN &gt;= 7.6.4</code></p>
<p>安装完<code>paddlepaddle</code>之后，再安装<code>paddleVideo</code></p>
<h1 id="快速开始">快速开始</h1>
<p>这里官方教程写的不是很详细，安装完之后让直接用命令行方式启动程序，因为<code>paddleVideo</code>这个库里面有三个inference模型，分别是</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Inference models that Paddle provides are listed as follows:</span><br><span class="line"></span><br><span class="line">&#123;&#x27;TSN&#x27;, &#x27;ppTSM&#x27;, &#x27;TSM&#x27;&#125;</span><br></pre></td></tr></table></figure>
<p>这里安装好环境之后，直接命令行是会报错的，因为还没有下载模型参数以及label，会报<code>'If you want to use your own model, Please input model_file as model path!'</code>的错误。</p>
<p>这时候进入python环境，跑以下程序:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> ppvideo <span class="keyword">import</span> PaddleVideo</span><br><span class="line">clas = PaddleVideo(model_name=<span class="string">&#x27;ppTSM&#x27;</span>,use_gpu=<span class="literal">False</span>,use_tensorrt=<span class="literal">False</span>)</span><br><span class="line">video_file=<span class="string">&#x27;data/example.avi&#x27;</span></span><br><span class="line">result=clas.predict(video_file)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<p>当你enter第二句话的时候，程序就会开始下载<code>InferenceModel</code>，大概有90.6兆，下载完之后再执行下面几行的程序，其中<code>video_file</code>这里要注意一下，程序里是相对路径。</p>
<p>下载的模型会放在
<code>C:\Users\XXXX\.paddlevideo_inference\inference_model\ppTSM</code></p>
<p>下载的label的txt在<code>'D:\\XXXXX\\anaconda_envs\\paddleVideo\\lib\\site-packages\\ppvideo\\tools\\../data/k400/Kinetics-400_label_list.txt'</code>也就是虚拟环境那个ppvideo里</p>
<h1 id="训练attention-lstm模型">训练attention-lstm模型</h1>
<p>前两节安装完之后，快速开始是为了测试安装正确与否。paddleVideo只有attention-lstm模型，并没有提供在youtube-8m上训练后的参数和label。所以这部分我们使用paddle框架来自己训练。</p>
<h2 id="下载youtube-8m数据集并转换为paddlepaddle处理的格式">下载youtube-8m数据集并转换为paddlepaddle处理的格式</h2>
<p>整个数据集包含3844个训练数据文件和3844个验证数据文件（TFRecord格式）</p>
<p>在linux系统下用curl下载，在windows下可以利用git的bash命令行的方式下载：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl data.yt8m.org/download.py | partition=2/frame/train mirror=asia python</span><br><span class="line"></span><br><span class="line">curl data.yt8m.org/download.py | partition=2/frame/validate mirror=asia python</span><br><span class="line"></span><br><span class="line">curl data.yt8m.org/download.py | partition=2/frame/test mirror=asia python</span><br></pre></td></tr></table></figure>
<p>数据下载完成之后，因为paddlepaddle需要使用pickle的数据格式，所以需要用<code>https://github.com/PaddlePaddle/PaddleVideo/</code>该官方仓库里data/yt8m下的脚本<code>tf2pkl.py</code>脚本进行转换。<code>tf2pkl.py</code>文件运行时需要两个参数，分别是数据源tf文件存放路径和转化后的pkl文件存放路径。</p>
<blockquote>
<p>由于TFRecord文件的读取需要用到Tensorflow，用户要先安装Tensorflow，或者在安装有Tensorflow的环境中转化完数据，再拷贝到data/dataset/youtube8m/pkl目录下。为了避免和PaddlePaddle环境冲突，建议先在其他地方转化完成再将数据拷贝过来。</p>
</blockquote>
<p>上面在转换数据格式的时候还要注意，<code>tf2pkl.py</code>文件用的tensorflow是1.X的版本，用2.0之后版本的需要重新创建虚拟环境，如果不想麻烦，可以直接在这个paddleVideo的虚拟环境里面安装<code>tensorflow-gpu==1.14.0</code></p>
<p>这里如果安装的是gpu版本，需要有对应的cuda版本支持，1.14.0需要10.0的cuda，我的服务器是11.2的cuda，所以这里我就直接安装的是cpu版本的tensorflow。其他的版本支持请查阅：https://www.tensorflow.org/install/source#gpu</p>
<p><img src="/2021/11/25/%E4%BD%BF%E7%94%A8PaddleVideo%E5%AE%9E%E7%8E%B0%E5%9C%A8attention-lstm%E6%A8%A1%E5%9E%8B%E4%B8%8A%E8%AE%AD%E7%BB%83youtube-8m%E6%95%B0%E6%8D%AE/image-20211126133025867.png"></p>
<ul>
<li>在linux中命令：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python tf2pkl.py ./tf/train ./pkl/train</span><br><span class="line">python tf2pkl.py ./tf/val ./pkl/val</span><br></pre></td></tr></table></figure>
<ul>
<li>在windows中我的方案：</li>
</ul>
<p>我看到<code>tf2pkl.py</code>脚本里是用sys命令行的方式调用的，然后他整个脚本都是在linux的模式下的模式。其中有几个地方改动一下就可以适用于windows：</p>
<ol type="1">
<li>删掉以下几行：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># assert (len(sys.argv) == 3)</span><br><span class="line"># source_dir = sys.argv[1]</span><br><span class="line"># target_dir = sys.argv[2]</span><br></pre></td></tr></table></figure>
<p>禁用命令行调用的模式</p>
<ol start="2" type="1">
<li>将record_dir
变成你想转化的文件的文件夹的路径，比如<code>\data\dataset\youtube8m\tf</code></li>
<li>将main函数中将outputdir路径改成你要存储pickle文件的文件夹的路径。</li>
</ol>
<p>完成上述步骤之后，直接在IDE中运行<code>tf2pkl.py</code>就可以开始转化文件了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=1</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>卷积和反卷积中的output shape计算</title>
    <url>/2022/08/29/%E5%8D%B7%E7%A7%AF%E5%92%8C%E5%8F%8D%E5%8D%B7%E7%A7%AF%E4%B8%AD%E7%9A%84output-shape%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<h1 id="conv2d">Conv2D</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_shape = (<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">3</span>)</span><br><span class="line">x = tf.random.normal(input_shape)</span><br><span class="line"></span><br><span class="line">y1 = tf.keras.layers.Conv2D(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&#x27;valid&#x27;</span>)(x) <span class="comment"># output_shape= (input_shape - filter_size + 1) * (input_shape - filter_size + 1)</span></span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">26</span>, <span class="number">26</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y2 = tf.keras.layers.Conv2D(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;same&quot;</span>)(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y3 = tf.keras.layers.Conv2D(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;valid&quot;</span>,strides=<span class="number">2</span>)(x) </span><br><span class="line"><span class="built_in">print</span>(y.shape) <span class="comment"># output_shape= [(input_shape - filter_size)/strides + 1] * [(input_shape - filter_size)/strides + 1]</span></span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">13</span>, <span class="number">13</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y4 = tf.keras.layers.Conv2D(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;same&quot;</span>,strides=<span class="number">2</span>)(x) </span><br><span class="line"><span class="built_in">print</span>(y.shape) <span class="comment"># output_shape= (input_shape - filter_size + 1) * (input_shape - filter_size + 1)</span></span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">14</span>, <span class="number">14</span>, <span class="number">2</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上例中输入是<code>(28,28,3)</code>的<code>shape</code>。
我们在推算<code>output shape</code>时要分两种情况：</p>
<ol type="1">
<li><code>padding="valid"</code>:
<code>output_shape= [(input_shape - filter_size)/strides + 1] * [(input_shape - filter_size)/strides + 1]</code>。如果<code>strides</code>除不尽，则向下取整（取比该数小的那个整数）。所以<code>y3</code>的<code>shape</code>是13</li>
<li><code>padding="same"</code>:
当<code>padding="same"</code>时计算很简单，得到的<code>output</code>的<code>shape</code>一定是<code>input_shape/strides</code>。所以当<code>strides=1</code>时，输入和输出的<code>shape</code>时相等的，比如上面的<code>y2</code>。</li>
</ol>
<p>如果需要更细节的原理可以参考<a href="https://towardsdatascience.com/understand-transposed-convolutions-and-build-your-own-transposed-convolution-layer-from-scratch-4f5d97b2967">博客</a>。这篇文章讲的特别好！重点推荐。</p>
<h1 id="conv2dtranspose">Conv2DTranspose</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y1 = tf.keras.layers.Conv2DTranspose(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;valid&quot;</span>)(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">30</span>, <span class="number">30</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y2 = tf.keras.layers.Conv2DTranspose(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;same&quot;</span>)(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y3 = tf.keras.layers.Conv2DTranspose(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;valid&quot;</span>,strides=<span class="number">2</span>)(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">57</span>, <span class="number">57</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y4 = tf.keras.layers.Conv2DTranspose(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;same&quot;</span>,strides=<span class="number">2</span>)(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">56</span>, <span class="number">56</span>, <span class="number">2</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>同样的总结下这个反卷积我们在推算<code>output shape</code>时也是分两种情况：
1. <code>padding="valid"</code>:
<code>output_length = input_length * stride + max(filter_size - stride, 0)</code>
2. <code>padding="same"</code>:
<code>output_shape=input_shape * strides</code>。当<code>strides=1</code>时，输出<code>shape</code>等于输入<code>shape</code></p>
<p>当<code>padding=valid</code>时的计算有点复杂。再细究一下的话，<code>tensorflow</code>中<code>Conv2DTranspose</code>还接受<code>output_padding</code>这个参数。如果有这个参数的话（默认是None），可以参考<code>tensorflow</code>官方文档给出的<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose">公式</a></p>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-学习笔记-part1:预备知识</title>
    <url>/2021/11/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-part1-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<p>这本书的地址：https://zh.d2l.ai/index.html
中文版，也有英文版，主页有链接过去。</p>
<blockquote>
<p>之前学习过吴恩达的深度学习课程，觉得很优秀。这本书从实践的角度，一步一步实现深度学习的算法，基础实用。有兴趣的还可以去B站follow李沐老师的课程，讲的很好，非常有实践性意义。开此博客是记录我在阅读和跟随这本书的实践过程中的一些心得，我会的知识我不会再记录，这里只会记录对我来说是新知识的知识点，其他详情可以去github上看完整的md文档<a href="https://github.com/d2l-ai/d2l-en" class="uri">https://github.com/d2l-ai/d2l-en</a>。</p>
</blockquote>
<h1 id="数据操作">数据操作</h1>
<p>在MXNet中，<code>NDArray</code>是一个类，也是存储和变换数据的主要工具。</p>
<p>我们也可以将多个<code>NDArray</code>连结（concatenate）。下面分别在行上（维度0，即形状中的最左边元素）和列上（维度1，即形状中左起第二个元素）连结两个矩阵。可以看到，输出的第一个<code>NDArray</code>在维度0的长度（66）为两个输入矩阵在维度0的长度之和（3+33+3），而输出的第二个<code>NDArray</code>在维度1的长度（88）为两个输入矩阵在维度1的长度之和（4+44+4）。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">In [15]:</span><br><span class="line">nd.concat(X, Y, dim=0), nd.concat(X, Y, dim=1)</span><br><span class="line">Out[15]:</span><br><span class="line">(</span><br><span class="line"> [[ 0.  1.  2.  3.]</span><br><span class="line">  [ 4.  5.  6.  7.]</span><br><span class="line">  [ 8.  9. 10. 11.]</span><br><span class="line">  [ 2.  1.  4.  3.]</span><br><span class="line">  [ 1.  2.  3.  4.]</span><br><span class="line">  [ 4.  3.  2.  1.]]</span><br><span class="line"> &lt;NDArray 6x4 @cpu(0)&gt;,</span><br><span class="line"> [[ 0.  1.  2.  3.  2.  1.  4.  3.]</span><br><span class="line">  [ 4.  5.  6.  7.  1.  2.  3.  4.]</span><br><span class="line">  [ 8.  9. 10. 11.  4.  3.  2.  1.]]</span><br><span class="line"> &lt;NDArray 3x8 @cpu(0)&gt;)</span><br></pre></td></tr></table></figure>
<h2 id="处理缺失值">处理缺失值</h2>
<p><em>get_dummies()</em> <em>fillna()</em></p>
<p>[<strong>对于<code>inputs</code>中的类别值或离散值，我们将“NaN”视为一个类别。</strong>]
由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”，
<code>pandas</code>可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。
巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。
缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">os.makedirs(os.path.join(&#x27;..&#x27;, &#x27;data&#x27;), exist_ok=True)</span><br><span class="line">data_file = os.path.join(&#x27;..&#x27;, &#x27;data&#x27;, &#x27;house_tiny.csv&#x27;)</span><br><span class="line">with open(data_file, &#x27;w&#x27;) as f:</span><br><span class="line">    f.write(&#x27;NumRooms,Alley,Price\n&#x27;)  # 列名</span><br><span class="line">    f.write(&#x27;NA,Pave,127500\n&#x27;)  # 每行表示一个数据样本</span><br><span class="line">    f.write(&#x27;2,NA,106000\n&#x27;)</span><br><span class="line">    f.write(&#x27;4,NA,178100\n&#x27;)</span><br><span class="line">    f.write(&#x27;NA,NA,140000\n&#x27;)</span><br><span class="line">    </span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line">print(data)</span><br><span class="line"></span><br><span class="line">inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]</span><br><span class="line">inputs = inputs.fillna(inputs.mean())</span><br><span class="line"></span><br><span class="line">inputs = pd.get_dummies(inputs, dummy_na=True)</span><br><span class="line">print(inputs)</span><br></pre></td></tr></table></figure>
<p>思考题：丢失掉缺失值最多的列</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">data.drop(data.isnull().sum().idxmax(),axis=1)</span><br></pre></td></tr></table></figure>
<p><code>idmax</code>返回sum()最多的id，这里<code>axis=1</code>不能少，drop默认丢弃行。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>图像的读取方式</title>
    <url>/2022/06/21/%E5%9B%BE%E5%83%8F%E7%9A%84%E8%AF%BB%E5%8F%96%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<h1 id="pil">PIL</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&quot;1.jpg&quot;</span>)</span><br><span class="line">img.load()</span><br><span class="line"></span><br><span class="line">array = np.asarrary(img)</span><br><span class="line"></span><br><span class="line">img.show() <span class="comment"># view the picture</span></span><br><span class="line"></span><br><span class="line">img.save(<span class="string">&quot;./new_img.jpg&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>PIL.Image包有很多其他的功能，比如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从array生成图片convert it to a Pillow image</span></span><br><span class="line">Image.fromarray(data,<span class="string">&#x27;RGB&#x27;</span>) <span class="comment"># 如果mode=&#x27;L&#x27;,那么只有一个通道</span></span><br></pre></td></tr></table></figure>
<h1 id="cv2">cv2</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">&quot;./1.jpg&quot;</span>) <span class="comment"># 返回的img的channel顺序是BGR</span></span><br><span class="line"></span><br><span class="line">grey_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) <span class="comment"># 转化为灰度图的模式</span></span><br><span class="line">rgb_img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) <span class="comment"># 转化为RGB的模式</span></span><br></pre></td></tr></table></figure>
<p><code>cv2</code>一个有用的method是resize</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv2.resize(raw_img,(width,height))</span><br></pre></td></tr></table></figure>
<h1 id="tensorflow">tensorflow</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">img = tf.keras.preprocessing.image.load_img(<span class="string">&quot;./i.jpg&quot;</span>,target_size=(<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>)) <span class="comment"># Loads an image into PIL format</span></span><br><span class="line">img = tf.keras.preprocessing.image.img_to_array(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从array转化成PIL image 实例</span></span><br><span class="line">img = tf.keras.preprocessing.image.array_to_img(array) <span class="comment"># array是3D numpy array</span></span><br><span class="line"></span><br><span class="line">tf.keras.utils.save_img(path,array)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>命令行运行python脚本时传入参数的三种方式</title>
    <url>/2021/11/26/%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%BF%90%E8%A1%8Cpython%E8%84%9A%E6%9C%AC%E6%97%B6%E4%BC%A0%E5%85%A5%E5%8F%82%E6%95%B0%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<h1 id="sys.argv">sys.argv</h1>
<p>sys模块是很常用的模块，
它封装了与python解释器相关的数据，例如sys.modules里面有已经加载了的所有模块信息，sys.path里面是PYTHONPATH的内容，而sys.argv则封装了传入的参数数据。
使用sys.argv接收上面第一个命令中包含的参数方式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">print(sys.argv[0],sys.argv[1],sys.argv[2])</span><br></pre></td></tr></table></figure>
<p>其中<code>sys.argv[0]</code>是该脚本的名称，<code>sys.argv[1]</code>才是第一个参数，<code>sys.argv</code>是一个列表</p>
<p>用这种方式，命令行调用方式为：</p>
<p><code>python script.py parameter1 parameter2</code></p>
<h1 id="argparse">argparse</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;Process some integers.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;integers&#x27;</span>, metavar=<span class="string">&#x27;N&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, nargs=<span class="string">&#x27;+&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;an integer for the accumulator&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--sum&#x27;</span>, dest=<span class="string">&#x27;accumulate&#x27;</span>, action=<span class="string">&#x27;store_const&#x27;</span>,</span><br><span class="line">                    const=<span class="built_in">sum</span>, default=<span class="built_in">max</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;sum the integers (default: find the max)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"><span class="built_in">print</span>(args.accumulate(args.integers))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>如何对开源大语言模型微调？需要多少数据？</title>
    <url>/2023/06/08/%E5%A6%82%E4%BD%95%E5%AF%B9%E5%BC%80%E6%BA%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%EF%BC%9F%E9%9C%80%E8%A6%81%E5%A4%9A%E5%B0%91%E6%95%B0%E6%8D%AE%EF%BC%9F/</url>
    <content><![CDATA[<p>随着各大语言模型的开源，很多研究在没有计算资源的情况下，唯一可想的办法就是在各大开源模型上利用垂直领域的数据集来finetune开源大模型，充分利用pretrained模型的能力，而又能让它解决特定的任务。
斯坦福的羊驼alpaca模型的发布掀起了instruction
tuning的一阵狂潮，国内很多工作也在模仿Stanford这个工作方法做自己领域的大模型，其实在接触这些工作的时候我一直有一个疑问，就是什么时候我们该finetune？手里有多少数据的时候你可以finetune。</p>
<p>如果我们要开展微调，数据以及如何组织数据形式和微调的方式是两个主要问题。</p>
<h1 id="微调的方式">微调的方式</h1>
<p>现有的LLM规模太大，因此完全微调模型已并非易事。因此无论学界还是业界都需要一种高效的方法在下游任务数据上训练，这就为参数高效微调（Parameter-efficient
fine-tuning，PEFT）带来了研究空间。PEFT的目的是只训练一小部分参数（可以是大模型自身的，也可以是额外引入的）就能提升模型在下游任务的效果。</p>
<p><a href="http://arxiv.org/abs/2303.15647">Scaling Down to Scale Up: A
Guide to Parameter-Efficient Fine-Tuning</a>
一文总结了现有peft的主要方法，并做了分类：</p>
<figure>
<img src="/2023/06/08/%E5%A6%82%E4%BD%95%E5%AF%B9%E5%BC%80%E6%BA%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%EF%BC%9F%E9%9C%80%E8%A6%81%E5%A4%9A%E5%B0%91%E6%95%B0%E6%8D%AE%EF%BC%9F/image-20230608131344681.png" alt="PEFT methods分类">
<figcaption aria-hidden="true">PEFT methods分类</figcaption>
</figure>
<p>可以看到lora也就是我们现在经常用到的微调手段被分配到了reparametrization-based里面。并且在additive这个分类里，又分了两个子类：adapter-like和soft
prompts。对于每一个分类的解释可以看paper或者参考这篇<a href="https://zhuanlan.zhihu.com/p/619583361">知乎博客</a></p>
<p>我这里只做简单的对比：</p>
<ul>
<li>prefix tuning，prompt
tuning根本就是为了解决人工为每一个任务构造prompt太过于随机性，而且构造的模板有时候是离散化的；另外一个痛点就是如果对模型在下游任务上进行全参数调整，每一个任务得保存一份参数副本，对存储和训练都是考验。所以这一类方法就是让模型通过学习来自动寻找最合适的prompt，称之为soft
prompt.LLM那一部分的参数不做调整，只调整添加的这一部分参数。之后清华大学提出的ptuning和ptuning
v2版本，可以简单的将P-Tuning认为是针对Prompt Tuning的改进，P-Tuning
v2认为是针对Prefix Tuning的改进。</li>
<li>lora这一类方法（AdaLoRA，Qlora）基于的理论是：将LLM在下游任务进行微调时发现改变的参数都是哪些低秩矩阵，所以这些方法重构了一种低秩矩阵运算，这些矩阵里面的参数就可以代表下游任务的知识，这样将LLM的预训练参数和这部分参数进行合并之后就可以适配下游任务了。</li>
</ul>
<h1 id="微调实践applications">微调实践Applications</h1>
<p>这一章节主要介绍一些值得关注的微调实践工作，可以给我们在实际工作中提供一些微调的思路，比如看看别人数据集是如何组织的，有多少的数据量，针对什么任务有了performance的提高，还有做evaluation是咋做的。</p>
<h1 id="stanford-alpaca">Stanford Alpaca</h1>
<p>这是开创instruction
tuning工作的鼻祖，而且斯坦福的代码写的很优秀，特别是用GPT3生成instruction-input-output那一部分，把代码通读一遍可以加深self-instruct方法的理解。而且斯坦福的这个数据集只有52k条，但是有意思的是它在组织这52k条数据的时候是需要充分保持task的多样性的，毕竟我们知道instruction
tuning当时在 Finetuned Language Models Are Zero-Shot Learners
文中被提出的时候，其实是为了提高模型在unseen
task上的zero-shot能力，它有一个重要的前提是finetune的task要多样，这个非常重要！</p>
<blockquote>
<p>现在有一些国内的工作就是直接弄了一些数据就开始finetune，然后叫自己是instruction
tuning，我觉得不太合理。</p>
</blockquote>
<p>最近出了一个工作： LIMA: Less Is More for
Alignment，在LLaMa-65B基础上用1000条instruction数据训练的模型，在43%的情况下，LIMA可以超过或者和GPT4平齐，这真的很厉害了，毕竟只用了1000条数据，而且作者也用斯坦福的方法复刻了52k微调llama-65B的大羊驼，发现还是LIMA优秀一点，作者猜测是因为数据集质量，这1000条数据是精心策划的。</p>
<h1 id="chatglm-6b-p-tuning">Chatglm-6B p-tuning</h1>
<p>基于chatglm-6B的微调项目超级多，chatglm有天然的中文优势，所以国内好多语言模型都是基于清华的这个语言模型做的工作。chatglm-6B给出的官方github
repo中包含了p-tuning v2的<a href="https://github.com/THUDM/P-tuning-v2">代码</a>, p tuning
v2的原理就是将应该人工写的那一部分prompt用参数来学习，LLM预训练好的那一部分参数固定住，只更新添加的这部分参数。参考chatglm-6B自己给出的再ADGEN（广告生成的数据集）上finetuneg
chatglm6B的代码：
https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning，
这部分代码的数据组织部分蛮有意思的，数据集长这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;content&quot;</span>: <span class="string">&quot;类型#上衣*版型#宽松*版型#显瘦*图案#线条*衣样式#衬衫*衣袖型#泡泡袖*衣款式#抽绳&quot;</span>,</span><br><span class="line">    <span class="string">&quot;summary&quot;</span>: <span class="string">&quot;这件衬衫的款式非常的宽松，利落的线条可以很好的隐藏身材上的小缺点，穿在身上有着很好的显瘦效果。领口装饰了一个可爱的抽绳，漂亮的绳结展现出了十足的个性，配合时尚的泡泡袖型，尽显女性甜美可爱的气息。&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>也就是不像alpaca数据集有instruction了，只是一个映射关系，而且在main.py训练代码里也没有见到处理数据sample时要给每一个数据前加instruction，唯一加的就是在content前加了一个“问：”，在summary前加了一个“答：”。</p>
<h2 id="gorilla">Gorilla</h2>
<p>这是一个微调LLaMa-7B让LLM实现用语言方式让LLM返回API调用代码的工作。</p>
<p>参考：</p>
<ul>
<li>https://gorilla.cs.berkeley.edu/</li>
<li><a href="https://github.com/ShishirPatil/gorilla">Gorilla Github
Repo</a></li>
<li><a href="http://arxiv.org/abs/2305.15334">Gorilla: Large Language
Model Connected with Massive APIs</a></li>
</ul>
<figure>
<img src="/2023/06/08/%E5%A6%82%E4%BD%95%E5%AF%B9%E5%BC%80%E6%BA%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%EF%BC%9F%E9%9C%80%E8%A6%81%E5%A4%9A%E5%B0%91%E6%95%B0%E6%8D%AE%EF%BC%9F/image-20230608133459274.png" alt="image-20230608133459274">
<figcaption aria-hidden="true">image-20230608133459274</figcaption>
</figure>
<p>产生数据集的方式跟alpaca如出一辙，利用了self-instruct方式，让GPT-4来产生instruction-API的训练数据。而且它分了两种模式来训练，一种是有retriever的模式，在instruction中添加了数据库里关于这个API的帮助信息，一种就是instruction-API的格式。因为repo里并没有开源这部分的训练数据，所以我们也只能看论文来猜测数据是长这样的，等作者公布了数据可以再补充这部分数据到底长什么样子。</p>
<p>我们在使用这个model进行推理时，输入给他的就是一串我呢本，告诉它你想获得一个怎么样的API，比如“I
would like to identify the objects in an image”或者更模糊一点：“I am
going to the zoo, and would like to track
animals”。在zero-shot模式下这个instruction会直接给到gorilla，模型会给你返回一串API调用的代码，像这样：</p>
<figure>
<img src="/2023/06/08/%E5%A6%82%E4%BD%95%E5%AF%B9%E5%BC%80%E6%BA%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%EF%BC%9F%E9%9C%80%E8%A6%81%E5%A4%9A%E5%B0%91%E6%95%B0%E6%8D%AE%EF%BC%9F/image-20230608134439167.png" alt="image-20230608134439167">
<figcaption aria-hidden="true">image-20230608134439167</figcaption>
</figure>
<p>总体来说这个项目想法蛮有意思的，就是很多东西暂时还没开源，我们拭目以待吧，现在就用用就行。</p>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>李沐-斯坦福21秋季：实用机器学习（学习笔记）Part I: Basic ML Modeling</title>
    <url>/2021/11/16/%E6%9D%8E%E6%B2%90-%E6%96%AF%E5%9D%A6%E7%A6%8F21%E7%A7%8B%E5%AD%A3%EF%BC%9A%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%89Part-I-Basic-ML-Modeling/</url>
    <content><![CDATA[<h1 id="数据部分-data-part">数据部分 Data Part</h1>
<h2 id="数据获取-data-acquisition">数据获取 Data Acquisition</h2>
<p>比较popular的数据集：</p>
<p><img src="/2021/11/16/%E6%9D%8E%E6%B2%90-%E6%96%AF%E5%9D%A6%E7%A6%8F21%E7%A7%8B%E5%AD%A3%EF%BC%9A%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%89Part-I-Basic-ML-Modeling/image-20211112163641407.png"></p>
<p>更多的数据集list见 : <a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research" class="uri">https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research</a></p>
<p>哪里找到数据集:</p>
<p><img src="/2021/11/16/%E6%9D%8E%E6%B2%90-%E6%96%AF%E5%9D%A6%E7%A6%8F21%E7%A7%8B%E5%AD%A3%EF%BC%9A%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%89Part-I-Basic-ML-Modeling/image-20211112163931292.png"></p>
<p>数据的生成：（在数据不够的情况下的技术）</p>
<ul>
<li>GAN 无监督生成数据</li>
<li>数据增强 Data Augmentations</li>
</ul>
<p>例子：Back Translation 语义相同，句子不一样的句子</p>
<h2 id="探索性数据分析-exploratory-data-analysis">探索性数据分析
Exploratory data analysis</h2>
<p><code>data = pd.read_csv('house_sales.zip')</code>pandas可以支持读取压缩文件，text文本文件建议存储成zip，处理起来更迅速。</p>
<p><code>seaborn</code>库能画比matplotlib更多的图形。</p>
<p>自留作业：将数据下载下来看一遍，过一遍代码（To do）</p>
<h2 id="数据清理-data-cleaning">数据清理 Data Cleaning</h2>
<p>data very noisy ---&gt; data cleaning</p>
<p>数据清洗工具Trifacta Wrangler<a href="https://www.trifacta.com/" class="uri">https://www.trifacta.com/</a></p>
<h2 id="数据变换-data-transformation">数据变换 Data Transformation</h2>
<h3 id="对数值化column的normalization的方式">对数值化column的Normalization的方式:</h3>
<p><img src="/2021/11/16/%E6%9D%8E%E6%B2%90-%E6%96%AF%E5%9D%A6%E7%A6%8F21%E7%A7%8B%E5%AD%A3%EF%BC%9A%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%89Part-I-Basic-ML-Modeling/image-20211115153221386.png"></p>
<p>Z-score: 最常见，使得均值为0，方差为1。</p>
<h3 id="图片-picture">图片 Picture</h3>
<ol type="1">
<li>reduce image size减小图片尺寸，ML good at low-resolution images</li>
<li>对于jpeg格式的文件，如果存储为中等质量的图，会导致精度下降1%。最好在下采样时采用高质量存储</li>
<li>Image whitening</li>
</ol>
<h3 id="视频-video">视频 Video</h3>
<ol type="1">
<li>often use short video clips(&lt;10s)</li>
</ol>
<h3 id="文本text">文本text</h3>
<ol type="1">
<li>stemming and lemmatization 词根化和语法化: a word ---&gt; a common
base form</li>
<li>Tokenization 词源化：text ---&gt; a list of tokens</li>
</ol>
<p>词源可以是一个词，亦可以是一个character，或者是一个subwords</p>
<h3 id="summary">Summary</h3>
<p>Transform data into formats preferred by ML algorithms</p>
<ul>
<li><p>Tabular: normalize real value features</p></li>
<li><p>Images: cropping, downsampling, whitening</p></li>
<li><p>Videos: clipping, sampling frames</p></li>
<li><p>Text: stemming, lemmatization, tokenization</p></li>
</ul>
<p>Need to balance storage, quality, and loading speed</p>
<h2 id="特征工程-feature-engineering">特征工程 Feature Engineering</h2>
<h3 id="tabular-data-features">tabular data features</h3>
<ul>
<li>Int/float: directly use or or bin to unique int values</li>
<li>种类数据：one-hot encoding</li>
</ul>
<p>对于很多类的情况，将常见的类别保留，其他的类都归类为"Unkown"</p>
<ul>
<li>Date-time：转化为list</li>
</ul>
<p><code>[year, month, day, day_of_year, week_of_year, day_of_week]</code></p>
<ul>
<li>Feature combination: 用于组合比较相关的特征</li>
</ul>
<h3 id="文本text-features">文本text features</h3>
<ul>
<li>用token features表示text
<ol type="1">
<li>Bag-of-words(BoW) model</li>
</ol>
词典 --&gt; one-hot 没有语义信息了</li>
</ul>
<p>​ 2. Word Embeddings(e.g. Word2Vec)</p>
<ul>
<li>pre-trained language models(e.g. BERT,GPT-3)</li>
</ul>
<h3 id="图片视频-features">图片、视频 features</h3>
<ul>
<li>pre-trained model</li>
</ul>
<ol type="1">
<li>ResNet: trained with ImageNet (image classification)</li>
<li>I3D: trained with Kinetics (action classification)</li>
</ol>
<h3 id="总结">总结</h3>
<p>对于文本，图片和视频，一般采取DL的方式抽取特征，但对于Tabular的数据，暂时还没有特别好的深度学习模型来抽取，一般靠手动。</p>
<h2 id="data-part部分总结">Data part部分总结</h2>
<ul>
<li>label的质量 vs data的量之间要trade-off</li>
<li>大数据的管理：存储，处理，版本控制，安全</li>
<li>数据质量</li>
</ul>
<ol type="1">
<li>多样性</li>
<li>unbiased</li>
<li>fairness</li>
</ol>
<h1 id="ml-model-part">ML model part</h1>
<p>监督学习的种类：</p>
<p><img src="/2021/11/16/%E6%9D%8E%E6%B2%90-%E6%96%AF%E5%9D%A6%E7%A6%8F21%E7%A7%8B%E5%AD%A3%EF%BC%9A%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%89Part-I-Basic-ML-Modeling/image-20211116094045676.png"></p>
<h2 id="决策树-decision-tree">决策树 Decision Tree</h2>
<p>pros:</p>
<ol type="1">
<li>可以解释</li>
<li>可以处理数值型数据，也可以处理分类型数据（classification&amp;regression）</li>
</ol>
<p>cons:</p>
<ol type="1">
<li>non-robust(可以用ensemble集成 learning)</li>
<li>Complex tree</li>
<li>难以部署和并发执行</li>
</ol>
<p>特点：</p>
<ul>
<li>树模型是在工业界用的比较多的 Simple，fine-tune，often gives
satisfied results</li>
<li>可以通过ensemble trees去reduce bias&amp;variance
<ul>
<li>RF: trees trained in parallel with randomness</li>
<li>GBDT: train in sequential on residuals</li>
</ul></li>
</ul>
<h3 id="提升tree-model的方法">提升Tree Model的方法：</h3>
<p>Random Forest：</p>
<ol type="1">
<li>每一棵树都是单独训练的</li>
<li>对于分类任务，采取majority
voting；对于回归任务，对所有树的结果取平均</li>
<li>随机性来自于哪里？</li>
</ol>
<ul>
<li>随机采样训练样本来训练树：Bagging with
replacement，一个样本在一次训练中可以出现多次</li>
<li>随机采样feature中的subset来训练</li>
</ul>
<p>Gradient Boosting Decision Tree:</p>
<ol type="1">
<li>和RF不一样的是，GBDT是顺序地train多个树</li>
<li>在第<code>t</code>个时间步，前<code>t-1</code>个训练好的树的sum<code>Ft()</code></li>
</ol>
<p>训练一个新的树<code>f(t)</code>on
<code>&#123;(xi,yi-Ft(xi))&#125; i from 1...t-1</code></p>
<h2 id="线性模型-linear-model">线性模型 linear model</h2>
<h3 id="线性模型做回归">线性模型做回归</h3>
<ul>
<li>回归问题的output是一个实数值，所以objective通常为MSE（均方误差）</li>
</ul>
<h3 id="线性模型做分类多分类">线性模型做分类（多分类）</h3>
<ul>
<li>output用向量表示，y用one-hot</li>
<li>minimize loss MSE（注意：这里是向量的MSE）</li>
<li>predict label为向量的元素中最大值的那个类别</li>
</ul>
<p>以上方式有一个cons:将不重要的分类的值的loss也算进去了，MSE计算的是向量的每一个元素的差，所以可以用Softmax
Regression来解决：</p>
<p>将上述输出的向量<code>o</code>输入到Softmax操作子中，输出仍然是一个向量，但是向量中的元素的和是为1的，而且每一个元素的范围都在(0,1)之间。这里虽然做了非线性的变化，但是还是一个线性模型。</p>
<p><img src="/2021/11/16/%E6%9D%8E%E6%B2%90-%E6%96%AF%E5%9D%A6%E7%A6%8F21%E7%A7%8B%E5%AD%A3%EF%BC%9A%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%89Part-I-Basic-ML-Modeling/image-20211116111024159.png"></p>
<p>在求loss的阶段，比较两个概率型的向量用cross-entropy loss来做:</p>
<p><img src="/2021/11/16/%E6%9D%8E%E6%B2%90-%E6%96%AF%E5%9D%A6%E7%A6%8F21%E7%A7%8B%E5%AD%A3%EF%BC%9A%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%89Part-I-Basic-ML-Modeling/image-20211116111220963.png"></p>
<p>以上方式将模型转移到更加关注正确类的性能上来，而不是放在别的类上。</p>
<ul>
<li>线性模型通常使用MSE作为loss function</li>
<li>Softmax Regression用于多分类的问题
<ul>
<li>将输出的向量用softmax处理为概率型向量，并使用cross-entropy当做loss
function</li>
<li>cross-entropy用于求两个向量间的loss</li>
</ul></li>
</ul>
<h2 id="neural-network神经网络">Neural Network神经网络</h2>
<p>课程在这里介绍了三种模型：MLP（感知机），CNN和RNN。总体来说介绍的比较简单，如果想深入了解模型细节，建议移步吴恩达老师的深度学习课程，吴恩达的课程中这三种模型都花了一课时的时间来讲，特别是CNN部分讲的特别的好，结合吴恩达老师的作业学起来很受益。</p>
<h3 id="cnn">CNN</h3>
<p>在图片中寻找object的两个宗旨:</p>
<ol type="1">
<li>translation invariance
模型对于图片上不同位置的object的输出是类似的</li>
</ol>
<p>在卷积中的体现就是：每一次卷积核Kernel对某个位置计算完之后，会平移至下一个区域继续用该权重值进行计算。</p>
<ol type="1">
<li>locality 图片中相邻像素点的pixel相似</li>
</ol>
<p>在卷积中的体现就是只会计算图片中局部位置的加权和，如3*3大小的地方。</p>
<h4 id="pooling-layer-汇聚层meanmax-pooling">Pooling Layer
汇聚层：（mean/max pooling）</h4>
<p>why pooling？卷积计算的output值对于位置相当敏感。</p>
<h4 id="cnn的作用">cnn的作用</h4>
<p>用于抽取空间信息，task只要满足以上说的两种特征，就可以使用CNN来做。</p>
<ul>
<li>卷积层之后要有Activation</li>
<li>using Pooling layer 去reduce location sensitivity</li>
</ul>
<p>现在常用的CNN有: AlexNet, VGG, Inceptions, ResNet, MobileNet</p>
<h2 id="model-selection">Model Selection</h2>
<p>Tabular数据：Trees，线性模型，MLP（多个全连接层堆起来，中间加入non-linear
activations）</p>
<p>Text数据：RNNs（全连接层中接入过去的信息）</p>
<p>Images/audio/video: CNNs</p>
<p>对于Text和Images/audio/video，前者是需要处理时序性特征（1维），后者是空间性特征（2维），对于这两个任务，<code>Transformers</code>都可以解决。</p>
<p>​</p>
<p>​</p>
]]></content>
      <categories>
        <category>李沐-实用机器学习(学习笔记)</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>浅析stanford alpaca羊驼代码</title>
    <url>/2023/06/20/%E6%B5%85%E6%9E%90stanford-alpaca%E7%BE%8A%E9%A9%BC%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<p>斯坦福的羊驼alpaca开启了大家微调大语言模型的先河，现在很多国内的工作都是基于斯坦福的羊驼模型的范式来微调chatglm-6B,alpaca的<a href="https://github.com/tatsu-lab/stanford_alpaca">github</a>.</p>
<p>之前我一个很厉害的师兄说过，要想写代码写的厉害或者有所提高，最重要的就是要多读别人优秀的代码，多思考别人为什么这么写，如果让自己写的话是不是可以做到如此高效。这就是我想写这篇博客的原因，我是最近几个月才接触的huggingface的transformer库，发现很多API虽然设计的很简单，但里面功能丰富，不可能一下子就掌握住，所以我的办法是多看别人是如何写的，我的主要参考repo就是alpaca和chatglm-6B官方repo给出的那些finetune
LLM的代码。</p>
<p>回到羊驼alpca这份代码，不得不说斯坦福的这份代码写的真的很优秀，值得一句一句去debug。</p>
<h1 id="数据准备部分">数据准备部分</h1>
<p>代码在<code>generate_instruction.py</code>内。</p>
<p>这部分代码主要功能是实现由seed_tasks.jsonl作为模板，让GPT3.5来根据两个seed生成一些instruction和input，output。思想基于self-instruct的理念，在我另外一篇博客instrcution
tuning中有所详细介绍。</p>
<p>这里的启动函数是<code>def generate_instruction_following_data()</code>,
首先会将seed_tasks读取进来，然后从中随机选取num_prompt_instructions个数据，默认是三个：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)</span><br></pre></td></tr></table></figure>
<p>注意这里羊驼采用了向chatgpt传输batch请求的方式，也就是一次性向chatgpt传输多个prompt，程序里默认是5个prompt一起传输给gpt，然后每一个prompt长什么样子呢？</p>
<p>举一个简单的例子, 下面这是一个seed_task</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;&quot;id&quot;: &quot;seed_task_1&quot;, &quot;name&quot;: &quot;antonym_relation&quot;, &quot;instruction&quot;: &quot;What is the relation between the given pairs?&quot;, &quot;instances&quot;: [&#123;&quot;input&quot;: &quot;Night : Day :: Right : Left&quot;, &quot;output&quot;: &quot;The relation between the given pairs is that they are opposites.&quot;&#125;], &quot;is_classification&quot;: false&#125;</span><br></pre></td></tr></table></figure>
<p>作者将三个seed_task拼接在一起，然后前面加上事先定义好的prompt：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_prompt</span>(<span class="params">prompt_instructions</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Encode multiple prompt instructions into a single string.&quot;&quot;&quot;</span></span><br><span class="line">    prompt = <span class="built_in">open</span>(<span class="string">&quot;./prompt.txt&quot;</span>).read() + <span class="string">&quot;\n&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, task_dict <span class="keyword">in</span> <span class="built_in">enumerate</span>(prompt_instructions):</span><br><span class="line">        (instruction, <span class="built_in">input</span>, output) = task_dict[<span class="string">&quot;instruction&quot;</span>], task_dict[<span class="string">&quot;input&quot;</span>], task_dict[<span class="string">&quot;output&quot;</span>]</span><br><span class="line">        instruction = re.sub(<span class="string">r&quot;\s+&quot;</span>, <span class="string">&quot; &quot;</span>, instruction).strip().rstrip(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">        <span class="built_in">input</span> = <span class="string">&quot;&lt;noinput&gt;&quot;</span> <span class="keyword">if</span> <span class="built_in">input</span>.lower() == <span class="string">&quot;&quot;</span> <span class="keyword">else</span> <span class="built_in">input</span></span><br><span class="line">        prompt += <span class="string">f&quot;###\n&quot;</span></span><br><span class="line">        prompt += <span class="string">f&quot;<span class="subst">&#123;idx + <span class="number">1</span>&#125;</span>. Instruction: <span class="subst">&#123;instruction&#125;</span>\n&quot;</span></span><br><span class="line">        prompt += <span class="string">f&quot;<span class="subst">&#123;idx + <span class="number">1</span>&#125;</span>. Input:\n<span class="subst">&#123;<span class="built_in">input</span>&#125;</span>\n&quot;</span></span><br><span class="line">        prompt += <span class="string">f&quot;<span class="subst">&#123;idx + <span class="number">1</span>&#125;</span>. Output:\n<span class="subst">&#123;output&#125;</span>\n&quot;</span></span><br><span class="line">    prompt += <span class="string">f&quot;###\n&quot;</span></span><br><span class="line">    prompt += <span class="string">f&quot;<span class="subst">&#123;idx + <span class="number">2</span>&#125;</span>. Instruction:&quot;</span></span><br><span class="line">    <span class="keyword">return</span> prompt</span><br></pre></td></tr></table></figure>
<p>事先定义好的prompt长这样：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">You are asked to come up with a set of 20 diverse task instructions. These task instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions.</span><br><span class="line"></span><br><span class="line">Here are the requirements:</span><br><span class="line">1. Try not to repeat the verb for each instruction to maximize diversity.</span><br><span class="line">2. The language used for the instruction also should be diverse. For example, you should combine questions with imperative instrucitons.</span><br><span class="line">3. The type of instructions should be diverse. The list should include diverse types of tasks like open-ended generation, classification, editing, etc.</span><br><span class="line">4. A GPT language model should be able to complete the instruction. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a reminder because it cannot perform any action.</span><br><span class="line">5. The instructions should be in English.</span><br><span class="line">6. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a question is permitted.</span><br><span class="line">7. You should generate an appropriate input to the instruction. The input field should contain a specific example provided for the instruction. It should involve realistic data and should not contain simple placeholders. The input should provide substantial content to make the instruction challenging but should ideally not exceed 100 words.</span><br><span class="line">8. Not all instructions require input. For example, when a instruction asks about some general information, &quot;what is the highest peak in the world&quot;, it is not necssary to provide a specific context. In this case, we simply put &quot;&lt;noinput&gt;&quot; in the input field.</span><br><span class="line">9. The output should be an appropriate response to the instruction and the input. Make sure the output is less than 100 words.</span><br><span class="line"></span><br><span class="line">List of 20 tasks:</span><br></pre></td></tr></table></figure>
<p>理解起来就是作者在list of 20 tasks后面跟上了三个seed
tasks，也就是给gpt打个样，让它知道按这个模式去生成。这里有个地方值得参考的：</p>
<blockquote>
<p>用模板的时候给每一个example配上分隔符，这里作者采用了###,
不仅如此，作者还采用了序号的方式，这些都是为了方便后面对gpt返回的text进行处理</p>
</blockquote>
<p>这里还有一个值得学习的地方在utils.py内，我们在使用openai的api获取回复时，有时候会遇到prompt过长的问题，羊驼catch了这个报错，将prompt的长度变为原来的80%，然后再向gpt发送请求，正是由于这份耐心，这个代码的耦合性就没那么高，所以易用性非常强，非常值得野生程序员学习。</p>
<hr>
<p>gpt返回的文本羊驼模型还做了similarity的计算，将相似度超过一定阈值的instrcution剔除掉。这部分代码可以直接复用。</p>
<h1 id="train中的数据处理">train中的数据处理</h1>
<p>羊驼模型采用了一个函数生成transformers库所需要的参数：
<code>make_supervised_data_module</code>，该函数返回一个dict，其中字典的key就是我们初始化Trainer类所需要的train_dataset,eval_dataset和data_collator。这个函数里首先是构建数据集的类<code>SupervisedDataset</code>，继承自Dataset.
注意pytorch里规定如果你想要创建一个自建的Dataset，这个继承自Dataset的子类必须重写<code>__len__</code>和<code>__getitem__</code>两个方法，羊驼这里写的是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SupervisedDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Dataset for supervised fine-tuning.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data_path: <span class="built_in">str</span>, tokenizer: transformers.PreTrainedTokenizer</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SupervisedDataset, self).__init__()</span><br><span class="line">        logging.warning(<span class="string">&quot;Loading data...&quot;</span>)</span><br><span class="line">        list_data_dict = utils.jload(data_path)</span><br><span class="line"></span><br><span class="line">        logging.warning(<span class="string">&quot;Formatting inputs...&quot;</span>)</span><br><span class="line">        prompt_input, prompt_no_input = PROMPT_DICT[<span class="string">&quot;prompt_input&quot;</span>], PROMPT_DICT[<span class="string">&quot;prompt_no_input&quot;</span>]</span><br><span class="line">        sources = [</span><br><span class="line">            prompt_input.format_map(example) <span class="keyword">if</span> example.get(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;&quot;</span>) != <span class="string">&quot;&quot;</span> <span class="keyword">else</span> prompt_no_input.format_map(example)</span><br><span class="line">            <span class="keyword">for</span> example <span class="keyword">in</span> list_data_dict</span><br><span class="line">        ]</span><br><span class="line">        targets = [<span class="string">f&quot;<span class="subst">&#123;example[<span class="string">&#x27;output&#x27;</span>]&#125;</span><span class="subst">&#123;tokenizer.eos_token&#125;</span>&quot;</span> <span class="keyword">for</span> example <span class="keyword">in</span> list_data_dict]</span><br><span class="line"></span><br><span class="line">        logging.warning(<span class="string">&quot;Tokenizing inputs... This may take some time...&quot;</span>)</span><br><span class="line">        data_dict = preprocess(sources, targets, tokenizer)</span><br><span class="line"></span><br><span class="line">        self.input_ids = data_dict[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">        self.labels = data_dict[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.input_ids)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, i</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(input_ids=self.input_ids[i], labels=self.labels[i]) <span class="comment"># 这里key必须是input_ids和labels,这是由于是llama模型的规定。</span></span><br></pre></td></tr></table></figure>
<p>羊驼模型用的DataCollator是自定义的，首先解释下datacollator是什么东西，transformers的官方文档的解释是：</p>
<blockquote>
<p>Data collators are objects that will form a batch by using a list of
dataset elements as input. These elements are of the same type as the
elements of <code>train_dataset</code> or <code>eval_dataset</code>.</p>
</blockquote>
<p>也就是你把所有的文本用tokenizer转化成input_ids和labels之后，要把他们组织成batch的形式，不仅如此，collator还能做一些数据处理的工作。它的输入就是我们之前的数据集，注意我们数据集的组织形式每一个数据sample它是一个字典，字典有两个key。所以羊驼这里首先将其拆分,
一句话解决，非常善于利用[ for
句式]，让我写的话应该是写成非常冗余的两个for循环。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_ids, labels = <span class="built_in">tuple</span>([instance[key] <span class="keyword">for</span> instance <span class="keyword">in</span> instances] <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">&quot;input_ids&quot;</span>, <span class="string">&quot;labels&quot;</span>))</span><br></pre></td></tr></table></figure>
<p>后面就是很简单的train了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.train()</span><br><span class="line">trainer.save_state()</span><br><span class="line">trainer.save_model(output_dir=training_args.output_dir)</span><br></pre></td></tr></table></figure>
<p>我当时看到这里的时候有点奇怪，因为我之前的习惯还是tensorflow那一套，基本上你把数据处理完之后还要prefetech，batch等，但是这里感觉transformer全部做了集成，可以仔细看羊驼模型的训练启动命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">torchrun --nproc_per_node=4 --master_port=&lt;your_random_port&gt; train.py \</span><br><span class="line">    --model_name_or_path <span class="string">&quot;facebook/opt-6.7b&quot;</span> \</span><br><span class="line">    --data_path ./alpaca_data.json \</span><br><span class="line">    --bf16 True \</span><br><span class="line">    --output_dir &lt;your_output_dir&gt; \</span><br><span class="line">    --num_train_epochs 3 \</span><br><span class="line">    --per_device_train_batch_size 4 \</span><br><span class="line">    --per_device_eval_batch_size 4 \</span><br><span class="line">    --gradient_accumulation_steps 8 \</span><br><span class="line">    --evaluation_strategy <span class="string">&quot;no&quot;</span> \</span><br><span class="line">    --save_strategy <span class="string">&quot;steps&quot;</span> \</span><br><span class="line">    --save_steps 2000 \</span><br><span class="line">    --save_total_limit 1 \</span><br><span class="line">    --learning_rate 2e-5 \</span><br><span class="line">    --weight_decay 0. \</span><br><span class="line">    --warmup_ratio 0.03 \</span><br><span class="line">    --lr_scheduler_type <span class="string">&quot;cosine&quot;</span> \</span><br><span class="line">    --logging_steps 1 \</span><br><span class="line">    --fsdp <span class="string">&quot;full_shard auto_wrap&quot;</span> \</span><br><span class="line">    --fsdp_transformer_layer_cls_to_wrap <span class="string">&#x27;OPTDecoderLayer&#x27;</span> \</span><br><span class="line">    --tf32 True</span><br></pre></td></tr></table></figure>
<p>其中的per_device_train_batch_size就指定了一个device上弄几个数据，也就是batch_size是多少，另外loss计算这些都是pretrained模型定好的，所以不用管，包括optimizer，所以我们在训练的时候只需要指定训练过程中用到的参数，比如保存步数，学习率，训练多少个epoch等。这是finetune大语言模型和之前做深度学习模型不一样的地方，技术往往更新的太快，都快看不懂大家写的代码了，怎么咔咔一两句就开始训练了，所以函数集成太厉害也不是只有好处。</p>
<h1 id="推荐阅读">推荐阅读</h1>
<ul>
<li>https://mp.weixin.qq.com/s/ehEM04xmeJyqB4z7rmLKBQ
讲解self-instruct方法</li>
</ul>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>量化大模型技术</title>
    <url>/2023/07/05/%E9%87%8F%E5%8C%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF/</url>
    <content><![CDATA[<p>接触LLM的时候，特别是阅读chatglm-6B的代码仓库时，有一个参数叫量化等级，选项有FP16,INT8和INT4.如果采用int8来量化模型的话，最低GPU显存就可以从13G降低到8G。虽然知道大概意思是什么意思,但具体技术原理并不太懂。所以开一个博客记录下自己了解量化大模型的过程。</p>
<p>首先了解一下量化是什么东西，参考的paper是<a href="https://arxiv.org/abs/2205.07877">A Comprehensive Survey on Model
Quantization for Deep Neural Networks</a>.
可以简单的把量化理解成将一些连续的数据用离散的数据来表示，这样就会显著减少所需要的存储。这是符合我们的认知的，就像我们在c++编程中，将一个变量赋值为float还是int，在内存地址中占据的bit量是不一样的。</p>
<p>神经网络中三种类型的数据可以被量化，weights，activations和gradients。</p>
<figure>
<img src="/2023/07/05/%E9%87%8F%E5%8C%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF/image-20230705105510600.png" alt="quantized components">
<figcaption aria-hidden="true">quantized components</figcaption>
</figure>
<p>上面表格中总结了各种量化方法，其中W代表只是对神经网络中的weights进行量化，以此类推。</p>
<p>time of
quantization总结了在什么阶段对这些参数进行量化。有两种类型QAT表示在训练阶段进行量化，而PTQ表示在训练完之后进行量化。</p>
<p>QAT:
训练和量化同时进行，模型是以量化后的值进行训练的。这其实给模型的收敛带来了很多挑战，往往需要更多的迭代次数</p>
<p>PTQ：模型以full-precision的方式训练，训练完之后，到了inference阶段我们使用该方法对模型的参数进行量化。可以想象，经过量化之后的模型，模型的准确率会有一定的下降，所以现在大部分方法提出需要对量化之后的模型再进行finetune，从而让模型达到一个可接受的accuracy。<img src="/2023/07/05/%E9%87%8F%E5%8C%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF/image-20230705110321750.png" alt="PTQ and retrain"></p>
<p>从上图的总结看来，QAT方法采用的比较多，根本原因还在于QAT的方式比PTQ的方式准确率要更高一点。</p>
<h1 id="bit-matrix-multiplication">8-bit Matrix multiplication</h1>
<p>属于量化大模型的论文的必读paper之一：<a href="http://arxiv.org/abs/2208.07339">LLM.int8(): 8-bit Matrix
Multiplication for Transformers at Scale</a></p>
<p>做的事情：提出了一个针对transformer网络中的feed-forward和attention
projection矩阵计算的int8方式，可以有效减少模型在inference阶段的占用内存。175B参数的模型（以16/32
bit类型存储的checkpoints）以int8类型的方式load进gpu，不仅可以load而且performance也不会下降。好牛逼！</p>
<figure>
<img src="/2023/07/05/%E9%87%8F%E5%8C%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF/image-20230705114034989.png" alt="llm.int(8)">
<figcaption aria-hidden="true">llm.int(8)</figcaption>
</figure>
<p>具体咋做的呢？</p>
<p>对于这种方法而言，我们拿到的是一个以混合精度也就是FP16训练好的模型，然后它本身是一个transformer的架构，我们对其中的feed-forward和attention
projection计算里的weights进行量化。那么对于inference阶段我们拿过来的就是FP16精度的输入和FP16精度的weights们，对于正常的值我们走上面的那一条计算逻辑，对于outliers走下面的计算路径，最终将两者的结果相加。其实我们可以发现tricks都在上面那条线，就是如果将FP16精度的x和w先转化为int8类型，然后让两者相乘得到int32类型，然后再将int32类型的结果转化为fp16，技术上是可行的，但最终的是要在这个“降维&amp;升维”的过程中不损失精度，这就很tricky了。可以参考阅读<a href="https://huggingface.co/blog/hf-bitsandbytes-integration">blog</a></p>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>训练一个information retrival</title>
    <url>/2023/06/30/%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AAinformation-retrival/</url>
    <content><![CDATA[<p>近期中文大语言模型出现了好多产品，独领风骚的chatglm-6B确实表现很不错，所以大家就纷纷下场想做一个自己领域的大语言模型，最近看到一篇<a href="http://arxiv.org/abs/2305.15062">Lawyer LLaMA Technical
Report</a>，作者基于llama模型做了一个法律的语言模型。我觉得这篇文章值得想在垂直领域做语言模型的研究者参考，特别是它所采取的路径：</p>
<figure>
<img src="/2023/06/30/%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AAinformation-retrival/image-20230630105909352.png" alt="training process of lawyer LLaMA">
<figcaption aria-hidden="true">training process of lawyer
LLaMA</figcaption>
</figure>
<p>也有<a href="https://mp.weixin.qq.com/s/XYJMDND4Od41WECqlxt3Fw">博客</a>粗略介绍了这篇文章。</p>
<p>我重点关注的是这个工作里的信息抽取：</p>
<figure>
<img src="/2023/06/30/%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AAinformation-retrival/image-20230630110134378.png" alt="image-20230630110134378">
<figcaption aria-hidden="true">image-20230630110134378</figcaption>
</figure>
<p>思路大概就是和斯坦福的dsp一个思路，用户问一个问题，第一种方式就是直接把这个问题抛给llm，让它去回答，第二种就是在把问题抛给llm之前先过一道retrival模型，这个模型会抽取一些和回答这个问题相关的一些context，将这些context加入prompt中，然后再抛给llm。这有一个专门的名词，在Demonstrate-Search-Predict:
Composing retrieval and language models for knowledge-intensive NLP
中有所介绍：
<em>retrieve-then-read</em>。有兴趣的可以再看一下斯坦福在做这部分工作时出的<a href="https://github.com/stanfordnlp/dsp/blob/main/intro.ipynb">notebook</a>介绍，看完你就知道为啥要retrieve一些context加入到prompt中去。</p>
<p>可惜的是laywer
llama这篇文章并没有详细的介绍它的retrieve咋做的，文章中也是一笔带过，数据规模包括组织形式一概没说。我主要是参考斯坦福的dsp这框架中retrieve的实现，它是基于ColBERTv2做的抽取模块，但其实colbert预训练的模型是在微软的msmarco数据集上训练的，也就是没有中文，那必然对中文的适配度应该就不太行，所以我就想找找有咩有好心的博主写过自己用colbert在自己组织的语料库上训练的过程，发现没有，所以这就是我写这篇文章的动机啦，希望能给后面想用自己组建的数据集训练一个colbert做信息抽取的童鞋一点参考。</p>
<p>以下的内容参考<a href="https://github.com/stanford-futuredata/ColBERT">colbert
github</a></p>
<h1 id="数据组织">数据组织</h1>
<h1 id="模型使用">模型使用</h1>
<h2 id="用官方训练好的checkpoints">用官方训练好的checkpoints</h2>
<p>参考https://github.com/stanford-futuredata/ColBERT/blob/main/docs/intro.ipynb</p>
<p>里面的输就LoTTE的url已经失效了，包括checkpoints的下载地址。可以到hugging
face的仓库找</p>
<ul>
<li>colbertv2.0 checkpoints :
https://huggingface.co/colbert-ir/colbertv2.0/tree/main</li>
<li>lotte 数据集地址：
https://huggingface.co/datasets/colbertv2/lotte/tree/main</li>
</ul>
<p>注意上面这个lotte的数据集的组织方式和colbert官方github仓库里的介绍不一样，这一点上colbert2这个代码仓库的维护做的蛮糟糕的，比羊驼可差远了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.insert(<span class="number">0</span>, <span class="string">&#x27;../&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面这些包是colbert写好的类，可以导入预训练的checkpoints和你自己的querys，documents</span></span><br><span class="line"><span class="keyword">from</span> colbert.infra <span class="keyword">import</span> Run, RunConfig, ColBERTConfig</span><br><span class="line"><span class="keyword">from</span> colbert.data <span class="keyword">import</span> Queries, Collection</span><br><span class="line"><span class="keyword">from</span> colbert <span class="keyword">import</span> Indexer, Searcher</span><br><span class="line"></span><br><span class="line">dataroot = <span class="string">&#x27;downloads/lotte&#x27;</span> <span class="comment"># 假设你已经把lotte放到downloads文件夹下</span></span><br><span class="line">dataset = <span class="string">&#x27;lifestyle&#x27;</span></span><br><span class="line">datasplit = <span class="string">&#x27;dev&#x27;</span></span><br><span class="line"></span><br><span class="line">queries = os.path.join(dataroot, dataset, datasplit, <span class="string">&#x27;questions.search.tsv&#x27;</span>) <span class="comment"># questions.search.txv没找到</span></span><br><span class="line">collection = os.path.join(dataroot, dataset, datasplit, <span class="string">&#x27;collection.tsv&#x27;</span>) <span class="comment"># collections.tsv也没找到</span></span><br><span class="line"></span><br><span class="line">queries = Queries(path=queries)</span><br><span class="line">collection = Collection(path=collection)</span><br><span class="line"></span><br><span class="line"><span class="string">f&#x27;Loaded <span class="subst">&#123;<span class="built_in">len</span>(queries)&#125;</span> queries and <span class="subst">&#123;<span class="built_in">len</span>(collection):,&#125;</span> passages&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># indexing 主要用于对所有的document进行index</span></span><br><span class="line">nbits = <span class="number">2</span>   <span class="comment"># encode each dimension with 2 bits</span></span><br><span class="line">doc_maxlen = <span class="number">300</span>   <span class="comment"># truncate passages at 300 tokens</span></span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&#x27;downloads/colbertv2.0&#x27;</span></span><br><span class="line">index_name = <span class="string">f&#x27;<span class="subst">&#123;dataset&#125;</span>.<span class="subst">&#123;datasplit&#125;</span>.<span class="subst">&#123;nbits&#125;</span>bits&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Run().context(RunConfig(nranks=<span class="number">4</span>, experiment=<span class="string">&#x27;notebook&#x27;</span>)):  <span class="comment"># nranks specifies the number of GPUs to use.</span></span><br><span class="line">    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits)</span><br><span class="line"></span><br><span class="line">    indexer = Indexer(checkpoint=checkpoint, config=config)</span><br><span class="line">    indexer.index(name=index_name, collection=collection, overwrite=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># search </span></span><br><span class="line"><span class="keyword">with</span> Run().context(RunConfig(experiment=<span class="string">&#x27;notebook&#x27;</span>)):</span><br><span class="line">    searcher = Searcher(index=index_name)</span><br><span class="line">query = queries[<span class="number">37</span>]   <span class="comment"># or supply your own query</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;#&gt; <span class="subst">&#123;query&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find the top-3 passages for this query</span></span><br><span class="line">results = searcher.search(query, k=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the top-k retrieved passages</span></span><br><span class="line"><span class="keyword">for</span> passage_id, passage_rank, passage_score <span class="keyword">in</span> <span class="built_in">zip</span>(*results):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\t [<span class="subst">&#123;passage_rank&#125;</span>] \t\t <span class="subst">&#123;passage_score:<span class="number">.1</span>f&#125;</span> \t\t <span class="subst">&#123;searcher.collection[passage_id]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>ok
偶然看到官方仓库昨天更新了intro的notebook，我看到里面下载数据集的仓库变化了，然后导入包的时候也写得更清晰了，加了条件：https://github.com/stanford-futuredata/ColBERT/blob/main/docs/intro2new.ipynb</p>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>在linux上配置GPU深度学习环境</title>
    <url>/2021/12/03/%E5%9C%A8linux%E4%B8%8A%E9%85%8D%E7%BD%AEGPU%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<h1 id="配置云服务器">配置云服务器</h1>
<p>系统 ： ubuntu 18.04</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 更新所有环境</span><br><span class="line">sudo apt update </span><br><span class="line"></span><br><span class="line"># 安装开发所需的基本包</span><br><span class="line">sudo apt install build-essential</span><br><span class="line"></span><br><span class="line"># 安装cuda.这里会连同显卡驱动一起安装</span><br><span class="line">wget https://developer.download.nvidia.com/compute/cuda/11.5.1/local_installers/cuda_11.5.1_495.29.05_linux.run</span><br><span class="line">sudo sh cuda_11.5.1_495.29.05_linux.run</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 去anaconda官方复制miniconda的下载链接</span><br><span class="line">wget https://repo.anaconda.com/miniconda/Miniconda3-py38_4.10.3-Linux-x86_64.sh</span><br><span class="line"></span><br><span class="line"># 进入conda环境</span><br><span class="line">bash</span><br><span class="line"></span><br><span class="line">之后就继续用pip安装所需要的包</span><br><span class="line">apt-get install python3.8</span><br></pre></td></tr></table></figure>
<p>这里需要知道，如果我们在云端配置的环境，需要将云端的jupyter
notebook的运行端口映射到本地来，可以这样做：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh -L8888:localhost:8888 ubuntu@100.20.65.33</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果使用aws配置远端服务器，在本地连接时使用:</p>
<p>$ chomd 400 Downloads/d2l.pem</p>
<p>$ ssh -i Downloads/d2l.pem ubuntu@54.245.23.40</p>
<p>以上两条命令中将密钥path和ip地址都替换成自己的。</p>
</blockquote>
<h1 id="docker中的tensorflow-gpu配置">docker中的tensorflow-gpu配置</h1>
<p>首先在宿主机（本机）安装好<code>NVIDIA GPU</code>的驱动程序，然后对于每一个容器都要各自安装对应版本的<code>cuda</code>和<code>cudnn</code>。</p>
<h2 id="下载tensorflow-docker镜像">下载TensorFlow Docker镜像</h2>
<p>官方 TensorFlow Docker 映像位于 <a href="https://hub.docker.com/r/tensorflow/tensorflow/">tensorflow/tensorflow</a>
Docker Hub 代码库中。映像版本按照以下格式进行<a href="https://hub.docker.com/r/tensorflow/tensorflow/tags/">标记</a>：</p>
<table>
<colgroup>
<col style="width: 15%">
<col style="width: 84%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">标记</th>
<th style="text-align: left;">说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>latest</code></td>
<td style="text-align: left;">TensorFlow CPU
二进制映像的最新版本。（默认版本）</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>nightly</code></td>
<td style="text-align: left;">TensorFlow 映像的每夜版。（不稳定）</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em><code>version</code></em></td>
<td style="text-align: left;">指定 TensorFlow
二进制映像的版本，例如：2.1.0</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>devel</code></td>
<td style="text-align: left;">TensorFlow <code>master</code>
开发环境的每夜版。包含 TensorFlow 源代码。</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>custom-op</code></td>
<td style="text-align: left;">用于开发 TF
自定义操作的特殊实验性映像。详见<a href="https://github.com/tensorflow/custom-op">此处</a>。</td>
</tr>
</tbody>
</table>
<p>每个基本标记都有会添加或更改功能的变体：</p>
<table>
<colgroup>
<col style="width: 17%">
<col style="width: 82%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">标记变体</th>
<th style="text-align: left;">说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>tag-gpu</code></td>
<td style="text-align: left;">支持 GPU 的指定标记版本。（<a href="https://www.tensorflow.org/install/docker?hl=zh_cn#gpu_support">详见下文</a>）</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>tag-jupyter</code></td>
<td style="text-align: left;">针对 Jupyter 的指定标记版本（包含
TensorFlow 教程笔记本）</td>
</tr>
</tbody>
</table>
<p>您可以一次使用多个变体。例如，以下命令会将 TensorFlow
版本映像下载到计算机上：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker pull tensorflow/tensorflow                     # latest stable release</span><br><span class="line">docker pull tensorflow/tensorflow:devel-gpu           # nightly dev release w/ GPU support</span><br><span class="line">docker pull tensorflow/tensorflow:latest-gpu-jupyter  # latest release w/ GPU support and Jupyter</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意如果要用gpu版本的tensorflow，需要pull带有gpu tag的镜像.
<code>docker pull tensorflow/tensorflow:latest-gpu</code>。如果不带gpu标签，会默认下载CPU版本的tensorflow。</p>
</blockquote>
<h3 id="验证tensorflow-gpu">验证tensorflow gpu</h3>
<p>查看是否有GPU：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">gpu_device_name = tf.test.gpu_device_name()</span><br><span class="line"><span class="built_in">print</span>(gpu_device_name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># GPU是否可用,返回True或者False</span></span><br><span class="line">tf.test.is_gpu_available()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.client <span class="keyword">import</span> device_lib</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出所有的本地机器设备</span></span><br><span class="line">local_device_protos = device_lib.list_local_devices()</span><br><span class="line"><span class="comment"># 打印</span></span><br><span class="line"><span class="comment">#     print(local_device_protos)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只打印GPU设备</span></span><br><span class="line">[<span class="built_in">print</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> local_device_protos <span class="keyword">if</span> x.device_type == <span class="string">&#x27;GPU&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h2 id="docker中安装多种cuda版本并切换">docker中安装多种cuda版本并切换</h2>
<p>去<code>cuda</code>官网下载所需版本，以<code>.run</code>结尾。楼主系统为<code>linux</code>。</p>
<p>进入到放置 <code>cuda_9.0.176_384.81_linux.run</code> 的目录：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo chmod +x cuda_9.0.176_384.81_linux.run # 为 cuda_9.0.176_384.81_linux.run 添加可执行权限</span><br><span class="line">./cuda_9.0.176_384.81_linux.run # 安装 cuda_9.0.176_384.81_linux.run</span><br></pre></td></tr></table></figure>
<p>在安装过程中截取其中比较重要的几个选择：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Do you accept the previously read EULA?</span><br><span class="line">accept/decline/quit: accept</span><br><span class="line"></span><br><span class="line">Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 384.81?</span><br><span class="line">(y)es/(n)o/(q)uit: n # 如果在这之前已经安装好更高版本的显卡驱动就不需要再重复安装，如果需要重复安装就选择 yes,此外还需要关闭图形界面。</span><br><span class="line"></span><br><span class="line">Install the CUDA 9.0 Toolkit?</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line"></span><br><span class="line">Enter Toolkit Location</span><br><span class="line"> [ default is /usr/local/cuda-9.0 ]: # 一般选择默认即可，也可以选择安装在其他目录，在需要用的时候指向该目录或者使用软连接 link 到 /usr/local/cuda。</span><br><span class="line"></span><br><span class="line">/usr/local/cuda-9.0 is not writable.</span><br><span class="line">Do you wish to run the installation with &#x27;sudo&#x27;?</span><br><span class="line">(y)es/(n)o: y</span><br><span class="line"></span><br><span class="line">Please enter your password: </span><br><span class="line">Do you want to install a symbolic link at /usr/local/cuda? # 是否将安装目录通过软连接的方式 link 到 /usr/local/cuda，yes or no 都可以，取决于你是否使用 /usr/local/cuda 为默认的 cuda 目录。</span><br><span class="line">(y)es/(n)o/(q)uit: n</span><br><span class="line"></span><br><span class="line">Install the CUDA 9.0 Samples?</span><br><span class="line">(y)es/(n)o/(q)uit: n</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>选择的汇总： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Driver:   Not Selected</span><br><span class="line">Toolkit:  Installed in /usr/local/cuda-9.0</span><br><span class="line">Samples:  Not Selected</span><br><span class="line"></span><br><span class="line">Please make sure that</span><br><span class="line"> -   PATH includes /usr/local/cuda-9.0/bin</span><br><span class="line"> -   LD_LIBRARY_PATH includes /usr/local/cuda-9.0/lib64, or, add /usr/local/cuda-9.0/lib64 to /etc/ld.so.conf and run ldconfig as root</span><br><span class="line"></span><br><span class="line">To uninstall the CUDA Toolkit, run the uninstall script in /usr/local/cuda-9.0/bin</span><br><span class="line"></span><br><span class="line">Please see CUDA_Installation_Guide_Linux.pdf in /usr/local/cuda-9.0/doc/pdf for detailed information on setting up CUDA.</span><br><span class="line"></span><br><span class="line">***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 384.00 is required for CUDA 9.0 functionality to work.</span><br><span class="line">To install the driver using this installer, run the following command, replacing &lt;CudaInstaller&gt; with the name of this run file:</span><br><span class="line">    sudo &lt;CudaInstaller&gt;.run -silent -driver</span><br></pre></td></tr></table></figure> 安装完成后可以在 <code>/usr/local</code>
目录下看到：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cuda-11.1 # 之前安装的cuda-11.1 </span><br><span class="line">cuda-9.0 # 刚刚安装的cuda-9.0 </span><br><span class="line">cuda # cuda-10.0 的软连接</span><br></pre></td></tr></table></figure>
<p>多版本切换：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#在切换cuda版本时</span><br><span class="line">rm -rf /usr/local/cuda#删除之前创建的软链接</span><br><span class="line">sudo ln -s /usr/local/cuda-8.0/ /usr/local/cuda/</span><br><span class="line">nvcc --version #查看当前 cuda 版本</span><br><span class="line"></span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2016 NVIDIA Corporation</span><br><span class="line">Built on Mon_Jan_23_12:24:11_CST_2017</span><br><span class="line">Cuda compilation tools, release 8.0, V8.0.62</span><br><span class="line"></span><br><span class="line">#cuda8.0 切换到 cuda9.0 </span><br><span class="line">rm -rf /usr/local/cuda</span><br><span class="line">sudo ln -s /usr/local/cuda-9.0/ /usr/local/cuda/</span><br><span class="line">nvcc --version</span><br></pre></td></tr></table></figure>
<p>上面的前提是<code>linux</code>系统的环境变量中<code>(~./bashrc文件)</code>，<code>cuda</code>的路径是<code>/usr/local/cuda</code></p>
<h2 id="tensorflow_decision_forests使用">tensorflow_decision_forests使用</h2>
<p>我一开始pull了tensorflow-gpu版本的docker环境，想用一下tensorflow的tensorflow_decision_forests库，该库是随机森林的集合库，内有很多算法可以用。官方使用document在[https://www.tensorflow.org/decision_forests/tutorials/beginner_colab?hl=zh_cn]。</p>
<p>我一开始没注意，后来发现我在docker环境中直接用pip安装该库时，帮我又安装了cpu版本的tensorflow，这样就和我的gpu版本冲突了，然后去网上搜了一下，发现该库现在还有很多使用限制：1.
仅仅支持linux，不支持windows和mac 2. 仅支持cpu
，还没有gpu版本，见[https://github.com/tensorflow/decision-forests/issues/38].作者的意思是用gpu训练会更复杂，更详细的我就没看了。</p>
<p>然后我的做法是在该tensorflow-gpu的docker环境中使用<strong>virtualenv</strong>创建一个tensorfow的cpu虚拟环境，然后再用pip安装TF-DF这个包。避免污染docker主环境中的tensorflow-gpu。</p>
<blockquote>
<p>之所以不用miniconda，是因为conda会默认替代掉我容器自带的python以及安装好的tensorflow-gpu。我只是想用一下TFDF这个库，不想太折腾。</p>
</blockquote>
<p>此处贴virtualenv的命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install virtualenv</span><br><span class="line"></span><br><span class="line">virtualenv venv # 在项目目录中执行，会创建一个名称为venv的虚拟环境，venv文件夹下包含python和pip</span><br><span class="line"></span><br><span class="line">source venv/bin/activate # 激活环境</span><br><span class="line"></span><br><span class="line">激活完了之后就pip安装tensorflow就行了</span><br></pre></td></tr></table></figure>
<h1 id="本地机器vscode配置使用远程服务器中运行的容器">本地机器vscode配置使用远程服务器中运行的容器</h1>
<p>参考 [https://zhuanlan.zhihu.com/p/80099904]</p>
<p>其中有几个地方需要注意一下的是：</p>
<ol type="1">
<li><p>如果遇到在本地机器vscode中使用插件remote
ssh连接不上容器的问题时，需要<code>vim /etc/ssh/sshd_config    ,将PermitRootLogin的值改为yes（去掉前面的#号）</code></p></li>
<li><p>远程容器里面还需要有python插件，才能在本地机器vscode中debug代码</p></li>
<li><p>上面博主服务器端口映射的是容器的22端口，我没有尝试过用其他端口映射。22端口是ssh登陆的默认端口。</p></li>
</ol>
<h1 id="cudnn深度学习库的安装和验证">cudnn深度学习库的安装和验证</h1>
<p>参考[https://blog.csdn.net/caicaiatnbu/article/details/87626491]</p>
<p>注意点：</p>
<ol type="1">
<li>下载对应的linux版本的cudnn</li>
</ol>
<h1 id="conda安装cuda和cudnn">conda安装cuda和cudnn</h1>
<p>如果使用的是conda的环境，可以单独使用conda install
cuda来在虚拟环境中安装不同于本机版本的cuda和cudnn，而不需要使用我上面提到的那种方式(每次换cuda版本需要更换/usr/local/cuda的软链接的方式)。</p>
<p>conda创建了虚拟环境后，激活进入虚拟环境</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda install cudatoolkit=10.2</span><br></pre></td></tr></table></figure>
<p>以上命令安装了10.2版本的cuda，然后可以使用<code>cuda search cudnn</code>找一下合适版本的cudnn，然后还是用<code>cuda install cudnn=版本号</code>来安装</p>
<blockquote>
<p>注意：这里用conda安装的cuda和cudnn，是无法像在本机使用nvcc
-V来检查版本的，所以即使你在虚拟环境激活的情况下使用 nvcc
-V的命令会返回无此命令或者是返回的还是本机cuda的版本号！</p>
</blockquote>
<p>那可能有人会问，如果想在conda虚拟环境下测试cuda和cudnn是否安装成功怎么办？</p>
<p>目前的办法是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># In[]:</span><br><span class="line">import torch</span><br><span class="line"># cpu</span><br><span class="line">print(torch.__version__)</span><br><span class="line"># gpu</span><br><span class="line">print(torch.cuda.is_available())</span><br><span class="line"></span><br><span class="line"># In[]:</span><br><span class="line">import tensorflow as tf</span><br><span class="line"># cpu</span><br><span class="line">print(tf.__version__)</span><br><span class="line"># v1 to test gpu</span><br><span class="line">print(tf.test.is_gpu_available())</span><br><span class="line"># v2 to test gpu</span><br><span class="line">print(tf.config.list_physical_devices(&#x27;GPU&#x27;))</span><br></pre></td></tr></table></figure>
<p>参考[https://blog.csdn.net/qq_37774098/article/details/109895048]</p>
<h1 id="docker拉取的pytorch-gpu版找不到cuda和cudnn的位置为何">docker拉取的pytorch-gpu版找不到cuda和cudnn的位置，为何？</h1>
<p>参考 https://blog.csdn.net/ljp1919/article/details/106209358</p>
<h1 id="tensorflow和pytorch验证gpu是否可用">tensorflow和pytorch验证GPU是否可用</h1>
<h2 id="tensorflow">tensorflow</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"> </span><br><span class="line">gpu_device_name = tf.test.gpu_device_name()</span><br><span class="line"><span class="built_in">print</span>(gpu_device_name)</span><br><span class="line"><span class="comment"># 查看是否可用</span></span><br><span class="line">tf.test.is_gpu_available()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.client <span class="keyword">import</span> device_lib</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 列出所有的本地机器设备</span></span><br><span class="line">local_device_protos = device_lib.list_local_devices()</span><br><span class="line"><span class="comment"># 打印</span></span><br><span class="line"><span class="comment"># print(local_device_protos)</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 只打印GPU设备</span></span><br><span class="line">[<span class="built_in">print</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> local_device_protos <span class="keyword">if</span> x.device_type == <span class="string">&#x27;GPU&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h2 id="pytorch">pytorch</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">flag = torch.cuda.is_available()</span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;CUDA可使用&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;CUDA不可用&quot;</span>)</span><br><span class="line"></span><br><span class="line">ngpu= <span class="number">1</span></span><br><span class="line"><span class="comment"># Decide which device we want to run on</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> (torch.cuda.is_available() <span class="keyword">and</span> ngpu &gt; <span class="number">0</span>) <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;驱动为：&quot;</span>,device)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;GPU型号： &quot;</span>,torch.cuda.get_device_name(<span class="number">0</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> 	torch</span><br><span class="line"><span class="keyword">import</span>  time</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br><span class="line"><span class="comment"># print(&#x27;hello, world.&#x27;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">10000</span>, <span class="number">1000</span>)</span><br><span class="line">b = torch.randn(<span class="number">1000</span>, <span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line">c = torch.matmul(a, b)</span><br><span class="line">t1 = time.time()</span><br><span class="line"><span class="built_in">print</span>(a.device, t1 - t0, c.norm(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">a = a.to(device)</span><br><span class="line">b = b.to(device)</span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line">c = torch.matmul(a, b)</span><br><span class="line">t2 = time.time()</span><br><span class="line"><span class="built_in">print</span>(a.device, t2 - t0, c.norm(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line">c = torch.matmul(a, b)</span><br><span class="line">t2 = time.time()</span><br><span class="line"><span class="built_in">print</span>(a.device, t2 - t0, c.norm(<span class="number">2</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="附录docker使用">附录：docker使用</h1>
<p>创建容器：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --gpus all -d -v /:/host -p 3333:22 tensorflow/tensorflow:2.4.0-gpu tail -f /var/log/dpkg.log # 创建容器并启动</span><br><span class="line">docker create --name postgres nginx:latest # 创建容器并不启动</span><br><span class="line"></span><br><span class="line">docker start # 启动一个或多个已经被停止的容器</span><br><span class="line"></span><br><span class="line">docker stop # 停止一个运行中的容器</span><br><span class="line"></span><br><span class="line">docker restart # 重启容器</span><br></pre></td></tr></table></figure>
<p>进入容器：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec -it [container_id] /bin/bash</span><br><span class="line"></span><br><span class="line">docker inpect # 获取容器/镜像的元数据</span><br></pre></td></tr></table></figure>
<p>停止容器运行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker kill -s KILL [container_id]</span><br><span class="line">或者 docker stop [container_id]</span><br></pre></td></tr></table></figure>
<p>删除一个或多个容器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker rm -f db01 db02 # 强制删除，容器可以是在运行着的状态</span><br><span class="line">或者 docker rm db01</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>docker save 与 docker export
区别参考:[https://jingsam.github.io/2017/08/26/docker-save-and-docker-export.html]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker save -o images.tar postgres:9.6 # 打包postgres镜像</span><br><span class="line">docker load -9 images.tar # 载入镜像</span><br></pre></td></tr></table></figure>
<p>docker
save的应用场景是，如果你的应用是使用docker-compose.yml编排的多个镜像组合，但你要部署的客户服务器并不能连外网。这时，你可以使用docker
save将用到的镜像打个包，然后拷贝到客户服务器上使用docker load载入。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker export -o postgres-export.tar postgres # 将容器postgres打包成一个tar</span><br><span class="line">docker import postgres-export.tar postgres:latest # 这是将tar包import成一个镜像，镜像和tag名字自定义</span><br></pre></td></tr></table></figure>
<p>docker
export的应用场景主要用来制作基础镜像，比如你从一个ubuntu镜像启动一个容器，然后安装一些软件和进行一些设置后，使用docker
export保存为一个基础镜像。然后，把这个镜像分发给其他人使用，比如作为基础的开发环境。</p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
</search>
