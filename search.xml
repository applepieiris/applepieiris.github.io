<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Adaptation Tuning of LLMs</title>
    <url>/2023/05/19/Adaptation-Tuning-of-LLMs/</url>
    <content><![CDATA[<p>让LLM适配specific的下游任务，两条线：1. 在prompt engineering上下功夫
2. Fine-tune LLM.
其实这两条线并不分家，中间也有一些技术是有overlap的。prompt
engineering并不只是手动设计prompt让LLM返回更好的结果，使得其在下游任务中得以使用，一些研究并不想自己手动设计prompt，那就产生了很多自动产生prompt的方式。刘鹏飞博士的review文章<a href="http://arxiv.org/abs/2107.13586">Pre-train, Prompt, and Predict: A
Systematic Survey of Prompting Methods in Natural Language
Processing</a>将这些技术统一到一个体系里来，分类方式也比较清晰：</p>
<figure>
<img src="/2023/05/19/Adaptation-Tuning-of-LLMs/image-20230524100148539.png" alt="characteristics of different tuning strategies">
<figcaption aria-hidden="true">characteristics of different tuning
strategies</figcaption>
</figure>
<h1 id="full-fine-tunepromptless-finetune">Full Fine-tune（Promptless
Finetune）</h1>
<p>Bert就是一个典型的应用，将模型在一个很大的语料库上pretrain之后，再在一些任务的数据集上对模型参数进行调整。注意这里模型的所有参数都会进行调整。</p>
<figure>
<img src="/2023/05/19/Adaptation-Tuning-of-LLMs/image-20230519142832464.png" alt="image-20230519142832464">
<figcaption aria-hidden="true">image-20230519142832464</figcaption>
</figure>
<p>对所有模型参数调整就带来很多问题：</p>
<ul>
<li>要维护每一个task上的模型，有一些模型的参数量都是亿级别的，这对存储是一个考验</li>
<li>finetune所有参数就需要数据集达到一定的数量级，这在特定领域不一定是可以达到的；如果没有很多数据，有可能finetune完之后还会引起perfomance的下降或者过拟合。</li>
<li>计算资源的限制</li>
</ul>
<h1 id="more-efficient-ways-of-tuning">More Efficient Ways of
Tuning</h1>
<p>或许有更合适的tuning方式，less overfitting and more efficient
finetuning and inference</p>
<h2 id="prefix-tuning">Prefix-Tuning</h2>
<p><a href="https://arxiv.org/pdf/2101.00190.pdf">Prefix-Tuning:
Optimizing Continuous Prompts for Generation</a></p>
<figure>
<img src="/2023/05/19/Adaptation-Tuning-of-LLMs/image-20230519150912298.png" alt="prefix Tuning">
<figcaption aria-hidden="true">prefix Tuning</figcaption>
</figure>
<p>一开始理解prefix
tuning其实是从“如果不调整所有参数，那么是不是可以调整部分参数”来思考这个模型的。但是看了原文之后会发现作者的思考路径其实有点不太一样。paper中说</p>
<blockquote>
<p>Prefix-tuning draws inspiration from prompting, allowing subsequent
tokens to attend to this prefix as if it were “virtual tokens”</p>
</blockquote>
<p>lilian的<a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#automatic-prompt-design">博客</a>对于这个也解释的蛮有意思：</p>
<blockquote>
<p>Prompt is a sequence of prefix tokens that increase the probability
of getting desired output given input. Therefore we can treat them as
trainable parameters and <a href="https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#smart-prompt-design">optimize
them directly</a> on the embedding space via gradient descent, such as
<strong>AutoPrompt</strong> (<a href="https://arxiv.org/abs/2010.15980">Shin et al., 2020</a>,
<strong>Prefix-Tuning</strong> (<a href="https://arxiv.org/abs/2101.00190">Li &amp; Liang (2021)</a>),
<strong>P-tuning</strong> (<a href="https://arxiv.org/abs/2103.10385">Liu et al. 2021</a>) and
<strong>Prompt-Tuning</strong> (<a href="https://arxiv.org/abs/2104.08691">Lester et al. 2021</a>). <a href="https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#smart-prompt-design">This
section in my “Controllable Neural Text Generation” post</a> has a good
coverage of them. The trend from AutoPrompt to Prompt-Tuning is that the
setup gets gradually simplified.</p>
</blockquote>
<p>也就是既然我们发现in-context
learning是可以促进大语言模型解决特定问题（因为我们让LLM以更高的概率输出我们想要的结果了），那么是不是可以可以把这一部分信息编码进模型参数里，从而在特定地数据集上单独训练这些参数。</p>
<p>所以研究者也想了一些办法如何以最小的成本为特定的任务增加一些参数，fix住预训练模型的大部分参数，而去finetune给每一个任务增加的那一部分参数。其中adapter-tuning就是一种。</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention and Transformer model</title>
    <url>/2022/10/27/Attention-and-Transformer-model/</url>
    <content><![CDATA[<p>斯坦福cs231n最新的课程中包含了attention的模型讲解，但是很可惜我们现在只能看到17年的老课程，在youtube上可以找到，课程主页是<a href="http://cs231n.stanford.edu/schedule.html">cs231n</a>。可以在课程主页中下载对应的slides和查看推荐的blog，都是学习attention机制的好教材。另外我在学习cs231n课程过程中，也参考了吴恩达对于sequence
model的讲解，它课程中也涉及到了attention机制，课后作业也包含了简单的attention机制的实现，可以作为辅助理解来看。这篇博客权当自己学习attention以及由此创造的attention系列模型比如transformer的记录。cs231n推荐的<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">博客</a>内容也是很通俗易懂，英文不好的同学有中文翻译可以参考。</p>
<h1 id="general-attention-model">general attention model</h1>
<p>RNN有多种类型的网络：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221027100649410.png" alt="image-20221027100649410">
<figcaption aria-hidden="true">image-20221027100649410</figcaption>
</figure>
<p>对于"many to
many"类型的网络，有可能输入的长度不等于输出的长度，在机器翻译的任务中很常见。这种网络也叫Sequence
to
Sequence，首先该网络会经由encoder对输入进行编码，然后再有decoder进行sequence的生成。但是这种网络在长句子中表现很差，如果输入句子的长度很长，encoder网络就很难记忆住所有信息，从而在decoder中翻译出准确的词语。由此，需要用到attention
model。从计算角度来说就是encoder每次都会产生一个固定长度的vector，这对于长句子来说fixed
length的向量很难记住很早之前的信息：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221027104026997.png" alt="image-20221027104026997">
<figcaption aria-hidden="true">image-20221027104026997</figcaption>
</figure>
<p>那为了解决一个fixed
length的vector很难记忆前序信息的缺陷，所以诞生了attention
机制！具体的就是在decoder阶段的每一个时间步利用都产生不同的context，这个context产生的过程就是attention计算的过程。主要思想是在产生y之前做一个attention的权重计算，这个权重指的是在计算某个时间步的y值时，我们应该对输入句子的每一个词给予多少关注，给予的关注多，权重就大。所以这里我们会基于initial
decoder state（previous hidden state of the (post-attention)
LSTM）和encoder网络的输出值计算权重，计算过程采用dense
layer.这些权重值的和是1。</p>
<p>对于很长输入的句子，encoder不再是输出一个固定的context。如下：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221027103845891.png" alt="image-20221027103845891">
<figcaption aria-hidden="true">image-20221027103845891</figcaption>
</figure>
<p>context的详细计算如下：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221027101307861.png" alt="吴恩达课程attention">
<figcaption aria-hidden="true">吴恩达课程attention</figcaption>
</figure>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221027101324129.png" alt="吴恩达课程attention">
<figcaption aria-hidden="true">吴恩达课程attention</figcaption>
</figure>
<p>上面两张图是吴恩达在深度学习专项课程中的讲解细节，在cs231n的图中可以看出来它将dense
layer的输出单独列出来了，也就是下图中alignment
scores（<code>e</code>），在e的基础上再计算attention
weights（<code>a</code>），所有的attention weights总和为1 <img src="/2022/10/27/Attention-and-Transformer-model/image-20221206141252321.png" alt="cs231n_attention"></p>
<p>但是总体来说两个的讲解方式是一致的，cs231n这门课为什么把其中的<code>e</code>单独拎出来也是有它的用意，主要为了后面讲解self-attention。这里我琢磨了好一会儿才明白。还有一点值得提一下，吴恩达的图里那个与hidden
state 一同输入进dense
layer的repeateVector不要搞混淆，其实这里每次在decoder的一个时间步计算context时用到的s都不一样，比如在上面的里，计算<code>c1</code>用的是encoder的最后一个hidden
state，而计算<code>C2</code>的时候我们需要用decoder的第一个时间步的hidden
state来计算：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221206142430931-16703078727111.png" alt="decoder第二个时间步">
<figcaption aria-hidden="true">decoder第二个时间步</figcaption>
</figure>
<p>这种attention计算机制如果用到image
caption上面如何做呢？获取图片的特征我们使用CNN来抓取，得到的feature
map用来当作rnn中的hidden state计算：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221206145117196-16703094790462.png" alt="image caption">
<figcaption aria-hidden="true">image caption</figcaption>
</figure>
<p>理解以上的原理很重要，再把上面这个图改写一下，将h变成decoder的query：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221206145317028-16703095989033.png" alt="query">
<figcaption aria-hidden="true">query</figcaption>
</figure>
<p>以上都是简单的attention机制，随后的transformer模型真正的将attention机制推广开来，见transformer的<a href="https://arxiv.org/abs/1706.03762">paper</a></p>
<h1 id="attention-is-all-you-need-2017">Attention is all you need
2017</h1>
<p>在transformer的<a href="https://arxiv.org/abs/1706.03762">paper</a>中，作者首先介绍本文：主流的sequence
tranduction模型主要基于复杂的RNN或者CNN模型，它们包含encoder和decoder两部分，其中表现最好的模型在encoder和decoder之间增加了attention
mechanism。本文提出了一个新的简单的网络结构名叫<em>transformer</em>，也是完全基于attention机制，"dispensing
with recurrence and convolutions entirely"!
根本无需循环和卷积！了不起的Network~</p>
<p>在阅读这篇文章之前需要提前了解我在另外一篇博客 Attention and
transformer
model中的知识，在translation领域我们的科学家们是如何从RNN循环神经网络过渡到CNN，然后最终是transformer的天下的状态。技术经过了一轮轮的迭代，每一种基础模型架构提出后，会不断的有文章提出新的改进，文章千千万，不可能全部读完，就精读一些经典文章就好，Vaswani这篇文章是NMT领域必读paper，文章不长，加上参考文献才12页，介绍部分非常简单，导致这篇文章的入门门槛很高（个人感觉）。我一开始先读的这篇文章，发现啃不下去，又去找了很多资料来看，其中对我非常帮助的有很多：</p>
<ul>
<li><a href="http://jalammar.github.io/illustrated-transformer/">非常通俗易懂的blog</a>
有中文版本的翻译</li>
<li><a href="http://arxiv.org/abs/1912.02047">Neural Machine
Translation: A Review and Survey</a>
虽然这篇paper很长，90+页。前六章可以作为参照，不多25页左右，写的非常好</li>
<li><a href="http://cs231n.stanford.edu/slides/2022/lecture_11_ruohan.pdf">stanford
cs231n课程的ppt</a>
斯坦福这个课程真的很棒，youtube上可以找到17年的视频，17年的课程中没有attention的内容，所以就姑且看看ppt吧，希望斯坦福有朝一日能将最新的课程分享出来，也算是做贡献了</li>
<li>cs231n推荐的阅读<a href="https://lilianweng.github.io/posts/2018-06-24-attention/">博客</a>
非常全面的整理，强烈建议食用.
这位作者也附上了自己的transformer实现，在它参考的那些github实现里，哈佛大学的<a href="http://nlp.seas.harvard.edu/2018/04/01/attention.html">pytorch实现</a>也值得借鉴。</li>
</ul>
<p>Transformer这篇文章有几个主要的创新点：</p>
<ol type="1">
<li>使用self-attention机制，并首次提出使用multi-head attention</li>
</ol>
<p>该机制作用是在编码当前word的时候，这个self-attention就会告诉我们编码这个词语我们应该放多少注意力在这个句子中其他的词语身上，说白了其实就是计算当前词语和其他词语的关系。这也是CNN用于解决NMT问题时用不同width的kernel来扫input
metric的原因。</p>
<p>multi-head的意思是我使用多个不同的self-attention
layer来处理我们的输入，直观感觉是训练的参数更多了，模型的表现力自然要好一点。</p>
<ol start="2" type="1">
<li>Positional embeddings</li>
</ol>
<p>前一个创新点解决了dependence的问题，那如何解决位置的问题呢？也就是我这个词在编码的时候或者解码的时候应该放置在句子的哪个位置上。文章就用pisitional
embedding来解决这个问题。这个positional embedding和input
embedding拥有相同的shape，所以两者可以直接相加。transformer这篇文章提供了两种encoding方式：</p>
<p>1） sunusoidal positional encoding</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230302133247664.png" alt="image-20230302133247664">
<figcaption aria-hidden="true">image-20230302133247664</figcaption>
</figure>
<p>其中，pos=1,...,L(L是input句子的长度)，i是某一个PE中的一个维度，取值范围是1到dmodel。python实现为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span>(<span class="params">length, depth</span>):</span></span><br><span class="line">  depth = depth/<span class="number">2</span></span><br><span class="line"></span><br><span class="line">  positions = np.arange(length)[:, np.newaxis]     <span class="comment"># (seq, 1)</span></span><br><span class="line">  depths = np.arange(depth)[np.newaxis, :]/depth   <span class="comment"># (1, depth)</span></span><br><span class="line"></span><br><span class="line">  angle_rates = <span class="number">1</span> / (<span class="number">10000</span>**depths)         <span class="comment"># (1, depth)</span></span><br><span class="line">  angle_rads = positions * angle_rates      <span class="comment"># (pos, depth)</span></span><br><span class="line"></span><br><span class="line">  pos_encoding = np.concatenate(</span><br><span class="line">      [np.sin(angle_rads), np.cos(angle_rads)],</span><br><span class="line">      axis=-<span class="number">1</span>) </span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> tf.cast(pos_encoding, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">pos_encoding = positional_encoding(length=<span class="number">2048</span>, depth=<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the shape.</span></span><br><span class="line"><span class="built_in">print</span>(pos_encoding.shape) <span class="comment"># (2014,512)</span></span><br></pre></td></tr></table></figure>
<p>2） learned positional encoding</p>
<p>整体上看，这篇文章提出的transformer模型在做translation的任务时，架构是这样的：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230301133249379.png" alt="image-20230301133249379">
<figcaption aria-hidden="true">image-20230301133249379</figcaption>
</figure>
<p>其中encoders部分包含了6个encoders的block，decoders部分也包含了6个decoders的block，将encoders的每一个block拆开来看，有两个sub
layer：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230301133424302.png" alt="image-20230301133424302">
<figcaption aria-hidden="true">image-20230301133424302</figcaption>
</figure>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230301133440152.png" alt="image-20230301133440152">
<figcaption aria-hidden="true">image-20230301133440152</figcaption>
</figure>
<p>其中decoder部分的block比encoder部分的block多了一个sub
layer，其中self-attention和encoder-decoder attention都是multi-head
attention layer，只不过decoder部分的第一个multi-head attention
layer是一个masked multi-head
attention，为了防止未来的信息泄露给当下（prevent positions from
attending to the future）.</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230301133608484.png" alt="image-20230301133608484">
<figcaption aria-hidden="true">image-20230301133608484</figcaption>
</figure>
<p>在transformer模型中，作者还使用了residual
connection，所以在encoder的每一个block中，数据的flow是:</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230307162830473-16781777124571.png" alt="transformer架构">
<figcaption aria-hidden="true">transformer架构</figcaption>
</figure>
<p>其中self-attention中涉及的运算details是：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230301134258180.png" alt="image-20230301134258180">
<figcaption aria-hidden="true">image-20230301134258180</figcaption>
</figure>
<p>可以发现其中涉及的运算都是矩阵的点乘，并没有RNN中那种时间步的概念，所以所有运算都是可以parallelizable，这就能使得模型的推理和训练更加的efficient。并且！Transformers也可以抓住distant的依赖，而不是像rnn那样对于长依赖并不是很擅长，因为它前面的信息如果像传递到很后面的单词推理上，需要经历很多时间步的计算，而transformer在推理每一个单词的时候都可以access到input句子中的每一个单词（毕竟我们的Z中包含了每一个单词跟其他单词的关系)。</p>
<p>其中positional encoding现在可以简单的理解成在我们编码的word
embedding上我们又加了一个positional
encoding，维度和我们的embedding一模一样。</p>
<blockquote>
<p>在tensorflow中有一个layer是<code>MultiHeadAttention</code>,如果我们想实现transformer里的这个self-attention，那就是query，key，value其实都是由input
vector计算来的。</p>
</blockquote>
<p>以上的理论计算看起来可能会有点模糊，可以同步参照<a href="https://lilianweng.github.io/posts/2018-06-24-attention/">博客</a>
参考 <a href="http://jalammar.github.io/illustrated-transformer/">illustrated
transformer</a>介绍的详细细节，基于tensorflow框架实现的<a href="https://github.com/lilianweng/transformer-tensorflow/blob/master/transformer.py">transformer</a>来帮助自己理解transformer模型。</p>
<h2 id="encoder部分">encoder部分</h2>
<p>encoder的每一个block由两个sub-layer组成，中间穿插resnet
connection。</p>
<p><img src="/2022/10/27/Attention-and-Transformer-model/transformer_encoder_block.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multihead_attention</span>(<span class="params">self, query, memory=<span class="literal">None</span>, mask=<span class="literal">None</span>, scope=<span class="string">&#x27;attn&#x27;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            query (tf.tensor): of shape (batch, q_size, d_model)</span></span><br><span class="line"><span class="string">            memory (tf.tensor): of shape (batch, m_size, d_model)</span></span><br><span class="line"><span class="string">            mask (tf.tensor): shape (batch, q_size, k_size)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:h</span></span><br><span class="line"><span class="string">            a tensor of shape (bs, q_size, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> memory <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment"># 如果memory是None，那么就是一个典型的self-attention layer</span></span><br><span class="line">            memory = query</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># Linear project to d_model dimension: [batch, q_size/k_size, d_model]</span></span><br><span class="line">            Q = tf.layers.dense(query, self.d_model, activation=tf.nn.relu)</span><br><span class="line">            K = tf.layers.dense(memory, self.d_model, activation=tf.nn.relu)</span><br><span class="line">            V = tf.layers.dense(memory, self.d_model, activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Split the matrix to multiple heads and then concatenate to have a larger</span></span><br><span class="line">            <span class="comment"># batch size: [h*batch, q_size/k_size, d_model/num_heads]</span></span><br><span class="line">            Q_split = tf.concat(tf.split(Q, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            K_split = tf.concat(tf.split(K, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            V_split = tf.concat(tf.split(V, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            mask_split = tf.tile(mask, [self.h, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Apply scaled dot product attention</span></span><br><span class="line">            out = self.scaled_dot_product_attention(Q_split, K_split, V_split, mask=mask_split)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Merge the multi-head back to the original shape</span></span><br><span class="line">            out = tf.concat(tf.split(out, self.h, axis=<span class="number">0</span>), axis=<span class="number">2</span>)  <span class="comment"># [bs, q_size, d_model]</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># The final linear layer and dropout.</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_model)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dropout(out, rate=self.drop_rate, training=self._is_training)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_forwad</span>(<span class="params">self, inp, scope=<span class="string">&#x27;ff&#x27;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Position-wise fully connected feed-forward network, applied to each position</span></span><br><span class="line"><span class="string">        separately and identically. It can be implemented as (linear + ReLU + linear) or</span></span><br><span class="line"><span class="string">        (conv1d + ReLU + conv1d).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            inp (tf.tensor): shape [batch, length, d_model]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        out = inp</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_ff, activation=tf.nn.relu)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dropout(out, rate=self.drop_rate, training=self._is_training)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_model, activation=None)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># by default, use_bias=True</span></span><br><span class="line">            out = tf.layers.conv1d(out, filters=self.d_ff, kernel_size=<span class="number">1</span>, activation=tf.nn.relu)</span><br><span class="line">            out = tf.layers.conv1d(out, filters=self.d_model, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out  </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encoder_layer</span>(<span class="params">self, inp, input_mask, scope</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            inp: tf.tensor of shape (batch, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            input_mask: tf.tensor of shape (batch, seq_len, seq_len)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        out = inp</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># One multi-head attention + one feed-forword</span></span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(out, mask=input_mask))</span><br><span class="line">            out = self.layer_norm(out + self.feed_forwad(out))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h2 id="decoder部分">decoder部分</h2>
<p><img src="/2022/10/27/Attention-and-Transformer-model/encoder-decoder.png"></p>
<p>在decoder部分，我们可以看到每一个decoder
block的输入有两个：<em>整个encoder部分的输出</em>以及<em>上一个decoder
block的输出（第一个decoder
block是词向量的输入）</em>，而encoder部分的输出是接到每一个decoder
block的第二个sublayer的。正如刚刚提到了，decoder部分的每一个block跟encoder部分的block有一个不一样的地方，那就是多了一个sublayer：
encoder-decoder
attention。至于encoder部分和decoder部分是如何connect的，</p>
<blockquote>
<p>The encoder start by processing the input sequence. The output of the
top encoder is then transformed into a set of attention vectors K and V.
These are to be used by each decoder in its “encoder-decoder attention”
layer which helps the decoder focus on appropriate places in the input
sequence</p>
</blockquote>
<p>也就是我们得到了encoder部分top layer（最后一个encoder
layer）的输出之后，我们将输出转化成K和V.
我们可以看到在<code>multihead_attention</code>里，memory是<code>enc_out</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder_layer</span>(<span class="params">self, target, enc_out, input_mask, target_mask, scope</span>):</span></span><br><span class="line">        out = target</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(</span><br><span class="line">                out, mask=target_mask, scope=<span class="string">&#x27;self_attn&#x27;</span>))</span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(</span><br><span class="line">                out, memory=enc_out, mask=input_mask)) <span class="comment"># 将encoder部分的输出结果作为输入</span></span><br><span class="line">            out = self.layer_norm(out + self.feed_forwad(out))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder</span>(<span class="params">self, target, enc_out, input_mask, target_mask, scope=<span class="string">&#x27;decoder&#x27;</span></span>):</span></span><br><span class="line">        out = target</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_enc_layers):</span><br><span class="line">                out = self.decoder_layer(out, enc_out, input_mask, target_mask, <span class="string">f&#x27;dec_<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>以上实现的transformer其实我觉得还是有一点点复杂，毕竟在tensorflow2.0+版本中已经有了官方实现好的<code>layers.MultiHeadAttention</code>可以使用，应该可以大大简化我们实现步骤，特别是上面的<code>def multihead_attention(self, query, memory=None, mask=None, scope='attn'):</code>。从刚刚的实现里我们可以发现，除了decoder部分每一个block的第二个sublayer的attention计算有一点不一样之外，其他的attention计算都是一模一样的。我在github上找了不少用TF2.0实现的transformer（最标准的也是Attention
is all you
need的模型），发现很多都都写得一般般，最终发现还是tensorflow官方文档写的<a href="https://www.tensorflow.org/text/tutorials/transformer#add_and_normalize">tutotial</a>写的最好.</p>
<p>现在对照tensorflow的tutorial以及上面transformer的计算过程，拆解一下官方给的代码。</p>
<p>首先定义一个baseAttention类，然后在此基础上我们再定义encoder和decoder中的attention：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseAttention</span>(<span class="params">tf.keras.layers.Layer</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)</span><br><span class="line">    self.layernorm = tf.keras.layers.LayerNormalization()</span><br><span class="line">    self.add = tf.keras.layers.Add()</span><br></pre></td></tr></table></figure>
<p>那么针对encoder结果输入到decoder的cross attention
layer怎么处理呢？这时候我们使用MultiHeadAttention时就需要将target
sequence x当作是query，将encoder输出当作是context
sequence也就是key/value。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossAttention</span>(<span class="params">BaseAttention</span>):</span> <span class="comment"># encoder结果输入到decoder的层</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x, context</span>):</span> <span class="comment"># 这里的x是target sequence,context是encoder的输出结果</span></span><br><span class="line">    attn_output, attn_scores = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        key=context,</span><br><span class="line">        value=context,</span><br><span class="line">        return_attention_scores=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Cache the attention scores for plotting later.</span></span><br><span class="line">    self.last_attn_scores = attn_scores</span><br><span class="line"></span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>然后我们再定义global attention，global
attention就是没有任何特殊操作的（比如上面的attention计算它有特别的context），而在transformor中更多的是self-attention，也就是我们传递给MultiHeadAttention的query,key,value都是同一个值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GlobalSelfAttention</span>(<span class="params">BaseAttention</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    attn_output = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        value=x,</span><br><span class="line">        key=x)</span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>最后我们定义causal self attention
layer，这个是在decoder的每一个block的第一个sublayer：self-attention
layer.其实这个layer是和global attention
layer差不多的，但还是有一点微小的差别。为什么呢？因为我们在decoder阶段，我们是一个词语一个词语的预测的，这其实包含了一层因果关系，我们在预测一个词语的时候，我们应该已知它前面一个词语是什么，RNN中的hidden
state传递到下一个时间步就是这个因果关系的传递。那么如果我们使用刚刚我们实现的global
attention layer来实现这个self
attention，并没有包含这个因果关系，不仅如此，如果我们使用常规的self
attention的计算，将target
sequence全部当作输入输入到decoder中的第一个block中，会有未来的数据提前被当前时刻看到的风险，所以在Transformer这篇文章中，作者提出使用mask的技术来避免这个问题。</p>
<p>在tensorflow中实现很简单，就只需要给MultiHeadAttention传递一个use_causal_mask
= True的参数即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CausalSelfAttention</span>(<span class="params">BaseAttention</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    attn_output = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        value=x,</span><br><span class="line">        key=x,</span><br><span class="line">        use_causal_mask = <span class="literal">True</span>) <span class="comment"># The causal mask ensures that each location only has access to the locations that come before it</span></span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这样就可以保证先前的sequence并不依赖于之后的elements。这里我本来有一个疑问是，这样一来这个causal
layer并不能实现bi-rnn的能力？但后来一想并不是，因为双向的RNN的后向是指后面的词语先输入，其实就是从后往前输入，这样就可以知道一个sequence当前词语依赖于后面的词语的权重。</p>
<h2 id="why-transformer">why transformer?</h2>
<p>这里想补充一个东西，在从encoder-decoder过渡到transformer的时候，我一直的疑问是为什么要用transformer呢？为什么Effective
Approaches to Attention-based Neural Machine
Translation这篇paper介绍的方法就渐渐不被人所用了呢？一开始我去看了下transformer的原文，发现paper介绍的非常简单，所以就去找了下博客，找到一篇解释为什么transformer比LSTM快的<a href="https://voidful.medium.com/why-transformer-faster-then-lstm-on-generation-c3f30977d747">博客</a></p>
<p>文章说在传统的也就是paper：Neural Machine Translation by Jointly
Learning to Align and
Translate中介绍的用LSTM的encoder-decoder架构来做机器翻译的问题，一个问题在于：在RNN模型中，我们在计算当前时间步时，需要使用到前一个时间步的hidden_state，这就造成一个问题是无法并行训练，你必须要等到前面的东西都输完了你才能计算当前时间步的结果。transformer就可以解决这个问题，它完全摒弃了RNN的结构，基本上是一个FCNN。首先我们都知道输入到模型中来的是一个序列，序列中的每一个单词我们都转化为了词向量。如果是传统的RNN模型，这时候就要一个vector一个vector的往RNN里输入了，但transformer不是，它是将这个embedding变幻成了三个向量空间，也就是我们后面看到的Q,K,V.</p>
<p>后面又去谷歌了一番，找到一个<a href="https://ai.stackexchange.com/questions/20075/why-does-the-transformer-do-better-than-rnn-and-lstm-in-long-range-context-depen">stack</a>上的问题，也有人有这个疑问，被采取的回答总结起来就是：</p>
<ol type="1">
<li>transformer避免了recursion，从而可以方便并行运算（减少了训练时间），这个说法其实跟上面博客一个意思</li>
<li>同时transformer在长句子的dependency上提高了performance</li>
</ol>
<blockquote>
<ul>
<li><strong>Non sequential</strong>: sentences are processed as a whole
rather than word by word.</li>
<li><strong>Self Attention</strong>: this is the newly introduced 'unit'
used to compute similarity scores between words in a sentence.</li>
<li><strong>Positional embeddings</strong>: another innovation
introduced to replace recurrence. The idea is to use fixed or learned
weights which encode information related to a specific position of a
token in a sentence.</li>
</ul>
</blockquote>
<p>transformer并没有long
dependency的问题，为什么？我们知道transformer的做法是将整个sequence作为一个整体输入到模型。也就是它在预测当前时间步的word时并不依赖于上一个时间步的状态，模型看到的是整个序列，也许有人会质疑双向RNN不是也可以解决这个问题吗，但是这个作者说双向RNN仍然不能彻底解决长句子的依赖问题。</p>
<p>其实在机器翻译领域，为了解决长句子的依赖问题，CNN曾经也被广泛用于解决这个问题，不仅如此，CNN还有共享参数的优点，也就是可以在GPU上并行计算。如何用CNN处理句子可以参考<a href="Convolutional%20Neural%20Networks%20for%20Sentence%20Classification">paper</a>,CNN解决依赖问题是用不同宽度的kernel去学习依赖，比如width=2就学习两个词之间的依赖关系，width=3就学习三个词之间的依赖，但长句子的依赖很可能会有很多组合，所以就需要使用到很多不同宽度的kernel，这是不现实的。虽然现在CNN不怎么用来解决S2S的问题，但我觉得它是RNN结构的模型过度到transformer的一个中间桥梁，同时也可以帮助我们理解attention
is all your need这篇文章。感兴趣的可以读一下<a href="https://arxiv.org/abs/1705.03122">Convolutional Sequence to
Sequence Learning 2017</a></p>
<h2 id="drawbacks-and-variants-of-transformer">drawbacks and variants of
transformer</h2>
<p>transformer的一个很大的问题是它的计算量是随着sequence长度的增加而指数型增加的，从矩阵运算我们就可以发现每一个单词都和句子中的其他单词做了attention
score的计算。所以自从17年transformer模型出来之后，很多用于改进transformer计算效率的小改动paper出了不少，详见<a href="http://arxiv.org/abs/2102.11972">Do Transformer Modifications
Transfer Across Implementations and Applications?</a>.
但这篇文章的作者发现：</p>
<blockquote>
<p>Surprisingly, we find that most modifications do not meaningfully
improve performance.</p>
</blockquote>
<p>也就是说这些文章的改动更多的依赖于实现细节，并没有对tansformer的性能有本质上的提高。</p>
<p>另外，对于positional encoding也有不少researcher做了功课，比如relative
linear postion attention,dependency syntax-based position等。</p>
<h2 id="tensorflow-api实现补充介绍">tensorflow API实现补充介绍</h2>
<h3 id="tf.keras.layers.multiheadattention">tf.keras.layers.MultiHeadAttention</h3>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention">doc</a></p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230309150541287.png" alt="image-20230309150541287">
<figcaption aria-hidden="true">image-20230309150541287</figcaption>
</figure>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230309150600806.png" alt="image-20230309150600806">
<figcaption aria-hidden="true">image-20230309150600806</figcaption>
</figure>
<p>注意，return的结果包含两个，其中attention_output的shape的第二维是和target
sequence的长度是一致的，并且E是和query的最后一维是一致的。</p>
<p>https://dl.acm.org/doi/10.5555/3305381.3305510)</p>
<h1 id="attention-family">Attention Family</h1>
<p>这个章节整理于<a href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/">blog</a>，这个作者之前写了一篇介绍attention的文章，后面在2023年一月的时候又更新了两篇博客，详细介绍了从2020年以来出现的新的Transformer
models。权当自己学习记录一些我还需要补充的知识。</p>
<blockquote>
<p>The <strong>Transformer</strong> (which will be referred to as
“vanilla Transformer” to distinguish it from other enhanced versions; <a href="https://arxiv.org/abs/1706.03762">Vaswani, et al., 2017</a>) model
has an encoder-decoder architecture, as commonly used in many <a href="https://lilianweng.github.io/posts/2018-06-24-attention/#born-for-translation">NMT</a>
models. Later simplified Transformer was shown to achieve great
performance in language modeling tasks, like in encoder-only <a href="https://lilianweng.github.io/posts/2019-01-31-lm/#bert">BERT</a>
or decoder-only <a href="https://lilianweng.github.io/posts/2019-01-31-lm/#openai-gpt">GPT</a>.</p>
</blockquote>
<h1 id="pretraining">pretraining</h1>
<p>pretraining的技术在NLP领域取得了空前的发展，比如我们熟知的GPT系列模型以及Bert模型。自从transformer应运而生之后，pretrain的技术就发展开来。</p>
<p>model pretrain的方式有三种：decoders,encoders,encoder-decoders:</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230315113701127.png" alt="pretrain for three types of architectures">
<figcaption aria-hidden="true">pretrain for three types of
architectures</figcaption>
</figure>
<hr>
<p>上图中第一个将transformer中decoders拿来做pre-train的模型是GPT，paper是：<a href="https://openai.com/research/language-unsupervised">Improving
Language Understanding by Generative Pre-Training</a></p>
<p>GPT模型分为两个阶段，第一个阶段是unsupervised
learning，第二个是supervised learning。首先第一阶段是一个典型的language
modeling模型，目标函数是：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230315141236557.png" alt="image-20230315141236557">
<figcaption aria-hidden="true">image-20230315141236557</figcaption>
</figure>
<p>其中k是context窗口的大小，也就是我们预测一个单词的时候，只看它前k个单词，在前k个单词的基础上使得出现当前单词的概率最大。至于这个模型是什么？其实就是我们熟知的transformer
decoder部分，包含multi-head
self-attention(注意这里是masked的attention，原因我们只看前k个单词，并不看后面的部分)和feed-forward：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230315141542314.png" alt="Transformer used in GPT paper">
<figcaption aria-hidden="true">Transformer used in GPT
paper</figcaption>
</figure>
<p>我们取最上层的decoder的输出，然后使用softmax来预测p(u)。以上的decoder模型在一个很大的语料库上进行训练之后，我们预训练部分就做完了，通过这一步我们拥有了一个decoder，它的作用是当我们每次输入一个sentence，它会告诉我们紧接着的那个单词是什么。</p>
<p>接着第二部分是我们的监督学习，原文说监督学习部分需要在不同的task上进行fine-tune：</p>
<blockquote>
<p>First, we use a language modeling objective on the unlabeled data to
learn the initial parameters of a neural network model. Subsequently, we
adapt these parameters to a target task using the corresponding
supervised objective.</p>
</blockquote>
<hr>
<p>第二种用于pre-train的模型架构是encoders。它和用decoder来做非监督学习不一样的地方在于，摘录自博客<a href="https://medium.com/dataseries/openai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4">OpenAI
GPT: Generative Pre-Training for Language Understanding</a></p>
<p>Open AI GPT uses a <strong>Transformer Decoder</strong> architecture
as opposed to <a href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af">BERT’s</a>
Transformer Encoder architecture. I have already covered the difference
between the Transformer Encoder and Decoder in <a href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af">this</a>
post; however, it is as follows:</p>
<ul>
<li><strong>The Transformer Encoder</strong> is essentially a
Bidirectional Self-Attentive Model, that uses all the tokens in a
sequence to attend each token in that sequence</li>
</ul>
<blockquote>
<p>i.e. for a given word, the attention is computed using all the words
in the sentence and not just the words preceding the given word in one
of the left-to-right or right-to-left traversal order.</p>
</blockquote>
<ul>
<li>While the <strong>Transformer Decoder</strong>, is a Unidirectional
Self-Attentive Model, that uses only the tokens preceding a given token
in the sequence to attend that token</li>
</ul>
<blockquote>
<p>i.e. for a given word, the attention is computed using only the words
preceding the given word in that sentence according to the traversal
order, left-to-right or right-to-left.</p>
</blockquote>
<p>— from <a href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af">BERT:
Pre-Training of Transformers for Language Understanding</a></p>
<p>Thus, <strong>GPT gets its auto-regressive nature from this
directionality provided by the Transformer Decoder</strong> as it uses
just the previous tokens from the sequence to predict the next
token.</p>
<p>在这里典型的代表就是<a href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af">Bert</a>.</p>
<hr>
<p>参考读物：</p>
<ul>
<li><a href="https://medium.com/dataseries/openai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4">OpenAI
GPT: Generative Pre-Training for Language Understanding</a></li>
<li><a href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af">Bert's
Transformer Encoder architecture</a></li>
<li></li>
</ul>
<h1 id="bertpre-training-of-deep-bidirectional-transformers-for-language-understanding-2019">Bert：Pre-training
of Deep Bidirectional Transformers for Language Understanding 2019</h1>
<p>这篇文章出现在openai的GPT模型之前，前身是ELMo。</p>
<p>有两种方式将pre-trained language
representations用于下游任务，第一种是feature-based，第二种是fine-tine，其中第一种代表是ELMo，将pre-trained
representations用于附加的features输入到下游任务中；第二种是得到pre-trained的representation之后，将模型接入下游任务，然后同时fine-tune所有的参数。</p>
<p>作者在finetune模型时，采取11个下游任务：</p>
<ol type="1">
<li>GLUE general language understanding evalation</li>
</ol>
<p>它的输入主要是sentence
pair。比如MNLI数据集就是区分输入的两个句子，后者跟前者是entailment，contradiction还是中立的关系。另外GLUE
benchmark中还包含其他的几个数据集：QQP（比较两个question是不是语义上近似的），QNLI（问答数据集，sentence中是否包含能够回答question的answer）等。</p>
<p>Bert在处理此类任务的时候，不是单独对两个句子分别编码的，而是将两个句子拼接在一起，共同输入给self-attention，并且在两个句子中间加了一个标志[SEP]。在fine-tune的时候，直接将输出的hidden
states的中的第一个token的向量输出到全连接层中接softmax分类器。</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230321101507418.png" alt="sentence pair classification tasks">
<figcaption aria-hidden="true">sentence pair classification
tasks</figcaption>
</figure>
<ol start="2" type="1">
<li>SQuAD v1.1 stanford question answering dataset</li>
</ol>
<p>数据集的结构是一个question，一个passage，这个passage里面包含answer。该任务是预测answer在passage中的哪儿。在fine-tune的过程中引入了start和end两个vector，毕竟我们想知道这个answer在passage的哪个位置：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230321102053181.png" alt="SQuAD v1.1">
<figcaption aria-hidden="true">SQuAD v1.1</figcaption>
</figure>
<p>这里做分类的时候不是用的全连接层，而是使用的dot product：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230321102257845.png" alt="image-20230321102257845">
<figcaption aria-hidden="true">image-20230321102257845</figcaption>
</figure>
<p>其中，S是start vector，i是指位置i的token
word，Pi指位置i的word成为start的概率。</p>
<p>作者还在SQuAD
v2.0上做了实验，这个数据集和1.1不一样的地方在于数据集中包含不存在answer的情况。</p>
<ol start="3" type="1">
<li>SWAG situation with adversatial generations</li>
</ol>
<hr>
<p>在阅读本篇文章的过程中一个有一个疑问就是，其实bert的思想并不陌生，有点像之前的word2vec，那为什么现在大家不用预训练的word2vec来解决问题了呢？在Bert原文的第二章节，作者解答了这个问题：</p>
<blockquote>
<p>The advantage of these approaches is that few parameters need to be
learned from scratch.</p>
</blockquote>
<p>同样在cs224n的课件上我们也可以看到为什么大家转而从静态的词向量投向了GPT，Bert：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230321093156801.png" alt="pretrained word embeddings">
<figcaption aria-hidden="true">pretrained word embeddings</figcaption>
</figure>
<p>上面这张图说明的是以往的用预训练的词向量用于模型的方式，可以看到大多数的模型参数是随机初始化的，只是模型的输入部分是我们预训练的词向量。</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230321093348933.png" alt="pretrain whole models">
<figcaption aria-hidden="true">pretrain whole models</figcaption>
</figure>
<p>但是在model
nlp内，我们可以看到所有的参数在初始化的时候都是使用的预训练好的参数。这一类模型可以在预训练的过程中学习到如何表示一整个句子。</p>
<h1 id="improving-language-understanding-by-generative-pre-training-2018">Improving
Language Understanding by Generative Pre-Training 2018</h1>
<p>GPT stands for "Generative pretrained transformer" or "generative
pre-trained"</p>
<p>two main reasons for leveraging more than word-level information from
unlabeled text data</p>
<ol type="1">
<li>unclear about what type of optimization objectives are most
effective at learning text representations that are useful for transfer
缺乏很好的优化函数</li>
<li>Second, there is no consensus on the most effective way to transfer
these learned representations to the target task
如何更有效的将pre-train的知识transfer到target task还没有很好的方法</li>
</ol>
<p>作者在introduction部分强调了自己提出的模型在fine-tune阶段只需要对模型架构进行微调便可以适应target
task，在知识迁移时，采用了paper： Reasoning about entailment with neural
attention，which process structured text input as a single contiguous
sequence of tokens.</p>
<p><strong>Related work</strong> : LSTM using as the pre-trained network
to capture the representation but they has lots of restrictions. This
paper use transformer networks to allow capturing longer range
linguistic structure. Further, in fine-tuning process, the model only
requires minimal changes to model architecture other than involving
substantial amount of new parameters.</p>
<h2 id="framework">Framework</h2>
<h3 id="unsupervised-pre-train-process">1. unsupervised pre-train
process</h3>
<p><img src="/2022/10/27/Attention-and-Transformer-model/unsupervised%20pre-train%20process.png"></p>
<p>以当前token的前k个单词为context，预测当前token。典型的language
modeling。</p>
<h3 id="supervised-fine-tuning">2. Supervised fine-tuning</h3>
<p>这里和Bert不一样的是，作者采用了两个目标，第一个目标是target
task的目标（比如分类），第二个目标是pre-train时候的language
modeling的目标，即预测当前词语。为什么这么设定？作者的意思是将language
modeling的目标加入模型的fine-tune阶段可以增加模型的泛化能力和加速收敛。所以整体的模型架构是：</p>
<p><img src="/2022/10/27/Attention-and-Transformer-model/whole-picture%20of%20GPT.png"></p>
<p>截至到目前，如果我们的下游任务是分类任务，模型的fine-tune是很简单的，就将decoder的输出的最后一个token的vector拿出来接一个classifier就可以了，但是如果处理QA，texttual
entailment的数据集任务，这一类的任务的输入往往是句子的pair或者question，answer的组合。这时候作者想到一个办法是，将他们都当成一个连续的sequence，并在其中增加了随机初始化的vectors比如start/END.
同时作者指出之前有一些论文在这个方面也做了不少research，但都是“re-introduce
a significant amount of task-specific customization and does not use
transfer learning for these additional architectural components”.
那么具体是如何对模型的输入进行小改造的，如图：</p>
<p><img src="/2022/10/27/Attention-and-Transformer-model/whole%20architectures.png"></p>
<p>其中对于QA的task，作者的处理有一丢丢特殊，我们拿到的数据是一系列context，question和一系列answers，作者将document
context，question和answer拼接在一起变成一整个sequence输入到decoder中，然后计算当前question和answer的匹配程度。</p>
<h2 id="experiment">Experiment</h2>
<p>dataset ： BooksCorpus dataset contains 7000 unique books</p>
<h2 id="analysis">Analysis</h2>
<ol type="1">
<li>随着decoder部分层数的增加，performance会越来越好。作者得出结论
<em>”This indicates that each layer in the pre-trained model contains
useful functionality for solving target tasks“</em></li>
<li>zero-shot</li>
<li>ablation studies 作者在fine-tune阶段将LM任务去除，其实这时候和Bert
finetune阶段是一模一样的，只有target
task的任务需要优化。作者从这个实验中得出的结论是：LM可以在NLI和QQP两个任务上帮助提高performance。同时也发现，更大的数据集会从LM任务中获利更多，但小数据集并不会获利。同时作者将模型不进行pretrain，直接在数据集上训练，发现如果没有pretrain，所有的任务的performance都会下降，以此证明pre-train确实给所有任务都带来了performance的提升</li>
</ol>
<h2 id="conclusions">Conclusions</h2>
<p>GPT的两个关键词： generative pre-training &amp; discriminative
fine-tuning</p>
<p>同时作者得出结论是：在Transformer(what models)上利用text with long
range dependencies(which dataset to use)会收获好的performance。</p>
<h2 id="本文推荐阅读材料">本文推荐阅读材料</h2>
<ul>
<li><a href="https://huggingface.co/docs/transformers/tokenizer_summary#summary-of-the-tokenizers">summary
of tokenizers</a></li>
<li><a href="https://towardsdatascience.com/word-subword-and-character-based-tokenization-know-the-difference-ea0976b64e17">Word,
Subword, and Character-Based Tokenization: Know the Difference</a></li>
<li></li>
</ul>
<h1 id="reference">Reference</h1>
<p>在读transformer论文的时候，有几个概念key，query，value三个概念一下子就抛出来了。在讲transformer的<a href="https://www.youtube.com/watch?v=OyFJWRnt_AY">lecture
video</a>里,我看到有不少评论反应讲者没有将这三个概念讲清楚。我一开始在看cs231n的lecture
ppt时也有点疑惑，老师刚说完general attention layer转到讲self-attention
layer，就直接从h变成了q，确实有点云里雾里。</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221206170135727-16703172985855.png" alt="image-20221206170135727">
<figcaption aria-hidden="true">image-20221206170135727</figcaption>
</figure>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221206170101403.png" alt="image-20221206170101403">
<figcaption aria-hidden="true">image-20221206170101403</figcaption>
</figure>
<p>可以从上面的ppt中看出，原来是仅仅有q这个变量的，这是从一开始的h演变来的，而我们可以看到为了“add
more expressivity to the layer”，所以我们在1.输入x输入到FC得到alignment
score之前又加了一个不同的FC 2. 对输入x用attention weights进行weight
sum的过程也加了一个完全不同的FC layer。而我们可以看到这里加的这两个FC
layer是为了增加模型的表现力。这两个FC
layer的输出也就是成了我们所说的key和value。后面的过程就清晰了，首先利用query和key计算attention
weights，然后用attention weights和value进行计算得到context。</p>
<blockquote>
<p>An attention layer does a fuzzy lookup like this, but it's not just
looking for the best key. It combines the <code>values</code> based on
how well the <code>query</code> matches each <code>key</code>.</p>
<p>How does that work? In an attention layer the <code>query</code>,
<code>key</code>, and <code>value</code> are each vectors. Instead of
doing a hash lookup the attention layer combines the <code>query</code>
and <code>key</code> vectors to determine how well they match, the
"attention score". The layer returns the average across all the
<code>values</code>, weighted by the "attention scores</p>
</blockquote>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title>Anaconda使用</title>
    <url>/2021/11/05/Anaconda%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>Anaconda 安装包可以到 <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/" class="uri">https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/</a>
下载。</p>
<p>TUNA 还提供了 Anaconda
仓库与第三方源（conda-forge、msys2、pytorch等，查看完整列表）的镜像，各系统都可以通过修改用户目录下的
.condarc 文件。Windows 用户无法直接创建名为 .condarc 的文件，可先执行
conda config --set show_channel_urls yes 生成该文件之后再修改。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: true</span><br><span class="line">channel_alias: https://mirrors.tuna.tsinghua.edu.cn/anaconda</span><br><span class="line">default_channels:</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line">  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br></pre></td></tr></table></figure>
<p>一般表示 conda
应用程序的配置文件，在用户的家目录（windows：C:\users\username\，linux：/home/username/）。但对于.condarc配置文件，是一种可选的（optional）运行期配置文件，其默认情况下是不存在的，但当用户第一次运行
conda config命令时，将会在用户的家目录创建该文件。</p>
<h3 id="换国内源">换国内源</h3>
<ul>
<li>查看现在的源地址：<code>conda config --show-sources</code></li>
<li>设置搜索时显示通道地址
<code>conda config --set show_channel_urls yes</code></li>
<li>添加镜像源
<code>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</code></li>
</ul>
<h1 id="虚拟环境">虚拟环境</h1>
<p>常用命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda list 查看安装了那些包</span><br><span class="line"></span><br><span class="line">conda env list或者conda info -e 查看当前存在哪些虚拟环境</span><br><span class="line"></span><br><span class="line">conda update conda 更新当前conda</span><br><span class="line"></span><br><span class="line">conda create -n env_name python=3.6 创建虚拟环境</span><br><span class="line"></span><br><span class="line">activate env_name 激活某虚拟环境</span><br><span class="line"></span><br><span class="line">python --version 检查当前python版本</span><br><span class="line"></span><br><span class="line">conda install -n env_name [package] 安装某个包至某一个虚拟环境下</span><br><span class="line"></span><br><span class="line">deactivate 关闭当前虚拟环境</span><br><span class="line"></span><br><span class="line">conda remove -n env_name --all 删除某虚拟环境</span><br><span class="line"></span><br><span class="line">conda remove package_name 删除某个包</span><br><span class="line">pip unistall package_name</span><br><span class="line"></span><br><span class="line">conda env export &gt; requirements.yaml 将当前环境下安装的包保存为yaml文件</span><br><span class="line">conda env update -f=/path/requirements.yaml</span><br><span class="line"></span><br><span class="line">(如果不要conda，用pip的时候导出的是txt文件)</span><br><span class="line">pip freeze &gt; requirements.txt</span><br><span class="line">pip install -r /path/requirements.txt</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>其他技术</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM评测/Evaluation</title>
    <url>/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/</url>
    <content><![CDATA[<p>最近在关注模型performance评估的问题，打算在这个主题上做一个整理，也是受到很多博客和文章的启发写这篇文章，所以就将所有推荐阅读的文章放在前面，感兴趣的小伙伴可以拓展阅读。</p>
<ol type="1">
<li>老刘说NLP 公众号中8.10发的一篇文章《如何让自己的大模型榜单评分更高》
这篇文章有点借鉴了hugging face的<a href="https://huggingface.co/blog/zh/evaluating-mmlu-leaderboard">Open
LLM 排行榜近况</a></li>
<li>https://mp.weixin.qq.com/s/WiF3yRU5MZS7ulLTP8FIpw</li>
</ol>
<p>首先说一下这个<a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">LLM榜单</a>,
有四个benchmark，其中上面的博客就是重点讲了为什么同样一个模型比如LLaMA在MMLU上评测的结果会不如llama文章中提的效果，trick就在作者使用MMLU这个benchmark的方式有很大不同，这里来看看MMLU这个benchmark。</p>
<h1 id="mmlu-benchmark">MMLU benchmark</h1>
<p>首先看一下这个数据集到底是什么数据集，长什么样子，先给出文章中的定义：</p>
<blockquote>
<p><strong>MMLU</strong> (<strong>Massive Multitask Language
Understanding</strong>) is a new benchmark designed to measure knowledge
acquired during pretraining by evaluating models exclusively in
zero-shot and few-shot settings.</p>
</blockquote>
<p>这个评测集合里包含了57个学科，也就是57个task。原始的数据集长这样，里面的每个问题包含四个可能选项，且每个问题只有一个正确答案。：</p>
<figure>
<img src="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/image-20230816145305589.png" alt="image-20230816145305589">
<figcaption aria-hidden="true">image-20230816145305589</figcaption>
</figure>
<p>可以看到基本上就是question，answer的组织。注意这里看到原始数据的时候我还有点没看明白，作者的readme中也没写，还是对beginner有点不友好，第一列表示question，第二到第四列表示四个选项，最后一列是答案。所以可以看到原作者在<a href="https://github.com/hendrycks/test/blob/master/evaluate.py">evaluation</a>的代码中这样处理的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">choices = [<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>, <span class="string">&quot;C&quot;</span>, <span class="string">&quot;D&quot;</span>] <span class="comment"># 首先定义选项</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval</span>(<span class="params">args, subject, engine, dev_df, test_df</span>):</span></span><br><span class="line">    cors = []</span><br><span class="line">    all_probs = []</span><br><span class="line">    answers = choices[:test_df.shape[<span class="number">1</span>]-<span class="number">2</span>] <span class="comment"># 对于每一个csv文件读取进来后取answers</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">label = test_df.iloc[i, test_df.shape[<span class="number">1</span>]-<span class="number">1</span>] <span class="comment"># label这里其实是取得最后一列，也就是答案</span></span><br></pre></td></tr></table></figure>
<p>但这个评测数据集在用来评测LLM的过程中衍生出了很多版本，基本是prompt的变化：</p>
<figure>
<img src="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/640.png" alt="MMLU的不同实现">
<figcaption aria-hidden="true">MMLU的不同实现</figcaption>
</figure>
<p>同样的问答对，比如上面的选择题，Harness没有指令，并且衍生的两个版本也就是helm和harness版本还加了Question这个前缀，harness在选线之前还加了Choices。就这么一点差距，就导致同一个llm的出来的分数不一样：</p>
<figure>
<img src="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/LLM-01-bis-01.png" alt="LLM在不同MMLU实现上的评分">
<figcaption aria-hidden="true">LLM在不同MMLU实现上的评分</figcaption>
</figure>
<blockquote>
<p>关于如何使用这个benchmark，参考<a href="https://github.com/hendrycks/test/blob/master/evaluate.py">MMLU原始实现</a>，作者写的是用chatgpt来产生答案，prompt为：<code>prompt = "The following are multiple choice questions (with answers) about &#123;&#125;.\n\n".format(format_subject(subject))</code></p>
</blockquote>
<p>这三种实现方式不仅prompt的形式不同，也就是上面提到的。并且它在计算F1score的时候的机制也不同。</p>
<ol type="1">
<li>原始实现</li>
</ol>
<p>在原始实现中的评估的代码是这样写的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ans <span class="keyword">in</span> answers:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        lprobs.append(c[<span class="string">&quot;choices&quot;</span>][<span class="number">0</span>][<span class="string">&quot;logprobs&quot;</span>][<span class="string">&quot;top_logprobs&quot;</span>][-<span class="number">1</span>][<span class="string">&quot; &#123;&#125;&quot;</span>.<span class="built_in">format</span>(ans)]) <span class="comment"># c是chatgpt的回答</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Warning: &#123;&#125; not found. Artificially adding log prob of -100.&quot;</span>.<span class="built_in">format</span>(ans))</span><br><span class="line">    lprobs.append(-<span class="number">100</span>)</span><br><span class="line">    pred = &#123;<span class="number">0</span>: <span class="string">&quot;A&quot;</span>, <span class="number">1</span>: <span class="string">&quot;B&quot;</span>, <span class="number">2</span>: <span class="string">&quot;C&quot;</span>, <span class="number">3</span>: <span class="string">&quot;D&quot;</span>&#125;[np.argmax(lprobs)]</span><br><span class="line">    probs = softmax(np.array(lprobs))</span><br><span class="line"></span><br><span class="line">    cor = pred == label</span><br><span class="line">    cors.append(cor)</span><br><span class="line">    all_probs.append(probs)</span><br></pre></td></tr></table></figure>
<p>该方法在评估的时候，仅仅比较了模型对四个选项字母的预测概率，哪个选项的概率高就选哪个，即便是在极端情况下四个选项的概率值都很低的情况下也会选择某个选项，但其实模型有时候会回答很多不相关的东西（都是很高的概率的token），所以这种方式有点”放水“，整体评估出来的分数会偏高。</p>
<ol start="2" type="1">
<li><a href="https://github.com/stanford-crfm/helm">HELM实现</a></li>
</ol>
<p>HELM实现是根据模型预测的下一个输出词元的概率来选择输出文本，并将生成的文本与正确答案的文本进行对比。这种方式有效避免了如果模型的答案中出现概率高的token不是选项中的任意一个，那么就会判为错误答案。</p>
<p>看了helm的代码仓库，着实有点丰富。内容很多我都没有找到在哪个文件里做的evaluation的计算，只知道了读取csv的地方。有好心的小伙伴可以私信我告诉我在哪里。</p>
<ol start="3" type="1">
<li>harness实现</li>
</ol>
<p>这是hugging
face的llm榜单所用的实现。它不再是只是统计选项，而是连同选项字母以及后面的答案一起被考虑进来，计算的是整个序列的概率（获取每个词元的概率
(与上面其他实现一样)
并求它们的联合概率），那么很容易一些长文本的联合概率会比短文本的联合概率大，所以作者说可以在联合概率的基础上在做一个归一化，也就是用对数联合概率/
token数。</p>
<figure>
<img src="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/image-20230817100952429.png" alt="MMLU三种实现对于模型输出的总结">
<figcaption aria-hidden="true">MMLU三种实现对于模型输出的总结</figcaption>
</figure>
<p>例如实现如下，基于GPT2计算句子联合概率的一个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [</span><br><span class="line">    <span class="string">&quot;A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN)&quot;</span>,</span><br><span class="line">    <span class="string">&quot;The term MLP is used ambiguously, sometimes loosely to any feedforward ANN, sometimes strictly to refer to networks composed of multiple layers of perceptrons (with threshold activation); see § Terminology&quot;</span>,</span><br><span class="line">    <span class="string">&#x27;Multilayer perceptrons are sometimes colloquially referred to as &quot;vanilla&quot; neural networks, especially when they have a single hidden layer.[1]&#x27;</span>,</span><br><span class="line">    <span class="string">&quot;An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function.&quot;</span>,</span><br><span class="line">]</span><br><span class="line">model = transformers.GPT2LMHeadModel.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line">tok = transformers.GPT2Tokenizer.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line">tgs = []</span><br><span class="line"><span class="keyword">for</span> dat <span class="keyword">in</span> data:</span><br><span class="line">    random.seed(dat)</span><br><span class="line">    <span class="comment"># print(model(tok.encode(dat, return_tensors=&quot;pt&quot;))[0][0])</span></span><br><span class="line">    toks = tok.encode(dat, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">    ind = random.randrange(<span class="built_in">len</span>(toks[<span class="number">0</span>]) - <span class="number">1</span>)</span><br><span class="line">    logits = F.log_softmax(model(toks)[<span class="number">0</span>], dim=-<span class="number">1</span>)[:, :-<span class="number">1</span>]  <span class="comment"># [batch, seq, vocab]</span></span><br><span class="line">    res = torch.gather(logits, <span class="number">2</span>, toks[:, <span class="number">1</span>:].unsqueeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    tgs.append(<span class="built_in">float</span>(res[ind:].<span class="built_in">sum</span>()))</span><br></pre></td></tr></table></figure>
<p>在“老刘说NLP”的博客中也提到了一点，就是上面的方式都是开源模型，所以很容易就能得到每一个token的预测概率，所以返回结果可以拆的这么细致来分析。如果是闭源模型只返回response的话，这时候就需要用正则的方式来抽取回答内容里的选项，比如<a href="https://arxiv.org/abs/2308.04813">CEVAL</a>的测试方案：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_cot_answer</span>(<span class="params">self, line, gen_ans</span>):</span></span><br><span class="line">    m = re.findall(<span class="string">r&#x27;所以答案是(.+?)。&#x27;</span>, gen_ans, re.M)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(m) &gt; <span class="number">0</span> <span class="keyword">and</span> m[-<span class="number">1</span>] <span class="keyword">in</span> self.choices:</span><br><span class="line">        <span class="keyword">return</span> m[-<span class="number">1</span>], <span class="literal">True</span></span><br><span class="line">    answer_patterns = [</span><br><span class="line">        <span class="string">r&#x27;([ABCD])是正确的&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;选项([ABCD])正确&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;答案为([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;答案是([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;答案([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;选择([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;答案：([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;选择答案([ABCD])&#x27;</span></span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># RE extraction</span></span><br><span class="line">    <span class="keyword">for</span> answer_pattern <span class="keyword">in</span> answer_patterns:</span><br><span class="line">        m = re.search(answer_pattern, gen_ans, re.M)</span><br><span class="line">        <span class="keyword">if</span> m:</span><br><span class="line">            answer = m.group(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> answer, <span class="literal">False</span></span><br><span class="line">        <span class="comment"># only containing one choice-character</span></span><br><span class="line">        m = re.findall(<span class="string">r&#x27;[ABCD]&#x27;</span>, gen_ans, re.M)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(m) == <span class="number">1</span>:</span><br><span class="line">            answer = m[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">return</span> answer, <span class="literal">False</span></span><br><span class="line">        answer_word_counter = <span class="number">0</span></span><br><span class="line">        <span class="comment"># only containing one choice-context</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> self.choices:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(line[<span class="string">f&#x27;<span class="subst">&#123;c&#125;</span>&#x27;</span>]) <span class="keyword">in</span> gen_ans:</span><br><span class="line">                answer = c</span><br><span class="line">                answer_word_counter += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> answer_word_counter == <span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">return</span> answer, <span class="literal">False</span></span><br><span class="line">                <span class="keyword">return</span> <span class="string">&#x27;-&#x27;</span>, <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>对CLEVA评测平台感兴趣的可以看原文paper或者参考<a href="https://mp.weixin.qq.com/s/WiF3yRU5MZS7ulLTP8FIpw">文章</a>。原文说CLEVA是专门为评估中文语言模型而设计的平台。</p>
</blockquote>
<h1 id="section"></h1>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM大模型推理加速</title>
    <url>/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/</url>
    <content><![CDATA[<p>最近框架vLLM在LLaMA-13B以及7B模型上的推理速度相比较Tranformers有了质的提升。之前写过一篇大模型量化技术的文章，量化技术算是大模型出来初期大家使用的普遍比较多的方法之一，这里强调一点，我这里所说的模型加速是指在推理阶段我们让模型运行的更快，而且返回的结果要和原来的大参数模型差不多。这里重点强调的原因是我在看一些资料的时候发现有不少博客分享的是在模型训练阶段或者finetune阶段如何让模型训练的更快，这里就涉及到efficient
finetuning的技术（p-tuning,
prefix-tuning等），我这篇博客只关注模型训练完成之后如何在推理阶段让它更快，在同样时间内处理更多sequence（吞吐量througout），显存占用更低。大模型推理加速技术为什么这么受关注还是因为想在一些消费级别显卡上部署一个大模型为用户所用，而不是仅仅停留在实验室阶段。</p>
<blockquote>
<p>我在看这个topic下的文章的时候，发现往往一些方法提出来有一些是减少了显存占用，有一些是提高了吞吐量（跟减少latency一回事），所以具体在实现时应用哪个办法加速你的模型推理还要根据实际情况去对比分析，或者你每个方法都尝试一下也行。当然又一些方法集成地很好，比如量化模型中的GPQT已经集成进transformers库，用起来很方便。如果碰到一些很复杂的，比如prune“剪枝”就有点难以快速验证。</p>
</blockquote>
<p><a href="https://vllm.ai/">vLLM</a>
暂时还没有文章发出来，我在谷歌搜寻有没有review介绍大模型加速文章的时候也没找到很新的文章，不过找到了一篇微软在23年发布的<a href="http://arxiv.org/abs/2304.04487">LLMA</a>,
我本来觉得思想类似，但是后来仔细看了下文章发现并不是一回事，我觉得文章标题有点误导人，起的太大了，本质上文章其实就是发现了decoding时候生成的句子和之前的句子有一部分的文字重叠，所以作者考虑这部分重叠内容其实不需要让模型再去decoding了，那就想了个办法，在decoding的时候把前面一步的结果保存下来，比较当前步骤和前一步骤token的差距，差距小的就不再进行计算了</p>
<blockquote>
<p>一点不成熟的想法：这个文章思路可取，但创造力有限。</p>
</blockquote>
<p>不过它在introduction章节介绍了四种比较通用的加速方法：quantization,
pruning, compression and distillation，同样的分类也可以在<a href="https://arxiv.org/pdf/2205.07877.pdf">A Comprehensive Survey on
Model Quantization for Deep Neural Networks</a>文章中找到，
不过两者介绍的有一点点的不同：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230828093646830.png" alt="image-20230828093646830">
<figcaption aria-hidden="true">image-20230828093646830</figcaption>
</figure>
<p><a href="https://arxiv.org/pdf/2205.07877.pdf">Survey</a>将四种技术统一包含在了模型压缩里。我觉得review里这种分类比较合理，因为微软这篇文章compression引用的文章是</p>
<p><a href="https://arxiv.org/abs/2002.02925">Bert-of-theseus:
Compressing bert by progressive module replacing</a>,
compression应该是一种统称。后面我看到知乎有一篇<a href="https://zhuanlan.zhihu.com/p/642412124">文章</a>更详细的介绍了大模型的推理优化技术，它这个分类也符合我的理解，模型压缩（model
compression）里包含模型量化，pruning，low-rank
approximation和知识蒸馏这些技术。而且知乎这篇文章的分类也符合survey里的介绍：</p>
<blockquote>
<p>In designing accelerators, researchers concentrate on network
compression, parallel processing, and optimizing memory transfers for
processing speed-up.</p>
</blockquote>
<p>我这里做个思维导图总结一下：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230830133030762-16933734335261.png" alt="image-20230830133030762">
<figcaption aria-hidden="true">image-20230830133030762</figcaption>
</figure>
<p>题外话，根据LLMA文章的意思，它提出的这种帮助reduce the serving
cost的方式不属于上述任意一类，它认为以transformer为基础的生成模型，推理阶段主要消耗的时间瓶颈在autoregressive
decoding。这里贴原文便于理解</p>
<blockquote>
<p>While there are general methodologies that help reduce the serving
cost of LLMs such as quantization(Dettmers &amp; Zettlemoyer, 2023),
pruning (Frantar &amp; Alistarh, 2023), compression (Xu et al., 2020)
and distillation (Wang et al., 2020), <strong>the inference efficiency
bottleneck of these transformer-based generative models (e.g., GPT) is
mainly associated with autoregressive decoding: at test time, output
tokens must be decoded (sequentially) one by one, which poses
significant challenges for the LLMs to be deployed at scale.</strong>
这里补充介绍一下AI模型中精度，你会在各种场合下碰到FP32，FP16，int8，int4等名词。</p>
</blockquote>
<p>32-bit：也称全精度(Single precision)，<code>fp32</code>,
采用32位（4字节）来对数据进行编码。能够表达的数据动态区间是 <span class="math display">\[
1.4 * 10^{-45} - 1.7 * 10 ^ {38}
\]</span></p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/Float_example-16933762107103.svg" alt="Float_example">
<figcaption aria-hidden="true">Float_example</figcaption>
</figure>
<p>16-bit：半精度（half precision）,<code>fp16</code>,
能够表达的数据动态区间是 <span class="math display">\[
6 * 10^{-8} - 65504
\]</span></p>
<p><img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/IEEE_754r_Half_Floating_Point_Format-16933761704442.svg"></p>
<p>BF-16： 也称为半精度，可以表达比FP16更大的数，但是精度比fp16差</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230830145435142-16933784764394.png" alt="image-20230830145435142">
<figcaption aria-hidden="true">image-20230830145435142</figcaption>
</figure>
<p>int8,int4顾名思义就是 8bit和4个bit来表示数字，int8的表达数值范围是
<code>-128~127</code> <a href="https://blog.csdn.net/ordmeng/article/details/99620804">why</a>，无符号范围是<code>0~255</code>，int4的表达数值范围是<code>-8~7</code>。注意这里的计算方式和上面的浮点数可不一样，上面的浮点数中的8bits的exponent是指数表达，所以将指数那一部分的表达加和之后还要取2的指数，见<a href="https://blog.csdn.net/qq_36533552/article/details/105885714">具体计算</a>.
再详细一点的介绍见<a href="https://huggingface.co/blog/zh/hf-bitsandbytes-integration">hugging
face 博客</a></p>
<h1 id="模型压缩-model-compression">模型压缩 model compression</h1>
<h2 id="quatization-量化">Quatization 量化</h2>
<h3 id="参考文献">参考文献</h3>
<ul>
<li><a href="https://arxiv.org/pdf/2205.07877.pdf">A Comprehensive
Survey on Model Quantization for Deep Neural Networks</a></li>
<li>https://huggingface.co/docs/transformers/main_classes/quantization#autogptq-integration
huggingface的document，其中放在最上面的就是<a href="https://arxiv.org/pdf/2210.17323.pdf">GPTQ</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/627436535">大语言模型的模型量化(INT8/INT4)技术</a>
讲解了<code>LLM.int8()</code></li>
<li><a href="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/">LLM.int8()
and Emergent Features</a></li>
</ul>
<p>模型的量化可以分为两种方式：</p>
<ol type="1">
<li>post-training quantization
模型训练好之后，将模型的参数们降低精度</li>
<li>quantization-aware Training
在模型训练的过程中使用量化的方式，优点是比前者performance要好，但是需要更多的计算资源去训练</li>
</ol>
<p>量化顾名思义要把原来用高精度表达的值映射到一个低精度的空间，目标呢就是让模型的performance不能有很大的降低。那如何映射和对哪一些值进行映射，这两个方向是现在量化方法的主攻方向。</p>
<p>很典型的LLM.int8()算法，不是大刀阔斧地对所有值一次性量化，也不是把矩阵中所有值一起量化，而是先找出那些离群值，然后对这些离群值再按照居正中行列来进行量化：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230831103106998.png" alt="LLM.int8()">
<figcaption aria-hidden="true">LLM.int8()</figcaption>
</figure>
<p>最重要的是上面那一部分。计算拆解见<a href="https://huggingface.co/blog/zh/hf-bitsandbytes-integration">大规模
Transformer 模型 8 比特矩阵乘简介</a></p>
<h3 id="gptq">GPTQ</h3>
<p>读者可以自行阅读GPTQ的原文来了解它具体是如何做的，我喜欢找一些其他的文章来看别的作者是如何介绍自己的同行作品的，比如下面的这篇文章<a href="https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf">SmoothQuant:
Accurate and Efficient Post-Training Quantization for Large Language
Models</a> 的第六章节related
work里这样比较自己的smoothQuant和其他的量化模型方法的：</p>
<blockquote>
<p>GPTQ (Frantar et al., 2022) applies quantization only to weights but
not activations.
GPTQ这种方法只对weights做了量化，并没有对激活值做量化（我个人认为虽然这是事实，但有点硬凹的意思，因为对activations做量化映射并不会加速很多）</p>
<p>LLM.int8() uses mixed int8/fp16 decomposition to address the
activation outliers. However, such implementation leads to large latency
overhead, which can be even slower than FP16 inference.
意思是LLM.int8()这种方法只是减少了显存占用，并没有减少推理延迟，说白了就是慢，runtime没提高</p>
</blockquote>
<h2 id="sparsity">Sparsity</h2>
<h2 id="low-rank-approximation">low-Rank Approximation</h2>
<h3 id="lora"><a href="http://arxiv.org/abs/2106.09685">Lora</a></h3>
<p>用lora的方式替换全参数微调大模型已经成为好多研究者的选择，一个是它的确有效的降低了训练参数的比例，第二个很大的原因是它的performance还不错，也就是只训练低秩的那些参数矩阵完全可以得到一个高质量的模型.</p>
<blockquote>
<p>We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained
model weights and injects trainable rank decomposition matrices into
each layer of the Transformer architecture, greatly reducing the number
of trainable parameters for downstream tasks</p>
</blockquote>
<p>从文章的介绍可以看出，它主要是应用于transformer架构中的layer中，这个layer包含self-attention，也包含MLP。只要有矩阵乘的地方都可以用lora。</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20231228160733632.png" alt="lora图示">
<figcaption aria-hidden="true">lora图示</figcaption>
</figure>
<p>具体的在transformer中如何使用呢？我们知道transformer架构中涉及矩阵运算的地方：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20231228171113210.png" alt="image-20231228171113210">
<figcaption aria-hidden="true">image-20231228171113210</figcaption>
</figure>
<p>在上图中的self-attention中涉及四个weights矩阵：(Wq, Wk, Wv,
Wo)。在feed-forward(lora原文中叫MLP，我觉得原因在于这里是有两个线性变换的)，具体看attention
is all you need 原文：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20231228171318056.png" alt="image-20231228171318056">
<figcaption aria-hidden="true">image-20231228171318056</figcaption>
</figure>
<p>所以每一个encoder的layer都有6个weights矩阵可以实施lora。lora的作者仅仅对attention
weights做了lora，更简化的只是对其中的Wq和Wv做了lora变换。实现上由于现在有了peft库，只是几句话就能实现对这两个矩阵进行lora：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_peft_config</span>(<span class="params">model</span>):</span></span><br><span class="line">    <span class="keyword">from</span> peft <span class="keyword">import</span> (</span><br><span class="line">        get_peft_model,</span><br><span class="line">        LoraConfig,</span><br><span class="line">        TaskType,</span><br><span class="line">        prepare_model_for_int8_training,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    peft_config = LoraConfig(</span><br><span class="line">        task_type=TaskType.CAUSAL_LM,</span><br><span class="line">        inference_mode=<span class="literal">False</span>,</span><br><span class="line">        r=<span class="number">8</span>,</span><br><span class="line">        lora_alpha=<span class="number">32</span>,</span><br><span class="line">        lora_dropout=<span class="number">0.05</span>,</span><br><span class="line">        target_modules = [<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>] <span class="comment"># 这里仅仅对Q和V的变换矩阵做lora</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prepare int-8 model for training</span></span><br><span class="line">    model = prepare_model_for_int8_training(model)</span><br><span class="line">    model = get_peft_model(model, peft_config)</span><br><span class="line">    model.print_trainable_parameters()</span><br><span class="line">    <span class="keyword">return</span> model, peft_config</span><br><span class="line"></span><br><span class="line"><span class="comment"># create peft config</span></span><br><span class="line">model, lora_config = create_peft_config(model)</span><br></pre></td></tr></table></figure>
<h2 id="knowledge-distillation">Knowledge Distillation</h2>
<p>知识蒸馏出现的比较早，一开始也是在Bert上流行起来的。在LLM这种非常多参数的模型上用的不多。</p>
<p>这里罗列我看的对我理解很有帮助的文章：</p>
<ul>
<li><a href="https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/ch08.html#idm45146300040192"><strong>Natural
Language Processing with Transformers, Revised Edition</strong></a>
第八章“Making models smaller via Knowledge Distillation”
很详细的介绍了loss的计算（只是蒸馏的核心点）</li>
<li><a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/#distillation">lilian
wen blog</a></li>
</ul>
<h1 id="continuous-batching">Continuous Batching</h1>
<p><strong><em>参考文献：</em></strong></p>
<ul>
<li><a href="https://www.anyscale.com/blog/continuous-batching-llm-inference">How
continuous batching enables 23x throughput in LLM inference while
reducing p50 latency</a></li>
<li></li>
</ul>
<h2 id="vllm">vLLM</h2>
<p>vllm这个包本质上是将显存GPU高效利用了（PagedAttention技术），还有上面提到的LLMA.
不过它们的思路其实大同小异，本质上是为了解决transformer的decoder在文本生成时自回归结构带来的无法并发的开销。推荐阅读<a href="https://mp.weixin.qq.com/s/_4OxoRLxhOcjGf0Q4Tvp2Q">为什么现在大家都在用
MQA 和 GQA？</a></p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/640.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>那么既然时间开销都在右边这个decoding的阶段，那就想办法解决它。那就是刚刚那篇文章介绍的KV
Cache。作者提到的内存墙的问题也是这个问题的切入点，如何让计算单元更迅速的从存储单元获取数据，Paged
Attention和Flash
Attention都是来解决这个问题的。MQA的本质是减少了数据读取次数，第一次读取进来的K和V给所有的Q用，就放在缓存里。文章里详细讲解了MQA和GQA，这里不再赘述，但有一点值得注意的是，这两种办法再使用的时候可能并不能只是在推理的时候直接改变结构，也许要像作者说的那样：</p>
<blockquote>
<p>如果要用 MQA 和 GQA，可以是从头训练的时候就加上，也可以像 GQA
论文里面一样，用已有的开源模型，挑一些头取个 mean 用来初始化 MQA 或 GQA
继续训练一段时间。</p>
</blockquote>
<h2 id="text-generation-inference">Text Generation Inference</h2>
<h1 id="transformer结构优化">Transformer结构优化</h1>
<p>经典的Transformer架构出来之后，很多工作都在这个架构之上进行了魔改，希望能加快transformer的推理速度，推荐阅读survey
<a href="https://arxiv.org/pdf/2009.06732.pdf">Efficient Transformers: A
Survey</a> ,
目前该review已经是第三版本，最新版本是2022年3月份出的，所以内容里没有flash
Attention以及一些更新的技术，希望作者快快更新第四版本的review，技术迭代太快了，亟需大神肝的review总结。</p>
<p>大部分Transformer结构改进的方法的目标都是为了降低GPU的显存占用，也就是提高运算效率，将GPU的运算效率拉满，这就涉及到很底层的对于计算机系统结构的知识。</p>
<h2 id="flash-attention">flash Attention</h2>
<p>Flash Attention （Fast and Memory-Efficient Exact
Attention）详细介绍可以阅读<a href="https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad">ELI5:
FlashAttention</a>，这是除了了multi-query
attention技术之外用的比较多的加速推理的方式，当然Paged
Attention算是vllm火起来之后的后起之秀。当然也可以阅读<a href="https://arxiv.org/pdf/2205.14135.pdf">paper</a></p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230829155237303-16932955599782.png" alt="attention on GPT-2">
<figcaption aria-hidden="true">attention on GPT-2</figcaption>
</figure>
<p>在medium的这篇博客里作者首先澄清两个概念，一个是FLOPs（每秒钟浮点运算次数）和IO，前者在GPU的更新换代的情况下获得了高速发展，而GPU的计算单元和显存间的通信却没有获得同样数量级的增长。而从上图可以看出来（原文paper中的），Attention的计算过程中大部分时间都是memory-bound导向的运算，而计算密集型的操作比如矩阵乘法其实只占了耗费时间的一小部分。</p>
<p>传统的attention计算步骤：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230829162652549.png" alt="image-20230829162652549">
<figcaption aria-hidden="true">image-20230829162652549</figcaption>
</figure>
<p>注意这里的HBM是：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230829162732067.png" alt="image-20230829162732067">
<figcaption aria-hidden="true">image-20230829162732067</figcaption>
</figure>
<p>最上面一层是GPU的缓存，中间是高带宽内存，可以理解为GPU的显存，也就是你去买显卡，标注在显卡上的存储，这部分存储会大一点，运算单元需要从这里拿数据到计算单元去计算，可以直接交互，也可以先存储在缓存SRAM内，缓存会比HBM快很多。</p>
<p>从标准的attention计算看到有很多不需要把计算中间结果写回HBM的环节。至于FlashAttention计算推导部分我看了上面的英文博客和<a href="https://zhuanlan.zhihu.com/p/638468472">从 FlashAttention 到
PagedAttention, 如何进一步优化 Attention
性能</a>，还是没能理解，感兴趣的小伙伴还是自己去知乎这篇文章里好好看一下。</p>
<h2 id="paged-attention">Paged Attention</h2>
<h2 id="flat-attention">FLAT Attention</h2>
<h1 id="并行-parallel-processing">并行 Parallel Processing</h1>
<h1 id="推荐阅读">推荐阅读</h1>
<ul>
<li><a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/#methods-overview">Large
Transformer Model Inference Optimization</a>
lilian的博客，在文首推荐的知乎文章大抵参考了这篇博客，虽然是1月份的文章，但还是推荐阅读食用</li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>instruction-following language models</title>
    <url>/2023/04/14/Instruction-Finetuned-LLM/</url>
    <content><![CDATA[<p>nlp领域很多新出现的名词或者火热的研究方向，没有一个统一的标准。我在接触这些新的概念的时候往往会很糊涂，需要找大量的文献来看，然后捋清楚模型或者技术路线的发展脉络。instructed
LM，它是需要对pre-trained
LLM进行finetune的，在这之前也有一种技术叫做prompt
engineering，它是一种给大模型指令输入的手段，通过调整给大模型的输入，从而使得大模型能够返回更好的输出，解决我们的问题。也有更好的解释引用自<a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/">blog</a></p>
<blockquote>
<p><strong>Prompt Engineering</strong>, also known as <strong>In-Context
Prompting</strong>, refers to methods for how to communicate with LLM to
steer its behavior for desired outcomes <em>without</em> updating the
model weights. It is an empirical science and the effect of prompt
engineering methods can vary a lot among models, thus requiring heavy
experimentation and heuristics</p>
</blockquote>
<p>prompt engineering得益于LLM拥有zero-shot learning和few-shot
learning的两种prompt 模型的方法的发展。它更多的来源于经验。</p>
<p>prompt engineering领域也出现了非常多的文章，就正如<a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/">blog</a>里的观点一样，我同样觉得有一些文章只需要很少的文字就能讲明白它提出的方法是什么，但还是花了很多的篇幅，一个通用的benchmark才是我们需要的，现在有的只是一些零零碎碎的方法论。prompt
engineering不是我的关注重点，它受制于很多因素的影响，比如如果你使用的是GPT-3模型来开展你的任务或者搭建你的application，你可能会因为输入过多的文字而超出limit，而且GPT可是按照字符数收费的，所以可能会比较贵。</p>
<p>那么除了使用prompt
engineering的方式来让LLM输出能让我们满意的结果，另外一种方式是fine-tune整个LLM，直接让它在特定的数据集上调整参数（整体调整或者局部调整，比如Lora，prefix-tuning）或者使用增强学习训练一个打分模型，这也属于fine-tune的一个大分支。</p>
<p>2013年的综述文章<a href="http://arxiv.org/abs/2303.18223">A Survey of
Large Language Models</a> 在第五章介绍了详细的adaptation tuning of
LLMs的方法，也就是我一个pretrain好的LLM，如何让它在不同的任务上得到更好的泛化能力，这时候就要tuning
LLM。作者介绍其中有两种方法，一个是instruction Tuning，第二个是alignment
tuning。后者就是利用增强学习让模型从人类的反馈中去改进自己生成的文本，InstructGPT采用了这种方法。第一种会稍微复杂一点，但原理很简单，就是创造一系列的instruction和问答对，让LLM在这些新instruction上重新finetune，loss为sequence-to-sequence的loss。</p>
<blockquote>
<p>[My personal spicy take]
这里这篇综述我觉得写的不完整，有点误导读者。这篇综述第五章只介绍了adaptation
tuning模型中的两种，但在instruction
tuning出现之前，还有不少技术能够帮助我们“further adapt LLM according to
specific goals”. 不仅如此，这篇综述也没有很好的解释instruction
tuning为什么就能帮助我们在不同任务上有了performance的提高。所以我就想写一篇博客来记录如果我们拥有了一个pretrained的大模型，我们可以有什么样的做法来使得大模型在特定的任务上为我们所用。详见另一篇博客“Adaptation
Tuning of LLMs”</p>
</blockquote>
<p>在接触羊驼模型后，我一直有一个疑问，为什么instruction
finetuned模型performance有了提高，或者说它在什么样的任务上有了提高？这个问题一直困扰我，直到我看到了google家的<a href="http://arxiv.org/abs/2109.01652">Finetuned Language Models Are
Zero-Shot Learners</a>.instruction
tuning这种finetune方式的提出是为了<strong>improve zero-shot performance
on unseen tasks</strong>，具体一点就是在一些任务上比如阅读归纳，question
answering和语言推理上，研究者发现GPT3的zero-shot learning比few-shot
能力差很多，作者说一个潜在的原因是因为如果没有一些context给到模型的话，模型在面对跟pretrain时候数据相差很大的prompt时候会很困难，说直白点，就是没有例子给它参考了，就不会做题了。instruction
tuning这种方式就提供了一种非常简单的方式，它在好多个task上finetune这个模型，这里每一个task的数据组织形式跟原来不一样了，现在被组织成了(instruction,[input],output)的形式。finetune完之后的模型在unseen
task上做evaluation，研究者发现被instruction
finetune之后的模型比原来的模型在同一任务上的zero-shot能力大大提升：</p>
<figure>
<img src="/2023/04/14/Instruction-Finetuned-LLM/image-20230522182724638-16848907566251.png" alt="performance">
<figcaption aria-hidden="true">performance</figcaption>
</figure>
<h1 id="instruction-tuning">instruction tuning</h1>
<p>想要做到instruction tuning有两个前提条件：1. 你有一个pretrained的模型
2.
有很多instructions。首先第一个条件可以看看市面上有哪些模型是已经开源了，参考<a href="http://arxiv.org/abs/2303.18223">A Survey of Large Language
Models</a>3.1的整理，2023年斯坦福的羊驼模型是基于meta的LLaMA，所以目前github上出现了很多用LLaMA为LLM，在上面做instruction
tuning工作的。</p>
<figure>
<img src="/2023/04/14/Instruction-Finetuned-LLM/image-20230418093127008.png" alt="LLMs">
<figcaption aria-hidden="true">LLMs</figcaption>
</figure>
<p>那第一个问题解决了，起码我们有开源的LLM可以load到本地来使用，感谢facebook的开源。第二个问题如何产生很多的instructions，斯坦福的<a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">羊驼模型Alpaca</a>采用的是下面文章介绍的方法，省时省力，花费上不超过600美金。当然也有其他的一些产生instruction的方法，详细可以参考<a href="http://arxiv.org/abs/2303.18223">A Survey of Large Language
Models</a>
，其中作者介绍了一系列可以从现有数据集生成instruction的方法，这些方法应该也是低成本快速产生instruction的方法。</p>
<hr>
<p><a href="http://arxiv.org/abs/2212.10560">Self-instruct: Aligning
Language Model with Self Generated Instructions</a>
这篇文章介绍了一种self generated
instructions的方法，简单说就是让LLM自己生成人类的问题的答案，然后将这些instructions
重新来fine-tune我们的LLM。这样做的一个前提条件是：1. Large
“instruction-tuned” language models (finetuned to respond to
instructions) have demonstrated a remarkable ability to generalize
zero-shot to new tasks. 2. 产生instruction
data非常的耗时，原来都是采用Human written的方式。具体步骤是：</p>
<figure>
<img src="/2023/04/14/Instruction-Finetuned-LLM/image-20230414131301078.png" alt="self-instruct whole picture">
<figcaption aria-hidden="true">self-instruct whole picture</figcaption>
</figure>
<p>作者首先使用175个手工写的instructions作为seed
set，利用这175个instructions用LLM再次生成更多的instructions，将这些instructions再次输入到LLM中我们就得到了很多input-output
pair。这些input-output pair将会用来做instruction tuning.
作者使用的LLM是GPT-3. 最终得到了52k个instructions，以及82k个input-output
pair。</p>
<h2 id="instruction-generation">Instruction generation</h2>
<p>用bootstrap的方式，以人工产生的instruction为基础，用GPT来自己生成更多的"new
and novel"instruction。</p>
<hr>
<p>自Alpaca之后，国内的一些团队也仿照斯坦福的这种模型，做了一些自己的LLM，例如https://github.com/LC1332/Chinese-alpaca-lora，instruction来自用GPT翻译的斯坦福产生的52k的instruction的数据，它基于的模型<a href="https://github.com/tloen/alpaca-lora">aplaca-lora</a>,lora的全称是Low-rank
adaptation，作者说自己"reproducing the <a href="https://github.com/tatsu-lab/stanford_alpaca">Stanford Alpaca</a>
results using <a href="https://arxiv.org/pdf/2106.09685.pdf">low-rank
adaptation (LoRA)</a>."，并且训练好的instructed
model提供的文本质量可以和text-davinci-003(GPT-3)媲美。不太了解这个LoRA，有兴趣的可以读原文：https://arxiv.org/pdf/2106.09685.pdf。</p>
<p>看了Alpaca的blog，我发现斯坦福在evaluation阶段是将alpaca的结果和gpt3来进行比较的，由此也引发了我的思考，就是我们如何去衡量一个LLM的performance。刚上文的review的第七章很好的解答了我的疑惑，包括一系列的基本评测任务以及高级的评测任务。当然作者在7.3也给出了一些公开的全面的benchmarks，而且是用的比较多的，其中有MMLU，BIG-bench，HELM，这些benchmark内都包含了很多个任务，可以综合评测一个LLM的performance。</p>
<h1 id="stanford-alpaca">stanford alpaca</h1>
<p>这是2023年斯坦福开源的一款基于meta的LLaMA的大语言模型,名字叫<a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">羊驼</a>，只有7个billion的参数。属于instruction
tuning的一个标杆。里面用了两个比较新的技术，第一个是上文提到的self-instruct，就是让GPT或者市面上的LLM在我们人工产生的种子instruction上去产生一系列更多的instruction，包括配套每一个instruction的input和output。斯坦福将这部分用GPT-3.5(text-davinci-003)产生的instruction数据慷慨开源，见<a href="https://github.com/tatsu-lab/stanford_alpaca">github</a>。不仅如此斯坦福还给出了产生这些instructions的代码，可谓是非常nice了，方便大家上手学习。</p>
<p>我比较关注用这些instructions数据如何finetune大模型LLaMA的过程，这里权当自己复现以及阅读斯坦福代码时候的记录。首先我本来是想在meta的LLaMA的7B开源模型上做实验，但发现想获取meta的weights需要提前申请，详细可参考huggingface的transformer页面。</p>
<p>斯坦福的代码仓库可以在<a href="https://github.com/tatsu-lab/stanford_alpaca">github</a>找到。</p>
<h1 id="reference">reference</h1>
<ol type="1">
<li><a href="http://arxiv.org/abs/2212.10560">Self-instruct: Aligning
Language Model with Self Generated Instructions</a></li>
<li><a href="http://arxiv.org/abs/2210.11416">Scaling
Instruction-Finetuned Language Models</a></li>
<li><a href="http://arxiv.org/abs/2109.01652">Finetuned Language Models
Are Zero-Shot Learners</a></li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLMs</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA系列模型浅析</title>
    <url>/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/</url>
    <content><![CDATA[<p>这篇博客主要记录博主在探索meta开源的llama大模型，包括它的数据，training代码以及评测方法。之所以想记录下来，主要是因为llama的论文写的极其的细致，完美践行了开源这个词(感谢meta!)，第二个原因是它的文档以及社区都很活跃，使用人群广泛，我们可以借助很多中文社区的复现情况去一探大公司在实现一个大模型时候的考量，以及思考它为什么会这样做。</p>
<p>之前写过一篇关于斯坦福的alpaca的代码的解析，后来看过很多关于微调大模型(supervised
finetuning)的代码仓库，大家的实现思路基本上都可以追溯到alpaca的这份代码。</p>
<p>首先我会将所有我参考的资料罗列在前面，方便大家查找： - <a href="https://github.com/facebookresearch/llama/tree/main">llama代码仓库</a>
这个仓库是介绍如何下载llama模型 - <a href="lll">llama "食谱"</a>
一开始我想在上一个llama仓库中找到相关的train代码，找了半天发现根本没有。后来才发现meta官方将所有finetune(pretrain
from scrach)的代码放在这个仓库，适合developer - <a href="http://arxiv.org/abs/2307.09288">Llama 2: Open Foundation and
Fine-Tuned Chat Models</a> llama2的research paper。强烈建议食用</p>
<p>中文社区的LLama的工作 - <a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2">Chinese LLaMA
Alpaca2</a></p>
<p>这个仓库同样有配套的文章<a href="https://arxiv.org/pdf/2304.08177.pdf">Efficient and Effective Text
Encoding for Chinese LLaMA and Alpaca</a></p>
<p>这个仓库的工作主要是两个：</p>
<ol type="1">
<li>扩充了llama原来的token，也就是中文的那部分</li>
<li>用新的中文数据在llama上进行了continue
pretraining，并且发布了在instruction数据上的微调模型</li>
</ol>
<p>研究思路很简单，在别人模型上继续预训练，并参照alpaca对预训练的模型进行instruction
finetune让其具备follow instructions的能力。我们首先从这个Chinese
LLaMA代码仓库看起。</p>
<h1 id="chinese-llama">Chinese LLaMA</h1>
<p>作者自述做这份工作的原因是原生llama模型的词汇表中仅包含1000+个中文字符，所以首要任务是要扩充llama的词表。他们首先训练了一个中文的tokenizer，然后将其与llama的tokenizer进行融合，融合后的tokenizer拥有49953个token,
那么输入的词汇表数就从32000扩充到了49953。作者的实验还发现用新的融合后的tokenizer去tokenize序列要比旧的tokenizer编码后的序列要短。那很自然的就减少了很多计算量。</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231212161342629.png" alt="image-20231212161342629">
<figcaption aria-hidden="true">image-20231212161342629</figcaption>
</figure>
<p>在准备好tokenizer之后就到了训练环节，作者在这里没有采用全参数微调而是采用了Lora这种高效微调的方式。其实我看到这里是有疑问的，当然作者也在issue中做了回答：</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231212162349967.png" alt="image-20231212162349967">
<figcaption aria-hidden="true">image-20231212162349967</figcaption>
</figure>
<blockquote>
<p>我个人认为continue
pretraining是需要全参数微调的，而且还是在扩充了词表的情况下。</p>
</blockquote>
<p><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/blob/main/scripts/training/run_clm_pt_with_peft.py">预训练脚本</a>,这个脚本是作者在<a href="https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py">transformers库的run_clm.py</a>上修改的，至于中文预训练数据部分，作者采用了20G的纯文本数据，并将他们分成了每个block
512个token。我们来看看代码是怎么写的，源代码在<a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/blob/main/scripts/training/run_clm_pt_with_peft.py">run_clm_pt_with_peft</a>,可以先将<a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/tree/main">Chinese-LLaMA-Alpaca-2</a>拉到本地，在文件姐scripts里可以看到training文件夹里有两个训练代码，一个是pretrain的，一个是sft的。我们先看前面这个pretrain的，它的训练任务很好理解，就是用decoder这种模型架构训练一个输入序列的下一个单词。</p>
<p>作者在这个仓库里没有放训练数据，我们先在该仓库里创建一个<code>./data</code>,里面放一些txt格式的数据用于测试，比如一些小说啥的，训练脚本在处理数据时会自动对他们进行读取并chunk成512长度的序列。作者在paper里提到的他们team训练的tokenizer也一并在scripts的tokenizer文件夹内，要跑通train这个代码需要在run_pt.sh内将这些参数都制定好。</p>
<p>先来看load数据以及处理部分的重点代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">files = [file.name <span class="keyword">for</span> file <span class="keyword">in</span> path.glob(<span class="string">&quot;*.txt&quot;</span>)]</span><br><span class="line"><span class="keyword">for</span> idx, file <span class="keyword">in</span> <span class="built_in">enumerate</span>(files):<span class="comment"># files为./data文件夹内所有的txt</span></span><br><span class="line">    data_file = os.path.join(path, file)</span><br><span class="line">    filename = <span class="string">&#x27;&#x27;</span>.join(file.split(<span class="string">&quot;.&quot;</span>)[:-<span class="number">1</span>])</span><br><span class="line">    cache_path = os.path.join(data_args.data_cache_dir, filename+<span class="string">f&quot;_<span class="subst">&#123;block_size&#125;</span>&quot;</span>)</span><br><span class="line">    os.makedirs(cache_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        processed_dataset = datasets.load_from_disk(cache_path, keep_in_memory=<span class="literal">False</span>) <span class="comment"># 首先使用datasets导入</span></span><br><span class="line">        logger.info(<span class="string">f&#x27;training datasets-<span class="subst">&#123;filename&#125;</span> has been loaded from disk&#x27;</span>)</span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            cache_dir = os.path.join(data_args.data_cache_dir, filename+<span class="string">f&quot;_text_<span class="subst">&#123;block_size&#125;</span>&quot;</span>)</span><br><span class="line">            os.makedirs(cache_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">            raw_dataset = load_dataset(<span class="string">&quot;text&quot;</span>, data_files=data_file, cache_dir=cache_dir, keep_in_memory=<span class="literal">False</span>)</span><br><span class="line">            logger.info(<span class="string">f&quot;<span class="subst">&#123;file&#125;</span> has been loaded&quot;</span>)</span><br><span class="line">            tokenized_dataset = raw_dataset.<span class="built_in">map</span>(</span><br><span class="line">                tokenize_function,</span><br><span class="line">                batched=<span class="literal">True</span>,</span><br><span class="line">                num_proc=data_args.preprocessing_num_workers,</span><br><span class="line">                remove_columns=<span class="string">&quot;text&quot;</span>,</span><br><span class="line">                load_from_cache_file=<span class="literal">True</span>,</span><br><span class="line">                keep_in_memory=<span class="literal">False</span>,</span><br><span class="line">                cache_file_names = &#123;k: os.path.join(cache_dir, <span class="string">&#x27;tokenized.arrow&#x27;</span>) <span class="keyword">for</span> k <span class="keyword">in</span> raw_dataset&#125;,</span><br><span class="line">                desc=<span class="string">&quot;Running tokenizer on dataset&quot;</span>,</span><br><span class="line">            )</span><br><span class="line">            grouped_datasets = tokenized_dataset.<span class="built_in">map</span>(</span><br><span class="line">                group_texts,</span><br><span class="line">                batched=<span class="literal">True</span>,</span><br><span class="line">                num_proc=data_args.preprocessing_num_workers,</span><br><span class="line">                load_from_cache_file=<span class="literal">True</span>,</span><br><span class="line">                keep_in_memory=<span class="literal">False</span>,</span><br><span class="line">                cache_file_names = &#123;k: os.path.join(cache_dir, <span class="string">&#x27;grouped.arrow&#x27;</span>) <span class="keyword">for</span> k <span class="keyword">in</span> tokenized_dataset&#125;,</span><br><span class="line">                desc=<span class="string">f&quot;Grouping texts in chunks of <span class="subst">&#123;block_size&#125;</span>&quot;</span>,</span><br><span class="line">            ) <span class="comment"># </span></span><br><span class="line">            processed_dataset = grouped_datasets</span><br><span class="line">            processed_dataset.save_to_disk(cache_path)</span><br><span class="line">            <span class="keyword">if</span> idx == <span class="number">0</span>: <span class="comment"># </span></span><br><span class="line">                lm_datasets = processed_dataset[<span class="string">&#x27;train&#x27;</span>]</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 如果有多于2个txt,那么将这些数据叠加起来</span></span><br><span class="line">                <span class="keyword">assert</span> lm_datasets.features.<span class="built_in">type</span> == processed_dataset[<span class="string">&quot;train&quot;</span>].features.<span class="built_in">type</span></span><br><span class="line">                lm_datasets = concatenate_datasets([lm_datasets, processed_dataset[<span class="string">&quot;train&quot;</span>]])</span><br></pre></td></tr></table></figure>
<p>内有两个帮助函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_function</span>(<span class="params">examples</span>):</span></span><br><span class="line">        <span class="keyword">with</span> CaptureLogger(tok_logger) <span class="keyword">as</span> cl:</span><br><span class="line">            output = tokenizer(examples[<span class="string">&quot;text&quot;</span>]) <span class="comment"># 仅仅做了tokenize这一个动作,而且会在每一个序列的结尾都加上EOS,由于设置了tokenizer.add_eos_token = True</span></span><br><span class="line">        <span class="comment"># clm input could be much much longer than block_size</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;Token indices sequence length is longer than the&quot;</span> <span class="keyword">in</span> cl.out:</span><br><span class="line">            tok_logger.warning(</span><br><span class="line">                <span class="string">&quot;^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits&quot;</span></span><br><span class="line">                <span class="string">&quot; before being passed to the model.&quot;</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_texts</span>(<span class="params">examples</span>):</span> <span class="comment"># 在这个函数里程序将tokenize之后的input_ids和attention_mask进行chunk，保证每个chunk大小都是block_size的</span></span><br><span class="line">    <span class="comment"># Concatenate all texts.</span></span><br><span class="line">    concatenated_examples = &#123;k: <span class="built_in">list</span>(chain(*examples[k])) <span class="keyword">for</span> k <span class="keyword">in</span> examples.keys()&#125;</span><br><span class="line">    total_length = <span class="built_in">len</span>(concatenated_examples[<span class="built_in">list</span>(examples.keys())[<span class="number">0</span>]])</span><br><span class="line">    <span class="comment"># We drop the small remainder, we could add padding if the model supported it instead of this drop, you can</span></span><br><span class="line">    <span class="comment"># customize this part to your needs.</span></span><br><span class="line">    <span class="keyword">if</span> total_length &gt;= block_size:</span><br><span class="line">        total_length = (total_length // block_size) * block_size</span><br><span class="line">        <span class="comment"># Split by chunks of max_len.</span></span><br><span class="line">        result = &#123;</span><br><span class="line">            k: [t[i : i + block_size] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, total_length, block_size)]</span><br><span class="line">            <span class="keyword">for</span> k, t <span class="keyword">in</span> concatenated_examples.items()</span><br><span class="line">        &#125;</span><br><span class="line">        result[<span class="string">&quot;labels&quot;</span>] = result[<span class="string">&quot;input_ids&quot;</span>].copy() <span class="comment"># 这里labels设置成和input_ids一模一样</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<p>可以看到基本采用transformer的库来实现的数据的导入以及process，总体来说使用datasets还是比较方便的。</p>
<p>再来看如何做的lora train：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, TaskType, get_peft_model, PeftModel, get_peft_model_state_dict</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> training_args.full_finetuning: <span class="comment"># 默认模型是全参数微调</span></span><br><span class="line">        <span class="keyword">if</span> training_args.peft_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            logger.info(<span class="string">&quot;Peft from pre-trained model&quot;</span>)</span><br><span class="line">            model = PeftModel.from_pretrained(model, training_args.peft_path, device_map=device_map)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            logger.info(<span class="string">&quot;Init new peft model&quot;</span>)</span><br><span class="line">            target_modules = training_args.trainable.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">            modules_to_save = training_args.modules_to_save</span><br><span class="line">            <span class="keyword">if</span> modules_to_save <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                modules_to_save = modules_to_save.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">            lora_rank = training_args.lora_rank</span><br><span class="line">            lora_dropout = training_args.lora_dropout</span><br><span class="line">            lora_alpha = training_args.lora_alpha</span><br><span class="line">            logger.info(<span class="string">f&quot;target_modules: <span class="subst">&#123;target_modules&#125;</span>&quot;</span>)</span><br><span class="line">            logger.info(<span class="string">f&quot;lora_rank: <span class="subst">&#123;lora_rank&#125;</span>&quot;</span>)</span><br><span class="line">            peft_config = LoraConfig(</span><br><span class="line">                task_type=TaskType.CAUSAL_LM,</span><br><span class="line">                target_modules=target_modules,</span><br><span class="line">                inference_mode=<span class="literal">False</span>,</span><br><span class="line">                r=lora_rank, lora_alpha=lora_alpha,</span><br><span class="line">                lora_dropout=lora_dropout,</span><br><span class="line">                modules_to_save=modules_to_save) <span class="comment"># LoraConfig是PEFT这个包内的</span></span><br><span class="line">            model = get_peft_model(model, peft_config)</span><br><span class="line">        model.print_trainable_parameters()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>该仓库的instruction
finetune的代码和alpaca的思路一样，很多写法都一模一样。不过因为作者在做pretrain的时候用的是lora的形式，所以在sft的时候也需要在这个基础模型上进行微调。作者在<code>run_clm_sft_with_peft.py</code>中是类似于pt脚本中的写法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> training_args.peft_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    logger.info(<span class="string">&quot;Peft from pre-trained model&quot;</span>)</span><br><span class="line">    model = PeftModel.from_pretrained(model, training_args.peft_path, device_map=device_map)</span><br></pre></td></tr></table></figure>
<p>这里的peft_path是需要在train的时候传入参数的，也就是我们在pretrain时候通过call_back函数保存的lora参数,
模型组装好之后训练。博主认为这时候是所有参数一起调整了，包含lora部分以及llama2基础模型部分。</p>
<blockquote>
<p>一点题外话：在阅读Chinese
LLaMA这份代码的时候发现了其中一个作者崔一鸣的博客，内有一个关于大模型的纵览介绍挺适合初学者熟悉大模型的相关技术，也适合面试的盆友回顾以及对自己还没掌握透的知识进行查漏补缺的。<a href="https://ymcui.com/talk/20230826_baai_llm_tutorial.pdf"><strong>[Methods
and Practices for Large Pre-trained Language
Models](https://ymcui.com/talk/20230826_baai_llm_tutorial.pdf)</strong></a></p>
<p>建议配合<a href="https://karpathy.ai/stateofgpt.pdf">stateofgpt</a>食用</p>
</blockquote>
<h1 id="llama">LLaMA</h1>
<h2 id="拓展补充介绍">拓展补充介绍</h2>
<p>LLaMA的1和2版本在模型架构上大多数相似，其中三个关键技术使羊驼模型区别于其他模型，这里摘一下llama2
research paper中的描述：</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231213162604171.png" alt="image-20231213162604171">
<figcaption aria-hidden="true">image-20231213162604171</figcaption>
</figure>
<h3 id="rmsnorm">RMSNorm</h3>
<p>在介绍RMSNorm之前补充一下Batch Normalization以及Layer
Normalization</p>
<p>参考：</p>
<ul>
<li><a href="https://medium.com/@florian_algo/batchnorm-and-layernorm-2637f46a998b">BatchNorm
and LayerNorm</a></li>
<li><a href="https://tungmphung.com/deep-learning-normalization-methods/">Deep
Learning normalization methods</a></li>
<li><a href="https://ai.stackexchange.com/questions/27309/what-are-the-consequences-of-layer-norm-vs-batch-norm">[What
are the consequences of layer norm vs batch
norm?](https://ai.stackexchange.com/questions/27309/what-are-the-consequences-of-layer-norm-vs-batch-norm)</a></li>
<li><a href="https://stackoverflow.com/questions/70065235/understanding-torch-nn-layernorm-in-nlp">[Understanding
torch.nn.LayerNorm in
nlp](https://stackoverflow.com/questions/70065235/understanding-torch-nn-layernorm-in-nlp)</a></li>
</ul>
<p><img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/0_K45DoPRbhC5-dqq1-17025178394161.webp"></p>
<p>上面图片中，每一行属于一个batch的数据，不用管这个batch内的数据是2维的还是1维的。</p>
<h4 id="batch-normalization">Batch Normalization</h4>
<blockquote>
<p>for each dimension of the input, all data points in the batch are
gathered and normalized with the same mean and standard deviation</p>
<p>BN的所有计算都在一个batch以内，也就是我们用到的数据只是这个batch内的数据，不会涉及到其他batch的数据</p>
</blockquote>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214093939577.png" alt="image-20231214093939577">
<figcaption aria-hidden="true">image-20231214093939577</figcaption>
</figure>
<p>上面的伪代码中的<code>x</code>可以是一个向量，如果是向量的情况下涉及到的x的相加都是向量的运算。值得注意的是在卷积层里，<code>dimension</code>指的是channel维度的</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214094523587.png" alt="image-20231214094523587">
<figcaption aria-hidden="true">image-20231214094523587</figcaption>
</figure>
<p>也就是不同channel计算出的μ和σ是不同的。</p>
<blockquote>
<p>the input data is normalized separately for each channel in a
convolutional layer.</p>
</blockquote>
<p>而在全连接层，<code>dimension</code>就是指feature维度。</p>
<h4 id="layer-normalization">Layer Normalization</h4>
<blockquote>
<p>with LayerNorm, we normalize each data point separately. Moreover,
each data point’s mean and variance are shared over all hidden units
(i.e. neurons) of the layer</p>
</blockquote>
<p>跟batch没关系，在layer层面去计算均值和方差。比如在全连接层，输入是125个神经元的话，就对这些神经元进行归一化。也就是数据中的每一个data
points都是独立进行归一化的，和其他data
points无关。那么对于卷积层来说的话就有两种计算方式：<a href="https://blog.csdn.net/weixin_41978699/article/details/122778085">参考</a></p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214100917596.png" alt="image-20231214100917596">
<figcaption aria-hidden="true">image-20231214100917596</figcaption>
</figure>
<p>看pytorch的<a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">doc</a>多采取前一种全部一股脑求平均和方差的方式。</p>
<p>RMSNorm</p>
<p>RMSNorm的research paper写着一部分写的特别清楚，推荐查看原文<a href="https://proceedings.neurips.cc/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf">Root
Mean Square Layer Normalization</a></p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214103540564.png" alt="image-20231214103540564">
<figcaption aria-hidden="true">image-20231214103540564</figcaption>
</figure>
<p>RMSNorm去除了LN的求平均数的过程，并且将LN中的除以方差变成了除以<code>root mean square</code>。来看llama中的代码实现：<a href="https://github.com/facebookresearch/llama/blob/main/llama/model.py#L63">llama/llama/model.py</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_norm</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply the RMSNorm normalization to the input tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x (torch.Tensor): The input tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            torch.Tensor: The normalized tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x * torch.rsqrt(x.<span class="built_in">pow</span>(<span class="number">2</span>).mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) + self.eps) <span class="comment"># eps防止除以0</span></span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="swiglu">SwiGLU</h3>
<p>阅读知乎这篇博客<a href="https://zhuanlan.zhihu.com/p/650237644">大模型基础｜激活函数｜从ReLU
到SwiGLU</a></p>
<h3 id="rotary-embedding-rope">Rotary Embedding, RoPE</h3>
<h4 id="attention-is-all-you-need中的position-embedding">Attention is
All you need中的position embedding</h4>
<p>首先回顾下在Attention is all you
need原文paper中对于位置编码的公式：</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231215135557606.png" alt="image-20231215135557606">
<figcaption aria-hidden="true">image-20231215135557606</figcaption>
</figure>
<p>我一开始理解这两个公式的时候很困难，后来查了一些资料，发现很多人也在这里由一些困惑，包括tensorflow官方的实现方式<a href="https://www.tensorflow.org/tutorials/text/transformer?hl=zh-cn#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%88positional_encoding%EF%BC%89">位置编码</a>，tensorflow的官方给出的代码是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_angles</span>(<span class="params">pos, i, d_model</span>):</span></span><br><span class="line">  angle_rates = <span class="number">1</span> / np.power(<span class="number">10000</span>, (<span class="number">2</span> * (i//<span class="number">2</span>)) / np.float32(d_model))</span><br><span class="line">  <span class="keyword">return</span> pos * angle_rates</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span>(<span class="params">position, d_model</span>):</span></span><br><span class="line">  angle_rads = get_angles(np.arange(position)[:, np.newaxis],</span><br><span class="line">                          np.arange(d_model)[np.newaxis, :],</span><br><span class="line">                          d_model)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 将 sin 应用于数组中的偶数索引（indices）；2i</span></span><br><span class="line">  angle_rads[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(angle_rads[:, <span class="number">0</span>::<span class="number">2</span>]) <span class="comment"># 不懂双冒号切片的可参考： https://stackoverflow.com/questions/3453085/what-is-double-colon-in-python-when-subscripting-sequences</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 将 cos 应用于数组中的奇数索引；2i+1</span></span><br><span class="line">  angle_rads[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(angle_rads[:, <span class="number">1</span>::<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">  pos_encoding = angle_rads[np.newaxis, ...]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> tf.cast(pos_encoding, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<p>在<code>get_angles</code>方法里10000的指数系数中tensorflow的实现多加了一个<code>i//2</code>。这里我非常困惑，后来发现stackflow上也有同样的发问：</p>
<ul>
<li>[<a href="https://datascience.stackexchange.com/questions/79995/explanation-about-i-2-in-positional-encoding-in-tensorflow-tutorial-about-trans/126053#126053">Explanation
about i//2 in positional encoding in tensorflow tutorial about
transformers</a></li>
<li>[<a href="https://stackoverflow.com/questions/69036265/why-does-the-i-need-to-be-divided-by-2-in-caculating-positional-encoding">Why
does the 'i' need to be divided by 2 in caculating positional
encoding?</a></li>
</ul>
<p>推荐阅读一下<a href="https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/">A
Gentle Introduction to Positional Encoding in Transformer Models, Part
1</a>。该作者的实现方式更符合人类的理解方式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPositionEncoding</span>(<span class="params">seq_len, d, n=<span class="number">10000</span></span>):</span></span><br><span class="line">    P = np.zeros((seq_len, d))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(seq_len):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="built_in">int</span>(d/<span class="number">2</span>)): <span class="comment"># 这里只循环d//2次</span></span><br><span class="line">            denominator = np.power(n, <span class="number">2</span>*i/d)</span><br><span class="line">            P[k, <span class="number">2</span>*i] = np.sin(k/denominator)</span><br><span class="line">            P[k, <span class="number">2</span>*i+<span class="number">1</span>] = np.cos(k/denominator)</span><br><span class="line">    <span class="keyword">return</span> P</span><br><span class="line"> </span><br><span class="line">P = getPositionEncoding(seq_len=<span class="number">4</span>, d=<span class="number">4</span>, n=<span class="number">100</span>)</span><br><span class="line"><span class="built_in">print</span>(P)</span><br></pre></td></tr></table></figure>
<p>那么该怎么理解paper中的公式以及tensorflow//2的这个实现呢。就拿某一个sequence中的token来举例子，如果我们想要编码的向量长度是20,也就是d=20。那么tensorflow的做法是首先创建一个长度为20的向量，然后依次求其中的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">该token的position encoding所有应该求值得index</span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>]</span><br><span class="line">angle_rates = <span class="number">1</span> / np.power(<span class="number">10000</span>, (<span class="number">2</span> * (i//<span class="number">2</span>)) / np.float32(d_model)) 这句话</span><br><span class="line"><span class="number">10000</span>的指数(<span class="number">2</span> * (i//<span class="number">2</span>))是</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">18</span>, <span class="number">18</span>]</span><br></pre></td></tr></table></figure>
<p>所以对照paper中的公式表达的意思就是：</p>
<p>在向量的偶数index位置，比如0，2...等，公式里的2i就等于它的index</p>
<p>在向量的奇数index位置，比如1,3...等，公式里的10000的指数也就是2i的位置应该取这个奇数的前一个偶数值。</p>
<p>那么我们来看看tensorflow的这份代码就对上了：</p>
<p>10000的指数部分出现的值为：
<code>[0, 0, 2, 2, 4, 4, 6, 6, 8, 8, 10, 10, 12, 12, 14, 14, 16, 16, 18, 18]</code></p>
<p>所以paper里的这个公式要将2i当作一个整体来看。</p>
<h4 id="roperotary-position-embedding"><a href="https://arxiv.org/pdf/2104.09864v5.pdf">RoPE(rotary Position
Embedding)</a></h4>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231215152735803.png" alt="image-20231215152735803">
<figcaption aria-hidden="true">image-20231215152735803</figcaption>
</figure>
<p>RoPE进一步改进了绝对位置编码，是一种在transformer
attention中的Q和K上添加相对位置信息的方法</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231215152803294.png" alt="image-20231215152803294">
<figcaption aria-hidden="true">image-20231215152803294</figcaption>
</figure>
<p>首先作者将隐藏层的向量每两个维度编成一组，看成2维的向量；然后对于特定位置m的x1,x2，将他们旋转mθ角度，用新的x1,x2值替换老的值加入到query和key中。</p>
<h3 id="grouped-query-attention-gqa">Grouped-Query Attention (GQA)</h3>
<p>GQA是llama2相较于llama1新采用的技术，它是一种提升推理速度的方法，主要针对多头注意力机制进行改进，与KV
Cache搭配使用</p>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>QA(question answering)</title>
    <url>/2023/05/09/QA-question-answering/</url>
    <content><![CDATA[<h1 id="qa是什么">QA是什么？</h1>
<p>直观上理解，QA就是用户给出一个question，系统（模型）给出一个answer。它和reading
comprehension还是不一样的，在<a href="Latent%20Retrieval%20for%20Weakly%20Supervised%20Open%20Domain%20Question%20Answering">paper</a>中对于这两个问题做了归纳：</p>
<figure>
<img src="/2023/05/09/QA-question-answering/image-20230508115516457.png" alt="Comparison of assumptions">
<figcaption aria-hidden="true">Comparison of assumptions</figcaption>
</figure>
<p>reading
comprehension问题，我们的任务是从一个paragraph中找出answer：comprehend a
passage of text and answer questions about its content (P,Q)-&gt;
A，也就是我们事先不仅有question，还有一段paragraph，我们只需要找出answer在这个paragrapg中的位置（span），但是对于open-domain的QA来说，我们事先是不知道anwer在哪儿的，比如上图中unsupervised
QA。那么对于这一类open-domaiin的问题如何解决？最常见的就是retriver-reader架构，就是我们先从一大堆语料库中挑选出我们的paragraph，然后对于这一些paragraph，我们用reading
comprehension的技术再来定位answer在哪里。上面这篇paper就是应用的这种思想，提出了ORQA，这是19年的文章，想法很典型。</p>
<figure>
<img src="/2023/05/09/QA-question-answering/image-20230508120439778.png" alt="ORQA">
<figcaption aria-hidden="true">ORQA</figcaption>
</figure>
<p>上文提到的斯坦福的数据集SQuAD是使用的最广泛的reading
comprehension数据集，它的组成形式是（passage,
question,answer），每一个answer都是passgae里面的一个segment。</p>
<figure>
<img src="/2023/05/09/QA-question-answering/image-20230508121850697.png" alt="SQuAD example">
<figcaption aria-hidden="true">SQuAD example</figcaption>
</figure>
<p>这就有它的局限性，因为有一些问题的回答是不会在passage中找到的，所以它不能用于open-domain的task，它完全是一个监督性任务的数据集，目前在这个数据集上最好的模型的表现已经超越了人类，可以说是"almost
solved"。</p>
<h1 id="reading-comprehension">Reading Comprehension</h1>
<p>首先我们可以先从比较简单的问题开始解决，reading
comprehension可以说是QA的一个子问题。第一步搞明白问题定义：</p>
<figure>
<img src="/2023/05/09/QA-question-answering/image-20230508122338691.png" alt="problem formulation">
<figcaption aria-hidden="true">problem formulation</figcaption>
</figure>
<p><strong>2016年-2018年，主要用于解决RC的方法是LSTM+attention的模型架构，这些模型主要有15年的attentive
reader、16，17年的stanford attentive
reader、17年的match-LSTM、17年的BiDAF、17年的dynamic coattention
network、17年的DrQA、17年的R-Net、17年的ReadoNet。2019年开始，主要是18年Bert出来之后，大家普遍开始采用finetune
BERT来解决这个问题，值得一提的是在BERT的原文paper中下游任务也使用了SQuAD来做了实验，作者用于预测start和end的方式也设计的很精巧。</strong></p>
<h2 id="stanford-attentive-reader">stanford attentive reader</h2>
<p>这篇文章在logistic
regression的基础上（SQuAD数据集）有了巨大的进步，算是18年之前lstm+attention架构的集大成者。思想很简单，但里面很多细节，比如在编码passage的时候用了好多来源的vector进行拼接，然后再输入斤lstm中。但整体架构和机器翻译领域的发展历程是一样的，在transformer没有出现之前，大家都在lstm上做了很多创新，attention加入计算是其中一种，DrQA将多个来源的向量拼接也是一种，感兴趣的可以读一下原文：Reading
Wikipedia to Answer Open-Domain
Questions。作者是cs224n的QA这门课的讲师，也是一位华人小姐姐。其实我在看cs224n
QA这一讲的ppt时感觉斯坦福的这门课着重讲解了stanford attentive
reader这一系列的模型，不知道有没有私心。</p>
<figure>
<img src="/2023/05/09/QA-question-answering/image-20230508150255263.png" alt="DrQA">
<figcaption aria-hidden="true">DrQA</figcaption>
</figure>
<p>这之后还出现了bidirectional
attention的，例如BiDAF，就是不仅计算了query2Context的attention
weights，也计算了Context2Query的attention
weights，将这两者和context原来的lstm值再输入进一个双层LSTM进行start，end的预测（很复杂就是了），虽然也不知道为啥这么做就会有一个不错的performance，模型架构越来越复杂，卷死了。</p>
<p>但不管怎么说，在transformer被大家普遍使用之前，rnn+attention的这种模式是最好的解决办法了，俗称SOAT。</p>
<h2 id="bert用作reading-comprehension">Bert用作reading
comprehension</h2>
<p>自从18年开始，在reading
comprehension这个任务上，谷歌出了bert，成就了新的历史，一下子F1从79.4（DrQA）升级到了91.8，人类在SQuAD上的表现才91.2，所以算是超越人类表现了。</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language
Understanding
这篇文章的4.2节详细介绍了预训练完成之后的bert如何在SQuAD这个下游数据集上进行finetune的。</p>
<figure>
<img src="/2023/05/09/QA-question-answering/image-20230508170709211.png" alt="Bert QA">
<figcaption aria-hidden="true">Bert QA</figcaption>
</figure>
<p>上面这幅图更好的解释了Bert在finetune的时候如何做RC这个任务的。首先我们将question和paragraph拼接在一起，中间使用[SEP]连接。然后将这一串字符串整个输入进预训练好的Bert。对应的bert输出了同样长度的一连串向量，这时我们只取paragrapgh那一部分对应的向量们。第二步加了两个独特的向量，这两个向量是下游任务新增的，需要通过finetune时更新参数，分别是start
S和end
E向量。为了得到paragrapgh中每一个token它时span-start的概率，采用如下公式：</p>
<figure>
<img src="/2023/05/09/QA-question-answering/image-20230508171252656.png" alt="image-20230508171252656">
<figcaption aria-hidden="true">image-20230508171252656</figcaption>
</figure>
<p>同样对于每一个token成为泡影span-end的概率，也对E采用同样的计算方式。到这一步我们就得到了paragrapgh中每一个token它们成为span-start和span-end的概率。第三步如何通过这些概率值得出我们的answer所在的span？</p>
<p>作者的做法是对每一个candidate span都计算一个分数： <span class="math display">\[
S*T_i+E*T_j
\]</span>
选取得分最大的那个span作为预测值。计算loss时有一点不太一样，刚刚说的是predict，loss的计算是用的负log
<span class="math display">\[
L = - logP_(start^s) - logP_(end^e)
\]</span>
这里可能会有点迷惑。其实训练数据拿来时，对于paragraph中每一个token我们都会有一个label表示它是否是start还是end，是就是1，不是就是0.那么我们在做预测的时候，上面的第三步骤已经得到了每一个token的成为start还是end的概率，那么这时候我们就能用交叉墒来计算loss了。注意这里我们用的是softmax，所以我们应该是计算softmax的loss。另外这里只会计算到真正的start和end的那两个token上的loss，因为其他token的groudtruth
label都是0。</p>
<h2 id="spanbert">SpanBERT</h2>
<p>这篇是在Bert基础上改进的，主要改进点在于improve了Bert在pretrain时候的两个task：1.
mask 2. next sentence prediction。同样是chen的文章：(Joshi &amp; Chen et
al., 2020): SpanBERT: Improving Pre-training by Representing and
Predicting Spans</p>
<blockquote>
<p>To finish</p>
</blockquote>
<p>SpanBert在谷歌的bert基础上又将performance提高了许多。</p>
<h1 id="open-domain-question-answering">open-domain question
answering</h1>
<p>这个问题在Speech and language
processing的第三版的14章节进行了详细介绍，它也可以叫做是<em>information-retrieval(IR)
based
QA</em>。它首先要做的是从很大的语料库中搜索出相关的passage，然后第二步运动reading
comprehension的算法从这些passage中找出answer（spans of
text）。从一堆语料库里找到相关的passage，这个过程称之为information
retrival。</p>
<p>IR能想到的最最简单的做法就是将query和语料库中的每一个passage都编码成一个向量，然后计算这些向量之间的相似度，也就是score，分数越高的就是跟query越相似，那么就可以得出语料库中和query最相关的那些passage了。那么将query和passage们编码成向量有很多种方法，最简单的方法就是TFIDF，还有tfidf的变种BM25。这些方法现在已经不再采用，更多的是用Bert，俗称dense
vectors，这是和tfidf这种稀疏向量相对应的叫法。</p>
<p>具体做法是：</p>
<figure>
<img src="/2023/05/09/QA-question-answering/image-20230509133739252.png" alt="ORQA">
<figcaption aria-hidden="true">ORQA</figcaption>
</figure>
<p>分别用两个不同的bert分别编码query和passage们，然后将query的向量[CLS]和passage的向量[CLS]点积，这个点积的结果就作为query和passage的相似度得分。上面这张图片是将retriver和reader一起训练的，当然也可以单独用query和answer训练retriver</p>
<h1 id="用生成模型来做qa">用生成模型来做QA</h1>
<p>由于LLM的兴起，大家开始发现用生成模型来做QA更能回答复杂的问题。比如现在的GPT系列模型。它完全摈弃了抽取信息和从passage中寻找answer所在位置的环节，有点黑科技，就好像模型将所有的知识都记到脑子里去了。但也带来了新的问题，用户没有办法知道模型是从哪里找到的答案。</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Python3.7: error while loading shared libraries: libpython3.7m.so.1.0</title>
    <url>/2021/12/29/Python3-7-error-while-loading-shared-libraries-libpython3-7m-so-1-0/</url>
    <content><![CDATA[<p>在ubuntu上安装了其他版本的python之后遇到如下报错：</p>
<p>参考[https://stackoverflow.com/questions/58649177/python3-7-error-while-loading-shared-libraries-libpython3-7m-so-1-0]</p>
<p>对于我的这种情况，先查看<code>libpython3.7m.so.1.0</code>这个文件在哪里？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">locate libpython3.7m.so.1.0</span><br></pre></td></tr></table></figure>
<p>上面的命令如果返回无，要先更新下系统内文件系统的index字典</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">updatedb</span><br></pre></td></tr></table></figure>
<p>找出libpython3.7m.so.1.0在哪一个lib文件夹内，然后将该lib路径加入搜索路径，可以通过：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=/lib:/usr/lib:/usr/local/lib</span><br></pre></td></tr></table></figure>
<p>以上方式只对该session起作用，如果reboot了系统就会失效，想要永久的方式:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ldconfig /usr/local/lib </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2023/09/19/RLHF%E7%BB%86%E8%8A%82%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>写这篇博客的初衷是自己一直以来都在关注supervised
finetuning，但对强化学习这一块一直都没有过多的涉猎，一方面是因为它是大模型技术模块里相对成本比较高的过程，还有一方面是我对强化学习没有系统性的学习，觉得有一丢丢的难理解，躺在list里的斯坦福的强化学习课程也一直搁浅，传送门：
<a href="https://web.stanford.edu/class/cs234/">cs234n</a>,
课程视频和PPT都是可以免费下载的。</p>
<p>我这篇博客主要是受llama2模型的<a href="http://arxiv.org/abs/2307.09288">paper</a>的启发，觉得这篇文章在RLHF方面写的非常之细致，并且代码也进行了开源，可以对照代码进行学习.
移步这篇文章的3.2节。当然也有很多博客详细介绍了这篇文章的强化学习部分的细节，参考<a href="https://zhuanlan.zhihu.com/p/644697081">【LLM】Meta LLaMA
2中RLHF技术细节</a></p>
<p>首先RLHF包含了两个步骤，第一个就是训练一个reward
modeling来对LM生成的回答进行打分，这个分数是一个数值型的数据；第二部分就是用这个RM去调整我们的LM，使得LM能output更符合人类期望的回答。也有作者将SFT放到了RLHF的第一阶段，比如<a href="http://arxiv.org/abs/2303.18223">A Survey of Large Language
Models</a> 的5.2.3节将RLHF分为了三阶段：</p>
<figure>
<img src="/2023/09/19/RLHF%E7%BB%86%E8%8A%82%E8%A7%A3%E6%9E%90/image-20231011143645416.png" alt="image-20231011143645416">
<figcaption aria-hidden="true">image-20231011143645416</figcaption>
</figure>
<p>不过我认为SFT还是隔离开讲比较好。</p>
<h1 id="reward-modeling">Reward Modeling</h1>
<h2 id="数据">数据</h2>
<p>prompt好准备，那么打分这个就要靠人来打分了，人打分有一定的主观臆测性，所以就换成了比较哪一种回答比较好，像LLAMA2的做法就是分了四个等级：significantly
better, better, slightly better or negligibly better / unsure。</p>
<h2 id="rm模型">RM模型</h2>
<p>hugging face <a href="https://huggingface.co/blog/zh/rlhf">blog</a>
中有一段话：</p>
<blockquote>
<p>这个过程中一个有趣的产物是目前成功的 RLHF 系统使用了和生成模型具有
不同 大小的 LM (例如 OpenAI 使用了 175B 的 LM 和 6B 的 RM，Anthropic
使用的 LM 和 RM 从 10B 到 52B 大小不等，DeepMind 使用了 70B 的
Chinchilla 模型分别作为 LM 和 RM)
。一种直觉是，偏好模型和生成模型需要具有类似的能力来理解提供给它们的文本</p>
</blockquote>
<h2 id="rm的训练">RM的训练</h2>
<p>直观上理解我们现在有了prompts，也有了这些prompts的generation在我们的LM上的的generation的评分ranking，那么怎么来用这些数据训练呢？</p>
<p>就拿LLAMA2的做法来说，它用了一个和预训练模型一模一样的模型作为RM的初始模型，唯一不同的是将LM中用作预测下一个token的分类头替换成了另一个可以输出分值的回归头就像下面这样：</p>
<p><img src="/2023/09/19/RLHF%E7%BB%86%E8%8A%82%E8%A7%A3%E6%9E%90/image-20230918142104723.png"></p>
<p>上图出自state of GPT。</p>
<p>loss的计算采用的是ouyang 2022的<a href="http://arxiv.org/abs/2203.02155">Training language models to
follow instructions with human feedback</a> 提出的计算方式：</p>
<figure>
<img src="/2023/09/19/RLHF%E7%BB%86%E8%8A%82%E8%A7%A3%E6%9E%90/image-20231011143044591.png" alt="image-20231011143044591">
<figcaption aria-hidden="true">image-20231011143044591</figcaption>
</figure>
<p>其中r是RM输出的标量值代表分值。不过llama2的做法在这个loss基础上加了一个margin，刚刚提到它在人工标注这些generation的时候分了四个档次，有的回答会比另一个对手super
better，有的只是稍微好一点，所以这种“好的程度”可以在loss中区分出来，所以作者在loss的的计算里加了一个margin：</p>
<figure>
<img src="/2023/09/19/RLHF%E7%BB%86%E8%8A%82%E8%A7%A3%E6%9E%90/image-20231011143054960.png" alt="image-20231011143054960">
<figcaption aria-hidden="true">image-20231011143054960</figcaption>
</figure>
<p>super better的就用一个比较大的m值。</p>
<h1 id="rl-fine-tuning">RL Fine-tuning</h1>
<h1 id="推荐阅读">推荐阅读</h1>
<ul>
<li>https://github.com/opendilab/awesome-RLHF</li>
<li><a href="https://zhuanlan.zhihu.com/p/644697081">LLM Meta LLaMA
2中RLHF技术细节</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>R-CNN vs SPP vs Fast R-CNN vs Faster R-CNN</title>
    <url>/2022/11/02/R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/</url>
    <content><![CDATA[<p>最近在object
detection任务上读这几篇文章，见识到神仙打架。一开始我只是关注image
segmentation的任务，其中instance
segmentation任务中Mask-RCNN是其中比较火的一个model，所以就把跟这个模型相关的几个模型都找出来看了看。这里想记录下这几天看这几篇论文的心得体会，如果有写的不正确的地方，欢迎批评指正。</p>
<p>其实去仔细看这几篇论文很有意思，梳理一下时间线就是：</p>
<ol type="1">
<li><p><strong>2014</strong>年Girshick提出了RCNN，用于解决accurate
object detection 和 semantic
segmentation。该模型有一个drawbacks是每次一张图片输入进来，需要产生~2000个region
proposals，这些region的大小都是不一致的，但我们对图片进行分类的下游网络都是需要fixed
size的图片，那怎么办呢？作者提出使用wraped方法，具体可以参考作者的论文。总之最终我们输入到SVM也就是分类器的region图片大小都是一致的。</p></li>
<li><p>为了解决每次输入网络的图片大小怎么样才能变成fixed
size的vector，<strong>2015</strong>年he kaiming提出了SPP（spatial
pyramid pooling），跟前者RCNN不一样的地方在于：1) 将region
proposal的方法用在了图片输入cnn网络得到的feature map上，2)从feature
map选择出来的region
proposal不还是不一样大小么？作者没有使用wrap的方法，而是提出了一个SPP
layer,这个layer可以接受任何大小的图片，最终都会转化成一个fixed
size的向量，这样就可以轻松输入进SVM或者Dense layer进行分类了。</p></li>
<li><p>收到SPP的启发，Girshick在<strong>2015</strong>年提出了Fast-RCNN，将SPP
layer重新替换成ROI Pooling，经过ROI pooling，输出的并不是SPP
layer输出的金字塔式的向量了，而是只有一个。 <a href="https://analyticsindiamag.com/r-cnn-vs-fast-r-cnn-vs-faster-r-cnn-a-comparative-guide/">参考博客</a></p></li>
<li><p>经过前一轮的battle，虽然各自的模型都提出了自己的独特方法，但是无论是SPP
Net还是Fast-RCNN都没有提出在选择ROI(region of
interest)的方法。<strong>2016</strong>年He
Kaiming再次强势入场，提出了产生region
proposal的方法，它使用了一个单独的CNN网络来获取region
proposal.得到了这些proposal之后再将他们传递给Roi Pooling
layer，后面的过程和fast RCNN一致。 这篇Faster-RCNN的方法作者中有he
kaiming和Girshick，这里致敬下sun jian，感谢为computer
vision领域贡献的灵感和创造。</p></li>
</ol>
<h1 id="mask-rcnn">Mask-RCNN</h1>
<p>Mask-RCNN是Region-based CNN系列中的一个算法，用于解决instance
segmentation的问题，instance segmentation的难点在于我们不仅要做object
detection，而且需要将object的准确轮廓给识别出来，同时做出分类这是什么object。在<a href="https://arxiv.org/abs/1703.06870">Mask-RCNN</a></p>
<p>中，related
work一章节对RCNN这一系列的模型做了准确概括，建议大家读原文：</p>
<blockquote>
<p>The Region-based CNN (R-CNN) approach [13] to bounding-box object
detection is to attend to a manageable number of candidate object
regions [42, 20] and evaluate convolutional networks [25, 24]
independently on each RoI. R-CNN was extended [18, 12] to allow
attending to RoIs on feature maps using RoIPool, leading to fast speed
and better accuracy. Faster R-CNN [36] advanced this stream by learning
the attention mechanism with a Region Proposal Network (RPN). Faster
R-CNN is flexible and robust to many follow-up improvements (e.g., [38,
27, 21]), and is the current leading framework in several
benchmarks.</p>
</blockquote>
<p>在Mask-RCNN的文章中提出了一种新的ROIAlign
Layer，主要是为了解决Faster-Rcnn的网络中ROI pooling
layer的问题。在此补充下ROI pooling是怎么将不同size的ROI（region of
interest）都变成fixed-size的feature map的：</p>
<figure>
<img src="/2022/11/02/R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/image-20221128165901678-16696259446601.png" alt="ROI Pooling Layer">
<figcaption aria-hidden="true">ROI Pooling Layer</figcaption>
</figure>
<p>上图是将5*4大小的ROI变成了2✖2大小的feature
map。这种方式带来的影响就是可能在提取的extracted
features和ROI之间造成misalignments。但是这种misalignments并不会在faster-rcnn中对分类造成很大的影响，但如果要用这个ROI做segmentation的话就可能会造成巨大的影响，因此作者提出了一个ROIAlign
Layer。</p>
<blockquote>
<p>如果有小伙伴想对照code看这篇paper，可以参考<a href="https://github.com/matterport/Mask_RCNN/blob/3deaec5d902d16e1daf56b62d5971d428dc920bc/mrcnn/model.py#L486">tensorflow实现</a>。如果你对pytorch更熟悉可以参考paper中给出的官方github地址的实现。有一篇博客详细介绍了tensorlfow的实现，参见<a href="https://blog.paperspace.com/mask-r-cnn-in-tensorflow-2-0/">blog</a></p>
</blockquote>
<p>在MaskRCNN中使用了faster-rcnn中提出的RPN来产生ROI，然后才使用上面提到的ROI
Algin。RPN的具体细节参见fatser-rcnn原文和本博客的<a href="#RPN(Region%20Proposal%20Network)">第二章节</a></p>
<h1 id="rpnregion-proposal-network">RPN(Region Proposal Network)</h1>
<p>RPN
是在faster-rcnn中提出来的网络，主要是为了解决在rcnn和fast-rcnn两个前置模型中产生ROI的耗时问题，之前产生ROI主要是依靠Selected
Search。在读mask-rcnn的paper时发现这个网络的细节不甚了解，这里补充记录一下。感兴趣的朋友可以阅读<a href="http://arxiv.org/abs/1506.01497">paper</a>的第3.2节。</p>
<p>RPN在output长方体的ROI的同时，也会给每一个ROI产生一个Objectness
score。看原文的时候作者是以sliding
window的方式来讲解的，一开始看的有点懵。但其实就是卷积层的计算过程的拆解，我们先按照作者的思路来看RPN做了什么。</p>
<figure>
<img src="/2022/11/02/R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/image-20221129160354668-16697090363711.png" alt="faster-Rcnn">
<figcaption aria-hidden="true">faster-Rcnn</figcaption>
</figure>
<p>RPN的输入是经过一系列卷积层之后的feature
map，在这个map上，我们在上面再做一些运算：</p>
<figure>
<img src="/2022/11/02/R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/image-20221129160504387-16697091055822.png" alt="RPN network">
<figcaption aria-hidden="true">RPN network</figcaption>
</figure>
<p>针对一个window（n✖n），RPN做的就是将这个window映射到一个低维度的feature上，图上是256维的向量，然后我们再接两个dense
layer，一个用于预测box，一个用于做分类。k的意思是在每一个sliding
window上我们都维护了k个anchor，这个概念和yolo里一致。所以在每一个sliding-window上我们都可以得出来4k个box和2k个分类结果（是否有object的概率），这个anchor
box是和sliding-window的中心点绑定的，所以如果RPN的输入是一个W×H的feature
map，那么我们就会有W×H×k个anchors。</p>
<p>那么我们知道RPN是怎么计算的了，然后在训练阶段还有一些tricks。作者对每一个anchor都赋予了一个class
label，赋予positive的anchor为 1) anchor和 groud truth的box有最高的IOU 2)
如果anchor与groud
truth的box的IOU大于0.7。这两种anchor都会被赋予positive的标签，也就是代表它里面有object。对于哪些和任何groud
truth
box的IOU都小于0.3的anchor，赋予negtive的标签。在为RPN产生训练数据时，对于所有的anchors都有一个class
label，也就是它里面是否包含object。对于box的训练数据的处理有一点不一样的地方。可以参考<a href="https://github.com/matterport/Mask_RCNN/blob/3deaec5d902d16e1daf56b62d5971d428dc920bc/mrcnn/model.py#L486">tensorflow实现</a>，在<code>mrcnn/model.py</code>的<code>build_rpn_target</code>函数中：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># RPN Match: 1 = positive anchor, -1 = negative anchor, 0 = neutral</span></span><br><span class="line">rpn_match = np.zeros([anchors.shape[<span class="number">0</span>]], dtype=np.int32)</span><br><span class="line"><span class="comment"># RPN bounding boxes: [max anchors per image, (dy, dx, log(dh), log(dw))]</span></span><br><span class="line">rpn_bbox = np.zeros((config.RPN_TRAIN_ANCHORS_PER_IMAGE, <span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<p>rpn_bbox的数量是提前设定好的，也就是不是对每一个anchor都会有一个box来对应，对于那些标记为positive的anchor
box才会有bbox的target。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Generate RPN trainig targets</span></span><br><span class="line"><span class="comment"># target_rpn_match is 1 for positive anchors, -1 for negative anchors</span></span><br><span class="line"><span class="comment"># and 0 for neutral anchors.</span></span><br><span class="line">target_rpn_match, target_rpn_bbox = modellib.build_rpn_targets(</span><br><span class="line">    image.shape, model.anchors, gt_class_id, gt_bbox, model.config)</span><br><span class="line">log(<span class="string">&quot;target_rpn_match&quot;</span>, target_rpn_match)</span><br><span class="line">log(<span class="string">&quot;target_rpn_bbox&quot;</span>, target_rpn_bbox)</span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">target_rpn_match         shape: (65472,)              min:   -1.00000  max:    1.00000</span><br><span class="line">target_rpn_bbox          shape: (256, 4)              min:   -5.19860  max:    2.59641</span><br></pre></td></tr></table></figure>
<p>从上面可以看出，在全局变量设置中设置的是每一张图片最多有256个anchors，所以产生的rpn_bbox
shape就是(256,4),而用于RPN训练的class
label是全部的anchors的分类label。进一步的我们可以查看其中一张图片的anchors：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">positive_anchor_ix = np.where(target_rpn_match[:] == <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">negative_anchor_ix = np.where(target_rpn_match[:] == -<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">neutral_anchor_ix = np.where(target_rpn_match[:] == <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">positive_anchors = model.anchors[positive_anchor_ix]</span><br><span class="line">negative_anchors = model.anchors[negative_anchor_ix]</span><br><span class="line">neutral_anchors = model.anchors[neutral_anchor_ix]</span><br><span class="line">log(<span class="string">&quot;positive_anchors&quot;</span>, positive_anchors)</span><br><span class="line">log(<span class="string">&quot;negative_anchors&quot;</span>, negative_anchors)</span><br><span class="line">log(<span class="string">&quot;neutral anchors&quot;</span>, neutral_anchors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply refinement deltas to positive anchors</span></span><br><span class="line">refined_anchors = utils.apply_box_deltas(</span><br><span class="line">    positive_anchors,</span><br><span class="line">    target_rpn_bbox[:positive_anchors.shape[<span class="number">0</span>]] * model.config.RPN_BBOX_STD_DEV)</span><br><span class="line">log(<span class="string">&quot;refined_anchors&quot;</span>, refined_anchors, )</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">positive_anchors         shape: (14, 4)               min:    5.49033  max:  973.25483</span><br><span class="line">negative_anchors         shape: (242, 4)              min:  -22.62742  max: 1038.62742</span><br><span class="line">neutral anchors          shape: (65216, 4)            min: -362.03867  max: 1258.03867</span><br><span class="line">refined_anchors          shape: (14, 4)               min:    0.00000  max: 1023.99994</span><br></pre></td></tr></table></figure>
<p>即便是有65472个anchors，但其实正负anchor所占比重很小，大多数是neutral的anchor。其中对于positive的anchor，我们拥有在他们的bbox上refine过的box。</p>
<figure>
<img src="/2022/11/02/R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/positive_refined_box.png" alt="positive anchor">
<figcaption aria-hidden="true">positive anchor</figcaption>
</figure>
<p>上图中虚线画出来的是positive
anchor，实线框出来的是在这些anchor上refine过的box。</p>
<p>在训练阶段，计算loss时，对于regression loss，模型只会计算postive
anchors的regression
loss，也就是只计算那些被打上positive标签的anchor预测的box与groud-truth
box的回归loss。如果一个anchor，它的class
label在anatation阶段就是negtive的，模型并不会将它预测出来的box和ground-truth
box进行比较，他们的loss不会被计算进总的Loss。</p>
<p>关于如何训练的问题，在Faster-Rcnn的文章中给出了三种训练的算法，第一种也是文中采用的方法就是：1.
alternating training :
先把RPN单独训练，然后使用RPN产生的proposals去训练fast r-cnn.
然后将fine-tune过的RPN作为初始参数，然后再去产生proposal，再去训练fast
rcnn. 具体来说就是4步：</p>
<ol type="1">
<li>单独train RPN ： 用ImageNet pre-trained 参数做初始化，然后在region
proposal这个task上fine tune</li>
<li>利用第一步产生的proposal训练Fast-Rcnn，也就是<a href="R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/image-20221129160354668-16697090363711.png">架构图</a>中的最上面一部分，该网络也会使用ImageNet
pre-trained
参数做初始化。注意一直到这一步，两个网络都没有share任何卷积layers</li>
<li>第三步我们使用detector的network去初始化RPN，同时fix住最下面的卷积层，也就是两个网络共享的那些卷积层。这一步骤单独fine-tune
RPN的layers。</li>
<li>最后一步，fine-tune Fast-RCNN的unique的layers。</li>
</ol>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG中PDF的解析</title>
    <url>/2024/06/27/RAG%E4%B8%ADPDF%E7%9A%84%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>今天新开一篇文章。前段时间一直忙于项目周期中琐碎的事情，没有好好总结和思考技术核心的东西。探索RAG的应用也有一段时间了，市面上的应用也看的不少，很多的应用包括langchain-chatchat，Dify,
都是整体搭建了一个最basic版本的RAG框架，至于很多细节点，并没有提供更精细化的实现，今天要写的这部分：PDF上传到知识库之后如何parse和chunking，直接影响后续retrival的表现，目前还没有很全面的review来总结这部分内容。这里就对我看到的一些技术做一些整理。</p>
<p>解析PDF文档的难点主要在于如何精确地捕捉页面的整体布局，并将包括<code>表格</code>，<code>标题</code>,
段落以及图片在内的内容转译为文档的文字形式。这一过程涉及到多个技术点，布局的检测，图片中文字的抽取，表格中行与列的识别（如何正确将PDF中的表格识别成可用结构化形式表示的表格，也就是能还原出原表格来）。</p>
<p>目前解析PDF文档主要有三种主流方式：</p>
<ol type="1">
<li>基于规则的方法：这种方法根据文档的组织特性来确定每个部分的样式和内容，代表库：<a href="https://pypi.org/project/pypdf/">pypdf</a>，
这种方法的适用性比较差，很难通过预设的规则覆盖所有PDF的情形。</li>
<li>基于深度学习模型的方法：一个流行的解决方案是结合了物体检测和OCR模型，代表
Chatdoc</li>
<li>基于多模态大模型的方法：通过这种方法可以解析PDF中复杂结构或者提取关键信息</li>
</ol>
<h1 id="基于规则的方法">基于规则的方法</h1>
<p>基于规则的PDF处理库真的太多了，每一次看到一个应用使用的新的PDF解析器，都要重新看看怎么处理的。比如langchain-chatchat用的是pyMuPDF中的fitz包，见<a href="https://github.com/chatchat-space/Langchain-Chatchat/blob/master/libs/chatchat-server/chatchat/server/file_rag/document_loaders/mypdfloader.py">langchain-chatchat源代码</a>，这一段处理特别粗糙，我贴上来：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> fitz  <span class="comment"># pyMuPDF里面的fitz包，不要与pip install fitz混淆</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">ocr = get_ocr()</span><br><span class="line">doc = fitz.<span class="built_in">open</span>(filepath)</span><br><span class="line">resp = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">b_unit = tqdm.tqdm(</span><br><span class="line">    total=doc.page_count, desc=<span class="string">&quot;RapidOCRPDFLoader context page index: 0&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">for</span> i, page <span class="keyword">in</span> <span class="built_in">enumerate</span>(doc):</span><br><span class="line">    b_unit.set_description(</span><br><span class="line">        <span class="string">&quot;RapidOCRPDFLoader context page index: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i)</span><br><span class="line">    )</span><br><span class="line">    b_unit.refresh()</span><br><span class="line">    text = page.get_text(<span class="string">&quot;&quot;</span>)</span><br><span class="line">    resp += text + <span class="string">&quot;\n&quot;</span></span><br><span class="line"></span><br><span class="line">    img_list = page.get_image_info(xrefs=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> img <span class="keyword">in</span> img_list:</span><br><span class="line">        <span class="keyword">if</span> xref := img.get(<span class="string">&quot;xref&quot;</span>):</span><br><span class="line">                bbox = img[<span class="string">&quot;bbox&quot;</span>]</span><br><span class="line">                <span class="comment"># 检查图片尺寸是否超过设定的阈值</span></span><br><span class="line">                <span class="keyword">if</span> (bbox[<span class="number">2</span>] - bbox[<span class="number">0</span>]) / (page.rect.width) &lt; PDF_OCR_THRESHOLD[</span><br><span class="line">                    <span class="number">0</span></span><br><span class="line">                ] <span class="keyword">or</span> (bbox[<span class="number">3</span>] - bbox[<span class="number">1</span>]) / (</span><br><span class="line">                    page.rect.height</span><br><span class="line">                ) &lt; PDF_OCR_THRESHOLD[<span class="number">1</span>]:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                    pix = fitz.Pixmap(doc, xref)</span><br><span class="line">                    samples = pix.samples</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">int</span>(page.rotation) != <span class="number">0</span>:  <span class="comment"># 如果Page有旋转角度，则旋转图片</span></span><br><span class="line">                        img_array = np.frombuffer(</span><br><span class="line">                            pix.samples, dtype=np.uint8</span><br><span class="line">                        ).reshape(pix.height, pix.width, -<span class="number">1</span>)</span><br><span class="line">                        tmp_img = Image.fromarray(img_array)</span><br><span class="line">                        ori_img = cv2.cvtColor(np.array(tmp_img), cv2.COLOR_RGB2BGR)</span><br><span class="line">                        rot_img = rotate_img(img=ori_img, angle=<span class="number">360</span> - page.rotation)</span><br><span class="line">                        img_array = cv2.cvtColor(rot_img, cv2.COLOR_RGB2BGR)</span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            img_array = np.frombuffer(</span><br><span class="line">                                pix.samples, dtype=np.uint8</span><br><span class="line">                            ).reshape(pix.height, pix.width, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                            result, _ = ocr(img_array)</span><br><span class="line">                            <span class="keyword">if</span> result:</span><br><span class="line">                                ocr_result = [line[<span class="number">1</span>] <span class="keyword">for</span> line <span class="keyword">in</span> result]</span><br><span class="line">                                resp += <span class="string">&quot;\n&quot;</span>.join(ocr_result)</span><br></pre></td></tr></table></figure>
<p>注意它对PDF中图片的处理，是将某一页中所有的图片存入一个img_list，然后遍历这个list，用ocr算法抠出里面的文字，将这些文字都放到该页text的末尾，很大程度上丧失了图片在PDF中应该表达的语义，不仅如此，我觉得还影响了retrival的performance，加入了噪声。</p>
<p>再来看看<a href="https://github.com/langgenius/dify/blob/main/api/core/rag/extractor/pdf_extractor.py">Dify</a>怎么处理的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pypdfium2</span><br><span class="line">    <span class="keyword">with</span> blob.as_bytes_io() <span class="keyword">as</span> file_path:</span><br><span class="line">        pdf_reader = pypdfium2.PdfDocument(file_path, autoclose=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">for</span> page_number, page <span class="keyword">in</span> <span class="built_in">enumerate</span>(pdf_reader):</span><br><span class="line">                text_page = page.get_textpage()</span><br><span class="line">                content = text_page.get_text_range()</span><br><span class="line">                text_page.close()</span><br><span class="line">                page.close()</span><br><span class="line">                metadata = &#123;<span class="string">&quot;source&quot;</span>: blob.source, <span class="string">&quot;page&quot;</span>: page_number&#125;</span><br><span class="line">                <span class="keyword">yield</span> Document(page_content=content, metadata=metadata)</span><br><span class="line">                <span class="keyword">finally</span>:</span><br><span class="line">                    pdf_reader.close()</span><br></pre></td></tr></table></figure>
<p>换了一个pypdfium2的库，<strong>图片完全舍弃</strong>。我也去搜了有没有对两者的比较，见<a href="https://www.reddit.com/r/learnpython/comments/1796l3g/pypdf_or_pymupdf/">pypdf
or pymupdf?</a>，更有作者做了一个repo用于比较各种library的效果：<a href="https://github.com/py-pdf/benchmarks">传送门</a>。结论是如果单论文本抽取的质量，pypdfium2是第一</p>
<blockquote>
<p>对于RAG中PDF的处理，我觉得最理想的目标应该是，query能够link到PDF中的图片，不仅如此，这张图片也应该作为reference，目前市面上的reference只能link到相应文件的纯文本。远远不够精细，不过做起来确实有点困难，也需要一点耐心。</p>
</blockquote>
<figure>
<img src="/2024/06/27/RAG%E4%B8%ADPDF%E7%9A%84%E8%A7%A3%E6%9E%90/image-20240628100839214.png" alt="image-20240628100839214">
<figcaption aria-hidden="true">image-20240628100839214</figcaption>
</figure>
<p>更多其他工具的比较可参考<a href="https://ad-publications.cs.uni-freiburg.de/benchmark.pdf">A
Benchmark and Evaluation for Text Extraction from PDF</a></p>
<figure>
<img src="/2024/06/27/RAG%E4%B8%ADPDF%E7%9A%84%E8%A7%A3%E6%9E%90/image-20240628101040870.png" alt="image-20240628101040870">
<figcaption aria-hidden="true">image-20240628101040870</figcaption>
</figure>
<p>基于规则的方法有一个最大的缺点就是：它会将每一行视为由换行符“”分隔的序列，如果那一行确实是以句号为结尾，影响还稍微小一点，但是如果下一行还在表述这句话，语义上就完全断掉了。要知道在chunking的阶段，大部分的做法是以<code>\n</code>作分隔符的。</p>
<h1 id="基于深度学习模型的方法">基于深度学习模型的方法</h1>
<p>这种方法的又是是它能准确识别整个文档的布局，包括表格和段落。它甚至能理解表格内的结构。这意味着解析出来的表格能完整的解析成表格原本的样子。局限性就在于对象检测和OCR的识别两个阶段可能会耗时。这时候可以考虑采用GPU加速或者多进程和多线程的方式进行处理。</p>
<p>这里有几个开源的代表框架：</p>
<ul>
<li>Unstructured：它是langchain官方推荐的方式。在启用infer_table_structure=True的hi_res策略下，表格的识别效果良好。fast策略下表现不佳</li>
<li>Layout-parser：如果需要识别复杂结构的PDF，建议使用最大的模型，虽然可能会稍慢一点</li>
<li><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/main/ppstructure/README_ch.md">PP-StructureV2</a>：
采用多种模型组合进行文档的分析，性能优于平均水平。这是百度的飞桨出品的文档智能模型。</li>
</ul>
<p>另外有一些闭源付费的工具诸如ChatDoc和LLama Parse,
这些在现实的落地中存在一定阻碍，毕竟调用API是一定会存在数据上传到别人服务器的风险的，如果是处理非敏感数据，付费API可以做考虑。</p>
<p>这里对Unstructured处理PDF做一些探索。</p>
<p><a href="https://docs.unstructured.io/open-source/core-functionality/partitioning#partition-pdf">Unstructured</a>对PDF的处理首先是对layout进行检测(detectron2模型)，然后使用tesseract实现OCR的功能，用table
transformer处理table。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> unstructured.partition.pdf <span class="keyword">import</span> partition_pdf</span><br><span class="line"><span class="comment"># Get elements</span></span><br><span class="line">raw_pdf_elements = partition_pdf(</span><br><span class="line">    filename=test_file,</span><br><span class="line">    <span class="comment"># Using pdf format to find embedded image blocks</span></span><br><span class="line">    extract_images_in_pdf=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># Use layout model (YOLOX) to get bounding boxes (for tables) and find titles</span></span><br><span class="line">    <span class="comment"># Titles are any sub-section of the document</span></span><br><span class="line">    infer_table_structure=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># Post processing to aggregate text once we have the title</span></span><br><span class="line">    chunking_strategy=<span class="string">&quot;by_title&quot;</span>,</span><br><span class="line">    <span class="comment"># Chunking params to aggregate text blocks</span></span><br><span class="line">    <span class="comment"># Attempt to create a new chunk 3800 chars</span></span><br><span class="line">    <span class="comment"># Attempt to keep chunks &gt; 2000 chars</span></span><br><span class="line">    <span class="comment"># Hard max on chunks</span></span><br><span class="line">    max_characters=<span class="number">4000</span>,</span><br><span class="line">    new_after_n_chars=<span class="number">3800</span>,</span><br><span class="line">    combine_text_under_n_chars=<span class="number">2000</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tables = [el <span class="keyword">for</span> el <span class="keyword">in</span> raw_pdf_elements <span class="keyword">if</span> el.category == <span class="string">&#x27;Table&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tables[<span class="number">0</span>].text)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tables[<span class="number">0</span>].metadata.text_as_html)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上述的第二个输出：将表格转化为html格式，如果将其保存为html，它可以在浏览器中打开，打开后还是一个完整的表格。unstructured的好处就在于它保证了一个表格的完整性。官方给出的<a href="https://colab.research.google.com/drive/1BJYYyrPVe0_9EGyXqeNyzmVZDrCRZwsg">示例代码</a>中就是将每一个element的text去做embedding，对于表格来说就是将表格的text去做embedding。但是我测试了两个例子发现，它把表格的标题以及表格的注解给弄到其他element里去了，这些文本本应该和表格一起，比如下面的表格：</p>
<figure>
<img src="/2024/06/27/RAG%E4%B8%ADPDF%E7%9A%84%E8%A7%A3%E6%9E%90/image-20240628160939056.png" alt="image-20240628160939056">
<figcaption aria-hidden="true">image-20240628160939056</figcaption>
</figure>
<p>它抽取出来的表格element就只包含纯表格那部分，上面关于Table1的介绍都包含在了上面一个element里</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">3.5 Positional Encoding</span><br><span class="line"></span><br><span class="line">Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the</span><br><span class="line"></span><br><span class="line">5</span><br><span class="line"></span><br><span class="line">Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.</span><br><span class="line">--------------------------------- 上面是另一个element的内容</span><br><span class="line">Layer Type Self-Attention Recurrent Convolutional Self-Attention (restricted) Complexity per Layer O(n2 · d) O(n · d2) O(k · n · d2) O(r · n · d) Sequential Maximum Path Length Operations O(1) O(n) O(1) O(1) O(1) O(n) O(logk(n)) O(n/r)</span><br></pre></td></tr></table></figure>
<h1 id="基于多模态大模型的方法">基于多模态大模型的方法</h1>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Sequence分类问题中处理不定长数据</title>
    <url>/2023/12/06/Sequence%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%AD%E5%A4%84%E7%90%86%E4%B8%8D%E5%AE%9A%E9%95%BF%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<p>之前写过一篇关于stanford
alpaca的代码的分析，最近在kaggle上看到一个检测某段长文本是否是AI生成的任务<a href="https://www.kaggle.com/c/llm-detect-ai-generated-text">LLM -
Detect AI Generated
Text</a>,自己也在尝试做这个任务的时候，发现斯坦福的这份代码真是常看常新。对于数据的准备部分，有很多选择，比如在创建Dataset的时候就把所有的字符串数据tokenize好，在get_item()的函数返回时就返回input_ids，也可以是像斯坦福的<a href="https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py">这份代码</a>一样，先把数据读取进来然后再用<code>DataCollator</code>处理（padding）。</p>
<p>我之前没有发现斯坦福这份代码这么写的真正原因，直到我自己来处理这种不定长的序列输入时才发现这样写的绝妙，因为我们都知道矩阵是每一行都需要是同样的size，所以斯坦福的写法在数据处理前期一直在用list，而不是batch。</p>
<p>先来说说transformer的对于序列分类的官方教程的写法<a href="https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/sequence_classification.ipynb">传送门</a></p>
<p>transformer的这个教程直接使用的是自己的数据集，已经规整为datasets了，首先它对数据集使用map函数做了截断的处理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;distilbert-base-uncased&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_function</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">&quot;text&quot;</span>], truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">tokenized_imdb = imdb.<span class="built_in">map</span>(preprocess_function, batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>然后重点来了，注意在上面的<code>preprocess_function</code>中并没有对序列进行padding，只是对过长的序列做了截断。接着作者使用了<code>datacollatorwithpadding</code>，给出的理由是：</p>
<blockquote>
<p>It's more efficient to <em>dynamically pad</em> the sentences to the
longest length in a batch during collation, instead of padding the whole
dataset to the maximum length.</p>
</blockquote>
<p>我们可以用官方的文档中看到对于datacollator的<a href="https://huggingface.co/docs/transformers/main_classes/data_collator">定义</a>：</p>
<blockquote>
<p>Data collators are objects that will form a batch by using a list of
dataset elements as input. These elements are of the same type as the
elements of <code>train_dataset</code> or <code>eval_dataset</code></p>
</blockquote>
<p>也就是data
collators的输入是一个list，list里的每一个元素跟train_dataset中的元素是一样的。在data
collator中你可以做一些processing，如padding，random
masking。我们接下来可以看到斯坦福的羊驼代码就是将padding的步骤放到了data
collator内。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorWithPadding</span><br><span class="line"></span><br><span class="line">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span><br></pre></td></tr></table></figure>
<p>在这里的教程里，作者使用了<code>DataCollatorWithPadding</code>，它会动态地pad
inputs。我看到文档里还有<code>class transformers.DataCollatorForTokenClassification</code>这个类，maybe可以处理不定长输入，留作后续探索。</p>
<p>transformer的这个教程还是过于简单了，在实际的case中情况会复杂一点。接下来我们看斯坦福的羊驼咋处理不定长sequence的。</p>
<p>首先它先把Dataset定义好：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    sources: <span class="type">Sequence</span>[<span class="built_in">str</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">    targets: <span class="type">Sequence</span>[<span class="built_in">str</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">    tokenizer: transformers.PreTrainedTokenizer,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="type">Dict</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Preprocess the data by tokenizing.&quot;&quot;&quot;</span></span><br><span class="line">    examples = [s + t <span class="keyword">for</span> s, t <span class="keyword">in</span> <span class="built_in">zip</span>(sources, targets)]</span><br><span class="line">    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) <span class="keyword">for</span> strings <span class="keyword">in</span> (examples, sources)]</span><br><span class="line">    input_ids = examples_tokenized[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">    labels = copy.deepcopy(input_ids)</span><br><span class="line">    <span class="keyword">for</span> label, source_len <span class="keyword">in</span> <span class="built_in">zip</span>(labels, sources_tokenized[<span class="string">&quot;input_ids_lens&quot;</span>]):</span><br><span class="line">        label[:source_len] = IGNORE_INDEX</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(input_ids=input_ids, labels=labels)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SupervisedDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Dataset for supervised fine-tuning.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data_path: <span class="built_in">str</span>, tokenizer: transformers.PreTrainedTokenizer</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SupervisedDataset, self).__init__()</span><br><span class="line">        logging.warning(<span class="string">&quot;Loading data...&quot;</span>)</span><br><span class="line">        list_data_dict = utils.jload(data_path)</span><br><span class="line"></span><br><span class="line">        logging.warning(<span class="string">&quot;Formatting inputs...&quot;</span>)</span><br><span class="line">        prompt_input, prompt_no_input = PROMPT_DICT[<span class="string">&quot;prompt_input&quot;</span>], PROMPT_DICT[<span class="string">&quot;prompt_no_input&quot;</span>]</span><br><span class="line">        sources = [</span><br><span class="line">            prompt_input.format_map(example) <span class="keyword">if</span> example.get(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;&quot;</span>) != <span class="string">&quot;&quot;</span> <span class="keyword">else</span> prompt_no_input.format_map(example)</span><br><span class="line">            <span class="keyword">for</span> example <span class="keyword">in</span> list_data_dict</span><br><span class="line">        ]</span><br><span class="line">        targets = [<span class="string">f&quot;<span class="subst">&#123;example[<span class="string">&#x27;output&#x27;</span>]&#125;</span><span class="subst">&#123;tokenizer.eos_token&#125;</span>&quot;</span> <span class="keyword">for</span> example <span class="keyword">in</span> list_data_dict] <span class="comment"># 将output之后加上[EOS]</span></span><br><span class="line"></span><br><span class="line">        logging.warning(<span class="string">&quot;Tokenizing inputs... This may take some time...&quot;</span>)</span><br><span class="line">        data_dict = preprocess(sources, targets, tokenizer)</span><br><span class="line"></span><br><span class="line">        self.input_ids = data_dict[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">        self.labels = data_dict[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.input_ids)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, i</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(input_ids=self.input_ids[i], labels=self.labels[i])</span><br></pre></td></tr></table></figure>
<p>玄机在：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_tokenize_fn</span>(<span class="params">strings: <span class="type">Sequence</span>[<span class="built_in">str</span>], tokenizer: transformers.PreTrainedTokenizer</span>) -&gt; <span class="type">Dict</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Tokenize a list of strings.&quot;&quot;&quot;</span></span><br><span class="line">    tokenized_list = [</span><br><span class="line">        tokenizer(</span><br><span class="line">            text,</span><br><span class="line">            return_tensors=<span class="string">&quot;pt&quot;</span>,</span><br><span class="line">            padding=<span class="string">&quot;longest&quot;</span>,</span><br><span class="line">            max_length=tokenizer.model_max_length,</span><br><span class="line">            truncation=<span class="literal">True</span>,</span><br><span class="line">        ) <span class="comment"># 这里做了循环，也就是对strings这个list里的每一个sequence单独做的tokenize，然后把这些不等长的input_ids一同放到一个list里。之所以用list，是因为list里可以存储不等长的list。一直到这一步都没有做padding</span></span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> strings</span><br><span class="line">    ]</span><br><span class="line">    input_ids = labels = [tokenized.input_ids[<span class="number">0</span>] <span class="keyword">for</span> tokenized <span class="keyword">in</span> tokenized_list]</span><br><span class="line">    input_ids_lens = labels_lens = [</span><br><span class="line">        tokenized.input_ids.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>().item() <span class="keyword">for</span> tokenized <span class="keyword">in</span> tokenized_list</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        labels=labels,</span><br><span class="line">        input_ids_lens=input_ids_lens,</span><br><span class="line">        labels_lens=labels_lens,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>当我们调用<code>SupervisedDataset</code>实例化数据后我们来看看数据长什么样子</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">train_dataset中有两个key，一个Input_ids,一个labels</span><br><span class="line">input_ids中的值长这样：</span><br><span class="line">tensor([    2, 45943,    16,    41, 15741,    14,  7448,    10,  3685,     4,</span><br><span class="line">        21062,    10,  1263,    14, 16574, 25830,     5,  2069,     4, 50118,</span><br><span class="line">        50118, 48134, 41241,    35, 50118, 31033,   130,  4965,    13,  4959,</span><br><span class="line">         2245,     4, 50118, 50118, 48134, 19121,    35,   134,     4, 43800,</span><br><span class="line">           10,  9320,  5626,     8,   146,   686,     7,   680,  2710,     9,</span><br><span class="line">        12849,     8,  8942,     4,  1437, 50118,   176,     4, 30450,  4595,</span><br><span class="line">            7,   489,   110,   809,  2171,     8,   670,     4,  1437, 50118,</span><br><span class="line">          246,     4,  2315,   615,  3581,     8,  3014,    10,  4292,  3581,</span><br><span class="line">         3078,     4,     2])</span><br><span class="line">labels中的值长这样：</span><br><span class="line">tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,</span><br><span class="line">         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,</span><br><span class="line">         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,</span><br><span class="line">         -100,  -100,  -100,  -100,  -100,  -100,  -100,   134,     4, 43800,</span><br><span class="line">           10,  9320,  5626,     8,   146,   686,     7,   680,  2710,     9,</span><br><span class="line">        12849,     8,  8942,     4,  1437, 50118,   176,     4, 30450,  4595,</span><br><span class="line">            7,   489,   110,   809,  2171,     8,   670,     4,  1437, 50118,</span><br><span class="line">          246,     4,  2315,   615,  3581,     8,  3014,    10,  4292,  3581,</span><br><span class="line">         3078,     4,     2])</span><br></pre></td></tr></table></figure>
<p>而且input_ids中每一个值的长度都是不同的，这是因为没有做padding的结果，<strong>仅仅</strong>是将所有的过长的sequence截断了。</p>
<p>羊驼的代码将所有的padding细节都放到了collator里：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataCollatorForSupervisedDataset</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Collate examples for supervised fine-tuning.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    tokenizer: transformers.PreTrainedTokenizer</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, instances: <span class="type">Sequence</span>[<span class="type">Dict</span>]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span></span><br><span class="line">        input_ids, labels = <span class="built_in">tuple</span>([instance[key] <span class="keyword">for</span> instance <span class="keyword">in</span> instances] <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">&quot;input_ids&quot;</span>, <span class="string">&quot;labels&quot;</span>))</span><br><span class="line">        input_ids = torch.nn.utils.rnn.pad_sequence(</span><br><span class="line">            input_ids, batch_first=<span class="literal">True</span>, padding_value=self.tokenizer.pad_token_id</span><br><span class="line">        )</span><br><span class="line">        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=<span class="literal">True</span>, padding_value=IGNORE_INDEX)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            labels=labels,</span><br><span class="line">            attention_mask=input_ids.ne(self.tokenizer.pad_token_id), <span class="comment"># see https://pytorch.org/docs/stable/generated/torch.ne.html#torch.ne</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>这里作者写了一个自己的类，继承自object。这里没有继承transformer的DefaultDataCollator，暂时不清楚用意，但我觉得应该也可以。这个类实现了一个<code>__call__</code>方法，接受的是一个Sequence（可迭代对象），对象中是字典（input_ids,
labels），我们上面在创建数据集的时候getitem每次返回一个dict，这个dict里有input_id和label。现在的collator接受的是这个字典的list，也就是有很多个数据（batch_size），我们对这个batch里的数据统一进行padding，这样就实现了在batch内部去pad，避免将所有的字符串都pad成最长的字符长度。</p>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>TF2中的custom layer&amp;model&amp;training</title>
    <url>/2023/02/13/TF2%E4%B8%AD%E7%9A%84custom-layer-model-training/</url>
    <content><![CDATA[<p>在上Coursera上关于<a href="https://www.coursera.org/specializations/tensorflow-advanced-techniques">Tensorflow的高级用法课程</a>时，老师简略介绍了custom
layer和custom
model的用法，但后来看到其实课程覆盖的内容比较简单，除了介绍了__init__和call两个可override的function外没有介绍其他的。偶然看到一篇博客详细介绍了在tensorflow中如何使用sub
classing来搭建模型，写的非常好，这里贴上<a href="https://towardsdatascience.com/model-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e">链接</a></p>
<p>我们知道在tensorflow中有三种搭建模型的方式： 1) sequential API
也就是想创建一个Sequential实例，然后通过add的方式把一个layer加到模型中去，如：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># declare input shape </span></span><br><span class="line">seq_model = tf.keras.Sequential()</span><br><span class="line">seq_model.add(tf.keras.Input(shape=imput_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block 1</span></span><br><span class="line">seq_model.add(tf.keras.layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, strides=<span class="number">2</span>, activation=<span class="string">&quot;relu&quot;</span>))</span><br><span class="line">seq_model.add(tf.keras.layers.MaxPooling2D(<span class="number">3</span>))</span><br><span class="line">seq_model.add(tf.keras.layers.BatchNormalization())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block 2</span></span><br><span class="line">seq_model.add(tf.keras.layers.Conv2D(<span class="number">64</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>))</span><br><span class="line">seq_model.add(tf.keras.layers.BatchNormalization())</span><br><span class="line">seq_model.add(tf.keras.layers.Dropout(<span class="number">0.3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now that we apply global max pooling.</span></span><br><span class="line">seq_model.add(tf.keras.layers.GlobalMaxPooling2D())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Finally, we add a classification layer.</span></span><br><span class="line">seq_model.add(tf.keras.layers.Dense(output_dim))</span><br></pre></td></tr></table></figure>
sequential的方式在researcher中用的不多，随着模型变得越来越复杂，可以看到tensorflow的application模块实现的官方模型代码中，已经见不到这种形式了。
2) Functional API 正如其名，就是用函数调用的方式来搭建模型：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># declare input shape </span></span><br><span class="line"><span class="built_in">input</span> = tf.keras.Input(shape=(imput_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block 1</span></span><br><span class="line">x = tf.keras.layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, strides=<span class="number">2</span>, activation=<span class="string">&quot;relu&quot;</span>)(<span class="built_in">input</span>)</span><br><span class="line">x = tf.keras.layers.MaxPooling2D(<span class="number">3</span>)(x)</span><br><span class="line">x = tf.keras.layers.BatchNormalization()(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block 2</span></span><br><span class="line">x = tf.keras.layers.Conv2D(<span class="number">64</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">x = tf.keras.layers.BatchNormalization()(x)</span><br><span class="line">x = tf.keras.layers.Dropout(<span class="number">0.3</span>)(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now that we apply global max pooling.</span></span><br><span class="line">gap = tf.keras.layers.GlobalMaxPooling2D()(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Finally, we add a classification layer.</span></span><br><span class="line">output = tf.keras.layers.Dense(output_dim)(gap)</span><br><span class="line"></span><br><span class="line"><span class="comment"># bind all</span></span><br><span class="line">func_model = tf.keras.Model(<span class="built_in">input</span>, output)</span><br></pre></td></tr></table></figure>
注意：这种方式最终要使用<code>tf.keras.Model()</code>来将inputs和outputs接起来。</p>
<ol start="3" type="1">
<li>Model sub-classing API 第三种方式是现在用的最多的方式。
之前我没理解layer和model两种调用方式的区别，我觉得就是一系列运算，我们把输入输进来，return
output结果的一个过程。但如果一个类它是Layer的子类，它比model的子类多了一个功能，它有state属性，也就是我们熟悉的weights。比如Dense
layer，我们知道它做了线性运算+激活函数，其中的weights就是我们assign给每一个feature的权重，但其实我们并不只是想要这一类别的运算，比如下面的：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleQuadratic</span>(<span class="params">Layer</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, units=<span class="number">32</span>, activation=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;Initializes the class and sets up the internal variables&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        <span class="built_in">super</span>(SimpleQuadratic, self).__init__()</span><br><span class="line">        self.units = units</span><br><span class="line">        self.activation = tf.keras.activations.get(activation)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, input_shape</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;Create the state of the layer (weights)&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># a and b should be initialized with random normal, c (or the bias) with zeros.</span></span><br><span class="line">        <span class="comment"># remember to set these as trainable.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        a_init = tf.random_normal_initializer()</span><br><span class="line">        b_init = tf.random_normal_initializer()</span><br><span class="line">        c_init = tf.zeros_initializer()</span><br><span class="line">        </span><br><span class="line">        self.a = tf.Variable(name = <span class="string">&quot;kernel&quot;</span>, initial_value = a_init(shape= (input_shape[-<span class="number">1</span>], self.units), </span><br><span class="line">                                                                    dtype= <span class="string">&quot;float32&quot;</span>), trainable = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        self.b = tf.Variable(name = <span class="string">&quot;kernel&quot;</span>, initial_value = b_init(shape= (input_shape[-<span class="number">1</span>], self.units), </span><br><span class="line">                                                                    dtype= <span class="string">&quot;float32&quot;</span>), trainable = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        self.c = tf.Variable(name = <span class="string">&quot;bias&quot;</span>, initial_value = c_init(shape= (self.units,), </span><br><span class="line">                                                                    dtype= <span class="string">&quot;float32&quot;</span>), trainable = <span class="literal">True</span>)</span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span> </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;Defines the computation from inputs to outputs&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        result = tf.matmul(tf.math.square(inputs), self.a) + tf.matmul(inputs, self.b) + self.c</span><br><span class="line">        <span class="keyword">return</span> self.activation(result)</span><br></pre></td></tr></table></figure>
上面的代码将inputs平方之后和a做乘积，之后再加上inputs和b的乘积，最终返回的是和。这样的运算是tf.keras.layer中没有的。这个时候我们自己customize
layer就很方便。还有一个很方便的地方在于很多模型其实是按模块来的，模块内部的layer很类似。这个时候我们就可以把这些模型内的layer包起来变成一个layer的子类（Module），再定义完这些module之后我们使用Model把这些module再包起来，这就是我们最终的model。这时候我们就可以看到Model和Layer子类的区别了，虽然两者都可以实现输入进来之后实现一系列运算返回运算结果，但后者可以实现更灵活的运算，而前者往往是在把每一个模块定义好之后最终定义我们训练模型的类。
&gt; In general, we use the Layer class to define the inner computation
blocks and will use the Model class to define the outer model,
practically the object that we will train. ---粘贴自博客</li>
</ol>
<blockquote>
<p>You can treat any model as if it were a layer by invoking it on an
<code>Input</code> or on the output of another layer. By calling a model
you aren't just reusing the architecture of the model, you're also
reusing its weights</p>
</blockquote>
<p>同样值得注意的是，model的子类也可以像layer那样使用functional
API来调用，比如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">encoder_input = keras.Input(shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>), name=<span class="string">&quot;original_img&quot;</span>)</span><br><span class="line">x = layers.Conv2D(<span class="number">16</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(encoder_input)</span><br><span class="line">x = layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">x = layers.MaxPooling2D(<span class="number">3</span>)(x)</span><br><span class="line">x = layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">x = layers.Conv2D(<span class="number">16</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">encoder_output = layers.GlobalMaxPooling2D()(x)</span><br><span class="line"></span><br><span class="line">encoder = keras.Model(encoder_input, encoder_output, name=<span class="string">&quot;encoder&quot;</span>)</span><br><span class="line">encoder.summary()</span><br><span class="line"></span><br><span class="line">decoder_input = keras.Input(shape=(<span class="number">16</span>,), name=<span class="string">&quot;encoded_img&quot;</span>)</span><br><span class="line">x = layers.Reshape((<span class="number">4</span>, <span class="number">4</span>, <span class="number">1</span>))(decoder_input)</span><br><span class="line">x = layers.Conv2DTranspose(<span class="number">16</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">x = layers.Conv2DTranspose(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">x = layers.UpSampling2D(<span class="number">3</span>)(x)</span><br><span class="line">x = layers.Conv2DTranspose(<span class="number">16</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">decoder_output = layers.Conv2DTranspose(<span class="number">1</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line"></span><br><span class="line">decoder = keras.Model(decoder_input, decoder_output, name=<span class="string">&quot;decoder&quot;</span>)</span><br><span class="line">decoder.summary()</span><br><span class="line"></span><br><span class="line">autoencoder_input = keras.Input(shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>), name=<span class="string">&quot;img&quot;</span>)</span><br><span class="line">encoded_img = encoder(autoencoder_input)</span><br><span class="line">decoded_img = decoder(encoded_img)</span><br><span class="line">autoencoder = keras.Model(autoencoder_input, decoded_img, name=<span class="string">&quot;autoencoder&quot;</span>)</span><br><span class="line">autoencoder.summary()</span><br></pre></td></tr></table></figure>
<p>我们以sub-classing的方式定义的model是没有办法调用summary来看模型架构的，作者也给出了解决方案：<a href="https://github.com/tensorflow/tensorflow/issues/31647#issuecomment-692586409">github
comments</a></p>
<p>方法就是在Model的子类中添加build_graph方法： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_graph</span>(<span class="params">self, raw_shape</span>):</span></span><br><span class="line">        x = tf.keras.layers.Input(shape=raw_shape)</span><br><span class="line">        <span class="keyword">return</span> Model(inputs=[x], outputs=self.call(x))</span><br></pre></td></tr></table></figure>
这样我们就可以正常调用summary() <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cm.build_graph(raw_input).summary()</span><br><span class="line"><span class="comment"># 不仅如此还能使用tf.keras.utils.plot_model来生成png</span></span><br><span class="line">tf.keras.utils.plot_model(</span><br><span class="line">    model.build_graph(raw_input),                      <span class="comment"># here is the trick (for now)</span></span><br><span class="line">    to_file=<span class="string">&#x27;model.png&#x27;</span>, dpi=<span class="number">96</span>,              <span class="comment"># saving  </span></span><br><span class="line">    show_shapes=<span class="literal">True</span>, show_layer_names=<span class="literal">True</span>,  <span class="comment"># show shapes and layer name</span></span><br><span class="line">    expand_nested=<span class="literal">False</span>                       <span class="comment"># will show nested block</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>作者同样推荐了一篇博客讲tensorflow中保存模型的各种方式：<a href="https://colab.research.google.com/drive/172D4jishSgE3N7AO6U2OKAA_0wNnrMOq#scrollTo=vnTvqAgfspGJ">博客地址</a>.非常推荐阅读</p>
<p>总结一下就是：</p>
<ol type="1">
<li>对于Functional
API创建的模型，最好的保存模型和导入模型的方式是：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(<span class="string">&#x27;path_to_my_model.h5&#x27;</span>)</span><br><span class="line"><span class="keyword">del</span> model</span><br><span class="line">model = keras.models.load_model(<span class="string">&#x27;path_to_my_model.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>以上方式会将模型的架构，weights以及训练过程中的设定（也就是model.compile()）的内容全部保存。</p>
<ol start="2" type="1">
<li>对于sub class创建的模型，推荐的方式是用save_weights</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save_weights(<span class="string">&#x27;path_to_my_weights&#x27;</span>, save_format=<span class="string">&#x27;tf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>如果想要加载weights，必须要知道原来用sub
class建立模型的code。不仅如此，还需要用原来的code先build起模型，让模型知道输入tensor的shape以及dtype，如果没有build这一步程序将会报错。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_model = MiniInception()</span><br><span class="line">new_model.build((<span class="literal">None</span>, x_train.shape[<span class="number">1</span>:])) <span class="comment"># or .build((x_train.shape))</span></span><br><span class="line">new_model.load_weights(<span class="string">&#x27;net.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="tf.function">tf.function</h1>
<p>在我们定义custum training
过程中时我们经常会用到这个装饰器@tf.function</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span>(<span class="params">step, x, y</span>):</span></span><br><span class="line">   <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">   input: x, y &lt;- typically batches </span></span><br><span class="line"><span class="string">   input: step &lt;- batch step</span></span><br><span class="line"><span class="string">   return: loss value</span></span><br><span class="line"><span class="string">   &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># start the scope of gradient </span></span><br><span class="line">   <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">      logits = model(x, training=<span class="literal">True</span>) <span class="comment"># forward pass</span></span><br><span class="line">      train_loss_value = loss_fn(y, logits) <span class="comment"># compute loss </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute gradient </span></span><br><span class="line">   grads = tape.gradient(train_loss_value, model.trainable_weights)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update weights</span></span><br><span class="line">   optimizer.apply_gradients(<span class="built_in">zip</span>(grads, model.trainable_weights))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update metrics</span></span><br><span class="line">   train_acc_metric.update_state(y, logits)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># write training loss and accuracy to the tensorboard</span></span><br><span class="line">   <span class="keyword">with</span> train_writer.as_default():</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;loss&#x27;</span>, train_loss_value, step=step)</span><br><span class="line">        tf.summary.scalar(</span><br><span class="line">            <span class="string">&#x27;accuracy&#x27;</span>, train_acc_metric.result(), step=step</span><br><span class="line">        ) </span><br><span class="line">   <span class="keyword">return</span> train_loss_value</span><br></pre></td></tr></table></figure>
<p>先看如果一个函数不加这个装饰器会如何：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Traced with&quot;</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    f(<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">f(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Traced with 2</span><br><span class="line">Traced with 2</span><br><span class="line">Traced with 2</span><br><span class="line">Traced with 2</span><br><span class="line">Traced with 2</span><br><span class="line">Traced with 3</span><br></pre></td></tr></table></figure>
<p>加上装饰器：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Traced with&quot;</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    f(<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">f(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Traced <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Traced <span class="keyword">with</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p>可以看到第二种加了装饰器的方式，即便是循环了5遍，我们仍然只有一行打印了2.</p>
<p>如果我们在上面的代码中print之前加上一行：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Traced with&quot;</span>, x)</span><br><span class="line">    <span class="comment"># add tf.print</span></span><br><span class="line">    tf.<span class="built_in">print</span>(<span class="string">&quot;Executed with&quot;</span>, x)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    f(<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">f(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>程序的输出就变成了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Traced <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Traced <span class="keyword">with</span> <span class="number">3</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p>可以看到<code>tf.print</code>就可以正常按<code>loop</code>运行。注意一点:
被<code>tf.function</code>装饰的函数只能包含<code>operations</code>而不能定义<code>variable</code>比如<code>tf.Variable()</code></p>
]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>cv2中bitwise_and()</title>
    <url>/2022/10/12/cv2%E4%B8%ADbitwise-and/</url>
    <content><![CDATA[<p>参考文档：</p>
<ol type="1">
<li><a href="https://docs.opencv.org/4.x/d0/d86/tutorial_py_image_arithmetics.html">opencv
tutorial</a></li>
<li><a href="https://docs.opencv.org/4.x/d2/de8/group__core__array.html#ga60b4d04b251ba5eb1392c34425497e14">opencv
官方文档</a></li>
<li><a href="https://stackoverflow.com/questions/44333605/what-does-bitwise-and-operator-exactly-do-in-opencv">stackoverflow
question</a></li>
</ol>
<p>没有多少文档能讲清楚具体是如何计算的。在stackoverflow中的这个问题，第一个高赞答案也只是选取了最普通的一种情况，也就是当image的pixel的值全部是0或者1的时候，and的运算。但是cv2中的bitwise_and()还有一个很重要的参数mask并没有讲清楚，并且当image的像素值有除了0或者1以外的其他值，这时候该如何运算？</p>
<p>官方文档中说mask是决定了哪些位置要进行运算，当mask中某个元素不为0时，我们对相应位置的像素做“与”运算，如果scr1和scr2(bitwise_and的两个参数)的值不是0或者1，会将十进制的值转化为2进制，然后对二进制再进行运算，运算完了之后又会转化为十进制(<a href="https://dsp.stackexchange.com/questions/58276/opencv-how-does-bitwise-not-work">参考</a>)
当mask相应位置的值=0时，任何操作都不会做，不仅如此，结果的图片相应位置的像素值会变成0，也就是黑色（这一点官方文档没说）.</p>
<p>所以经过bitwise_and之后的图片会看到mask中像素值为0的地方全部是黑色。这就相应的把mask中不为0的地方强调突出了。</p>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer拆解</title>
    <url>/2024/01/02/Transformer%E6%8B%86%E8%A7%A3/</url>
    <content><![CDATA[<p>这篇博客主要记录Transformer架构的代码实现。以下是参考资料 - <a href="http://arxiv.org/abs/1706.03762">Attention is All you need</a> -
<a href="https://lilianweng.github.io/posts/2018-06-24-attention/">Attention?
Attention!</a> - <a href="https://github.com/lilianweng/transformer-tensorflow">lilian
wen的tensorflow版本的实现</a> - <a href="https://jalammar.github.io/illustrated-transformer/">illustrated-transformer</a>
强烈建议看illustrated
transformer这篇博客，是跟paper介绍的transformer架构完全对齐的 - <a href="https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/transformer.py">pytorch
transformer实现</a></p>
<p>​ 这是pytorch的官方实现</p>
<ul>
<li><a href="https://github.com/harvardnlp/annotated-transformer/tree/master">annotated
transformer</a></li>
</ul>
<p>​ 斯坦福出的transformer架构的实现tutorial</p>
<p>我自己想实现的一遍的原因在于：</p>
<ol type="1">
<li>transformer的文章读了很多遍，但是很多细节还是没有去深究。</li>
<li>斯坦福的实现完全遵照的是paper的架构，但是我觉得还是实现的过于复杂了，我想遵循lilian的tensorflow实现把原生的tranformer架构实现一下</li>
<li>我对pytorch的掌握没有tensorflow好，感觉现在pytorch基本上成为深度学习网络的主流，特别是大模型出来之后，hugginggface的transformer库也是支持pytorch更好一点，更大的社区。（此时有点后悔当时系统学习的是tensorflow而不是pytorch）</li>
</ol>
<p>transformer的整体架构:
encoder-decoder两大模块，encoder模块内有重复的6个子模块，decoder模块内也有重复的6个子模块。</p>
<figure>
<img src="/2024/01/02/Transformer%E6%8B%86%E8%A7%A3/transformer.png" alt="Transformer model">
<figcaption aria-hidden="true">Transformer model</figcaption>
</figure>
<p>我们采用自上而下的方式来看这两个模块</p>
<h1 id="transformer整体架构">Transformer整体架构</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    define the whole architecture of Transformer in:</span></span><br><span class="line"><span class="string">        Vaswani et al. Attention is All You Need. NIPS 2017.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_heads=<span class="number">8</span>, d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, num_enc_layers=<span class="number">6</span>, num_dec_layers=<span class="number">6</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 drop_rate=<span class="number">0.1</span>, warmup_steps=<span class="number">400</span>, pos_encoding_type=<span class="string">&#x27;sinusoid&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ls_epsilon=<span class="number">0.1</span>, use_label_smoothing=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 model_name=<span class="string">&#x27;transformer&#x27;</span>, tf_sess_config=<span class="literal">None</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.h = num_heads</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.d_ff = d_ff</span><br><span class="line"></span><br><span class="line">        self.num_enc_layers = num_enc_layers</span><br><span class="line">        self.num_dec_layers = num_dec_layers</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Dropout regularization: added in every sublayer before layer_norm(...) and</span></span><br><span class="line">        <span class="comment"># applied to embedding + positional encoding.</span></span><br><span class="line">        self.drop_rate = drop_rate</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Label smoothing epsilon</span></span><br><span class="line">        self.ls_epsilon = ls_epsilon</span><br><span class="line">        self.use_label_smoothing = use_label_smoothing</span><br><span class="line">        self.pos_encoding_type = pos_encoding_type</span><br><span class="line"></span><br><span class="line">        <span class="comment"># For computing the learning rate</span></span><br><span class="line">        self.warmup_steps = warmup_steps</span><br><span class="line"></span><br><span class="line">        self.config = <span class="built_in">dict</span>(</span><br><span class="line">            num_heads=self.h,</span><br><span class="line">            d_model=self.d_model,</span><br><span class="line">            d_ff=self.d_ff,</span><br><span class="line">            num_enc_layers=self.num_enc_layers,</span><br><span class="line">            num_dec_layers=self.num_dec_layers,</span><br><span class="line">            drop_rate=self.drop_rate,</span><br><span class="line">            warmup_steps=self.warmup_steps,</span><br><span class="line">            ls_epsilon=self.ls_epsilon,</span><br><span class="line">            use_label_smoothing=self.use_label_smoothing,</span><br><span class="line">            pos_encoding_type=self.pos_encoding_type,</span><br><span class="line">            model_name=self.model_name,</span><br><span class="line">            tf_sess_config=self.tf_sess_config,</span><br><span class="line">        )</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span> <span class="comment"># 这里进行拼接，将encoder和decoder两大模块拼接在一起</span></span><br><span class="line">            enc_out = self.encoder(src, src_mask)</span><br><span class="line">            dec_out = self.decoder(enc_out, src_mask, tgt, tgt_mask)</span><br><span class="line">            <span class="keyword">return</span> dec_out</span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<h1 id="tranformer-encoder">Tranformer Encoder</h1>
<figure>
<img src="/2024/01/02/Transformer%E6%8B%86%E8%A7%A3/image-20240102135835011.png" alt="image-20240102135835011">
<figcaption aria-hidden="true">image-20240102135835011</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span>(<span class="params">module, N</span>):</span></span><br><span class="line">    <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder_layer, num_enc_layers</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        self.num_enc_layers = num_enc_layers</span><br><span class="line">        self.encoder_layers = clones(encoder_layer,num_enc_layers) <span class="comment"># 将encoder_layer复制6次</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src, src_mask</span>):</span></span><br><span class="line">        out = src</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.encoder_layers:</span><br><span class="line">            out = layer(out, src_mask)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>这里实现了一个<code>clones</code>帮助函数，我想过在这里用for循环，lilian在这里就是用的for循环：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out = inp  <span class="comment"># now, (batch, seq_len, embed_size)</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_enc_layers):</span><br><span class="line">        out = self.encoder_layer(out, input_mask, <span class="string">f&#x27;enc_<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>注意这里的每一个encoder_layer的参数都是独立的，也就是有6份encoder_layer的参数需要训练，tensorflow为什么可行？是因为它这里使用了variable_scope的概念，上面的tensorflow实现每一次out和input_mask进来都是和不同的数值进行的运算。如果在pytorch中想实现这种方式，要先把encoder_layer复制六遍，每一次输入进来都拿不同的layer做运算。</p>
<h2 id="encoder-layer">encoder layer</h2>
<p>接下来我们实现encoder layer中的细节部分，它包含两个sub-layer： 1）
self-attention + Add&amp;layer_Norm 2) position-wise feed forward +
Add&amp;layer_Norm</p>
<figure>
<img src="/2024/01/02/Transformer%E6%8B%86%E8%A7%A3/image-20240102135835011.png" alt="image-20240102135835011">
<figcaption aria-hidden="true">image-20240102135835011</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model: the number of expected features in the input (required).</span></span><br><span class="line"><span class="string">        nhead: the number of heads in the multiheadattention models (required).</span></span><br><span class="line"><span class="string">        dim_feedforward: the dimension of the feedforward network model (default=2048).</span></span><br><span class="line"><span class="string">    About:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># One multi-head attention + one feed-forward</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, n_head, dim_feedforward, dropout = <span class="number">0.1</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.self_attn = MultiheadAttention(d_model, n_head)</span><br><span class="line">        self.norm_1 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="comment"># Implementation of Feedforward model(Two linear transformation together with one dropout)</span></span><br><span class="line">        self.linear1 = nn.Linear(d_model, dim_feedforward, )</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.linear2 = nn.Linear(dim_feedforward, d_model)</span><br><span class="line">        self.norm_2 = nn.LayerNorm(d_model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__ff_block</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># feed forward layer contains two linear</span></span><br><span class="line">        out = F.relu(self.linear1(x))</span><br><span class="line">        out = self.dropout(out)</span><br><span class="line">        out = self.linear2(out)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src, src_mask</span>):</span></span><br><span class="line">        out = src</span><br><span class="line">        out = self.norm_1(out + self.self_attn(out, src_mask))<span class="comment"># 这里在pytorch的官方实现中在self_attention后还加了一个dropout</span></span><br><span class="line">        out = self.norm_2(out + self.__ff_block(out))</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h3 id="self-attention">self attention</h3>
<p>我一开始查阅的资料是<a href="https://jalammar.github.io/illustrated-transformer/">illustrated-transformer</a>,
这个博客内没有具体的实现。后来我参考的是lilian
wen的tensorflow实现。在lilian的实现里对于multihead
attention是这样写的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multihead_attention</span>(<span class="params">self, query, memory=<span class="literal">None</span>, mask=<span class="literal">None</span>, scope=<span class="string">&#x27;attn&#x27;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            query (tf.tensor): of shape (batch, q_size, d_model)</span></span><br><span class="line"><span class="string">            memory (tf.tensor): of shape (batch, m_size, d_model)</span></span><br><span class="line"><span class="string">            mask (tf.tensor): shape (batch, q_size, k_size)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:h</span></span><br><span class="line"><span class="string">            a tensor of shape (bs, q_size, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> memory <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            memory = query</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># Linear project to d_model dimension: [batch, q_size/k_size, d_model]</span></span><br><span class="line">            Q = tf.layers.dense(query, self.d_model, activation=tf.nn.relu)</span><br><span class="line">            K = tf.layers.dense(memory, self.d_model, activation=tf.nn.relu)</span><br><span class="line">            V = tf.layers.dense(memory, self.d_model, activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Split the matrix to multiple heads and then concatenate to have a larger</span></span><br><span class="line">            <span class="comment"># batch size: [h*batch, q_size/k_size, d_model/num_heads]</span></span><br><span class="line">            Q_split = tf.concat(tf.split(Q, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            K_split = tf.concat(tf.split(K, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            V_split = tf.concat(tf.split(V, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            mask_split = tf.tile(mask, [self.h, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Apply scaled dot product attention</span></span><br><span class="line">            out = self.scaled_dot_product_attention(Q_split, K_split, V_split, mask=mask_split)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Merge the multi-head back to the original shape</span></span><br><span class="line">            out = tf.concat(tf.split(out, self.h, axis=<span class="number">0</span>), axis=<span class="number">2</span>)  <span class="comment"># [bs, q_size, d_model]</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># The final linear layer and dropout.</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_model)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dropout(out, rate=self.drop_rate, training=self._is_training)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>以上的实现其实和博客内的内容有点相左，博客写的是：</p>
<blockquote>
<p>As we’ll see next, with multi-headed attention we have not only one,
but multiple sets of Query/Key/Value weight matrices (the Transformer
uses eight attention heads, so we end up with eight sets for each
encoder/decoder). Each of these sets is randomly initialized. Then,
after training, each set is used to project the input embeddings (or
vectors from lower encoders/decoders) into a different representation
subspace.</p>
</blockquote>
<p>结合作者给出的图片：</p>
<figure>
<img src="/2024/01/02/Transformer%E6%8B%86%E8%A7%A3/image-20240104131802468.png" alt="image-20240104131802468">
<figcaption aria-hidden="true">image-20240104131802468</figcaption>
</figure>
<p>我一开始的理解是每一个head都有一份单独的W
sets（WQ,WK,WV）。每一个head经过了scaled attention的计算</p>
<figure>
<img src="/2024/01/02/Transformer%E6%8B%86%E8%A7%A3/image-20240104132257007.png" alt="image-20240104132257007">
<figcaption aria-hidden="true">image-20240104132257007</figcaption>
</figure>
<p>得到的Z的shape都是<code>(batch, seq_len, embeded_size)</code>，所以才会有WO这个线性变化（blog里说的）：</p>
<figure>
<img src="/2024/01/02/Transformer%E6%8B%86%E8%A7%A3/image-20240104132406484.png" alt="image-20240104132406484">
<figcaption aria-hidden="true">image-20240104132406484</figcaption>
</figure>
<p>但我看完代码之后发现并不是我想的那样。我觉得这篇博客写的有点问题。后来又找到了一篇<a href="https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853">博客</a>，能够解答我的疑问。它最重要的话是：</p>
<blockquote>
<p>However, the important thing to understand is that this is a logical
split only. The Query, Key, and Value are not physically split into
separate matrices, one for each Attention head. A single data matrix is
used for the Query, Key, and Value, respectively, with logically
separate sections of the matrix for each Attention head. Similarly,
there are not separate Linear layers, one for each Attention head. All
the Attention heads share the same Linear layer but simply operate on
their ‘own’ logical section of the data matrix.</p>
</blockquote>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>decoding strategy in NG tasks</title>
    <url>/2023/04/21/decoding-strategy-in-NG-tasks/</url>
    <content><![CDATA[<p>在Neural Language
Generation的任务中，如何在每一个时间步产生一个token称为decoding
method.最常见的decoding方法就是用softmax激活函数计算概率分布之后，将概率最大的那个token列为当前的预测值（most
likely string）。这种argmax的方式在machine
translation等non-open-ended的任务中表现还可以，但是如果是在纯粹的开放性的任务中，比如写一首诗歌。这种方式会造成重复性的输出，并且输出的句子有时候连续性也比较差。</p>
<p>这时候很多研究工作就对decoding strategic展开了研究，比如beam
search，基本原理很简单，就是从取概率排名前k个token作为当前预测值。但是这还是会有一个问题，就是当我们概率分布均衡的时候，这个方法可以，但如果概率分布不均衡，也就是softmax计算出来的概率值只有几个token的概率比较大，其他概率都非常小，比如零点几，那这个时候其实我们不太需要考虑k个单词，只需要考虑概率比较大的那些tokens就够了。</p>
<h2 id="top-pnucleus-sampling">Top-p（Nucleus Sampling）</h2>
<p>所以这时候就有人提出了Top-p(nucleus) sampling的方法<a href="http://arxiv.org/abs/1904.09751">The Curious Case of Neural Text
Degeneration</a></p>
<p>具体做法就是维护一个动态的k值，这个k值随着softmax的输出概率分布决定，</p>
<figure>
<img src="/2023/04/21/decoding-strategy-in-NG-tasks/image-20230421134555223.png" alt="image-20230421134555223">
<figcaption aria-hidden="true">image-20230421134555223</figcaption>
</figure>
<p>作者的思路就是当概率分布比较flat的时候，我们应该把sample的池子定的大一点。但是当概率分布比较陡峭，也就是上图中的第三种情况时，我们就需要把这个sample的池子变得小一点。具体是如何操作的呢？</p>
<figure>
<img src="/2023/04/21/decoding-strategy-in-NG-tasks/image-20230421134938176.png" alt="image-20230421134938176">
<figcaption aria-hidden="true">image-20230421134938176</figcaption>
</figure>
<figure>
<img src="/2023/04/21/decoding-strategy-in-NG-tasks/image-20230421134952769.png" alt="image-20230421134952769">
<figcaption aria-hidden="true">image-20230421134952769</figcaption>
</figure>
<p>看公式可能会比较复杂，具体做法就是提前预设一个阈值p，然后对于某个时间步的概率分布P，我们寻找一个最小的top-p的一个token池子，这个池子里所有的token的概率值加起来要大于p，注意这里是找一个smallest
set，也就是我们在找这个token池子时，从概率最高的token往下捋，一直到概率和大于p。找到这个池子之后，我们将原来经过softmax函数计算之后的概率分布按照上图中（3）的公式重新计算得出一个新的概率分布。最终我们得到的概率分布不属于我们之前找的token池子里的token的概率全部置为0，至于在这个token池子里的token的概率值会除以这个池子里所有token概率值的<strong>和</strong>。然后我们从这个分布中去sample我们的预测token。</p>
<h2 id="sampling-with-temperature">Sampling with Temperature</h2>
<p>Temperature这个概念在GPT中也有，不同的温度值，你会得到不同的结果。温度值越低，它会对自己的输出结果更自信，而温度值越高，它会降低模型的确信值，也就会返回更多的结果给你。<a href="https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277">Blog</a>对上面提到的<a href="http://arxiv.org/abs/1904.09751">The Curious Case of Neural Text
Degeneration</a>文章进行了解答，但我发现有一点和paper中不一样的是：</p>
<figure>
<img src="/2023/04/21/decoding-strategy-in-NG-tasks/image-20230421144452222.png" alt="image-20230421144452222">
<figcaption aria-hidden="true">image-20230421144452222</figcaption>
</figure>
<p>temperature值t并不是取值是<code>[0,1)</code>，stanford224n的课件以及博客里对t的取值是可以大于1的：</p>
<figure>
<img src="/2023/04/21/decoding-strategy-in-NG-tasks/image-20230421144619190.png" alt="image-20230421144619190">
<figcaption aria-hidden="true">image-20230421144619190</figcaption>
</figure>
<p>在博客中我们可以看到作者对于t值大于1和小于1画出的图的区别：</p>
<figure>
<img src="/2023/04/21/decoding-strategy-in-NG-tasks/image-20230421144711636.png" alt="image-20230421144711636">
<figcaption aria-hidden="true">image-20230421144711636</figcaption>
</figure>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Sigmoid和Softmax使用区别（Tensorflow）</title>
    <url>/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/</url>
    <content><![CDATA[<p>一直都对这两个函数的一些概念理不清楚，今天就整理一下，结合吴恩达老师和李沐给出的Tensorflow的coding过程，一并整理厘清概念和这两者的区别。</p>
<h1 id="sigmoid">Sigmoid</h1>
<p>Sigmoid通常用于逻辑回归Logistic
Regression的二分类中，output出一个概率值（比如：预测一张图片是一只猫的概率）。它的公式为：
<span class="math display">\[
\frac{1}{1+e^{-z}}
\]</span> 图示为:</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/Sigmoid_function_01.png"></p>
<p>特点：y的值是介于[0,1]之间的，而x是负无穷到正无穷的。x=0时，y=0.5。</p>
<p>LR的网络结构为:</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117112204836.png"></p>
<p>在将Sigmoid用于LR任务中时，模型的输入是一个特征向量X，这时的loss
function使用：</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117104751440.png"></p>
<p>这里<strong>不会</strong>使用MSE，原因是：</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117104923278.png"></p>
<p>loss
function是在单个训练样本中定义的，它衡量的是算法在单个训练样本中表现如何，为了衡量算法在全部训练样本上的表现，需要定义代价函数（cost
function），在LR中代价函数即为对m个样本的损失函数求和再平均：</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117105155751.png"></p>
<p>虽然有这么多计算步骤，但是在tensorflow中，我们并不需要自己计算sigmoid后的值a，然后将a和y放到J函数中计算中整体的cost，只需要一个函数就可以帮助我们实现。</p>
<p><code>tf.nn.sigmoid_cross_entropy_with_logits(logits=z,labels=y)</code></p>
<blockquote>
<p>注意：上述的z是before the final sigmoid
activation的值，也就是还没有传入Sigmoid前，只是经过线性计算后的值。</p>
</blockquote>
<h1 id="softmax">Softmax</h1>
<p>softmax函数是对sigmoid的推广，用于处理<strong>多分类</strong>的问题。公式：</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117111137512.png"></p>
<p><strong>它的输入是一个向量，输出也是一个向量</strong>。不同于Sigmoid的输入（实数），输出（介于0,1之间的概率值）。Sigmoid的输出是一个向量，其中
向量中的每一个元素的范围都在(0,1)之间，它能将一个含任意<em>实数</em>的K维向量压缩到另一个K维向量中。</p>
<p>它在线性模型中的应用方式为：对接最后一层，输出一个向量。</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117111549950.png"></p>
<p>它的loss function是:</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117113151953.png"></p>
<p>其中q为输出向量y的维度。</p>
<p>那么它的cost function J应该是将整个训练集的损失总和:
通常叫做<strong>cross-entropy loss交叉熵损失函数</strong></p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117113353952.png"></p>
<p>在tensorflow中该过程只需要一个函数来实现:</p>
<p><code>tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))</code></p>
<blockquote>
<ul>
<li>其中logits也是传入softmax激活函数之前的结果，也就是经过线性计算之后的Z</li>
<li>logits和labels也必须是相同的shape （num of examples,
num_classes）</li>
</ul>
</blockquote>
<p>用tensorflow的完整实现即为:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span>(<span class="params">Z3, Y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)</span></span><br><span class="line"><span class="string">    Y -- &quot;true&quot; labels vector placeholder, same shape as Z3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - Tensor of the cost function</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>    </span><br><span class="line">    <span class="comment"># to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)</span></span><br><span class="line">    logits = tf.transpose(Z3)</span><br><span class="line">    labels = tf.transpose(Y)  </span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment"># Do the training loop</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line"></span><br><span class="line">        epoch_cost = <span class="number">0.</span>                       <span class="comment"># Defines a cost related to an epoch</span></span><br><span class="line">        num_minibatches = <span class="built_in">int</span>(m / minibatch_size) <span class="comment"># number of minibatches of size minibatch_size in the train set</span></span><br><span class="line">        seed = seed + <span class="number">1</span></span><br><span class="line">        minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Select a minibatch</span></span><br><span class="line">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line"></span><br><span class="line">            <span class="comment"># IMPORTANT: The line that runs the graph on a minibatch.</span></span><br><span class="line">            <span class="comment"># Run the session to execute the &quot;optimizer&quot; and the &quot;cost&quot;, the feedict should contain a minibatch for (X,Y).</span></span><br><span class="line">            _ , minibatch_cost = sess.run([optimizer, cost], feed_dict=&#123;X: minibatch_X, Y: minibatch_Y&#125;)</span><br><span class="line"></span><br><span class="line">            epoch_cost += minibatch_cost / num_minibatches</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Print the cost every epoch</span></span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="literal">True</span> <span class="keyword">and</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span> (<span class="string">&quot;Cost after epoch %i: %f&quot;</span> % (epoch, epoch_cost))</span><br><span class="line">                <span class="keyword">if</span> print_cost == <span class="literal">True</span> <span class="keyword">and</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                    costs.append(epoch_cost)    </span><br></pre></td></tr></table></figure>
<p>以上的实现是将里面的计算步骤都展示出来的实现，也就是我们得到Z之后再进一步的得到loss。在李沐的教程中，用tensorflow实现softmax回归使用了更高级的API来实现。</p>
<h2 id="tensorflow-keras模块对softmax更简洁的实现">tensorflow
keras模块对softmax更简洁的实现</h2>
<p>softmax回归的输出层是一个全连接层。因此，为了实现我们的模型，我们只需在<code>Sequential</code>中添加一个带有10个输出的全连接层。同样，在这里，<code>Sequential</code>并不是必要的，但我们可能会形成这种习惯。因为在实现深度模型时，<code>Sequential</code>将无处不在。我们仍然以均值0和标准差0.01随机初始化权重。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = tf.keras.models.Sequential()</span><br><span class="line">net.add(tf.keras.layers.Flatten(input_shape=(<span class="number">28</span>, <span class="number">28</span>)))</span><br><span class="line">weight_initializer = tf.keras.initializers.RandomNormal(mean=<span class="number">0.0</span>, stddev=<span class="number">0.01</span>)</span><br><span class="line">net.add(tf.keras.layers.Dense(<span class="number">10</span>, kernel_initializer=weight_initializer))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>以上实现的输入是28*28大小的灰度图片，分类类别数为10。</p>
</blockquote>
<p>在这里，我们使用学习率为0.1的小批量随机梯度下降作为优化算法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)</span><br><span class="line">trainer = tf.keras.optimizers.SGD(learning_rate=.1)</span><br><span class="line">num_epochs = 10</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo图片插入问题：在主页和文章详情页同时显示图片</title>
    <url>/2021/11/17/hexo%E5%9B%BE%E7%89%87%E6%8F%92%E5%85%A5%E9%97%AE%E9%A2%98%EF%BC%9A%E5%9C%A8%E4%B8%BB%E9%A1%B5%E5%92%8C%E6%96%87%E7%AB%A0%E8%AF%A6%E6%83%85%E9%A1%B5%E5%90%8C%E6%97%B6%E6%98%BE%E7%A4%BA%E5%9B%BE%E7%89%87/</url>
    <content><![CDATA[<h1 id="设置静态文件根目录">设置静态文件根目录</h1>
<p><code>_config.yml</code>中有个<code>url</code>和<code>root</code>参数
如果你部署的地址是<code>http://yoursite.com/child</code>，需要设置下面两个参数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">url: &#x27;http://yoursite.com/child&#x27; // 部署的域名</span><br><span class="line">root: &#x27;/child/&#x27; // 部署的根目录</span><br><span class="line">12</span><br></pre></td></tr></table></figure>
<h1 id="设置资源文件夹">设置资源文件夹</h1>
<p>资源（Asset）代表 source
文件夹中除了文章以外的所有文件，例如图片、CSS、JS
文件等。比方说，如果你的Hexo项目中只有少量图片，那最简单的方法就是将它们放在
<code>source/images</code> 文件夹中。然后通过类似于
<code>![](/images/image.jpg)</code> 的方法访问它们。</p>
<p>&lt; !--more--&gt;</p>
<p>文章资源文件夹
对于那些想要更有规律地提供图片和其他资源以及想要将他们的资源分布在各个文章上的人来说，Hexo也提供了更组织化的方式来管理资源。这个稍微有些复杂但是管理资源非常方便的功能可以通过<strong>将
<code>config.yml</code> 文件中的 <code>post_asset_folder</code> 选项设为
true 来打开</strong>。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">_config.yml</span><br><span class="line">post_asset_folder: true</span><br><span class="line">12</span><br></pre></td></tr></table></figure>
<p>当资源文件管理功能打开后，Hexo将会在你<strong>每一次通过
<code>hexo new [layout] &lt;title&gt;</code>
命令创建新文章时自动创建一个文件夹</strong>。这个资源文件夹将会有与这个
<code>markdown</code>
文件一样的名字。将所有与你的文章有关的资源放在这个关联文件夹中之后，你可以通过相对路径来引用它们，这样你就得到了一个更简单而且方便得多的工作流。</p>
<p>相对路径引用的标签插件 通过常规的 markdown
语法和相对路径来引用图片和其它资源可能会导致它们在存档页或者主页上显示不正确。在Hexo
2时代，社区创建了很多插件来解决这个问题。但是，随着Hexo 3
的发布，许多新的标签插件被加入到了核心代码中。这使得你可以更简单地在文章中引用你的资源。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% asset_path slug %&#125;</span><br><span class="line">&#123;% asset_img slug [title] %&#125;</span><br><span class="line">&#123;% asset_link slug [title] %&#125;</span><br><span class="line">123</span><br></pre></td></tr></table></figure>
<p>比如说：当你打开文章资源文件夹功能后，你把一个
<code>example.jpg</code>
图片放在了你的资源文件夹中，如果通过使用相对路径的常规 markdown 语法
<code>![](/example.jpg)</code> ，它将 不会
出现在首页上。（但是它会在文章中按你期待的方式工作）</p>
<p>正确的引用图片方式是使用下列的标签插件而不是 markdown ：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% asset_img example.jpg avatar %&#125;</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<p><strong>通过这种方式，图片将会同时出现在文章和主页以及归档页中。通过<code>&#123;% asset_img 图片名称 图片说明 %&#125;</code></strong></p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>git常用命令总结</title>
    <url>/2022/01/13/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>github官网给出的教程挺通俗易懂的，移步[https://docs.github.com/cn/get-started/using-git]</p>
<p>本地配置ssh和github，参考官方文档[https://docs.github.com/cn/authentication/connecting-to-github-with-ssh/about-ssh]</p>
<h1 id="远程仓库使用">远程仓库使用</h1>
<p>在本地设置推送到远程仓库的用户名:[https://docs.github.com/cn/get-started/getting-started-with-git/setting-your-username-in-git]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git config --global user.name &quot;your name&quot;</span><br></pre></td></tr></table></figure>
<p>远程URL是 Git 一种指示“您的代码存储位置”的绝佳方式，您只能推送到两类
URL 地址：</p>
<ul>
<li>HTTPS URL，如 <code>https://github.com/user/repo.git</code></li>
<li>SSH URL，如 <code>git@github.com:user/repo.git</code></li>
</ul>
<p>Git 将远程 URL 与名称相关联，您的默认远程通常名为
<code>origin</code></p>
<p>创建远程仓库，并将其命名为master</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote add master &lt;REMOTE_URL&gt; </span><br></pre></td></tr></table></figure>
<p>如果后面想更改url，使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote set-url origin &lt;new_url&gt;</span><br></pre></td></tr></table></figure>
<p>查看远程仓库设置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote -v</span><br></pre></td></tr></table></figure>
<p>重命名远程仓库</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote rename origin destination</span><br></pre></td></tr></table></figure>
<p>删除远程仓库</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote rm destination</span><br></pre></td></tr></table></figure>
<p>推送提交至远程仓库，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git push &lt;REMOTENAME&gt; &lt;LOCALBRANCHNAME&gt;:&lt;REMOTEBRANCHNAME&gt; </span><br></pre></td></tr></table></figure>
<p>拉取某远程仓库距离上一次抓取之后的工作：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git fetch &lt;remote_name&gt;</span><br><span class="line">必须注意 git fetch 命令只会将数据下载到你的本地仓库——它并不会自动合并或修改你当前的工作。 当准备好时你必须手动将其合并入你的工作。</span><br></pre></td></tr></table></figure>
<p>显示某远程仓库的信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote show &lt;remote_name&gt;</span><br></pre></td></tr></table></figure>
<h1 id="忽略文件">忽略文件</h1>
<p><a href="https://docs.github.com/cn/get-started/getting-started-with-git/ignoring-files">见文档</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">touch .gitignore #该命令在项目根目录会创建一个.gitignore文件,然后往该文件中填东西</span><br></pre></td></tr></table></figure>
<p>如果有些我们不需要跟踪的文件已经提交到了暂存区，那么使用下面的命令来删除暂存区的该文件，再将该文件写入.gitignore文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git rm --cached FILENAME 该命令直接将暂存区的那个版本删除了</span><br><span class="line">或者 git restore --staged &lt;file&gt; 该命令会用暂存区的代码覆盖掉工作区的代码</span><br></pre></td></tr></table></figure>
<h1 id="在自己的project中添加别人的project">在自己的project中添加别人的project</h1>
<p><a href="https://devconnected.com/how-to-add-and-update-git-submodules/" class="uri">https://devconnected.com/how-to-add-and-update-git-submodules/</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git submodule add &lt;remote_url&gt; &lt;destination_folder&gt;</span><br><span class="line"></span><br><span class="line">git commit -m &quot;Added the submodule to the project.&quot;</span><br><span class="line"></span><br><span class="line">git push</span><br><span class="line"></span><br><span class="line">git submodule update --init --recursive # 如果想要拉取别人仓库里的submodule到本地</span><br></pre></td></tr></table></figure>
<h1 id="分支">分支</h1>
<p>创建分支：当执行git init时，默认创建名字是master的分支</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git branch &lt;branch_name&gt;</span><br></pre></td></tr></table></figure>
<p>切换到某分支:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git checkout &lt;branch_name&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>分支切换会改变你工作目录中的文件。在切换分支时，一定要注意你工作目录里的文件会被改变。
如果是切换到一个较旧的分支，你的工作目录会恢复到该分支最后一次提交时的样子。
如果 Git 不能干净利落地完成这个任务，它将禁止切换分支。</p>
</blockquote>
<p>上面两条命令可以使用一条命令搞定：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git checkout -b &lt;branch_name&gt;</span><br></pre></td></tr></table></figure>
<p>在一条分支上，比如hotfix修改一些文件提交后，需要回到master分支并将hotfix上的分支的修改合并到master</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git merge hotfix</span><br></pre></td></tr></table></figure>
<p>这是一个Fast-forward。合并完之后master会和hotfix指向同一个位置，这时可以删除hotfix这个分支：<code>git branch -d hotfix</code></p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>image classification models总结</title>
    <url>/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>本博客旨在记录自己在了解<code>image classification</code>这个术语<code>computer vision</code>的一个子任务中常见的模型。耳熟能详的就是<code>ResNet</code>,<code>VGG</code>,<code>Inception</code>,<code>MobileNet</code>,
<code>Efficientnet</code>。每一个模型之间有什么区别，他们自身又有哪些变种，比如<code>VGG</code>，它拥有<code>VGG16,VGG19</code>等，<code>ResNet</code>又有很多，单是查看<code>Tensorflow</code>的官方文档就会发现在<code>tf.keras.applications</code>模块下，就有很多模型架构可选（也都有预训练参数）。整理这个博客的目的在于让自己对这些模型之间的差别有所了解，这样在不同的任务中才会知道使用什么样的模型架构来handle自己的数据。</p>
<p>在整理这篇博客的过程中，我也去搜了有没有<code>image classification</code>这个单任务上的<code>review</code>文章，文章都挺多的，筛选之后推荐这篇<a href="https://www.mdpi.com/2072-4292/13/22/4712">Review of Image
Classification Algorithms Based on Convolutional Neural
Networks</a>.这篇文章主要是介绍基于CNN的一些模型，共有三个章节。重点是第二章节梳理了<code>CNN-based</code>的一些模型，包括本文想要<code>cover</code>的<code>VGG</code>，<code>inception</code>，<code>resnet</code>，<code>mobilenet</code>。重点关注图像分类算法的小伙伴可以通读一下这篇文章。</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/image-20230207094857739-16757345394748.png" alt="Review of Image Classification Algorithms Based on Convolutional Neural Networks第二章节目录">
<figcaption aria-hidden="true">Review of Image Classification Algorithms
Based on Convolutional Neural Networks第二章节目录</figcaption>
</figure>
<h1 id="vgg">VGG</h1>
<p>首次提出在2014年的<a href="https://arxiv.org/abs/1409.1556">paper</a></p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/vgg16.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>上图中包含13个卷积层和3个全连接层，是<code>VGG16</code>的结构。而<code>VGG19</code>包含了16个卷积层和3个全连接层：</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/1dNYBNBDP7ZvckfOSYzHxIw.jpeg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><code>VGG</code>系列就是<code>VGG16</code>和<code>VGG19</code>，两者的区别在于19用了更多的卷积层。<code>Tensorflow</code>也提供了这两个模型的黑盒子实现供大家使用。</p>
<h1 id="resnet">ResNet</h1>
<p>首次提出在2016年的<a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">paper</a>，其中最重要的就是网络中的<code>residual block</code>:</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/1_nmPcwwnsHE-AC69ASkj9w.jpeg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>在作者的原文中我们可以发现，文章中提出的Resnet是34层，也就是ResNet34。在具体实现的时候，作者在每一个卷积操作之后（激活函数之前）加上了batch
Normalization。在<code>Module: tf.keras.applications.resnet</code><a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet">Tensorflow
applications
resnet</a>中现在只有ResNet101，152，50三个版本，其中ResNet50和ResNet34的区别在于：前者使用三个卷积一个block，后者是2个卷积一个block.
ResNet50表现更优异。ResNet101和ResNet152在一个block内使用了更多的卷积layer。</p>
<p>以上所说的都是resnet v1，后来同一个作者又发表了<a href="https://arxiv.org/pdf/1603.05027.pdf">Identity Mappings in Deep
Residual Networks</a>,提出了ResNet
v2。同样我们在tensorflow中也可以看到模块<code>Module: tf.keras.applications.resnet_v2</code>，同样的也有50，101，152三个版本的model。</p>
<p>v1和v2的区别在于：</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/image-20230206150618805-16756671800395.png" alt="ResNet v1 and ResNet v2">
<figcaption aria-hidden="true">ResNet v1 and ResNet v2</figcaption>
</figure>
<p>以上只是概念上的解释，看代码会更合适一点，其中<code>Deep Residual Learning for Image Recognition</code>文章中也给出了34，50，101，152等几个模型在实现中注意的细节：</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/image-20230206153053177-16756686549546.png" alt="image-20230206153053177">
<figcaption aria-hidden="true">image-20230206153053177</figcaption>
</figure>
<p>ResNet34 V1中，一个resnet
block是由两个卷积layer组成的，同时它和V2的一个区别就在于X进来后就先进行卷积运算，也就是上图中的weight</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras.activations <span class="keyword">import</span> relu</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers <span class="keyword">as</span> Layers</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResBlock</span>(<span class="params">Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, channels, stride=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ResBlock, self).__init__(name=<span class="string">&#x27;ResBlock&#x27;</span>)</span><br><span class="line">        self.flag = (stride != <span class="number">1</span>)</span><br><span class="line">        self.conv1 = Conv2D(channels, <span class="number">3</span>, stride, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.bn1 = BatchNormalization()</span><br><span class="line">        self.conv2 = Conv2D(channels, <span class="number">3</span>, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.bn2 = BatchNormalization()</span><br><span class="line">        self.relu = ReLU()</span><br><span class="line">        <span class="keyword">if</span> self.flag:</span><br><span class="line">            self.bn3 = BatchNormalization()</span><br><span class="line">            self.conv3 = Conv2D(channels, <span class="number">1</span>, stride)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x1 = self.conv1(x)</span><br><span class="line">        x1 = self.bn1(x1)</span><br><span class="line">        x1 = self.relu(x1)</span><br><span class="line">        x1 = self.conv2(x1)</span><br><span class="line">        x1 = self.bn2(x1)</span><br><span class="line">        <span class="keyword">if</span> self.flag:</span><br><span class="line">            x = self.conv3(x)</span><br><span class="line">            x = self.bn3(x)</span><br><span class="line">        x1 = Layers.add([x, x1])</span><br><span class="line">        x1 = self.relu(x1)</span><br><span class="line">        <span class="keyword">return</span> x1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet34</span>(<span class="params">Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ResNet34, self).__init__(name=<span class="string">&#x27;ResNet34&#x27;</span>)</span><br><span class="line">        self.conv1 = Conv2D(<span class="number">64</span>, <span class="number">7</span>, <span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.bn = BatchNormalization()</span><br><span class="line">        self.relu = ReLU()</span><br><span class="line">        self.mp1 = MaxPooling2D(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.conv2_1 = ResBlock(<span class="number">64</span>)</span><br><span class="line">        self.conv2_2 = ResBlock(<span class="number">64</span>)</span><br><span class="line">        self.conv2_3 = ResBlock(<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">        self.conv3_1 = ResBlock(<span class="number">128</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv3_2 = ResBlock(<span class="number">128</span>)</span><br><span class="line">        self.conv3_3 = ResBlock(<span class="number">128</span>)</span><br><span class="line">        self.conv3_4 = ResBlock(<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">        self.conv4_1 = ResBlock(<span class="number">256</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv4_2 = ResBlock(<span class="number">256</span>)</span><br><span class="line">        self.conv4_3 = ResBlock(<span class="number">256</span>)</span><br><span class="line">        self.conv4_4 = ResBlock(<span class="number">256</span>)</span><br><span class="line">        self.conv4_5 = ResBlock(<span class="number">256</span>)</span><br><span class="line">        self.conv4_6 = ResBlock(<span class="number">256</span>)</span><br><span class="line"></span><br><span class="line">        self.conv5_1 = ResBlock(<span class="number">512</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv5_2 = ResBlock(<span class="number">512</span>)</span><br><span class="line">        self.conv5_3 = ResBlock(<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">        self.pool = GlobalAveragePooling2D()</span><br><span class="line">        self.fc1 = Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.dp1 = Dropout(<span class="number">0.5</span>)</span><br><span class="line">        self.fc2 = Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.dp2 = Dropout(<span class="number">0.5</span>)</span><br><span class="line">        self.fc3 = Dense(<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.mp1(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2_1(x)</span><br><span class="line">        x = self.conv2_2(x)</span><br><span class="line">        x = self.conv2_3(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv3_1(x)</span><br><span class="line">        x = self.conv3_2(x)</span><br><span class="line">        x = self.conv3_3(x)</span><br><span class="line">        x = self.conv3_4(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv4_1(x)</span><br><span class="line">        x = self.conv4_2(x)</span><br><span class="line">        x = self.conv4_3(x)</span><br><span class="line">        x = self.conv4_4(x)</span><br><span class="line">        x = self.conv4_5(x)</span><br><span class="line">        x = self.conv4_6(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv5_1(x)</span><br><span class="line">        x = self.conv5_2(x)</span><br><span class="line">        x = self.conv5_3(x)</span><br><span class="line"></span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.dp1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.dp2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = ResNet34()</span><br><span class="line">model.build(input_shape=(<span class="number">1</span>, <span class="number">480</span>, <span class="number">480</span>, <span class="number">3</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<h1 id="inception">Inception</h1>
<p>已经有不少博客在科普Inception系列模型的区别<a href="https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202">A
Simple Guide to the Versions of the Inception Network</a></p>
<p>首先提出该模型的是2014年的<a href="https://arxiv.org/pdf/1409.4842v1.pdf">Going deeper with
convolutions</a> Inception
V1（GoogleNet）,然后又分别有了好几个变体：<code>Inception V2，Inception V3，Inception V4</code>，<a href="http://arxiv.org/abs/1602.07261">Inception-ResNet-v2</a>。和<code>ResNet</code>一样，<code>Inception</code>网络中一个重要的<code>module</code>是<code>Inception Module</code>：</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/image-20230206161239990-16756711610527.png" alt="Inception Module">
<figcaption aria-hidden="true">Inception Module</figcaption>
</figure>
<p>其中这些Network中被广泛使用的是<a href="https://arxiv.org/pdf/1512.00567v3.pdf">Inception_v3</a>和<a href="https://arxiv.org/pdf/1602.07261.pdf">Inception-ResNet-v2</a>.</p>
<h1 id="mobilenet">MobileNet</h1>
<blockquote>
<p>The idea behind MobileNet is to use depthwise separable convolutions
to build loghter deep neural networks. In regular convolutional layer,
the convolution kernel or filter is applied to all of the channels of
the input image, by doing weighted sum of the input pixels with the
filter and then slides to the next input pixels across the
images.MobileNet uses this regular convolution only in the first layer.
The next layers are the depthwise separable convolutions which are the
combination of the depthwise and pointwise convolution. The depthwise
convolution does the convolution on each channel separately. If the
image has three channels, therefore, the output image also has three
channels. This depthwise convolution is used to filter the input
channels. The next step is the pointwise convolution, which is similar
to the regular convolution but with a 1x1 filter. The purpose of
pointwise convolution is to merge the output channels of the depthwise
convolution to create new features. By doing so, the computational work
needed to be done is less than the regular convolutional networks.</p>
<p>引用自<a href="https://medium.com/@fransiska26/the-differences-between-inception-resnet-and-mobilenet-e97736a709b0">the-differences-between-inception-resnet-and-mobilenet</a></p>
</blockquote>
<p><code>MobileNet</code>使用了两种卷积形式，<code>depthwise</code>和<code>pointwise</code>，后者就是我们常见的卷积操作，只是使用的是1✖1的卷积核，input
image有多少个<code>channel</code>，<code>filter</code>就会延展为几个<code>channel</code>，比如输入进来的<code>channel</code>数是3，那么一个<code>3*3</code>大小的filter就会extend成3✖3✖3的一个立方体，然后这27个数分别禹输入image对应的区域做乘积之后相加取和。但是<code>depthwise</code>卷积是对每一个<code>channel</code>分别做卷积，如果输入图片有三个<code>channel</code>，那么输出的也会是三个<code>channel</code>。如图：</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/image-20230207132331415.png" alt="depthwise convolution">
<figcaption aria-hidden="true">depthwise convolution</figcaption>
</figure>
<p><code>tensorflow</code>中有<code>DepthwiseConv2D</code>这个<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D">layer</a>,它对于<code>depthwise convolution</code>的解释是：</p>
<blockquote>
<p>Depthwise convolution is a type of convolution in which each input
channel is convolved with a different kernel (called a depthwise
kernel). You can understand depthwise convolution as the first step in a
depthwise separable convolution.</p>
</blockquote>
<p><code>MobileNetV2</code>主要引进了<code>Inverted residuals</code>和<code>linear bottlenecks</code>去解决在<code>depthwise</code>卷积操作中卷积核的参数往往是0的问题</p>
<h1 id="other-topics">other topics</h1>
<p>在阅读<a href="https://www.mdpi.com/2072-4292/13/22/4712">Review of
Image Classification Algorithms Based on Convolutional Neural
Networks</a>的最后一章节时，作者不仅介绍了现在research和industry领域用的比较多的image
classification的模型，也给出了各个模型在image-net数据集上的accuracy。在总结章，作者还提出了一些结论性的发现，我觉得蛮收益的，将文章的观点整理在这里。</p>
<ol type="1">
<li>2012年到2017年主要提供了日后用于分类的basic
CNN模型架构，这期间的模型架构有2012的alexnet，2014年的vgg，2014年的inception，2015年的resnet，2017年提出了attention加cnn的架构</li>
<li>attention加入到cnn之后形成了新的模型，也因此提高了模型的performance。现在很多模型会将SE
block嵌入到模型架构中去，我查了下这个SE
block是SEnet中的一个block，squeeze and excitation block。<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf">SEnet</a>是在2017年提出的，这个知识点待补充</li>
<li>超参数的选择对于CNN网络的performance影响很大，很多的工作在着力于减少超参数的个数以及replace
them with other composite coefficients。</li>
<li>手动设计一个performance很好的网络往往需要很多effort，<a href="https://en.wikipedia.org/wiki/Neural_architecture_search">NAS
search</a> （neural architecture search）可以让这个过程变得更简单</li>
<li>想要提升模型的performance，不仅仅需要将关注力放在模型架构的设计上，data
augmentation,transfer learning,training
strategies也可以帮助我们提高模型的准确度。在transfer learning上，paper：
Large Scale Learning of General Visual Representations for Transfer
总结了一些在不同的task上如何利用transfer
learning取得很好的performance的办法。</li>
</ol>
<hr>
<p>CNN model 还面临的挑战：</p>
<ol type="1">
<li>lightweight
models比如mobileNet系列的轻量级模型往往需要牺牲accuracy来提高efficiency。未来在embedded系统上，CNN的运行效率值得去explore</li>
<li>cnn模型在semi-supervised和unsupervised上的发挥不如NLP领域。</li>
</ol>
<hr>
<p>future directions:</p>
<ol type="1">
<li>重视vision transformer.
如何将卷积和transformer有效结合起来是当前的一个热点。目前的SOTA
network是 <a href="https://arxiv.org/abs/2106.04803">CoAtNet</a>，在image
net数据集上的accuracy是90.88，确实是目前在image
net数据集上performance最高的模型架构。值得读一下，mark！</li>
<li>有一些关于CNN的传统技术可能会成为阻碍CNN发展的重要因素，诸如：activation
function的选择，dropout，batch normalization。</li>
</ol>
<h1 id="senet-2017">SENet 2017</h1>
<p>原文 <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf">SEnet</a>，是由自动驾驶公司Momenta在2017年公布的一种全新的图像识别结构，它通过对特征通道间的相关性进行建模，把重要的特征进行强化来提升准确率。这个结构是2017
ILSVR竞赛的冠军，top5的错误率达到了2.251%，比2016年的第一名还要低25%，可谓提升巨大。</p>
<h1 id="coatnet">CoAtNet</h1>
<h1 id="reference">Reference</h1>
<ol type="1">
<li><a href="https://towardsdatascience.com/architecture-comparison-of-alexnet-vggnet-resnet-inception-densenet-beb8b116866d">Architecture
comparison of AlexNet, VGGNet, ResNet, Inception, DenseNet</a></li>
<li><a href="https://medium.com/analytics-vidhya/vggnet-architecture-explained-e5c7318aa5b6">VGGNet
Architecture Explained</a></li>
<li><a href="https://viso.ai/deep-learning/resnet-residual-neural-network/">resnet-residual-neural-network
Resnet系列网络架构的区别</a></li>
</ol>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>linux命令</title>
    <url>/2022/03/15/linux%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h1 id="tar-压缩以及解压">tar 压缩以及解压</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar -czvf test.tar.gz a.txt   //压缩 a.c文件为test.tar.gz</span><br><span class="line">tar -xzvf test.tar.gz // 解压</span><br><span class="line">tar -tzvf test.tar.gz //列出压缩文件的内容</span><br></pre></td></tr></table></figure>
<h1 id="linux中环境变量">linux中环境变量</h1>
<p>参考https://www.cjavapy.com/article/2250/</p>
<h2 id="介绍">1、介绍</h2>
<p>Linux中环境变量包括系统级和用户级</p>
<p>1）系统级</p>
<p><strong>/etc/environment</strong>：系统在登录时读取的第一个文件，用于为所有进程设置环境变量。系统使用此文件时并不是执行此文件中的命令，而是根据<code>KEY=VALUE</code>模式的代码，对<code>KEY</code>赋值以<code>VALUE</code>，因此文件中如果要定义<code>PATH</code>环境变量，只需加入类似如<code>PATH=$PATH:/xxx/bin</code>的代码即可。</p>
<p><strong>/etc/profile</strong>：是系统登录时执行的第二个文件，可以用于设定针对全系统所有用户的环境变量。该文件一般是调用<code>/etc/bash.bashrc</code>文件。</p>
<p><strong>/etc/bash.bashrc</strong>：系统级的<code>bashrc</code>文件，为每一个运行bash
shell的用户执行此文件。此文件会在用户每次打开shell时执行一次。</p>
<p><strong>注意</strong>：<code>/etc/environment</code>是设置整个系统的环境，而<code>/etc/profile</code>是设置所有用户的环境，前者与登录用户无关，后者与登录用户有关。
这两个文件修改后一般都要重启系统才能生效。</p>
<p>2）用户级</p>
<p><strong>~/.profile:</strong>
是对应当前登录用户的<code>profile</code>文件，用于定制当前用户的个人工作环境。</p>
<p>每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次。默认情况下，会设置一些环境变量，执行用户的<code>.bashrc</code>文件。</p>
<p><strong>~/.bashrc</strong>:
是对应当前登录用户的bash初始化文件，当用户每次打开shell时，系统都会执行此文件一次。通常设置环境变量修改这个文件。</p>
<p>上述配置文件执行先后顺序如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/etc/enviroment `–&gt; `/etc/profile` –&gt; `~/.profile` –&gt; `/etc/bash.bashrc `–&gt; `~/.bashrc</span><br></pre></td></tr></table></figure>
<h2 id="环境变量的作用">2、环境变量的作用</h2>
<p>环境变量相当于给系统或用户应用程序设置的一些参数，具体起什么作用这当然和具体的环境变量相关。比如<code>PATH</code>，是告诉系统，当要求系统运行一个程序而没有告诉它程序所在的完整路径时，系统除了在当前目录下面寻找此程序外，还应到哪些目录下去寻找；再如tc或vc++中，<code>set include=path1;path2</code>;
是告诉编译程序到哪里去找.h类型的文件；当然不仅仅是指定什么路径，还有其它的作用的，如<code>set dircmd=/4</code>
设置一个环境变量的作用是在使用dir命令时会把<code>/4</code>作为缺省的参数添加到你的dir命令之后，就像你的每个命令都加了/4参数，它实际上是给命令解释程序<code>command</code>设置的一个环境变量，并且是给<code>dir</code>这个内部命令。</p>
<h2 id="配置环境变量的方法">3、配置环境变量的方法</h2>
<p>1）临时环境变量</p>
<p>linux下设定环境变量时，如果只是临时用一下，可以直接在shell下用<code>set</code>或<code>export</code>命令设定环境变量。但是只能在当前shell环境下可以用，切换或关闭重新进入就会失效。具体配置方法，如下，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#终端输入：</span><br><span class="line">export MYSQLPATH=/home/mysql  #MYSQLPATH设置为该路径</span><br><span class="line">#终端查看一个特定环境变量包含的内容，比如，MYSQLPATH，PATH</span><br><span class="line">echo $PATH</span><br><span class="line">echo $MYSQLPATH</span><br></pre></td></tr></table></figure>
<p>2）永久环境变量</p>
<p>设置的环境变量，需要经常使用的，而不是临时使用，把上面的设置环境变量命令写到上面提到的相应配置文件中即可，则可以每次开机或打开shell时自动设置，</p>
<p><strong>例如，</strong></p>
<p>只需要当前用户生效的环境变量：</p>
<p>终端中输入：<code>sudo vi ~/.bashrc</code>，编辑这个文件，在其末尾添加：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export MYSQLPATH=/home/mysql:$MYSQLPATH</span><br><span class="line"># path采用:来分隔,冒号左右不需要空格.</span><br><span class="line"># :$MYSQLPATH在后面新添加的优先搜索，$MYSQLPATH:在前面说明新添加的最后搜索，不加代表新路径设置为MYSQLPATH路径。</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：在终端执行，<code>source ~/.bashrc</code>
，使其立即生效，或者重启电脑即可。</p>
<p>设置所有用户生效的环境变更：</p>
<p>终端中输入：<code>sudo vi /etc/profile</code>，编辑这个文件，在其末尾添加：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export MYSQLPATH=/home/mysql:$MYSQLPATH</span><br><span class="line"># path采用:来分隔,冒号左右不需要空格.</span><br><span class="line"># :$MYSQLPATH在后面新添加的优先搜索，$MYSQLPATH:在前面说明新添加的最后搜索，不加代表新路径设置为MYSQLPATH路径。</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：在终端执行，<code>source /etc/profile</code>
，使其立即生效，或者重启电脑即可。</p>
<h1 id="查看linux版本信息">查看linux版本信息</h1>
<p><a href="https://blog.csdn.net/lu_embedded/article/details/44350445" class="uri">https://blog.csdn.net/lu_embedded/article/details/44350445</a></p>
<h1 id="查看进程">查看进程</h1>
<p><a href="https://blog.csdn.net/lechengyuyuan/article/details/16337233" class="uri">https://blog.csdn.net/lechengyuyuan/article/details/16337233</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ps -ef # 查看所有本机进程</span><br><span class="line">ps -ef |grep python # 查看python进程</span><br><span class="line"></span><br><span class="line">kill -9 pid # 杀死某个进程</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/li528405176/article/details/83379164" class="uri">https://blog.csdn.net/li528405176/article/details/83379164</a></p>
<h1 id="将程序留在后台运行">将程序留在后台运行</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nohup python -u test.py &gt; out.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>
<p>参考 <a href="https://www.jianshu.com/p/4041c4e6e1b0" class="uri">https://www.jianshu.com/p/4041c4e6e1b0</a></p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>image segmentation(图像分割)中的IOU,Dice的计算</title>
    <url>/2022/08/22/image-segmentation-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E4%B8%AD%E7%9A%84IOU-Dice%E7%9A%84%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<p>在图像分割任务中，通常需要将loss设置成dice或者IOU的值，这里总结一下他们的使用方式：</p>
<h1 id="二分类问题mask只有0或者1">二分类问题（mask只有0或者1）</h1>
<p>这种task网络的最后一层通常会加sigmoid激活函数，比如unet实现中最后一层就是一个卷积层：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv10 = Conv2D(<span class="number">1</span>, (<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">&#x27;sigmoid&#x27;</span>)(conv9)</span><br></pre></td></tr></table></figure> 因此输出的feature
map中每一个pixel的值就是0~1之间的值。</p>
<p>dice loss可以如下计算： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> keras.losses <span class="keyword">import</span> binary_crossentropy</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line">SMOOTH = <span class="number">1.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dice_coef</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    y_true_f = K.flatten(y_true)</span><br><span class="line">    y_pred_f = K.flatten(y_pred)</span><br><span class="line">    intersection = K.<span class="built_in">sum</span>(y_true_f * y_pred_f)</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">2.</span> * intersection + SMOOTH) / (K.<span class="built_in">sum</span>(y_true_f) + K.<span class="built_in">sum</span>(y_pred_f) + SMOOTH)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iou_coef</span>(<span class="params">y_true, y_pred, smooth=<span class="number">1</span></span>):</span></span><br><span class="line">    intersection = K.<span class="built_in">sum</span>(K.<span class="built_in">abs</span>(y_true * y_pred), axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">    union = K.<span class="built_in">sum</span>(y_true,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])+K.<span class="built_in">sum</span>(y_pred,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])-intersection</span><br><span class="line">    iou = K.mean((intersection + smooth) / (union + smooth), axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> iou</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bce_dice_loss</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred) <span class="comment"># 这里也可以用 + (1-dice_coef)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(Adam(learning_rate=<span class="number">1e-4</span>),</span><br><span class="line">              bce_dice_loss,</span><br><span class="line">              metrics=[binary_crossentropy, dice_coef])</span><br></pre></td></tr></table></figure></p>
<h1 id="多分类问题mask有除了0和1以外的其他值">多分类问题（mask有除了0和1以外的其他值）</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">class_wise_metrics</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Computes the class-wise IOU and Dice Score.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    y_true (tensor) - ground truth label maps</span></span><br><span class="line"><span class="string">    y_pred (tensor) - predicted label maps</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  class_wise_iou = []</span><br><span class="line">  class_wise_dice_score = []</span><br><span class="line"></span><br><span class="line">  smoothing_factor = <span class="number">0.00001</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">    intersection = np.<span class="built_in">sum</span>((y_pred == i) * (y_true == i)) <span class="comment"># 计算true positive的pixel个数</span></span><br><span class="line">    y_true_area = np.<span class="built_in">sum</span>((y_true == i)) <span class="comment"># 计算pixcel=i的像素个数</span></span><br><span class="line">    y_pred_area = np.<span class="built_in">sum</span>((y_pred == i))</span><br><span class="line">    combined_area = y_true_area + y_pred_area</span><br><span class="line">    </span><br><span class="line">    iou = (intersection) / (combined_area - intersection + smoothing_factor)</span><br><span class="line">    class_wise_iou.append(iou)</span><br><span class="line">    </span><br><span class="line">    dice_score =  <span class="number">2</span> * ((intersection) / (combined_area + smoothing_factor))</span><br><span class="line">    class_wise_dice_score.append(dice_score)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> class_wise_iou, class_wise_dice_score</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在上述计算过程中需要注意的是，这里的y_pred是要经过np.argmax()的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results = model.predict(test_dataset, steps=test_steps)</span><br><span class="line"><span class="built_in">print</span>(results.shape) <span class="comment"># (192, 64, 84, 11)</span></span><br><span class="line"></span><br><span class="line">results = np.argmax(results, axis=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">integer_slider = <span class="number">105</span> <span class="comment"># 取第105个图片</span></span><br><span class="line">iou, dice_score = class_wise_metrics(np.argmax(y_true_segments[integer_slider], axis=<span class="number">3</span>), results[integer_slider]) </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>llm赋能的全自动Agents</title>
    <url>/2023/07/13/llm%E8%B5%8B%E8%83%BD%E7%9A%84%E5%85%A8%E8%87%AA%E5%8A%A8Agents/</url>
    <content><![CDATA[<p>这篇文章来源于liianwen的<a href="https://lilianweng.github.io/posts/2023-06-23-agent/">blog</a>,初看这篇博客时感觉太多新技术看不懂，再者今天突然看到新智元公众号发了一篇<a href="https://mp.weixin.qq.com/s/6gu_m739yOKhRBl_2rvWhg">文章</a>，乍一看特别熟悉，对比了下确实是完全照搬翻译，让人读起来一头雾水，不仅如此，跟原博客相比缺失了很多内容。</p>
<p>强烈建议先食用<a href="https://blog.salesforceairesearch.com/large-action-models/">blog</a>,
很通俗的讲解了LLM发展到现在成为Agents的原因，这里引用博客中的一句话：</p>
<blockquote>
<p>Recent months have seen the emergence of a powerful new trend in
which large language models are augmented to become “agents”—software
entities capable of performing tasks on their own, ultimately in the
service of a goal, rather than simply responding to queries from human
users</p>
</blockquote>
<p>也就是研究人员已经不满足于让LLM仅仅是根据query回答问题，更希望它能帮我们完成一些任务，成为我们工作生活的“助手”，就好像你有一个秘书一样，你让他去定一个航班，秘书可能会进行一系列的操作，比如他要考量你的时间安排，还要考虑航班的情况等等，你最终就是拿到了秘书给你的机票，但其实秘书在中间做了超级多事情。那我们现在就希望能把LLM培养成这样的角色，他不仅能接受命令还能自己做决策，然后把任务完成了。刚刚提到的博客里还讲了一个购买车的例子。</p>
<p>那我们知道我们的终极目标是要实现一个高级别的私人助理，那么实现这个目标需要哪些技术呢，这时候才到了lilian
wen的这篇博客部分。引言就是现在一些agents的雏形比如autoGPT,
GPT-engineer和BabyAGI出现了。</p>
<p>lilian的博客认为agents是以LLM作为大脑，配置三个主要的components：planing，memory和tool。Planning主要是将复杂任务拆分，不仅如此它还要负责自我反省，吸取以往错误的教训，从而能够产生更好的结果。</p>
<figure>
<img src="https://lilianweng.github.io/posts/2023-06-23-agent/agent-overview.png" alt="overview of a llm-powered autonomous agent system">
<figcaption aria-hidden="true">overview of a llm-powered autonomous
agent system</figcaption>
</figure>
<p>Memory包含短期记忆和长期记忆，前者可以理解成in-context
learning中应用的记忆，后者主要是应用外部的向量数据库或者本地知识库抽取的知识。Tool就是agent可以拥有调用各个外部API的能力，就像你的武器库一样，不同的武器适合不同的作战场景，这些API就可以弥补预训练完的模型所欠缺的能力，比如对于当下实时信息的获取。</p>
<h1 id="component-1-planning">Component 1： Planning</h1>
<p>拆解任务有两种主流办法：1. CoT chain of thought 2. Tree of
Thoughts</p>
<p>前者被讲烂了，后者是对CoT的扩展，将任务拆解成一个子任务树，然后采用宽度优先搜索或者广度优先搜索的方式去决定接下去先解决哪个子任务。</p>
<p>planning这个子模块还有一个更重要的功能就是自我反思，人都是需要从错误中进步的，大语言模型也是一样。思想有点类似于增强学习。首先讲到的是<code>ReAct</code>,
说实话lilian博客里写的这一段我没看懂，所以还是找原文<a href="https://arxiv.org/pdf/2210.03629.pdf">paper</a>来看了下。</p>
<figure>
<img src="/2023/07/13/llm%E8%B5%8B%E8%83%BD%E7%9A%84%E5%85%A8%E8%87%AA%E5%8A%A8Agents/image-20230713145032112.png" alt="comparison results">
<figcaption aria-hidden="true">comparison results</figcaption>
</figure>
<p>从上面的例子就可以理解作者提出的办法就是将thought和action结合起来了，也就是单纯的思考比如chain
of
thought并不能很好的回答问题，受制于预训练模型自己模型内存储的知识，而如果只有action呢？就是不停的去搜索，如果搜索不到正确的答案那也是白搭。其实我理解就是作者提出我们要做一个通用的人工智能，你要告诉他在行动的时候也要思考，思考清楚之后再去考虑下一步已经采取什么样的行动，同时每一次行动也会从环境中得到反馈，比如作者举的第二个例子，你去countertop（台面）的时候，你看到了苹果，面包，胡椒粉瓶子和一个花瓶，既然我们要把胡椒粉瓶子放到抽屉里，那就可以拿走胡椒粉瓶子啦！其实这也好理解，一个优秀的人其实也是要边做边思考的，所以就形成了作者提出的prompt新范式：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Thought: ...</span><br><span class="line">Action: ...</span><br><span class="line">Observation: ...</span><br><span class="line">... (Repeated many times)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://react-lm.github.io/files/diagram.png" alt="frames">
<figcaption aria-hidden="true">frames</figcaption>
</figure>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>machine translation相关论文阅读</title>
    <url>/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<p>machine translation 这个任务一般是作为language
modeling的紧接一个话题。它的前身（2010年之前）是statistical machine
translation，但自从Neural machine
translation出来之后，用statistical的方式来做translation就少了很多。有兴趣的可以了解下statistical
machine translation的<a href="https://www.cs.upc.edu/~ageno/anlp/classeMT.pdf">具体细节</a>.
本博客主要记录NMT的主要论文和研究。NMT的架构主要是encoder-decoder架构，它其实是一个很典型的seq-to-seq的模型,
关于它的定义：</p>
<blockquote>
<p>Neural Machine Translation (NMT) is a way to do Machine Translation
with a single end-to-end neural network</p>
</blockquote>
<p>它的一般架构是这样的:</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230313132157791-16786849189061.png" alt="Seq2Seq">
<figcaption aria-hidden="true">Seq2Seq</figcaption>
</figure>
<p>NMT所有的模型都基于一个统一的数学公式：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230313134416540.png" alt="数学公式">
<figcaption aria-hidden="true">数学公式</figcaption>
</figure>
<p>注意这里和statistical machine translation的公式是不一样的：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230313134658535.png" alt="statistical machine translation">
<figcaption aria-hidden="true">statistical machine
translation</figcaption>
</figure>
<p>用统计翻译模型做的时候是分别解决translation model以及language
model的问题，涉及很多特征工程的问题，很复杂。</p>
<p>在machine
translation领域，encoder-decoder架构的模型经历了好几次演变，最终才转化成加入了attention机制，模型架构的整理可以参考<a href="http://arxiv.org/abs/1912.02047">Neural Machine Translation: A
Review and
Survey</a>。文章的第五章介绍了将encoder编码为固定长度的向量的用法。其中有两种使用这个<code>C</code>的用法，1.
作为decoder的初始化state 2.
作为decoder每一个时间步的固定输入和input一起去计算hidden state：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20221214151254068.png" alt="Encoder-decoder architectures with fixed-length sentence encodings">
<figcaption aria-hidden="true">Encoder-decoder architectures with
fixed-length sentence encodings</figcaption>
</figure>
<p>这些文章从<a href="https://arxiv.org/abs/1409.3215">Sequence to
Sequence Learning with Neural Networks</a>，再到<a href="http://arxiv.org/abs/1406.1078">Learning Phrase Representations
using RNN Encoder-Decoder for Statistical Machine Translation</a>.
然后就过度到attention时代了，所以作者在这篇review中只花了很少的第五章节就结束了。第六章就开始讲attentional
encoder-decoder networks。</p>
<blockquote>
<p>The concept of attention is no longer just a technique to improve
sentence lengths in NMT. Since its introduction by Bahdanau et al.
(2015) it has become a vital part of various NMT architectures,
culminating in the Transformer architecture</p>
</blockquote>
<p>这句话是6.1的精髓，attention的概念不再是我们上文所说的那些用于初始化呀，还是用作duplicate
context。Bahdanau 2015年的这篇文章，也就是引入multi-head
attention的这篇文章彻底打破了这个convention。因为我们可以看到transformer的架构中都没有RNN的身影，有的只是attention
weights的计算。</p>
<h1 id="learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation-2014">Learning
Phrase Representations using RNN Encoder-Decoder for Statistical Machine
Translation 2014</h1>
<p>这是在机器翻译领域encoder-decoder架构，在attention
机制提出之前表现最好的RNN模型。其实模型挺简单的，encoder负责将input
sequence编码成了一个固定的向量Context，然后基于这个向量，decoder每一个时间步产生一个单词。在decoder的每一个时间步进行的运算是：</p>
<p><img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20221212164533545.png" alt="image-20221212164533545"> <code>y_t</code>是由s_t得到的。</p>
<p>同样的，这篇文章可以结合<a href="https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb">代码</a>来看，轻易理解。该代码是用pytorch实现的。这个pytorch的实现是从<a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning
with Neural Networks</a>开始讲解的，Learning Phrase Representations
using RNN Encoder-Decoder for Statistical Machine
Translation这篇文章进步在</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20221216102034819.png" alt="image-20221216102034819">
<figcaption aria-hidden="true">image-20221216102034819</figcaption>
</figure>
<p>可以看到该篇文章介绍的模型优势在于预测y的时候加入了context以及<span class="math inline">\(y_{t-1}\)</span>,而不是仅仅依赖于<span class="math inline">\(s_t\)</span></p>
<p>以上的文章都是将input
sentence编码成一个fixed-length的vector，从下面这篇2015年Bahdanau的文章开始，attention就开始用于NMT。为了解决fixed-length
vector的问题，这样我们就不必要将input
sentence的所有信息都编码到一个固定长度的向量里。</p>
<h1 id="neural-machine-translation-by-jointly-learning-to-align-and-translate-2015">Neural
Machine Translation by Jointly Learning to Align and Translate 2015</h1>
<p>从这篇文章开始，attention的机制开始使用在翻译中。</p>
<p>在Introduction章节，最重要的一句话：</p>
<blockquote>
<p>The most important distinguishing feature of this approach from the
basic encoder–decoder is that it does not attempt to encode a whole
input sentence into a single fixed-length vector. Instead, it encodes
the input sentence into a sequence of vectors and chooses a subset of
these vectors adaptively while decoding the translation</p>
</blockquote>
<p>意即跟以往那种encoder-decoder的网络来做translation的model不同，虽然提出的模型也属于encoder-decoder架构，但不是将input
sentence编码成一个固定长度的向量，而是将input
sentence编码成一系列的向量并自适应的从中选择一个小子集的向量用来做decode。</p>
<p>截至文章发表，现有做机器翻译的模型中，表现最好的模型是RNN，内units用lstm。可以称之为RNN
Encoder-Decoder。</p>
<p>还有一个发现是，这些encoder和decoder block，里面基本上是stacked
rnns结构，也就是堆了好几层rnn。这个发现可以追溯到<a href="https://arxiv.org/pdf/1703.03906.pdf">paper</a>.
该作者发现在NMT任务上，high-performing rnns are usually multi-layer,
不仅如此，对于encoder rnn，2到4层是最好的，对于decoder
rnn，4层是最好的。通常情况下，2层堆叠的RNN比一层RNN要lot better;
为了解决long dependency的问题，用lstm
cell是必要的，但这也不够，需要使用一些其他的技术，比如skip-connection，dense-connections。</p>
<hr>
<p>这里值得一提的是，虽然Bahdanau
2015年出的这篇文章很火。但是后来通过学习cs224n和观察tensorflow的文档：<a href="https://www.tensorflow.org/text/tutorials/nmt_with_attention">Neural
machine translation with attention</a>,发现<a href="https://arxiv.org/abs/1508.04025v5">luong
2015</a>的这篇文章中的架构使用的更多，它的计算公式和Bahdanau介绍的有一点点不一样，再luong的文章中我们也可以看到它自己说的和Bahdanau不一样的地方：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230313171420621.png" alt="image-20230313171420621">
<figcaption aria-hidden="true">image-20230313171420621</figcaption>
</figure>
<blockquote>
<p>Comparison to (Bahdanau et al., 2015) – While our global attention
approach is similar in spirit to the model proposed by Bahdanau et al.
(2015), there are several key differences which reflect how we have both
simplified and generalized from the original model. First, we simply use
hidden states at the top LSTM layers in both the encoder and decoder as
illustrated in Figure 2. Bahdanau et al. (2015), on the other hand, use
the concatenation of the forward and backward source hidden states in
the bi-directional encoder and target hidden states in their
non-stacking unidirectional decoder. Second, our computation path is
simpler; we go from ht → at → ct → ̃ ht then make a prediction as
detailed in Eq. (5), Eq. (6), and Figure 2. On the other hand, at any
time t, Bahdanau et al. (2015) build from the previous hidden state ht−1
→ at → ct → ht, which, in turn, goes through a deep-output and a maxout
layer before making predictions.7 Lastly, Bahdanau et al. (2015) only
experimented with one alignment function, the concat product; whereas we
show later that the other alternatives are better.</p>
</blockquote>
<p>所以关于用attention来做machine
translation的模型，我们只需要记住下面的计算过程就行，因为它也不是现在流行的machine
translation的方法（毕竟2015年的时候transformer还没出来）：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230313171823584.png" alt="attention in equations">
<figcaption aria-hidden="true">attention in equations</figcaption>
</figure>
<p>以上的模型给我们解决了标准的seq2seq的模型在做NMT任务时的一些问题：</p>
<ul>
<li>improves NMT performance</li>
<li>provides more "human-like" model: replace the fixed length vector
with dynamic vector according to the decoder hidden states</li>
<li>solves the bottleneck problem: allows decoder to look directly at
source</li>
<li>helps with the vanishing gradient problem</li>
<li>provides some interpretability</li>
</ul>
<p>注意，虽然attention机制首先是在NMT任务中提出并得到了应用，但是它并不是seq2seq的专属，你也可以将attention用在很多architectures和不同的tasks中。有一个关于attention的更general的定义是：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230314154515967.png" alt="general definition of attention">
<figcaption aria-hidden="true">general definition of
attention</figcaption>
</figure>
<p>我们有时候会说： <strong>query attends to the
values</strong>，例如在seq2seq2+attention的模型中，每一个decoder hidden
state就是query，attends to 所有的encoder hidden states(values).</p>
<h1 id="attention-is-all-you-need-2017">Attention is all you need
2017</h1>
<p>在transformer的<a href="https://arxiv.org/abs/1706.03762">paper</a>中，作者首先介绍本文：主流的sequence
tranduction模型主要基于复杂的RNN或者CNN模型，它们包含encoder和decoder两部分，其中表现最好的模型在encoder和decoder之间增加了attention
mechanism。本文提出了一个新的简单的网络结构名叫<em>transformer</em>，也是完全基于attention机制，"dispensing
with recurrence and convolutions entirely"!
根本无需循环和卷积！了不起的Network~</p>
<p>在阅读这篇文章之前需要提前了解我在另外一篇博客 Attention and
transformer
model中的知识，在translation领域我们的科学家们是如何从RNN循环神经网络过渡到CNN，然后最终是transformer的天下的状态。技术经过了一轮轮的迭代，每一种基础模型架构提出后，会不断的有文章提出新的改进，文章千千万，不可能全部读完，就精读一些经典文章就好，Vaswani这篇文章是NMT领域必读paper，文章不长，加上参考文献才12页，介绍部分非常简单，导致这篇文章的入门门槛很高（个人感觉）。我一开始先读的这篇文章，发现啃不下去，又去找了很多资料来看，其中对我非常帮助的有很多：</p>
<ul>
<li><a href="http://jalammar.github.io/illustrated-transformer/">非常通俗易懂的blog</a>
有中文版本的翻译</li>
<li><a href="http://arxiv.org/abs/1912.02047">Neural Machine
Translation: A Review and Survey</a>
虽然这篇paper很长，90+页。前六章可以作为参照，不多25页左右，写的非常好</li>
<li><a href="http://cs231n.stanford.edu/slides/2022/lecture_11_ruohan.pdf">stanford
cs231n课程的ppt</a>
斯坦福这个课程真的很棒，youtube上可以找到17年的视频，17年的课程中没有attention的内容，所以就姑且看看ppt吧，希望斯坦福有朝一日能将最新的课程分享出来，也算是做贡献了</li>
<li>cs231n推荐的阅读<a href="https://lilianweng.github.io/posts/2018-06-24-attention/">博客</a>
非常全面的整理，强烈建议食用.
这位作者也附上了自己的transformer实现，在它参考的那些github实现里，哈佛大学的<a href="http://nlp.seas.harvard.edu/2018/04/01/attention.html">pytorch实现</a>也值得借鉴。</li>
<li><a href="https://nlp.seas.harvard.edu/annotated-transformer/">The
annotated Transformer</a> 斯坦福出的关于Attention is All you
need学术文章的解析以及代码实现，强烈建议食用。</li>
</ul>
<p>Transformer这篇文章有几个主要的创新点：</p>
<ol type="1">
<li>使用self-attention机制，并首次提出使用multi-head attention</li>
</ol>
<p>该机制作用是在编码当前word的时候，这个self-attention就会告诉我们编码这个词语我们应该放多少注意力在这个句子中其他的词语身上，说白了其实就是计算当前词语和其他词语的关系。这也是CNN用于解决NMT问题时用不同width的kernel来扫input
metric的原因。</p>
<p>multi-head的意思是我使用多个不同的self-attention
layer来处理我们的输入，直观感觉是训练的参数更多了，模型的表现力自然要好一点。</p>
<ol start="2" type="1">
<li>Positional embeddings</li>
</ol>
<p>前一个创新点解决了dependence的问题，那如何解决位置的问题呢？也就是我这个词在编码的时候或者解码的时候应该放置在句子的哪个位置上。文章就用pisitional
embedding来解决这个问题。这个positional embedding和input
embedding拥有相同的shape，所以两者可以直接相加。transformer这篇文章提供了两种encoding方式：</p>
<p>1） sunusoidal positional encoding</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230302133247664.png" alt="image-20230302133247664">
<figcaption aria-hidden="true">image-20230302133247664</figcaption>
</figure>
<p>其中，pos=1,...,L(L是input句子的长度)，i是某一个PE中的一个维度，取值范围是1到dmodel。python实现为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span>(<span class="params">length, depth</span>):</span></span><br><span class="line">  depth = depth/<span class="number">2</span></span><br><span class="line"></span><br><span class="line">  positions = np.arange(length)[:, np.newaxis]     <span class="comment"># (seq, 1)</span></span><br><span class="line">  depths = np.arange(depth)[np.newaxis, :]/depth   <span class="comment"># (1, depth)</span></span><br><span class="line"></span><br><span class="line">  angle_rates = <span class="number">1</span> / (<span class="number">10000</span>**depths)         <span class="comment"># (1, depth)</span></span><br><span class="line">  angle_rads = positions * angle_rates      <span class="comment"># (pos, depth)</span></span><br><span class="line"></span><br><span class="line">  pos_encoding = np.concatenate(</span><br><span class="line">      [np.sin(angle_rads), np.cos(angle_rads)],</span><br><span class="line">      axis=-<span class="number">1</span>) </span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> tf.cast(pos_encoding, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">pos_encoding = positional_encoding(length=<span class="number">2048</span>, depth=<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the shape.</span></span><br><span class="line"><span class="built_in">print</span>(pos_encoding.shape) <span class="comment"># (2014,512)</span></span><br></pre></td></tr></table></figure>
<p>2） learned positional encoding</p>
<p>整体上看，这篇文章提出的transformer模型在做translation的任务时，架构是这样的：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230301133249379.png" alt="image-20230301133249379">
<figcaption aria-hidden="true">image-20230301133249379</figcaption>
</figure>
<p>其中encoders部分包含了6个encoders的block，decoders部分也包含了6个decoders的block，将encoders的每一个block拆开来看，有两个sub
layer：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230301133424302.png" alt="image-20230301133424302">
<figcaption aria-hidden="true">image-20230301133424302</figcaption>
</figure>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230301133440152.png" alt="image-20230301133440152">
<figcaption aria-hidden="true">image-20230301133440152</figcaption>
</figure>
<p>其中decoder部分的block比encoder部分的block多了一个sub
layer，其中self-attention和encoder-decoder attention都是multi-head
attention layer，只不过decoder部分的第一个multi-head attention
layer是一个masked multi-head
attention，为了防止未来的信息泄露给当下（prevent positions from
attending to the future）.</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230301133608484.png" alt="image-20230301133608484">
<figcaption aria-hidden="true">image-20230301133608484</figcaption>
</figure>
<p>在transformer模型中，作者还使用了residual
connection，所以在encoder的每一个block中，数据的flow是:</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230307162830473-16781777124571.png" alt="transformer架构">
<figcaption aria-hidden="true">transformer架构</figcaption>
</figure>
<p>其中self-attention中涉及的运算details是：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230301134258180.png" alt="image-20230301134258180">
<figcaption aria-hidden="true">image-20230301134258180</figcaption>
</figure>
<p>可以发现其中涉及的运算都是矩阵的点乘，并没有RNN中那种时间步的概念，所以所有运算都是可以parallelizable，这就能使得模型的推理和训练更加的efficient。并且！Transformers也可以抓住distant的依赖，而不是像rnn那样对于长依赖并不是很擅长，因为它前面的信息如果像传递到很后面的单词推理上，需要经历很多时间步的计算，而transformer在推理每一个单词的时候都可以access到input句子中的每一个单词（毕竟我们的Z中包含了每一个单词跟其他单词的关系)。</p>
<p>其中positional encoding现在可以简单的理解成在我们编码的word
embedding上我们又加了一个positional
encoding，维度和我们的embedding一模一样。</p>
<blockquote>
<p>在tensorflow中有一个layer是<code>MultiHeadAttention</code>,如果我们想实现transformer里的这个self-attention，那就是query，key，value其实都是由input
vector计算来的。</p>
</blockquote>
<p>以上的理论计算看起来可能会有点模糊，可以同步参照<a href="https://lilianweng.github.io/posts/2018-06-24-attention/">博客</a>
参考 <a href="http://jalammar.github.io/illustrated-transformer/">illustrated
transformer</a>介绍的详细细节，基于tensorflow框架实现的<a href="https://github.com/lilianweng/transformer-tensorflow/blob/master/transformer.py">transformer</a>来帮助自己理解transformer模型。</p>
<h2 id="encoder部分">encoder部分</h2>
<p>encoder的每一个block由两个sub-layer组成，中间穿插resnet
connection。</p>
<p><img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/transformer_encoder_block.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multihead_attention</span>(<span class="params">self, query, memory=<span class="literal">None</span>, mask=<span class="literal">None</span>, scope=<span class="string">&#x27;attn&#x27;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            query (tf.tensor): of shape (batch, q_size, d_model)</span></span><br><span class="line"><span class="string">            memory (tf.tensor): of shape (batch, m_size, d_model)</span></span><br><span class="line"><span class="string">            mask (tf.tensor): shape (batch, q_size, k_size)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:h</span></span><br><span class="line"><span class="string">            a tensor of shape (bs, q_size, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> memory <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment"># 如果memory是None，那么就是一个典型的self-attention layer</span></span><br><span class="line">            memory = query</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># Linear project to d_model dimension: [batch, q_size/k_size, d_model]</span></span><br><span class="line">            Q = tf.layers.dense(query, self.d_model, activation=tf.nn.relu)</span><br><span class="line">            K = tf.layers.dense(memory, self.d_model, activation=tf.nn.relu)</span><br><span class="line">            V = tf.layers.dense(memory, self.d_model, activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Split the matrix to multiple heads and then concatenate to have a larger</span></span><br><span class="line">            <span class="comment"># batch size: [h*batch, q_size/k_size, d_model/num_heads]</span></span><br><span class="line">            Q_split = tf.concat(tf.split(Q, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            K_split = tf.concat(tf.split(K, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            V_split = tf.concat(tf.split(V, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            mask_split = tf.tile(mask, [self.h, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Apply scaled dot product attention</span></span><br><span class="line">            out = self.scaled_dot_product_attention(Q_split, K_split, V_split, mask=mask_split)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Merge the multi-head back to the original shape</span></span><br><span class="line">            out = tf.concat(tf.split(out, self.h, axis=<span class="number">0</span>), axis=<span class="number">2</span>)  <span class="comment"># [bs, q_size, d_model]</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># The final linear layer and dropout.</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_model)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dropout(out, rate=self.drop_rate, training=self._is_training)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_forwad</span>(<span class="params">self, inp, scope=<span class="string">&#x27;ff&#x27;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Position-wise fully connected feed-forward network, applied to each position</span></span><br><span class="line"><span class="string">        separately and identically. It can be implemented as (linear + ReLU + linear) or</span></span><br><span class="line"><span class="string">        (conv1d + ReLU + conv1d).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            inp (tf.tensor): shape [batch, length, d_model]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        out = inp</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_ff, activation=tf.nn.relu)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dropout(out, rate=self.drop_rate, training=self._is_training)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_model, activation=None)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># by default, use_bias=True</span></span><br><span class="line">            out = tf.layers.conv1d(out, filters=self.d_ff, kernel_size=<span class="number">1</span>, activation=tf.nn.relu)</span><br><span class="line">            out = tf.layers.conv1d(out, filters=self.d_model, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out  </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encoder_layer</span>(<span class="params">self, inp, input_mask, scope</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            inp: tf.tensor of shape (batch, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            input_mask: tf.tensor of shape (batch, seq_len, seq_len)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        out = inp</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># One multi-head attention + one feed-forword</span></span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(out, mask=input_mask))</span><br><span class="line">            out = self.layer_norm(out + self.feed_forwad(out))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h2 id="decoder部分">decoder部分</h2>
<p><img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/encoder-decoder.png"></p>
<p>在decoder部分，我们可以看到每一个decoder
block的输入有两个：<em>整个encoder部分的输出</em>以及<em>上一个decoder
block的输出（第一个decoder
block是词向量的输入）</em>，而encoder部分的输出是接到每一个decoder
block的第二个sublayer的。正如刚刚提到了，decoder部分的每一个block跟encoder部分的block有一个不一样的地方，那就是多了一个sublayer：
encoder-decoder
attention。至于encoder部分和decoder部分是如何connect的，</p>
<blockquote>
<p>The encoder start by processing the input sequence. The output of the
top encoder is then transformed into a set of attention vectors K and V.
These are to be used by each decoder in its “encoder-decoder attention”
layer which helps the decoder focus on appropriate places in the input
sequence</p>
</blockquote>
<p>也就是我们得到了encoder部分top layer（最后一个encoder
layer）的输出之后，我们将输出转化成K和V.
我们可以看到在<code>multihead_attention</code>里，memory是<code>enc_out</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder_layer</span>(<span class="params">self, target, enc_out, input_mask, target_mask, scope</span>):</span></span><br><span class="line">        out = target</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(</span><br><span class="line">                out, mask=target_mask, scope=<span class="string">&#x27;self_attn&#x27;</span>))</span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(</span><br><span class="line">                out, memory=enc_out, mask=input_mask)) <span class="comment"># 将encoder部分的输出结果作为输入</span></span><br><span class="line">            out = self.layer_norm(out + self.feed_forwad(out))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder</span>(<span class="params">self, target, enc_out, input_mask, target_mask, scope=<span class="string">&#x27;decoder&#x27;</span></span>):</span></span><br><span class="line">        out = target</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_enc_layers):</span><br><span class="line">                out = self.decoder_layer(out, enc_out, input_mask, target_mask, <span class="string">f&#x27;dec_<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>以上实现的transformer其实我觉得还是有一点点复杂，毕竟在tensorflow2.0+版本中已经有了官方实现好的<code>layers.MultiHeadAttention</code>可以使用，应该可以大大简化我们实现步骤，特别是上面的<code>def multihead_attention(self, query, memory=None, mask=None, scope='attn'):</code>。从刚刚的实现里我们可以发现，除了decoder部分每一个block的第二个sublayer的attention计算有一点不一样之外，其他的attention计算都是一模一样的。我在github上找了不少用TF2.0实现的transformer（最标准的也是Attention
is all you
need的模型），发现很多都都写得一般般，最终发现还是tensorflow官方文档写的<a href="https://www.tensorflow.org/text/tutorials/transformer#add_and_normalize">tutotial</a>写的最好.</p>
<p>现在对照tensorflow的tutorial以及上面transformer的计算过程，拆解一下官方给的代码。</p>
<p>首先定义一个baseAttention类，然后在此基础上我们再定义encoder和decoder中的attention：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseAttention</span>(<span class="params">tf.keras.layers.Layer</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)</span><br><span class="line">    self.layernorm = tf.keras.layers.LayerNormalization()</span><br><span class="line">    self.add = tf.keras.layers.Add()</span><br></pre></td></tr></table></figure>
<p>那么针对encoder结果输入到decoder的cross attention
layer怎么处理呢？这时候我们使用MultiHeadAttention时就需要将target
sequence x当作是query，将encoder输出当作是context
sequence也就是key/value。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossAttention</span>(<span class="params">BaseAttention</span>):</span> <span class="comment"># encoder结果输入到decoder的层</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x, context</span>):</span> <span class="comment"># 这里的x是target sequence,context是encoder的输出结果</span></span><br><span class="line">    attn_output, attn_scores = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        key=context,</span><br><span class="line">        value=context,</span><br><span class="line">        return_attention_scores=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Cache the attention scores for plotting later.</span></span><br><span class="line">    self.last_attn_scores = attn_scores</span><br><span class="line"></span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>然后我们再定义global attention，global
attention就是没有任何特殊操作的（比如上面的attention计算它有特别的context），而在transformor中更多的是self-attention，也就是我们传递给MultiHeadAttention的query,key,value都是同一个值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GlobalSelfAttention</span>(<span class="params">BaseAttention</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    attn_output = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        value=x,</span><br><span class="line">        key=x)</span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>最后我们定义causal self attention
layer，这个是在decoder的每一个block的第一个sublayer：self-attention
layer.其实这个layer是和global attention
layer差不多的，但还是有一点微小的差别。为什么呢？因为我们在decoder阶段，我们是一个词语一个词语的预测的，这其实包含了一层因果关系，我们在预测一个词语的时候，我们应该已知它前面一个词语是什么，RNN中的hidden
state传递到下一个时间步就是这个因果关系的传递。那么如果我们使用刚刚我们实现的global
attention layer来实现这个self
attention，并没有包含这个因果关系，不仅如此，如果我们使用常规的self
attention的计算，将target
sequence全部当作输入输入到decoder中的第一个block中，会有未来的数据提前被当前时刻看到的风险，所以在Transformer这篇文章中，作者提出使用mask的技术来避免这个问题。</p>
<p>在tensorflow中实现很简单，就只需要给MultiHeadAttention传递一个use_causal_mask
= True的参数即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CausalSelfAttention</span>(<span class="params">BaseAttention</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    attn_output = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        value=x,</span><br><span class="line">        key=x,</span><br><span class="line">        use_causal_mask = <span class="literal">True</span>) <span class="comment"># The causal mask ensures that each location only has access to the locations that come before it</span></span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这样就可以保证先前的sequence并不依赖于之后的elements。这里我本来有一个疑问是，这样一来这个causal
layer并不能实现bi-rnn的能力？但后来一想并不是，因为双向的RNN的后向是指后面的词语先输入，其实就是从后往前输入，这样就可以知道一个sequence当前词语依赖于后面的词语的权重。</p>
<h2 id="补充介绍">补充介绍</h2>
<h3 id="tf.keras.layers.multiheadattention">tf.keras.layers.MultiHeadAttention</h3>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention">doc</a></p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230309150541287.png" alt="image-20230309150541287">
<figcaption aria-hidden="true">image-20230309150541287</figcaption>
</figure>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230309150600806.png" alt="image-20230309150600806">
<figcaption aria-hidden="true">image-20230309150600806</figcaption>
</figure>
<p>注意，return的结果包含两个，其中attention_output的shape的第二维是和target
sequence的长度是一致的，并且E是和query的最后一维是一致的。</p>
<h1 id="attention-family">Attention Family</h1>
<p>这个章节整理于<a href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/">blog</a>，这个作者之前写了一篇介绍attention的文章，后面在2023年一月的时候又更新了两篇博客，详细介绍了从2020年以来出现的新的Transformer
models。权当自己学习记录一些我还需要补充的知识。</p>
<blockquote>
<p>The <strong>Transformer</strong> (which will be referred to as
“vanilla Transformer” to distinguish it from other enhanced versions; <a href="https://arxiv.org/abs/1706.03762">Vaswani, et al., 2017</a>) model
has an encoder-decoder architecture, as commonly used in many <a href="https://lilianweng.github.io/posts/2018-06-24-attention/#born-for-translation">NMT</a>
models. Later simplified Transformer was shown to achieve great
performance in language modeling tasks, like in encoder-only <a href="https://lilianweng.github.io/posts/2019-01-31-lm/#bert">BERT</a>
or decoder-only <a href="https://lilianweng.github.io/posts/2019-01-31-lm/#openai-gpt">GPT</a>.</p>
</blockquote>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql常用命令</title>
    <url>/2022/03/17/mysql%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h1 id="select">select</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT * FROM table_name; # 选取所有内容</span><br><span class="line"></span><br><span class="line">SELECT DISTINCT column_name,column_name</span><br><span class="line">FROM table_name; # 一个列可能包含不同的值，该句可以列出不同的值</span><br><span class="line"></span><br><span class="line">SELECT column_name,column_name</span><br><span class="line">FROM table_name</span><br><span class="line">WHERE column_name operator value; # OPERATOR部分可以是: =,&gt;,&lt;,between,in,like</span><br><span class="line"># WHERE后可以跟逻辑符号 and,or,not</span><br></pre></td></tr></table></figure>
<p>mysql语句对大小写不敏感，select == SELECT</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from emp where not sal &gt; 1500;</span><br><span class="line"></span><br><span class="line">Select * from emp where comm is null;</span><br><span class="line"></span><br><span class="line">Select * from emp where sal in (5000,3000,1500);</span><br><span class="line"></span><br><span class="line">SELECT * FROM Websites WHERE alexa &gt; 15 AND (country=&#x27;CN&#x27; OR country=&#x27;USA&#x27;); # AND和OR可以结合使用</span><br></pre></td></tr></table></figure>
<p>Like模糊查询</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Select * from emp where ename like &#x27;M%&#x27;;</span><br></pre></td></tr></table></figure>
<p>查询 EMP 表中 Ename 列中有 M 的值，M 为要查询内容中的模糊信息。</p>
<ul>
<li><strong>%</strong> 表示多个字值，**_** 下划线表示一个字符；</li>
<li><strong>M%</strong> :
为能配符，正则表达式，表示的意思为模糊查询信息为 M 开头的。</li>
<li><strong>%M%</strong> : 表示查询包含M的所有内容。</li>
<li><strong>%M_</strong> : 表示查询以M在倒数第二位的所有内容。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT column_name,column_name</span><br><span class="line">FROM table_name</span><br><span class="line">ORDER BY column_name,column_name ASC|DESC; # 默认是升序</span><br><span class="line"></span><br><span class="line">INSERT INTO table_name (column1,column2,column3,...)</span><br><span class="line">VALUES (value1,value2,value3,...); # 这种方式可以向table_name表格中插入一行，可以只向某一列插入，其他自动</span><br><span class="line"></span><br><span class="line">UPDATE table_name</span><br><span class="line">SET column1=value1,column2=value2,...</span><br><span class="line">WHERE some_column=some_value; # 用于更新某条记录 </span><br><span class="line"># 比如：</span><br><span class="line">UPDATE Websites </span><br><span class="line">SET alexa=&#x27;5000&#x27;, country=&#x27;USA&#x27; </span><br><span class="line">WHERE name=&#x27;菜鸟教程&#x27;;</span><br><span class="line"></span><br><span class="line">DELETE FROM table_name</span><br><span class="line">WHERE some_column=some_value; # 删除某一行记录</span><br></pre></td></tr></table></figure>
<h1 id="高级用法">高级用法</h1>
<h2 id="join">join</h2>
<figure>
<img src="/2022/03/17/mysql%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/sql-join.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># QL INNER JOIN（简单的 JOIN）。 SQL INNER JOIN 从多个表中返回满足 JOIN 条件的所有行。</span><br><span class="line">SELECT Websites.id, Websites.name, access_log.count, access_log.date</span><br><span class="line">FROM Websites</span><br><span class="line">INNER JOIN access_log</span><br><span class="line">ON Websites.id=access_log.site_id;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>INNER JOIN</strong>：如果表中有至少一个匹配，则返回行</li>
<li><strong>LEFT
JOIN</strong>：即使右表中没有匹配，也从左表返回所有的行</li>
<li><strong>RIGHT
JOIN</strong>：即使左表中没有匹配，也从右表返回所有的行</li>
<li><strong>FULL JOIN</strong>：只要其中一个表中存在匹配，则返回行</li>
</ul>
<h2 id="union">union</h2>
<p>UNION 操作符用于合并两个或多个 SELECT 语句的结果集。</p>
<p>请注意，UNION 内部的每个 SELECT
语句必须拥有相同数量的列。列也必须拥有相似的数据类型。同时，每个 SELECT
语句中的列的顺序必须相同。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT column_name(s) FROM table1</span><br><span class="line">UNION</span><br><span class="line">SELECT column_name(s) FROM table2;</span><br></pre></td></tr></table></figure>
<p><strong>注释：</strong>默认地，UNION
操作符选取不同的值。如果允许重复的值，请使用 UNION ALL。</p>
<h3 id="sql-union-all-语法">SQL UNION ALL 语法</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT column_name(s) FROM table1</span><br><span class="line">UNION ALL</span><br><span class="line">SELECT *column_name(s) FROM table2;</span><br></pre></td></tr></table></figure>
<p><strong>注释：</strong>UNION 结果集中的列名总是等于 UNION 中第一个
SELECT 语句中的列名。</p>
<p>前者只会列出选出的值中不同的一些数据条，后者是所有的都返回。</p>
<h2 id="count">Count</h2>
<p>COUNT() 函数返回匹配指定条件的行数。</p>
<hr>
<h3 id="sql-countcolumn_name-语法">SQL COUNT(column_name) 语法</h3>
<p>COUNT(column_name) 函数返回指定列的值的数目（NULL 不计入）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT COUNT(column_name) FROM table_name;</span><br></pre></td></tr></table></figure>
<h3 id="sql-count-语法">SQL COUNT(*) 语法</h3>
<p>COUNT(*) 函数返回表中的记录数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT COUNT(*) FROM table_name;</span><br></pre></td></tr></table></figure>
<h3 id="sql-countdistinct-column_name-语法">SQL COUNT(DISTINCT
column_name) 语法</h3>
<p>COUNT(DISTINCT column_name) 函数返回指定列的不同值的数目：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT COUNT(DISTINCT column_name) FROM table_name;</span><br></pre></td></tr></table></figure>
<p><strong>注释：</strong>COUNT(DISTINCT) 适用于 ORACLE 和 Microsoft SQL
Server，但是无法用于 Microsoft Access。</p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>image-segmentation图像分割中的数据读取和处理</title>
    <url>/2022/08/23/image-segmentation%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%92%8C%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<h1 id="用tensorflow.image">用tensorflow.image</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> tensorflow_io <span class="keyword">as</span> tfio</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_image</span>(<span class="params">image_path, mask=<span class="literal">False</span></span>):</span></span><br><span class="line">    image = tf.io.read_file(image_path)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> mask:</span><br><span class="line">        image = tf.image.decode_png(image, channels=<span class="number">1</span>)</span><br><span class="line">        image.set_shape([<span class="literal">None</span>, <span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])</span><br><span class="line">        image = tf.cast(image, tf.int32)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        image = tf.image.decode_png(image, channels=<span class="number">3</span>)</span><br><span class="line">        image.set_shape([<span class="literal">None</span>, <span class="literal">None</span>, <span class="number">3</span>])</span><br><span class="line">        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])</span><br><span class="line">        image = image / <span class="number">255.</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">image_list, mask_list</span>):</span></span><br><span class="line">    image = read_image(image_list)</span><br><span class="line">    mask  = read_image(mask_list, mask=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> image, mask</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_generator</span>(<span class="params">image_list, mask_list, split=<span class="string">&#x27;train&#x27;</span></span>):</span></span><br><span class="line">    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))</span><br><span class="line">    dataset = dataset.shuffle(<span class="number">8</span>*BATCH_SIZE) <span class="keyword">if</span> split == <span class="string">&#x27;train&#x27;</span> <span class="keyword">else</span> dataset </span><br><span class="line">    dataset = dataset.<span class="built_in">map</span>(load_data, num_parallel_calls=tf.data.AUTOTUNE)</span><br><span class="line">    dataset = dataset.batch(BATCH_SIZE, drop_remainder=<span class="literal">True</span>)</span><br><span class="line">    dataset = dataset.prefetch(tf.data.AUTOTUNE)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line">IMAGE_SIZE = <span class="number">128</span></span><br><span class="line">BATCH_SIZE = <span class="number">86</span></span><br><span class="line"></span><br><span class="line">train_dataset = data_generator(images, masks)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Train Dataset:&quot;</span>, train_dataset)</span><br><span class="line"></span><br><span class="line">input_data = os.path.join(root, <span class="string">&#x27;train_images&#x27;</span>)</span><br><span class="line">images = <span class="built_in">sorted</span>(</span><br><span class="line">    [</span><br><span class="line">        os.path.join(input_data, fname)</span><br><span class="line">        <span class="keyword">for</span> fname <span class="keyword">in</span> os.listdir(input_data)</span><br><span class="line">        <span class="keyword">if</span> fname.endswith(exts) <span class="keyword">and</span> <span class="keyword">not</span> fname.startswith(<span class="string">&quot;.&quot;</span>)</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">target_data = os.path.join(root, <span class="string">&#x27;train_masks&#x27;</span>)</span><br><span class="line">masks = <span class="built_in">sorted</span>(</span><br><span class="line">    [</span><br><span class="line">        os.path.join(target_data, fname)</span><br><span class="line">        <span class="keyword">for</span> fname <span class="keyword">in</span> os.listdir(target_data)</span><br><span class="line">        <span class="keyword">if</span> fname.endswith(exts) <span class="keyword">and</span> <span class="keyword">not</span> fname.startswith(<span class="string">&quot;.&quot;</span>)</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of samples:&quot;</span>, <span class="built_in">len</span>(images), <span class="built_in">len</span>(masks))</span><br><span class="line"><span class="keyword">for</span> input_path, target_path <span class="keyword">in</span> <span class="built_in">zip</span>(images[:<span class="number">10</span>], masks[:<span class="number">10</span>]):</span><br><span class="line">    <span class="built_in">print</span>(input_path[-<span class="number">32</span>:], <span class="string">&quot;|&quot;</span>, target_path[-<span class="number">31</span>:], <span class="string">&#x27;|&#x27;</span>, np.unique(cv2.imread(target_path)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以上方式传入data_generator的是image和mask所在图片路径.</p>
<h1 id="用cv2">用cv2</h1>
<p>这种方式适合图片不多的情况下使用，直接读入ndarry里存储。
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">train_img_paths = <span class="built_in">sorted</span>(glob(<span class="string">&#x27;../output/kaggle/working/train/*.jpg&#x27;</span>))[:SAMPLE]</span><br><span class="line">train_mask_paths = <span class="built_in">sorted</span>(glob(<span class="string">&#x27;../output/kaggle/working/train_masks/*.gif&#x27;</span>))[:SAMPLE]</span><br><span class="line"></span><br><span class="line">train_imgs = np.array([cv2.resize(imageio.imread(path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">                        <span class="keyword">for</span> path <span class="keyword">in</span> train_img_paths])</span><br><span class="line"></span><br><span class="line">train_masks = np.array([cv2.resize(imageio.imread(path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">                        <span class="keyword">for</span> path <span class="keyword">in</span> train_mask_paths])</span><br><span class="line"></span><br><span class="line">train_masks = train_masks.astype(np.float32)</span><br><span class="line">train_masks[train_masks&lt;=<span class="number">127</span>] = <span class="number">0.</span></span><br><span class="line">train_masks[train_masks&gt;<span class="number">127</span>] = <span class="number">1.</span></span><br><span class="line">train_masks = np.reshape(train_masks, (*train_masks.shape, <span class="number">1</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h1 id="用generator方式">用generator方式</h1>
<p>推荐用这种方式，占用内存小。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img_paths = <span class="built_in">sorted</span>(glob(<span class="string">&#x27;../output/kaggle/working/train/*.jpg&#x27;</span>))[:<span class="number">500</span>]</span><br><span class="line">mask_paths = <span class="built_in">sorted</span>(glob(<span class="string">&#x27;../output/kaggle/working/train_masks/*.gif&#x27;</span>))[:<span class="number">500</span>]</span><br><span class="line">train_img_files,val_img_files,train_mask_files,val_mask_files = train_test_split(img_paths,mask_paths,test_size=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_mask</span>(<span class="params">masks</span>):</span></span><br><span class="line">    masks[masks&lt;=<span class="number">127</span>] = <span class="number">0.</span></span><br><span class="line">    masks[masks&gt;<span class="number">127</span>] = <span class="number">1.</span></span><br><span class="line">    masks = masks.astype(np.float32)</span><br><span class="line">    masks = np.reshape(masks, (*masks.shape, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> masks</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_img_mask_gen</span>():</span></span><br><span class="line">    <span class="keyword">for</span> img_path,mask_path <span class="keyword">in</span> <span class="built_in">zip</span>(train_img_files,train_mask_files):</span><br><span class="line">        img = cv2.resize(imageio.imread(img_path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">        img = img / <span class="number">127.5</span></span><br><span class="line">        mask = cv2.resize(imageio.imread(mask_path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">        mask = process_mask(mask)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">yield</span> img, mask</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val_img_mask_gen</span>():</span></span><br><span class="line">    <span class="keyword">for</span> img_path,mask_path <span class="keyword">in</span> <span class="built_in">zip</span>(val_img_files,val_mask_files):</span><br><span class="line">        img = cv2.resize(imageio.imread(img_path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">        mask = cv2.resize(imageio.imread(mask_path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">        img = img / <span class="number">127.5</span></span><br><span class="line">        mask = process_mask(mask)</span><br><span class="line">        <span class="keyword">yield</span> img, mask</span><br><span class="line"></span><br><span class="line">train_dataset = tf.data.Dataset.from_generator(train_img_mask_gen,</span><br><span class="line">                                              output_signature=(</span><br><span class="line">                                                    tf.TensorSpec(shape=(IMG_ROWS, IMG_COLS, <span class="number">3</span>), dtype=tf.float32),</span><br><span class="line">                                                    tf.TensorSpec(shape=(IMG_ROWS, IMG_COLS, <span class="number">1</span>), dtype=tf.float32))</span><br><span class="line">                                              )</span><br><span class="line">val_dataset = tf.data.Dataset.from_generator(val_img_mask_gen,</span><br><span class="line">                                             output_signature=(</span><br><span class="line">                                                    tf.TensorSpec(shape=(IMG_ROWS, IMG_COLS, <span class="number">3</span>), dtype=tf.float32),</span><br><span class="line">                                                    tf.TensorSpec(shape=(IMG_ROWS, IMG_COLS, <span class="number">1</span>), dtype=tf.float32))</span><br><span class="line">                                            )</span><br></pre></td></tr></table></figure>
上面这种方式需要创建一个generator函数，该函数不接受参数，如果需要传入参数可以另外新建一个函数，该函数会返回一个不接受任何参数的fun().比如：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img_masks_generator_from_files</span>(<span class="params">img_files, mask_files, sample_weights=<span class="literal">None</span>, width=<span class="number">512</span>, height=<span class="number">512</span>, shuffle=<span class="literal">True</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> sample_weights:</span><br><span class="line">        sample_weights = [[<span class="number">1</span>]] * <span class="built_in">len</span>(img_files) <span class="comment"># Make sample_weights equipped with a broadcastable shape when fed to a tf.Tensor</span></span><br><span class="line">    </span><br><span class="line">    sample_weights = np.array(sample_weights)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(sample_weights.shape) == <span class="number">1</span>:</span><br><span class="line">        sample_weights = sample_weights[..., np.newaxis] <span class="comment"># Make sample_weights equipped with a broadcastable shape when fed to a tf.Tensor</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(<span class="built_in">len</span>(sample_weights.shape) == <span class="number">2</span> <span class="keyword">and</span> sample_weights.shape[-<span class="number">1</span>] == <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        indexes = np.arange(<span class="built_in">len</span>(img_files))</span><br><span class="line">        np.random.shuffle(indexes)</span><br><span class="line">        img_files = [ img_files[i] <span class="keyword">for</span> i <span class="keyword">in</span> indexes ]</span><br><span class="line">        mask_files = [ mask_files[i] <span class="keyword">for</span> i <span class="keyword">in</span> indexes ]</span><br><span class="line">        sample_weights = [ sample_weights[i] <span class="keyword">for</span> i <span class="keyword">in</span> indexes ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f</span>():</span></span><br><span class="line">        <span class="keyword">for</span> img, masks, sample_weight <span class="keyword">in</span> <span class="built_in">zip</span>(img_files, mask_files, sample_weights):</span><br><span class="line">            <span class="keyword">yield</span> img_masks_from_file(img, masks, sample_weight)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> f</span><br><span class="line"></span><br><span class="line">train_img_masks_gen = img_masks_generator_from_files(train_img_files, train_mask_files, train_sample_weights, width=W, height=H)</span><br><span class="line"></span><br><span class="line">train_img_masks_dataset = tf.data.Dataset.from_generator(</span><br><span class="line">    train_img_masks_gen, </span><br><span class="line">    output_signature=(</span><br><span class="line">        tf.TensorSpec(shape=(W, H, <span class="number">1</span>), dtype=tf.float32),</span><br><span class="line">        tf.TensorSpec(shape=(W, H, <span class="built_in">len</span>(train_mask_files[<span class="number">0</span>])), dtype=tf.int32),</span><br><span class="line">        tf.TensorSpec(shape=(<span class="number">1</span>), dtype=tf.float32)</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
上面这种方式还可以换成另一种，在tensorflow.data.Dataset初始化时直接传递args参数:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ds_train = tf.data.Dataset.from_generator(noise_generator,args=[<span class="string">&#x27;train&#x27;</span>, mode],output_types=tf.int32,output_shapes=(<span class="literal">None</span>, <span class="literal">None</span>, n_channels))</span><br></pre></td></tr></table></figure>
<p>官方文档是这样写的：</p>
<blockquote>
<p>(Optional.) A tuple of <a href="https://www.tensorflow.org/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a>
objects that will be evaluated and passed to <code>generator</code> as
NumPy-array arguments.</p>
</blockquote>
<p>关于更多<code>tf.data.Dataset.from_generator</code>的用法可以参考<a href="https://vak.ai/tensorflow/TensorFlow2.0-dataset/">博客</a>。里面有一句话解答了我的疑惑：</p>
<blockquote>
<p>we need to have a python <a href="https://www.programiz.com/python-programming/generator">generator</a>
function which generates <strong>one</strong> training pair needed for
our model.</p>
</blockquote>
<p>这就是说我们在创建<code>generator</code>这个函数的时候，函数返回值应该是一个<code>training pair</code>，也就是<code>X</code>和<code>y</code></p>
<hr>
<p>visualize图片的可视化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span>(<span class="params">**images</span>):</span> <span class="comment"># **images是(key,item)的方式，*images是item list的方式</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;PLot images in one row.&quot;&quot;&quot;</span></span><br><span class="line">    n = <span class="built_in">len</span>(images)</span><br><span class="line">    plt.figure(figsize=(<span class="number">16</span>, <span class="number">5</span>))</span><br><span class="line">    <span class="keyword">for</span> i, (name, image) <span class="keyword">in</span> <span class="built_in">enumerate</span>(images.items()):</span><br><span class="line">        plt.subplot(<span class="number">1</span>, n, i + <span class="number">1</span>)</span><br><span class="line">        plt.xticks([])</span><br><span class="line">        plt.yticks([])</span><br><span class="line">        plt.title(<span class="string">&#x27; &#x27;</span>.join(name.split(<span class="string">&#x27;_&#x27;</span>)).title())</span><br><span class="line">        plt.imshow(image)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">image, mask = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataset.take(<span class="number">1</span>))) <span class="comment"># train_dataset</span></span><br><span class="line"><span class="comment"># image,mask = list(train_dataset.take(1)) </span></span><br><span class="line"><span class="built_in">print</span>(image.shape, mask.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (img, msk) <span class="keyword">in</span> <span class="built_in">zip</span>(image[:<span class="number">5</span>], mask[:<span class="number">5</span>]):</span><br><span class="line">    <span class="built_in">print</span>(mask.numpy().<span class="built_in">min</span>(), mask.numpy().<span class="built_in">max</span>())</span><br><span class="line">    visualize(</span><br><span class="line">        image=img.numpy(),</span><br><span class="line">        gt_mask=msk.numpy(), </span><br><span class="line">    )</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以下是mask标注大于两类（0或者1）的情况下可以观察数据的方式（需要被赋予color）
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Visualization Utilities</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># there are 11 classes in the dataset: one class for each digit (0 to 9) plus the background class</span></span><br><span class="line">n_classes = <span class="number">11</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># assign a random color for each class</span></span><br><span class="line">colors = [<span class="built_in">tuple</span>(np.random.randint(<span class="number">256</span>, size=<span class="number">3</span>) / <span class="number">255.0</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fuse_with_pil</span>(<span class="params">images</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Creates a blank image and pastes input images</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    images (list of numpy arrays) - numpy array representations of the images to paste</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    PIL Image object containing the images</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">  widths = (image.shape[<span class="number">1</span>] <span class="keyword">for</span> image <span class="keyword">in</span> images)</span><br><span class="line">  heights = (image.shape[<span class="number">0</span>] <span class="keyword">for</span> image <span class="keyword">in</span> images)</span><br><span class="line">  total_width = <span class="built_in">sum</span>(widths)</span><br><span class="line">  max_height = <span class="built_in">max</span>(heights)</span><br><span class="line"></span><br><span class="line">  new_im = PIL.Image.new(<span class="string">&#x27;RGB&#x27;</span>, (total_width, max_height))</span><br><span class="line"></span><br><span class="line">  x_offset = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> im <span class="keyword">in</span> images:</span><br><span class="line">    pil_image = PIL.Image.fromarray(np.uint8(im))</span><br><span class="line">    new_im.paste(pil_image, (x_offset,<span class="number">0</span>))</span><br><span class="line">    x_offset += im.shape[<span class="number">1</span>]</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> new_im</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">give_color_to_annotation</span>(<span class="params">annotation</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Converts a 2-D annotation to a numpy array with shape (height, width, 3) where</span></span><br><span class="line"><span class="string">  the third axis represents the color channel. The label values are multiplied by</span></span><br><span class="line"><span class="string">  255 and placed in this axis to give color to the annotation</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    annotation (numpy array) - label map array</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    the annotation array with an additional color channel/axis</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  seg_img = np.zeros( (annotation.shape[<span class="number">0</span>],annotation.shape[<span class="number">1</span>], <span class="number">3</span>) ).astype(<span class="string">&#x27;float&#x27;</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">    segc = (annotation == c)</span><br><span class="line">    seg_img[:,:,<span class="number">0</span>] += segc*( colors[c][<span class="number">0</span>] * <span class="number">255.0</span>)</span><br><span class="line">    seg_img[:,:,<span class="number">1</span>] += segc*( colors[c][<span class="number">1</span>] * <span class="number">255.0</span>)</span><br><span class="line">    seg_img[:,:,<span class="number">2</span>] += segc*( colors[c][<span class="number">2</span>] * <span class="number">255.0</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> seg_img</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_annotation_and_prediction</span>(<span class="params">image, annotation, prediction, iou_list, dice_score_list</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Displays the images with the ground truth and predicted label maps. Also overlays the metrics.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    image (numpy array) -- the input image</span></span><br><span class="line"><span class="string">    annotation (numpy array) -- the ground truth label map</span></span><br><span class="line"><span class="string">    prediction (numpy array) -- the predicted label map</span></span><br><span class="line"><span class="string">    iou_list (list of floats) -- the IOU values for each class</span></span><br><span class="line"><span class="string">    dice_score_list (list of floats) -- the Dice Score for each class</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">  new_ann = np.argmax(annotation, axis=<span class="number">2</span>)</span><br><span class="line">  true_img = give_color_to_annotation(new_ann)</span><br><span class="line">  pred_img = give_color_to_annotation(prediction)</span><br><span class="line"></span><br><span class="line">  image = image + <span class="number">1</span></span><br><span class="line">  image = image * <span class="number">127.5</span></span><br><span class="line">  image = np.reshape(image, (image.shape[<span class="number">0</span>], image.shape[<span class="number">1</span>],))</span><br><span class="line">  image = np.uint8(image)</span><br><span class="line">  images = [image, np.uint8(pred_img), np.uint8(true_img)]</span><br><span class="line"></span><br><span class="line">  metrics_by_id = [(idx, iou, dice_score) <span class="keyword">for</span> idx, (iou, dice_score) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(iou_list, dice_score_list)) <span class="keyword">if</span> iou &gt; <span class="number">0.0</span> <span class="keyword">and</span> idx &lt; <span class="number">10</span>]</span><br><span class="line">  metrics_by_id.sort(key=<span class="keyword">lambda</span> tup: tup[<span class="number">1</span>], reverse=<span class="literal">True</span>)  <span class="comment"># sorts in place</span></span><br><span class="line"></span><br><span class="line">  display_string_list = [<span class="string">&quot;&#123;&#125;: IOU: &#123;&#125; Dice Score: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(idx, iou, dice_score) <span class="keyword">for</span> idx, iou, dice_score <span class="keyword">in</span> metrics_by_id]</span><br><span class="line">  display_string = <span class="string">&quot;\n&quot;</span>.join(display_string_list)</span><br><span class="line"></span><br><span class="line">  plt.figure(figsize=(<span class="number">15</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> idx, im <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">3</span>, idx+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> idx == <span class="number">1</span>:</span><br><span class="line">      plt.xlabel(display_string)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.imshow(im)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_annotation_and_image</span>(<span class="params">image, annotation</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Displays the image and its annotation side by side</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    image (numpy array) -- the input image</span></span><br><span class="line"><span class="string">    annotation (numpy array) -- the label map</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  new_ann = np.argmax(annotation, axis=<span class="number">2</span>)</span><br><span class="line">  seg_img = give_color_to_annotation(new_ann)</span><br><span class="line">  </span><br><span class="line">  image = image + <span class="number">1</span></span><br><span class="line">  image = image * <span class="number">127.5</span></span><br><span class="line">  image = np.reshape(image, (image.shape[<span class="number">0</span>], image.shape[<span class="number">1</span>],))</span><br><span class="line"></span><br><span class="line">  image = np.uint8(image)</span><br><span class="line">  images = [image, seg_img]</span><br><span class="line">  </span><br><span class="line">  images = [image, seg_img]</span><br><span class="line">  fused_img = fuse_with_pil(images)</span><br><span class="line">  plt.imshow(fused_img)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">list_show_annotation</span>(<span class="params">dataset, num_images</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Displays images and its annotations side by side</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    dataset (tf Dataset) -- batch of images and annotations</span></span><br><span class="line"><span class="string">    num_images (int) -- number of images to display</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  ds = dataset.unbatch()</span><br><span class="line"></span><br><span class="line">  plt.figure(figsize=(<span class="number">20</span>, <span class="number">15</span>))</span><br><span class="line">  plt.title(<span class="string">&quot;Images And Annotations&quot;</span>)</span><br><span class="line">  plt.subplots_adjust(bottom=<span class="number">0.1</span>, top=<span class="number">0.9</span>, hspace=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> idx, (image, annotation) <span class="keyword">in</span> <span class="built_in">enumerate</span>(ds.take(num_images)):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">5</span>, idx + <span class="number">1</span>)</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    show_annotation_and_image(image.numpy(), annotation.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># get 10 images from the training set</span></span><br><span class="line">list_show_annotation(training_dataset, <span class="number">10</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h1 id="segmentation中checkpoint的设置">segmentation中checkpoint的设置</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DisplayCallback</span>(<span class="params">tf.keras.callbacks.Callback</span>):</span> <span class="comment"># 每间隔5个spoch显示一次结果</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dataset, epoch_interval=<span class="number">5</span></span>):</span></span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        self.epoch_interval = epoch_interval</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">display</span>(<span class="params">self, display_list, extra_title=<span class="string">&#x27;&#x27;</span></span>):</span></span><br><span class="line">        plt.figure(figsize=(<span class="number">15</span>, <span class="number">15</span>))</span><br><span class="line">        title = [<span class="string">&#x27;Input Image&#x27;</span>, <span class="string">&#x27;True Mask&#x27;</span>, <span class="string">&#x27;Predicted Mask&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(display_list) &gt; <span class="built_in">len</span>(title):</span><br><span class="line">            title.append(extra_title)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(display_list)):</span><br><span class="line">            plt.subplot(<span class="number">1</span>, <span class="built_in">len</span>(display_list), i+<span class="number">1</span>)</span><br><span class="line">            plt.title(title[i])</span><br><span class="line">            plt.imshow(display_list[i])</span><br><span class="line">            plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_mask</span>(<span class="params">self, pred_mask</span>):</span></span><br><span class="line">        pred_mask = (pred_mask &gt; <span class="number">0.5</span>).astype(<span class="string">&quot;int32&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> pred_mask[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show_predictions</span>(<span class="params">self, dataset, num=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="keyword">for</span> image, mask <span class="keyword">in</span> dataset.take(num):</span><br><span class="line">            pred_mask = model.predict(image)</span><br><span class="line">            self.display([image[<span class="number">0</span>], mask[<span class="number">0</span>], self.create_mask(pred_mask)])</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span>(<span class="params">self, epoch, logs=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> epoch <span class="keyword">and</span> epoch % self.epoch_interval == <span class="number">0</span>:</span><br><span class="line">            self.show_predictions(self.dataset)</span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&#x27;\nSample Prediction after epoch &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">30</span></span><br><span class="line">model.fit(</span><br><span class="line">    train_dataset, </span><br><span class="line">    epochs=epochs, </span><br><span class="line">    callbacks=[DisplayCallback(train_dataset)]</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>也需要有常规的checkpoint <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler</span><br><span class="line"></span><br><span class="line">filepath_dice_coeff = <span class="string">&quot;_val_loss.hdf5&quot;</span> <span class="comment"># 保存成一个单独的hdf5文件</span></span><br><span class="line">checkpointer = ModelCheckpoint(filepath_dice_coeff, monitor=<span class="string">&#x27;val_loss&#x27;</span>, verbose=<span class="number">1</span>, save_best_only=<span class="literal">True</span>, mode=<span class="string">&#x27;min&#x27;</span>)<span class="comment"># val_dice_coeff</span></span><br><span class="line">lr_reducer = ReduceLROnPlateau(factor=np.sqrt(<span class="number">0.1</span>), cooldown=<span class="number">0</span>, patience=<span class="number">30</span>, min_lr=<span class="number">0.5e-6</span>)</span><br><span class="line">early_stop = EarlyStopping(monitor=<span class="string">&#x27;val_loss&#x27;</span>, patience=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">hist = seg_classi_model.fit(train_data,</span><br><span class="line">                steps_per_epoch=(train_imgs.shape[<span class="number">0</span>] + batch_size - <span class="number">1</span>) // batch_size,</span><br><span class="line">                epochs=<span class="number">300</span>,</span><br><span class="line">                callbacks=[checkpointer, lr_reducer, early_stop],</span><br><span class="line">                validation_data=val_data,</span><br><span class="line">                validation_steps=(valid_imgs.shape[<span class="number">0</span>] + batch_size - <span class="number">1</span>) // batch_size) </span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>image-segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>numpy基础总结</title>
    <url>/2022/03/27/numpy%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="numpy.linalg.norm">numpy.linalg.norm</h1>
<p><code>linalg.norm(x,ord=None,axis=None,keepdims=False)</code></p>
<p>其中x可以传入array，也可以是matrix。</p>
<p>ord参数针对x是array还是matrix是不一样的计算：</p>
<table>
<thead>
<tr class="header">
<th>ord</th>
<th>norm for matrices</th>
<th>norm for vectors</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>None</td>
<td>Frobenius norm</td>
<td>2-norm</td>
</tr>
<tr class="even">
<td>‘fro’</td>
<td>Frobenius norm</td>
<td>–</td>
</tr>
<tr class="odd">
<td>‘nuc’</td>
<td>nuclear norm</td>
<td>–</td>
</tr>
<tr class="even">
<td>inf</td>
<td>max(sum(abs(x), axis=1))</td>
<td>max(abs(x))</td>
</tr>
<tr class="odd">
<td>-inf</td>
<td>min(sum(abs(x), axis=1))</td>
<td>min(abs(x))</td>
</tr>
<tr class="even">
<td>0</td>
<td>–</td>
<td>sum(x != 0)</td>
</tr>
<tr class="odd">
<td>1</td>
<td>max(sum(abs(x), axis=0))</td>
<td>as below</td>
</tr>
<tr class="even">
<td>-1</td>
<td>min(sum(abs(x), axis=0))</td>
<td>as below</td>
</tr>
<tr class="odd">
<td>2</td>
<td>2-norm (largest sing. value)</td>
<td>L2范数</td>
</tr>
<tr class="even">
<td>-2</td>
<td>smallest singular value</td>
<td>as below</td>
</tr>
<tr class="odd">
<td>other</td>
<td>–</td>
<td>sum(abs(x)<strong>ord)</strong>(1./ord)</td>
</tr>
</tbody>
</table>
<p>如果传入axis这个参数，axis=0表示每一列为向量，以每一列的向量为基础计算。然后就转化为了向量运算。</p>
<p>其中keepdims如果是True，那么规范化的轴将作为尺寸1留在结果中，使用此选项，结果将针对原始
x 正确广播。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.arange(<span class="number">9</span>) - <span class="number">4</span></span><br><span class="line">b = a.reshape((<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">a</span><br><span class="line">Out[<span class="number">7</span>]: array([-<span class="number">4</span>, -<span class="number">3</span>, -<span class="number">2</span>, -<span class="number">1</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>])</span><br><span class="line">b</span><br><span class="line">Out[<span class="number">8</span>]: </span><br><span class="line">array([[-<span class="number">4</span>, -<span class="number">3</span>, -<span class="number">2</span>],</span><br><span class="line">       [-<span class="number">1</span>,  <span class="number">0</span>,  <span class="number">1</span>],</span><br><span class="line">       [ <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line">LA.norm(b,axis=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">9</span>]: array([<span class="number">5.38516481</span>, <span class="number">1.41421356</span>, <span class="number">5.38516481</span>])</span><br><span class="line">LA.norm(b,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">Out[<span class="number">10</span>]: </span><br><span class="line">array([[<span class="number">5.38516481</span>],</span><br><span class="line">       [<span class="number">1.41421356</span>],</span><br><span class="line">       [<span class="number">5.38516481</span>]])</span><br></pre></td></tr></table></figure>
<p>从以上结果看出：如果keepdims等于True，那么输出的矩阵将会是3乘以1的矩阵，输入矩阵是3乘以3。如果用Input/
norm(input)是不会报错的。</p>
]]></content>
      <categories>
        <category>numpy</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title>pandas中count(),value_counts(),unique)区别</title>
    <url>/2021/12/13/pandas%E4%B8%ADcount-value-counts-unique-%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<h1 id="count">count()</h1>
<p>不管是Series还是DateFrame，两个类都有这个方法。</p>
<p>对于DateFrame，返回的是每一列中非空的元素的个数，可以指定是以行去统计还是以列去统计。</p>
<p>对于Series，返回的是Series中非空元素的个数。</p>
<p>对于groupby，返回的是每一组中元素的个数，这里的个数不包含空字段。</p>
<h1 id="value_counts">value_counts()</h1>
<p>对于DateFrame，返回的是unique行的个数，这里的unique行指的是只要其中一个column的值不一样就算不一样的行。</p>
<p>对于Series，返回的是每一个unique元素的个数，也就是每一个unique的元素，它会统计它在Series中出现了多少次。</p>
<h1 id="unique">unique()</h1>
<p>对于Series，返回的是所有unique的元素们，如果你想统计所有不一样的元素有多少个，可以统计list中元素的个数，如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">len(df[&#x27;a&#x27;].unique().tolist())  </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>pandas中对于时间TimeStamp的处理</title>
    <url>/2021/12/14/pandas%E4%B8%AD%E5%AF%B9%E4%BA%8E%E6%97%B6%E9%97%B4TimeStamp%E7%9A%84%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<h1 id="pandas.to_datetime"><code>pandas.to_datetime()</code></h1>
<p><strong>pandas.to_datetime(</strong><em>arg</em><strong>,</strong>
<em>errors='raise'</em><strong>,</strong>
<em>dayfirst=False</em><strong>,</strong>
<em>yearfirst=False</em><strong>,</strong>
<em>utc=None</em><strong>,</strong>
<em>format=None</em><strong>,</strong>
<em>exact=True</em><strong>,</strong>
<em>unit=None</em><strong>,</strong>
<em>infer_datetime_format=False</em><strong>,</strong>
<em>origin='unix'</em><strong>,</strong>
<em>cache=True</em><strong>)</strong>[<a href="https://github.com/pandas-dev/pandas/blob/v1.3.5/pandas/core/tools/datetimes.py#L676-L919">source]</a><a href="https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html?highlight=to_datetime#pandas.to_datetime">¶</a></p>
<p>Parameters</p>
<ul>
<li><p><strong>arg</strong> int, float, str, datetime, list, tuple, 1-d
array, Series, DataFrame/dict-like</p>
<p>The object to convert to a datetime</p></li>
<li><p><strong>format</strong> str, default None</p></li>
</ul>
<p>The strftime to parse time, eg “<code>%d/%m/%Y</code>”, note that
“<code>%f</code>” will parse all the way up to nanoseconds.</p>
<hr>
<p>该函数可以接受一个series，可以接受一个dateFrame。如果不确定它是否可以以默认的格式去解析你的时间，format参数可以不传递。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;year&#x27;</span>: [<span class="number">2015</span>, <span class="number">2016</span>],</span><br><span class="line"><span class="meta">... </span>                   <span class="string">&#x27;month&#x27;</span>: [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line"><span class="meta">... </span>                   <span class="string">&#x27;day&#x27;</span>: [<span class="number">4</span>, <span class="number">5</span>]&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.to_datetime(df)</span><br><span class="line"><span class="number">0</span>   <span class="number">2015</span>-02-04</span><br><span class="line"><span class="number">1</span>   <span class="number">2016</span>-03-05</span><br><span class="line">dtype: datetime64[ns]</span><br></pre></td></tr></table></figure>
<p>上面的这种产生<code>datetime</code>的方式，在创建dateframe的时候，可以指定缩写或者缩写的复数形式，其他形式不接受：<code>[‘year’, ‘month’, ‘day’, ‘minute’, ‘second’, ‘ms’, ‘us’, ‘ns’])</code></p>
<h1 id="pandas.to_timedelta"><code>pandas.to_timedelta()</code></h1>
<p>timedelta是两个时间之间的差值，该函数可以帮助我们求两个<code>timestamp</code>之间的差是多少（单位可以是<code>days,hours,minutes,seconds</code>）</p>
<p>Parameters</p>
<ul>
<li><p><strong>arg</strong> str, timedelta, list-like or Series</p></li>
<li><p><strong>unit</strong> str, optional</p>
<p>Denotes the unit of the arg for numeric arg. Defaults to
<code>"ns"</code>.</p>
<p>Possible values:</p>
<ul>
<li>‘W’</li>
<li>‘D’ / ‘days’ / ‘day’</li>
<li>‘hours’ / ‘hour’ / ‘hr’ / ‘h’</li>
<li>‘m’ / ‘minute’ / ‘min’ / ‘minutes’ / ‘T’</li>
<li>‘S’ / ‘seconds’ / ‘sec’ / ‘second’</li>
<li>‘ms’ / ‘milliseconds’ / ‘millisecond’ / ‘milli’ / ‘millis’ /
‘L’</li>
<li>‘us’ / ‘microseconds’ / ‘microsecond’ / ‘micro’ / ‘micros’ /
‘U’</li>
<li>‘ns’ / ‘nanoseconds’ / ‘nano’ / ‘nanos’ / ‘nanosecond’ / ‘N’</li>
</ul></li>
</ul>
<hr>
<p>这里如果传入的是str，是不允许再传入unit参数了，不然会报错。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; pd.to_timedelta(<span class="string">&#x27;15days 2hours&#x27;</span>)</span><br><span class="line">Timedelta(<span class="string">&#x27;15 days 02:00:00&#x27;</span>)</span><br><span class="line"></span><br><span class="line">&gt;&gt; pd.to_timedelta(<span class="string">&#x27;1 days 06:05:01.00003&#x27;</span>)</span><br><span class="line">Timedelta(<span class="string">&#x27;1 days 06:05:01.000030&#x27;</span>)</span><br><span class="line"></span><br><span class="line">&gt;&gt; pd.to_timedelta(<span class="number">4</span>,unit=<span class="string">&#x27;days&#x27;</span>)</span><br><span class="line">Timedelta(<span class="string">&#x27;4 days 00:00:00&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="series.dt"><code>Series.dt()</code></h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; seconds_series = pd.Series(pd.date_range(<span class="string">&quot;2000-01-01&quot;</span>, periods=<span class="number">3</span>, freq=<span class="string">&quot;s&quot;</span>))</span><br><span class="line">seconds_series</span><br><span class="line"><span class="number">0</span>   <span class="number">2000</span>-01-01 <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line"><span class="number">1</span>   <span class="number">2000</span>-01-01 <span class="number">00</span>:<span class="number">00</span>:01</span><br><span class="line"><span class="number">2</span>   <span class="number">2000</span>-01-01 <span class="number">00</span>:<span class="number">00</span>:02</span><br><span class="line">dtype: datetime64[ns]</span><br><span class="line">&gt;&gt; seconds_series.dt.second</span><br><span class="line"><span class="number">0</span>    <span class="number">0</span></span><br><span class="line"><span class="number">1</span>    <span class="number">1</span></span><br><span class="line"><span class="number">2</span>    <span class="number">2</span></span><br><span class="line">dtype: int64</span><br></pre></td></tr></table></figure>
<p>dt是Series的一个方法，当调用dt时，Series中必须是timestamp的格式。</p>
<p>当调用完dt后可以获取时间的具体年份等信息：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">seconds_series.dt.date <span class="comment"># 2000-01-01</span></span><br><span class="line">seconds_series.dt.hour <span class="comment"># 00</span></span><br><span class="line">seconds_series.dt.quarter <span class="comment"># 返回第几季度</span></span><br><span class="line">seconds_series.dt.time <span class="comment"># 00:00:00</span></span><br><span class="line">seconds_series.dt.year <span class="comment"># 2000</span></span><br><span class="line">seconds_series.dt.month <span class="comment"># 01</span></span><br><span class="line">seconds_series.dt.day <span class="comment"># 01</span></span><br><span class="line">seconds_series.dt.weekday <span class="comment"># 返回一个0-6的数，0表示周一，6表示周日</span></span><br><span class="line">seconds_series.dt.dayname() <span class="comment"># 会返回星期的名字：Monday</span></span><br></pre></td></tr></table></figure>
<p>有的时候我们想获取某一天是全年中的第几周，这时候weekday就不管用了，此时采用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dseries.dt.isocalendar()[<span class="string">&#x27;week&#x27;</span>] </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql查看当前使用的配置文件my.cnf的方法</title>
    <url>/2021/11/05/mysql%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8D%E4%BD%BF%E7%94%A8%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6my-cnf%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h1 id="mysql-查看当前使用的配置文件my.cnf的方法">mysql
查看当前使用的配置文件my.cnf的方法</h1>
<p>my.cnf是mysql启动时加载的配置文件，一般会放在mysql的安装目录中，用户也可以放在其他目录加载。</p>
<p>安装mysql后，系统中会有多个my.cnf文件，有些是用于测试的。</p>
<p>使用locate my.cnf命令可以列出所有的my.cnf文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">命令</span><br><span class="line">locate my.cnf</span><br><span class="line"></span><br><span class="line">输出</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/include/default_my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/federated/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/ndb/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/ndb_big/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/ndb_binlog/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/ndb_rpl/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/ndb_team/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/rpl/extension/bhs/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/rpl/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/rpl_ndb/my.cnf</span><br></pre></td></tr></table></figure>
<p>当我们需要修改配置文件时，需要找到mysql启动时是加载了哪个my.cnf文件。</p>
<h3 id="查看是否使用了指定目录的my.cnf">查看是否使用了指定目录的my.cnf</h3>
<p>启动mysql后，我们查看mysql的进程，看看是否有设置使用指定目录的my.cnf文件，如果有则表示mysql启动时是加载了这个配置文件。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">命令</span><br><span class="line">ps aux|grep mysql|grep &#x27;my.cnf&#x27;</span><br><span class="line"></span><br><span class="line">输出</span><br><span class="line">fdipzone         25174   0.0  0.0  3087244    600   ??  S     4:12下午   0:01.14 /usr/local/Cellar/mysql/5.6.24/bin/mysqld --defaults-file=/usr/local/Cellar/mysql/5.6.24/my.cnf --basedir=/usr/local/Cellar/mysql/5.6.24 --datadir=/usr/local/var/mysql --plugin-dir=/usr/local/Cellar/mysql/5.6.24/lib/plugin --bind-address=127.0.0.1 --log-error=/usr/local/var/mysql/TerrydeMacBook-Air.local.err --pid-file=/usr/local/var/mysql/TerrydeMacBook-Air.local.pid</span><br><span class="line">fdipzone         25064   0.0  0.0  2452824      4   ??  S     4:12下午   0:00.03 /bin/sh /usr/local/opt/mysql/bin/mysqld_safe --defaults-file=/usr/local/Cellar/mysql/5.6.24/my.cnf --bind-address=127.0.0.1 --datadir=/usr/local/var/mysql</span><br></pre></td></tr></table></figure>
<p>可以看到/usr/local/Cellar/mysql/5.6.24/my.cnf就是mysql启动加载的配置文件。</p>
<p>如果上面的命令没有输出，表示没有设置使用指定目录的my.cnf。</p>
<h3 id="查看mysql默认读取my.cnf的目录">查看mysql默认读取my.cnf的目录</h3>
<p>如果没有设置使用指定目录的my.cnf，mysql启动时会读取安装目录根目录及默认目录下的my.cnf文件。</p>
<p>查看mysql启动时读取配置文件的默认目录。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">命令</span><br><span class="line">mysql --help|grep &#x27;my.cnf&#x27;</span><br><span class="line"></span><br><span class="line">输出</span><br><span class="line">     order of preference, my.cnf, $MYSQL_TCP_PORT,</span><br><span class="line">/etc/my.cnf /etc/mysql/my.cnf /usr/local/etc/my.cnf ~/.my.cnf</span><br></pre></td></tr></table></figure>
<p>/etc/my.cnf, /etc/mysql/my.cnf, /usr/local/etc/my.cnf, ~/.my.cnf
这些就是mysql默认会搜寻my.cnf的目录，顺序排前的优先。</p>
<h3 id="启动时没有使用配置文件">启动时没有使用配置文件</h3>
<p>如果没有设置使用指定目录my.cnf文件及默认读取目录没有my.cnf文件，表示mysql启动时并没有加载配置文件，而是使用默认配置。</p>
<p>需要修改配置，可以在mysql默认读取的目录中，创建一个my.cnf文件(例如:/etc/my.cnf)，把需要修改的配置内容写入，重启mysql后即可生效。</p>
<h1 id="mysql使用group-by查询报错select-list-is-not-in-group-by-clause-and-contains-nonaggregated-column...解决方案">mysql使用group
by查询报错SELECT list is not in GROUP BY clause and contains
nonaggregated column...解决方案</h1>
<p><strong>MySQL5.7.5后only_full_group_by成为sql_mode的默认选项之一，这可能导致一些sql语句失效。</strong>
比如在使用<strong>group by</strong>进行分组查询报错</p>
<h3 id="查看自己的sql_mode配置">查看自己的sql_mode配置</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">在sql命令行中输入select @@sql_mode;这时我们能够看到自己的sql_mode配置,其中如果有ONLY_FULL_GROUP_BY,那它就是group by查询报错的罪魁祸首了</span><br></pre></td></tr></table></figure>
<h3 id="解决办法">解决办法</h3>
<p>命令行打开mysql.cnf,默认路径为/etc/mysql/conf.d/mysql.cnf,如果找不到可以使用whereis进行查询</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sql_mode = STRICT_TRANS_TABLES, NO_ZERO_IN_DATE, NO_ZERO_DATE, ERROR_FOR_DIVISION_BY_ZERO,      NO_AUTO_CREATE_USER, NO_ENGINE_SUBSTITUTION</span><br></pre></td></tr></table></figure>
<p>保存退出重启mysql</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo service mysqld restart #对于linux是mysql</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>其他技术</tag>
      </tags>
  </entry>
  <entry>
    <title>prompt engineering</title>
    <url>/2023/05/19/prompt-engineering/</url>
    <content><![CDATA[<p>在LLM兴起之后，很多工作都在如果更好的设计prompt的形式上下了很多苦功夫，设计提示词的这个学问就是prompt
engineering。我之前一直觉得这是一个静态的过程，建议刚接触的同学先学习吴恩达和openai合作制作的prompt
engineering的知道<a href="https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/">课程</a>,另外也有中国热心的网友也翻译了自己的中文版本，还加入了一些自己的思考，推荐<a href="https://github.com/LC1332/Prophet-Andrew-Ng/blob/main/content/2.%20%E6%8F%90%E7%A4%BA%E5%8E%9F%E5%88%99%20Guidelines.ipynb">骆驼</a>。</p>
<p>我是从斯坦福的羊驼模型开始接触instruction tuning的，instruction
tuning提供给了用户一种在特定数据集上低成本finetune模型的可能性，后来又接触到了斯坦福新出的paper：Demonstrate-Search-Predict:
Composing retrieval and language models for knowledge-intensive
NLP，这是一篇用信息抽取技术retrieval
models来扩充query，从而使得LM能够得到更多context的技术，这才让我了解到其实除了最基础的我们知道的那些prompting的方式，比如zero-shot,few-shot,chain-of-thought方式，还有一些方式在这些方式上进行了创新。</p>
<p>lilian weng的<a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/">博客</a>在chiin-of-thought章节之后就介绍了自动prompt
design和augmented language
models。这些对于我来说都是新东西，新技术发展的很快，我学习的过程其实也是在织自己知识结构中的这个网，从接触GPT以来写了好几篇博客，零零散散的，很多博客都是只局限于其中的技术，时不时我需要停下里思考这些技术之间的关联，从而把整个逻辑理顺。技术的发展或者一个模型一个方法的出现都是有路径可循的，我希望能把这些路径穿起来，摸清楚发展脉络。</p>
<h1 id="augmented-language-models">Augmented Language Models</h1>
<p>本节参考<a href="Augmented%20Language%20Models:%20a%20Survey">review</a>，我之前理解augmented
llm有点狭隘，最基本的方法是从一个知识库或者搜索引擎中搜索query相关的内容，将这些内容加到prompt里，从而一定程度上解决大模型的幻觉问题，这种思路的典型应用见<a href="http://arxiv.org/abs/2203.05115">Internet-augmented language
models through few-shot prompting for open-domain question
answering</a>.
但上面提到的这篇review把问题更细化一点，从更高的维度去分析现在的这些方法，第一个就是reasoning，也就是把复杂问题如何细化成LLM可以解决的简单问题，第二个是Tools，也就是我们上面介绍的应用的场景，tool帮我们抽取相关信息（document
retriever）。关于更具体地解释可以阅读review地1.2节。</p>
<h1 id="useful-links">Useful Links</h1>
<ul>
<li><a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#augmented-language-models">Prompt
engineering blog by Lilian Weng</a></li>
<li></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>pandas遇到的问题汇总</title>
    <url>/2022/03/17/pandas%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<h1 id="join">join()</h1>
<p>Examples</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df = pd.DataFrame(&#123;&#x27;key&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;, &#x27;K3&#x27;, &#x27;K4&#x27;, &#x27;K5&#x27;],</span><br><span class="line">...                    &#x27;A&#x27;: [&#x27;A0&#x27;, &#x27;A1&#x27;, &#x27;A2&#x27;, &#x27;A3&#x27;, &#x27;A4&#x27;, &#x27;A5&#x27;]&#125;)</span><br><span class="line">&gt;&gt;&gt; df</span><br><span class="line">  key   A</span><br><span class="line">0  K0  A0</span><br><span class="line">1  K1  A1</span><br><span class="line">2  K2  A2</span><br><span class="line">3  K3  A3</span><br><span class="line">4  K4  A4</span><br><span class="line">5  K5  A5</span><br><span class="line">&gt;&gt;&gt; other = pd.DataFrame(&#123;&#x27;key&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;],</span><br><span class="line">...                       &#x27;B&#x27;: [&#x27;B0&#x27;, &#x27;B1&#x27;, &#x27;B2&#x27;]&#125;)</span><br><span class="line">&gt;&gt;&gt; other</span><br><span class="line">  key   B</span><br><span class="line">0  K0  B0</span><br><span class="line">1  K1  B1</span><br><span class="line">2  K2  B2</span><br></pre></td></tr></table></figure>
<p>Join DataFrames using their indexes.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df.join(other, lsuffix=&#x27;_caller&#x27;, rsuffix=&#x27;_other&#x27;)</span><br><span class="line">  key_caller   A key_other    B</span><br><span class="line">0         K0  A0        K0   B0</span><br><span class="line">1         K1  A1        K1   B1</span><br><span class="line">2         K2  A2        K2   B2</span><br><span class="line">3         K3  A3       NaN  NaN</span><br><span class="line">4         K4  A4       NaN  NaN</span><br><span class="line">5         K5  A5       NaN  NaN</span><br></pre></td></tr></table></figure>
<p>如果不想以现有index为基础去Join，比如上面的例子想用key这个index去Join，可以：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt; df.join(other.set_index(&#x27;key&#x27;), on=&#x27;key&#x27;)</span><br><span class="line">  key   A    B</span><br><span class="line">0  K0  A0   B0</span><br><span class="line">1  K1  A1   B1</span><br><span class="line">2  K2  A2   B2</span><br><span class="line">3  K3  A3  NaN</span><br><span class="line">4  K4  A4  NaN</span><br><span class="line">5  K5  A5  NaN</span><br></pre></td></tr></table></figure>
<h1 id="merge">merge()</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df1 = pd.DataFrame(&#123;&#x27;lkey&#x27;: [&#x27;foo&#x27;, &#x27;bar&#x27;, &#x27;baz&#x27;, &#x27;foo&#x27;],</span><br><span class="line">                    &#x27;value&#x27;: [1, 2, 3, 5]&#125;)</span><br><span class="line">df2 = pd.DataFrame(&#123;&#x27;rkey&#x27;: [&#x27;foo&#x27;, &#x27;bar&#x27;, &#x27;baz&#x27;, &#x27;foo&#x27;],</span><br><span class="line">                    &#x27;value&#x27;: [5, 6, 7, 8]&#125;)</span><br><span class="line">df1</span><br><span class="line">    lkey value</span><br><span class="line">0   foo      1</span><br><span class="line">1   bar      2</span><br><span class="line">2   baz      3</span><br><span class="line">3   foo      5</span><br><span class="line">df2</span><br><span class="line">    rkey value</span><br><span class="line">0   foo      5</span><br><span class="line">1   bar      6</span><br><span class="line">2   baz      7</span><br><span class="line">3   foo      8</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="选择多列">选择多列</h1>
<p>有时候需要选择DataFrame中的多个列组成一个新的DataFrame，这时候要用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df[[&#x27;column1&#x27;,&#x27;column2&#x27;]] # 注意这里是双层中括号！</span><br></pre></td></tr></table></figure>
<h1 id="drop-满足条件的行">drop 满足条件的行</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_clear = df.drop(df[df[<span class="string">&#x27;x&#x27;</span>]&lt;<span class="number">0.01</span>].index)</span><br><span class="line"><span class="comment"># 也可以使用多个条件</span></span><br><span class="line">df_clear = df.drop(df[(df[<span class="string">&#x27;x&#x27;</span>]&lt;<span class="number">0.01</span>) | (df[<span class="string">&#x27;x&#x27;</span>]&gt;<span class="number">10</span>)].index) <span class="comment">#删除x小于0.01或大于10的行</span></span><br></pre></td></tr></table></figure>
<h1 id="dropna-丢掉某一列中有空的行">dropna 丢掉某一列中有空的行</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.dropna(subset=[<span class="string">&#x27;column_name&#x27;</span>])</span><br></pre></td></tr></table></figure>
<h1 id="applyapplymapmap">apply，applymap，map</h1>
<h2 id="apply">apply</h2>
<p>官方解释是 apply a function along an axis of the dataframe.</p>
<p>apply内函数的是Series，也就是在Series上做函数操作。index可以是dataframe的index，也可以是dataframe的columns(axis=1)。axis=0时，apply
the function to each
column，也就是每一列为一个整体去实施函数。axis=1时，每一行作为一个整体去实施函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.apply(np.<span class="built_in">sum</span>, axis=<span class="number">0</span>)</span><br><span class="line">df.apply(np.<span class="built_in">sum</span>, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="applymap">applymap</h2>
<p>官方解释是 apply a function to a dataframe
elementwise.和apply不一样的是对dataframe中每一个元素分别作函数。</p>
<p>applymap允许实施一个函数，这个函数接受和返回的都是一个标量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.applymap(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(<span class="built_in">str</span>(x)))</span><br></pre></td></tr></table></figure>
<h2 id="map">map</h2>
<p>map调用的对象只能是Series，而上面两个方法的调用对象是Dataframe。</p>
]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>python对Counter内容进行排序</title>
    <url>/2022/08/11/python-%E5%AF%B9Counter%E5%86%85%E5%AE%B9%E8%BF%9B%E8%A1%8C%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="对counter中的内容进行排序">对Counter中的内容进行排序</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">x = Counter(&#123;<span class="string">&#x27;a&#x27;</span>:<span class="number">5</span>, <span class="string">&#x27;b&#x27;</span>:<span class="number">3</span>, <span class="string">&#x27;c&#x27;</span>:<span class="number">7</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line">x.most_common() <span class="comment"># [(&#x27;c&#x27;, 7), (&#x27;a&#x27;, 5), (&#x27;b&#x27;, 3)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="built_in">sorted</span>(x, key=x.get, reverse=<span class="literal">True</span>) <span class="comment"># [&#x27;c&#x27;, &#x27;a&#x27;, &#x27;b&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"><span class="built_in">sorted</span>(x.items(), key=<span class="keyword">lambda</span> pair: pair[<span class="number">1</span>], reverse=<span class="literal">True</span>) <span class="comment"># [(&#x27;c&#x27;, 7), (&#x27;a&#x27;, 5), (&#x27;b&#x27;, 3)]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>一共三种方法，其中第二种方法中传入<code>key=x.get</code>最终返回的是所有的key，而其他两种方法返回的都是排完序之后的list</p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>python OS 文件/目录常用方法总结</title>
    <url>/2021/12/30/python-OS-%E6%96%87%E4%BB%B6-%E7%9B%AE%E5%BD%95%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.makedirs(path[, mode]) </span><br><span class="line">递归文件夹创建函数。像mkdir(), 但创建的所有intermediate-level文件夹需要包含子文件夹。</span><br><span class="line"></span><br><span class="line">os.walk(top[, topdown=<span class="literal">True</span>[, onerror=<span class="literal">None</span>[, followlinks=<span class="literal">False</span>]]])</span><br><span class="line">输出在文件夹中的文件名通过在树中游走，向上或者向下。返回的是一个三元组(root,dirs,files),是一个生成器类型，需要用<span class="keyword">for</span>遍历读取</span><br><span class="line"></span><br><span class="line">os.chdir(path)</span><br><span class="line">改变当前工作目录</span><br><span class="line">	</span><br><span class="line">os.listdir(path)</span><br><span class="line">返回path指定的文件夹包含的文件或文件夹的名字的列表。</span><br></pre></td></tr></table></figure>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(<span class="string">&quot;.&quot;</span>, topdown=<span class="literal">False</span>):</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> files:</span><br><span class="line">        <span class="built_in">print</span>(os.path.join(root, name))</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> dirs:</span><br><span class="line">        <span class="built_in">print</span>(os.path.join(root, name))</span><br><span class="line">        </span><br><span class="line">path = <span class="string">&quot;/var/www/html/&quot;</span></span><br><span class="line">dirs = os.listdir( path )</span><br><span class="line"><span class="comment"># 输出所有文件和文件夹</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> dirs:</span><br><span class="line">   <span class="built_in">print</span> (file)</span><br></pre></td></tr></table></figure>
<p><code>os.walk</code>和<code>os.listdir</code>两个函数的区别在于前者会遍历到子文件夹中的子文件，而后者只是返回你传入的path中的文件夹名字和文件名字。</p>
<p><code>os</code>库中有一个<code>path</code>的模块，专门用于处理文件<code>path</code>相关的属性信息。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">os.path.abspath(path)</span><br><span class="line">返回绝对路径</span><br><span class="line"></span><br><span class="line">os.path.basename(path)</span><br><span class="line">返回文件名</span><br><span class="line"></span><br><span class="line">os.path.exists(path)</span><br><span class="line">如果路径 path 存在，返回 True；如果路径 path 不存在，返回 False。</span><br><span class="line"></span><br><span class="line">os.path.join(path1[, path2[, ...]])</span><br><span class="line">把目录和文件名合成一个路径</span><br><span class="line"></span><br><span class="line">os.path.split(path)</span><br><span class="line">把路径分割成 dirname 和 basename，返回一个元组</span><br><span class="line"></span><br><span class="line">os.path.splitext(path)</span><br><span class="line">分割路径，返回路径名和文件扩展名的元组</span><br><span class="line"></span><br><span class="line">os.path.walk(path, visit, arg)</span><br><span class="line">遍历path，进入每个目录都调用visit函数，visit函数必须有3个参数(arg, dirname, names)，dirname表示当前目录的目录名，names代表当前目录下的所有文件名，args则为walk的第三个参数</span><br></pre></td></tr></table></figure>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>( os.path.basename(<span class="string">&#x27;/root/runoob.txt&#x27;</span>) )   <span class="comment"># 返回文件名</span></span><br><span class="line"><span class="built_in">print</span>( os.path.dirname(<span class="string">&#x27;/root/runoob.txt&#x27;</span>) )    <span class="comment"># 返回目录路径</span></span><br><span class="line"><span class="built_in">print</span>( os.path.split(<span class="string">&#x27;/root/runoob.txt&#x27;</span>) )      <span class="comment"># 分割文件名与路径</span></span><br><span class="line"><span class="built_in">print</span>( os.path.join(<span class="string">&#x27;root&#x27;</span>,<span class="string">&#x27;test&#x27;</span>,<span class="string">&#x27;runoob.txt&#x27;</span>) )  <span class="comment"># 将目录和文件名合成一个路径</span></span><br><span class="line"><span class="built_in">print</span>( os.path.splitext(<span class="string">&#x27;/root/runoob.txt&#x27;</span>) )</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">runoob.txt</span><br><span class="line">/root</span><br><span class="line">(&#x27;/root&#x27;, &#x27;runoob.txt&#x27;)</span><br><span class="line">root/test/runoob.txt</span><br><span class="line">(&#x27;/root/run/test&#x27;, &#x27;.txt&#x27;)</span><br></pre></td></tr></table></figure>
<p>上面的split和splitext，前者分割出了文件名和路径，而后者可以分割出路径名和扩展名，如果想要获得文件的扩展名，可以用splitext，传入文件的path就可以了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">os.mknod(filename) 创建</span><br></pre></td></tr></table></figure>
<h1 id="复制文件和删除文件移动文件">复制文件和删除文件,移动文件</h1>
<p>如果是删除一个目录，可以使用以下两种方式:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="comment"># method 1</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(root_dir):</span><br><span class="line">  shutil.rmtree(root_dir) <span class="comment"># 这里不可以使用os.removedirs(),removedirs只可以删除非空的文件夹，rmdir也是</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># method 2</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(os.listdir(root_dir)) &gt; <span class="number">0</span>:</span><br><span class="line">  <span class="keyword">for</span> file <span class="keyword">in</span> os.scandir(root_dir):</span><br><span class="line">    os.remove(file.path)</span><br></pre></td></tr></table></figure>
<p>如果想复制一个文件到另外一个文件夹，参考 <a href="https://zhuanlan.zhihu.com/p/35725217" class="uri">https://zhuanlan.zhihu.com/p/35725217</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">copyfile(source_file, destination_file)</span><br></pre></td></tr></table></figure>
<p>记住这里第二个参数一定要是可写入的文件名字，而不是目录。</p>
<h2 id="移动文件">移动文件</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"> </span><br><span class="line">file_source = <span class="string">&#x27;Path/Of/Directory&#x27;</span></span><br><span class="line">file_destination = <span class="string">&#x27;Path/Of/Directory&#x27;</span></span><br><span class="line"> </span><br><span class="line">get_files = os.listdir(file_source)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> g <span class="keyword">in</span> get_files:</span><br><span class="line">    shutil.move(file_source + g, file_destination)</span><br></pre></td></tr></table></figure>
<h1 id="读写csv文件">读写csv文件</h1>
<p>第一个方式是用pandas，具体不介绍。</p>
<p>这里总结一下csv库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取csv文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filename,<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">     csv_reader = csv.reader(csvfile)</span><br><span class="line">     <span class="keyword">for</span> row <span class="keyword">in</span> csv_reader:</span><br><span class="line">        <span class="built_in">print</span>(row[<span class="number">1</span>]) <span class="comment"># 用列表的index取值</span></span><br><span class="line"><span class="comment"># 写csv文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filename,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    csv_writer = csv.writer(csvfile)</span><br><span class="line">    spamwriter.writerow([<span class="string">&#x27;Spam&#x27;</span>, <span class="string">&#x27;Lovely Spam&#x27;</span>, <span class="string">&#x27;Wonderful Spam&#x27;</span>]) <span class="comment"># writerow接受一个list，所有值都会写在一行里</span></span><br><span class="line">    spamwriter.writerows([[],[],[]]) <span class="comment"># writerows写入多行，每一行是一个列表，传进去的是列表的列表</span></span><br></pre></td></tr></table></figure>
<p>除了写入list，还可以写字典类型的数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># my data rows as dictionary objects </span></span><br><span class="line">mydict =[&#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;COE&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;9.0&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Nikhil&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;2&#x27;</span>&#125;, </span><br><span class="line">         &#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;COE&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;9.1&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Sanchit&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;2&#x27;</span>&#125;, </span><br><span class="line">         &#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;IT&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;9.3&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Aditya&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;2&#x27;</span>&#125;, </span><br><span class="line">         &#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;SE&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;9.5&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Sagar&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;1&#x27;</span>&#125;, </span><br><span class="line">         &#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;MCE&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;7.8&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Prateek&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;3&#x27;</span>&#125;, </span><br><span class="line">         &#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;EP&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;9.1&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Sahil&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;2&#x27;</span>&#125;] </span><br><span class="line">    </span><br><span class="line"><span class="comment"># field names </span></span><br><span class="line">fields = [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;branch&#x27;</span>, <span class="string">&#x27;year&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>] </span><br><span class="line">    </span><br><span class="line"><span class="comment"># name of csv file </span></span><br><span class="line">filename = <span class="string">&quot;university_records.csv&quot;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># writing to csv file </span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> csvfile: </span><br><span class="line">    <span class="comment"># creating a csv dict writer object </span></span><br><span class="line">    writer = csv.DictWriter(csvfile, fieldnames = fields) </span><br><span class="line">        </span><br><span class="line">    <span class="comment"># writing headers (field names) </span></span><br><span class="line">    writer.writeheader() </span><br><span class="line">        </span><br><span class="line">    <span class="comment"># writing data rows </span></span><br><span class="line">    writer.writerows(mydict)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>python 全角转半角</title>
    <url>/2022/04/15/python-%E5%85%A8%E8%A7%92%E8%BD%AC%E5%8D%8A%E8%A7%92/</url>
    <content><![CDATA[<p>全角与半角转换在处理汉语语料中会经常出现，这里分别说明汉字、数字、字母的unicode编码范围。以及全角与半角的转换方法。最后给出wiki上全角和半角的编码对照表。这里Python需要用Python3版本。</p>
<h3 id="汉字的判断">汉字的判断</h3>
<p>汉字的unicode编码范围 u4e00 到 u9fa5。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_chinese</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断一个unicode是否是汉字&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> uchar &gt;= <span class="string">u&#x27;\u4e00&#x27;</span> <span class="keyword">and</span> uchar&lt;=<span class="string">u&#x27;\u9fa5&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="数字0-9的判断">数字0-9的判断</h3>
<p>数字的unicode编码范围根据全角和半角，有两个不同区域，半角数字 u0030
到 u0039，全角数字 uff10 到 uff19。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_number</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断一个unicode是否是半角数字&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> uchar &gt;= <span class="string">u&#x27;\u0030&#x27;</span> <span class="keyword">and</span> uchar&lt;=<span class="string">u&#x27;\u0039&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_Qnumber</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断一个unicode是否是全角数字&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> uchar &gt;= <span class="string">u&#x27;\uff10&#x27;</span> <span class="keyword">and</span> uchar &lt;= <span class="string">u&#x27;\uff19&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="大小写字母判断">大小写字母判断</h3>
<p>字母的unicode编码根据字母大小写，以及全角和半角共有四个区域。
半角大写字母：u0041 - u005a ，半角小写字母：u0061 - u007a ；
全角大写字母：uff21 - uff3a ， 全角小写字母：uff41 - uff5a 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_alphabet</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断一个unicode是否是半角英文字母&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> (uchar &gt;= <span class="string">u&#x27;\u0041&#x27;</span> <span class="keyword">and</span> uchar &lt;= <span class="string">u&#x27;\u005a&#x27;</span>) <span class="keyword">or</span> (uchar &gt;= <span class="string">u&#x27;\u0061&#x27;</span> <span class="keyword">and</span> uchar &lt;= <span class="string">u&#x27;\u007a&#x27;</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_Qalphabet</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断一个unicode是否是全角英文字母&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> (uchar &gt;= <span class="string">u&#x27;\uff21&#x27;</span> <span class="keyword">and</span> uchar &lt;= <span class="string">u&#x27;\uff3a&#x27;</span>) <span class="keyword">or</span> (uchar &gt;= <span class="string">u&#x27;\uff41&#x27;</span> <span class="keyword">and</span> uchar &lt;= <span class="string">u&#x27;\uff5a&#x27;</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="非汉字和数字字母的判断">非汉字和数字字母的判断</h3>
<p>判断除汉字、数字0-9、字母之外的字符。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_other</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断是否非汉字，数字和英文字符&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> (is_chinese(uchar) <span class="keyword">or</span> is_number(uchar) <span class="keyword">or</span> is_alphabet(uchar)):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="全角和半角的转换">全角和半角的转换</h3>
<p>全角半角转换需要用到上面的数字、字母等判断。</p>
<ol type="1">
<li>所有半角转全角，不是半角范围直接返回，空格半角特殊单独处理，其它半角和全角对应公式：半角
= 全角 - 0xfee0</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">B2Q</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;单个字符 半角转全角&quot;&quot;&quot;</span></span><br><span class="line">    inside_code = <span class="built_in">ord</span>(uchar)</span><br><span class="line">    <span class="keyword">if</span> inside_code &lt; <span class="number">0x0020</span> <span class="keyword">or</span> inside_code &gt; <span class="number">0x7e</span>: <span class="comment"># 不是半角字符就返回原来的字符</span></span><br><span class="line">        <span class="keyword">return</span> uchar </span><br><span class="line">    <span class="keyword">if</span> inside_code == <span class="number">0x0020</span>: <span class="comment"># 除了空格其他的全角半角的公式为: 半角 = 全角 - 0xfee0</span></span><br><span class="line">        inside_code = <span class="number">0x3000</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        inside_code += <span class="number">0xfee0</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">chr</span>(inside_code)</span><br></pre></td></tr></table></figure>
<ol type="1">
<li>所有全角转半角，和前面正好相反，公式对应：全角 = 半角 + 0xfee0</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Q2B</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;单个字符 全角转半角&quot;&quot;&quot;</span></span><br><span class="line">    inside_code = <span class="built_in">ord</span>(uchar)</span><br><span class="line">    <span class="keyword">if</span> inside_code == <span class="number">0x3000</span>:</span><br><span class="line">        inside_code = <span class="number">0x0020</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        inside_code -= <span class="number">0xfee0</span></span><br><span class="line">    <span class="keyword">if</span> inside_code &lt; <span class="number">0x0020</span> <span class="keyword">or</span> inside_code &gt; <span class="number">0x7e</span>: <span class="comment">#转完之后不是半角字符返回原来的字符</span></span><br><span class="line">        <span class="keyword">return</span> uchar</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">chr</span>(inside_code)</span><br></pre></td></tr></table></figure>
<ol type="1">
<li>把整个字符串全角转半角，也可以只转部分如数字和字母</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stringQ2B</span>(<span class="params">ustring</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;把字符串全角转半角&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join([Q2B(uchar) <span class="keyword">for</span> uchar <span class="keyword">in</span> ustring])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stringpartQ2B</span>(<span class="params">ustring</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;把字符串中数字和字母全角转半角&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join([Q2B(uchar) <span class="keyword">if</span> is_Qnumber(uchar) <span class="keyword">or</span> is_Qalphabet(uchar) <span class="keyword">else</span> uchar <span class="keyword">for</span> uchar <span class="keyword">in</span> ustring])</span><br></pre></td></tr></table></figure>
<p>测试上面的全角半角转换。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">text = <span class="string">&quot;电影《２０１２》讲述了２０１２年１２月２１日的世界末日，主人公Ｊａｃｋ以及世界各国人民挣扎求生的经历，灾难面前，尽现人间百态。&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;text原文：&quot;</span>, text, sep=<span class="string">&quot;\n&quot;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">text1 = stringQ2B(text)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;全角转半角：&quot;</span>, text1, sep=<span class="string">&quot;\n&quot;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">text2 = stringpartQ2B(text)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数字字母全角转半角：&quot;</span>, text2, sep=<span class="string">&quot;\n&quot;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果如下，只转数字字母与全部转是有区别的：</p>
<figure>
<img src="https:////upload-images.jianshu.io/upload_images/17669901-55fb5ccee96dcf60.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/980/format/webp" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>全角半角转换结果</p>
<h3 id="附全角和半角编码对应表">附全角和半角编码对应表</h3>
<ol type="1">
<li>ASCII内字符的全角和半角，包括数字0-9、大小写字母、标点符号等。</li>
</ol>
<table>
<thead>
<tr class="header">
<th>ASCII</th>
<th>全角字符</th>
<th>Unicode</th>
<th>半角字符</th>
<th>Unicode</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0x20</td>
<td>"　"</td>
<td>U+3000</td>
<td>" "</td>
<td>U+0020</td>
</tr>
<tr class="even">
<td>0x21</td>
<td>！</td>
<td>U+FF01</td>
<td>!</td>
<td>U+0021</td>
</tr>
<tr class="odd">
<td>0x22</td>
<td>＂</td>
<td>U+FF02</td>
<td>"</td>
<td>U+0022</td>
</tr>
<tr class="even">
<td>0x23</td>
<td>＃</td>
<td>U+FF03</td>
<td>#</td>
<td>U+0023</td>
</tr>
<tr class="odd">
<td>0x24</td>
<td>＄</td>
<td>U+FF04</td>
<td>$</td>
<td>U+0024</td>
</tr>
<tr class="even">
<td>0x25</td>
<td>％</td>
<td>U+FF05</td>
<td>%</td>
<td>U+0025</td>
</tr>
<tr class="odd">
<td>0x26</td>
<td>＆</td>
<td>U+FF06</td>
<td>&amp;</td>
<td>U+0026</td>
</tr>
<tr class="even">
<td>0x27</td>
<td>＇</td>
<td>U+FF07</td>
<td>'</td>
<td>U+0027</td>
</tr>
<tr class="odd">
<td>0x28</td>
<td>（</td>
<td>U+FF08</td>
<td>(</td>
<td>U+0028</td>
</tr>
<tr class="even">
<td>0x29</td>
<td>）</td>
<td>U+FF09</td>
<td>)</td>
<td>U+0029</td>
</tr>
<tr class="odd">
<td>0x2A</td>
<td>＊</td>
<td>U+FF0A</td>
<td>*</td>
<td>U+002A</td>
</tr>
<tr class="even">
<td>0x2B</td>
<td>＋</td>
<td>U+FF0B</td>
<td>+</td>
<td>U+002B</td>
</tr>
<tr class="odd">
<td>0x2C</td>
<td>，</td>
<td>U+FF0C</td>
<td>,</td>
<td>U+002C</td>
</tr>
<tr class="even">
<td>0x2D</td>
<td>－</td>
<td>U+FF0D</td>
<td>-</td>
<td>U+002D</td>
</tr>
<tr class="odd">
<td>0x2E</td>
<td>．</td>
<td>U+FF0E</td>
<td>.</td>
<td>U+002E</td>
</tr>
<tr class="even">
<td>0x2F</td>
<td>／</td>
<td>U+FF0F</td>
<td>/</td>
<td>U+002F</td>
</tr>
<tr class="odd">
<td>0x30</td>
<td>０</td>
<td>U+FF10</td>
<td>0</td>
<td>U+0030</td>
</tr>
<tr class="even">
<td>0x31</td>
<td>１</td>
<td>U+FF11</td>
<td>1</td>
<td>U+0031</td>
</tr>
<tr class="odd">
<td>0x32</td>
<td>２</td>
<td>U+FF12</td>
<td>2</td>
<td>U+0032</td>
</tr>
<tr class="even">
<td>0x33</td>
<td>３</td>
<td>U+FF13</td>
<td>3</td>
<td>U+0033</td>
</tr>
<tr class="odd">
<td>0x34</td>
<td>４</td>
<td>U+FF14</td>
<td>4</td>
<td>U+0034</td>
</tr>
<tr class="even">
<td>0x35</td>
<td>５</td>
<td>U+FF15</td>
<td>5</td>
<td>U+0035</td>
</tr>
<tr class="odd">
<td>0x36</td>
<td>６</td>
<td>U+FF16</td>
<td>6</td>
<td>U+0036</td>
</tr>
<tr class="even">
<td>0x37</td>
<td>７</td>
<td>U+FF17</td>
<td>7</td>
<td>U+0037</td>
</tr>
<tr class="odd">
<td>0x38</td>
<td>８</td>
<td>U+FF18</td>
<td>8</td>
<td>U+0038</td>
</tr>
<tr class="even">
<td>0x39</td>
<td>９</td>
<td>U+FF19</td>
<td>9</td>
<td>U+0039</td>
</tr>
<tr class="odd">
<td>0x3A</td>
<td>：</td>
<td>U+FF1A</td>
<td>:</td>
<td>U+003A</td>
</tr>
<tr class="even">
<td>0x3B</td>
<td>；</td>
<td>U+FF1B</td>
<td>;</td>
<td>U+003B</td>
</tr>
<tr class="odd">
<td>0x3C</td>
<td>＜</td>
<td>U+FF1C</td>
<td>&lt;</td>
<td>U+003C</td>
</tr>
<tr class="even">
<td>0x3D</td>
<td>＝</td>
<td>U+FF1D</td>
<td>=</td>
<td>U+003D</td>
</tr>
<tr class="odd">
<td>0x3E</td>
<td>＞</td>
<td>U+FF1E</td>
<td>&gt;</td>
<td>U+003E</td>
</tr>
<tr class="even">
<td>0x3F</td>
<td>？</td>
<td>U+FF1F</td>
<td>?</td>
<td>U+003F</td>
</tr>
<tr class="odd">
<td>0x40</td>
<td>＠</td>
<td>U+FF20</td>
<td>@</td>
<td>U+0040</td>
</tr>
<tr class="even">
<td>0x41</td>
<td>Ａ</td>
<td>U+FF21</td>
<td>A</td>
<td>U+0041</td>
</tr>
<tr class="odd">
<td>0x42</td>
<td>Ｂ</td>
<td>U+FF22</td>
<td>B</td>
<td>U+0042</td>
</tr>
<tr class="even">
<td>0x43</td>
<td>Ｃ</td>
<td>U+FF23</td>
<td>C</td>
<td>U+0043</td>
</tr>
<tr class="odd">
<td>0x44</td>
<td>Ｄ</td>
<td>U+FF24</td>
<td>D</td>
<td>U+0044</td>
</tr>
<tr class="even">
<td>0x45</td>
<td>Ｅ</td>
<td>U+FF25</td>
<td>E</td>
<td>U+0045</td>
</tr>
<tr class="odd">
<td>0x46</td>
<td>Ｆ</td>
<td>U+FF26</td>
<td>F</td>
<td>U+0046</td>
</tr>
<tr class="even">
<td>0x47</td>
<td>Ｇ</td>
<td>U+FF27</td>
<td>G</td>
<td>U+0047</td>
</tr>
<tr class="odd">
<td>0x48</td>
<td>Ｈ</td>
<td>U+FF28</td>
<td>H</td>
<td>U+0048</td>
</tr>
<tr class="even">
<td>0x49</td>
<td>Ｉ</td>
<td>U+FF29</td>
<td>I</td>
<td>U+0049</td>
</tr>
<tr class="odd">
<td>0x4A</td>
<td>Ｊ</td>
<td>U+FF2A</td>
<td>J</td>
<td>U+004A</td>
</tr>
<tr class="even">
<td>0x4B</td>
<td>Ｋ</td>
<td>U+FF2B</td>
<td>K</td>
<td>U+004B</td>
</tr>
<tr class="odd">
<td>0x4C</td>
<td>Ｌ</td>
<td>U+FF2C</td>
<td>L</td>
<td>U+004C</td>
</tr>
<tr class="even">
<td>0x4D</td>
<td>Ｍ</td>
<td>U+FF2D</td>
<td>M</td>
<td>U+004D</td>
</tr>
<tr class="odd">
<td>0x4E</td>
<td>Ｎ</td>
<td>U+FF2E</td>
<td>N</td>
<td>U+004E</td>
</tr>
<tr class="even">
<td>0x4F</td>
<td>Ｏ</td>
<td>U+FF2F</td>
<td>O</td>
<td>U+004F</td>
</tr>
<tr class="odd">
<td>0x50</td>
<td>Ｐ</td>
<td>U+FF30</td>
<td>P</td>
<td>U+0050</td>
</tr>
<tr class="even">
<td>0x51</td>
<td>Ｑ</td>
<td>U+FF31</td>
<td>Q</td>
<td>U+0051</td>
</tr>
<tr class="odd">
<td>0x52</td>
<td>Ｒ</td>
<td>U+FF32</td>
<td>R</td>
<td>U+0052</td>
</tr>
<tr class="even">
<td>0x53</td>
<td>Ｓ</td>
<td>U+FF33</td>
<td>S</td>
<td>U+0053</td>
</tr>
<tr class="odd">
<td>0x54</td>
<td>Ｔ</td>
<td>U+FF34</td>
<td>T</td>
<td>U+0054</td>
</tr>
<tr class="even">
<td>0x55</td>
<td>Ｕ</td>
<td>U+FF35</td>
<td>U</td>
<td>U+0055</td>
</tr>
<tr class="odd">
<td>0x56</td>
<td>Ｖ</td>
<td>U+FF36</td>
<td>V</td>
<td>U+0056</td>
</tr>
<tr class="even">
<td>0x57</td>
<td>Ｗ</td>
<td>U+FF37</td>
<td>W</td>
<td>U+0057</td>
</tr>
<tr class="odd">
<td>0x58</td>
<td>Ｘ</td>
<td>U+FF38</td>
<td>X</td>
<td>U+0058</td>
</tr>
<tr class="even">
<td>0x59</td>
<td>Ｙ</td>
<td>U+FF39</td>
<td>Y</td>
<td>U+0059</td>
</tr>
<tr class="odd">
<td>0x5A</td>
<td>Ｚ</td>
<td>U+FF3A</td>
<td>Z</td>
<td>U+005A</td>
</tr>
<tr class="even">
<td>0x5B</td>
<td>［</td>
<td>U+FF3B</td>
<td>[</td>
<td>U+005B</td>
</tr>
<tr class="odd">
<td>0x5C</td>
<td>＼</td>
<td>U+FF3C</td>
<td>\</td>
<td>U+005C</td>
</tr>
<tr class="even">
<td>0x5D</td>
<td>］</td>
<td>U+FF3D</td>
<td>]</td>
<td>U+005D</td>
</tr>
<tr class="odd">
<td>0x5E</td>
<td>＾</td>
<td>U+FF3E</td>
<td>^</td>
<td>U+005E</td>
</tr>
<tr class="even">
<td>0x5F</td>
<td>＿</td>
<td>U+FF3F</td>
<td>_</td>
<td>U+005F</td>
</tr>
<tr class="odd">
<td>0x60</td>
<td>｀</td>
<td>U+FF40</td>
<td>`</td>
<td>U+0060</td>
</tr>
<tr class="even">
<td>0x61</td>
<td>ａ</td>
<td>U+FF41</td>
<td>a</td>
<td>U+0061</td>
</tr>
<tr class="odd">
<td>0x62</td>
<td>ｂ</td>
<td>U+FF42</td>
<td>b</td>
<td>U+0062</td>
</tr>
<tr class="even">
<td>0x63</td>
<td>ｃ</td>
<td>U+FF43</td>
<td>c</td>
<td>U+0063</td>
</tr>
<tr class="odd">
<td>0x64</td>
<td>ｄ</td>
<td>U+FF44</td>
<td>d</td>
<td>U+0064</td>
</tr>
<tr class="even">
<td>0x65</td>
<td>ｅ</td>
<td>U+FF45</td>
<td>e</td>
<td>U+0065</td>
</tr>
<tr class="odd">
<td>0x66</td>
<td>ｆ</td>
<td>U+FF46</td>
<td>f</td>
<td>U+0066</td>
</tr>
<tr class="even">
<td>0x67</td>
<td>ｇ</td>
<td>U+FF47</td>
<td>g</td>
<td>U+0067</td>
</tr>
<tr class="odd">
<td>0x68</td>
<td>ｈ</td>
<td>U+FF48</td>
<td>h</td>
<td>U+0068</td>
</tr>
<tr class="even">
<td>0x69</td>
<td>ｉ</td>
<td>U+FF49</td>
<td>i</td>
<td>U+0069</td>
</tr>
<tr class="odd">
<td>0x6A</td>
<td>ｊ</td>
<td>U+FF4A</td>
<td>j</td>
<td>U+006A</td>
</tr>
<tr class="even">
<td>0x6B</td>
<td>ｋ</td>
<td>U+FF4B</td>
<td>k</td>
<td>U+006B</td>
</tr>
<tr class="odd">
<td>0x6C</td>
<td>ｌ</td>
<td>U+FF4C</td>
<td>l</td>
<td>U+006C</td>
</tr>
<tr class="even">
<td>0x6D</td>
<td>ｍ</td>
<td>U+FF4D</td>
<td>m</td>
<td>U+006D</td>
</tr>
<tr class="odd">
<td>0x6E</td>
<td>ｎ</td>
<td>U+FF4E</td>
<td>n</td>
<td>U+006E</td>
</tr>
<tr class="even">
<td>0x6F</td>
<td>ｏ</td>
<td>U+FF4F</td>
<td>o</td>
<td>U+006F</td>
</tr>
<tr class="odd">
<td>0x70</td>
<td>ｐ</td>
<td>U+FF50</td>
<td>p</td>
<td>U+0070</td>
</tr>
<tr class="even">
<td>0x71</td>
<td>ｑ</td>
<td>U+FF51</td>
<td>q</td>
<td>U+0071</td>
</tr>
<tr class="odd">
<td>0x72</td>
<td>ｒ</td>
<td>U+FF52</td>
<td>r</td>
<td>U+0072</td>
</tr>
<tr class="even">
<td>0x73</td>
<td>ｓ</td>
<td>U+FF53</td>
<td>s</td>
<td>U+0073</td>
</tr>
<tr class="odd">
<td>0x74</td>
<td>ｔ</td>
<td>U+FF54</td>
<td>t</td>
<td>U+0074</td>
</tr>
<tr class="even">
<td>0x75</td>
<td>ｕ</td>
<td>U+FF55</td>
<td>u</td>
<td>U+0075</td>
</tr>
<tr class="odd">
<td>0x76</td>
<td>ｖ</td>
<td>U+FF56</td>
<td>v</td>
<td>U+0076</td>
</tr>
<tr class="even">
<td>0x77</td>
<td>ｗ</td>
<td>U+FF57</td>
<td>w</td>
<td>U+0077</td>
</tr>
<tr class="odd">
<td>0x78</td>
<td>ｘ</td>
<td>U+FF58</td>
<td>x</td>
<td>U+0078</td>
</tr>
<tr class="even">
<td>0x79</td>
<td>ｙ</td>
<td>U+FF59</td>
<td>y</td>
<td>U+0079</td>
</tr>
<tr class="odd">
<td>0x7A</td>
<td>ｚ</td>
<td>U+FF5A</td>
<td>z</td>
<td>U+007A</td>
</tr>
<tr class="even">
<td>0x7B</td>
<td>｛</td>
<td>U+FF5B</td>
<td>{</td>
<td>U+007B</td>
</tr>
<tr class="odd">
<td>0x7C</td>
<td>｜</td>
<td>U+FF5C</td>
<td>|</td>
<td>U+007C</td>
</tr>
<tr class="even">
<td>0x7D</td>
<td>｝</td>
<td>U+FF5D</td>
<td>}</td>
<td>U+007D</td>
</tr>
<tr class="odd">
<td>0x7E</td>
<td>～</td>
<td>U+FF5E</td>
<td>~</td>
<td>U+007E</td>
</tr>
</tbody>
</table>
<ol type="1">
<li>其它特殊字符的全角和半角</li>
</ol>
<table>
<thead>
<tr class="header">
<th>半角字符</th>
<th>Unicode</th>
<th>全角字符</th>
<th>Unicode</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>⦅</td>
<td>U+2985</td>
<td>｟</td>
<td>U+FF5F</td>
</tr>
<tr class="even">
<td>⦆</td>
<td>U+2986</td>
<td>｠</td>
<td>U+FF60</td>
</tr>
<tr class="odd">
<td>¢</td>
<td>U+00A2</td>
<td>￠</td>
<td>U+FFE0</td>
</tr>
<tr class="even">
<td>£</td>
<td>U+00A3</td>
<td>￡</td>
<td>U+FFE1</td>
</tr>
<tr class="odd">
<td>¬</td>
<td>U+00AC</td>
<td>￢</td>
<td>U+FFE2</td>
</tr>
<tr class="even">
<td>¯</td>
<td>U+00AF</td>
<td>￣</td>
<td>U+FFE3</td>
</tr>
<tr class="odd">
<td>¦</td>
<td>U+00A6</td>
<td>￤</td>
<td>U+FFE4</td>
</tr>
<tr class="even">
<td>¥</td>
<td>U+00A5</td>
<td>￥</td>
<td>U+FFE5</td>
</tr>
<tr class="odd">
<td>₩</td>
<td>U+20A9</td>
<td>￦</td>
<td>U+FFE6</td>
</tr>
<tr class="even">
<td>￨</td>
<td>U+FFE8</td>
<td>│</td>
<td>U+2502</td>
</tr>
<tr class="odd">
<td>￩</td>
<td>U+FFE9</td>
<td>←</td>
<td>U+2190</td>
</tr>
<tr class="even">
<td>￪</td>
<td>U+FFEA</td>
<td>↑</td>
<td>U+2191</td>
</tr>
<tr class="odd">
<td>￫</td>
<td>U+FFEB</td>
<td>→</td>
<td>U+2192</td>
</tr>
<tr class="even">
<td>￬</td>
<td>U+FFEC</td>
<td>↓</td>
<td>U+2193</td>
</tr>
<tr class="odd">
<td>￭</td>
<td>U+FFED</td>
<td>■</td>
<td>U+25A0</td>
</tr>
<tr class="even">
<td>￮</td>
<td>U+FFEE</td>
<td>○</td>
<td>U+25CB</td>
</tr>
</tbody>
</table>
<h3 id="参考">参考</h3>
<p>[1]. <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fzh.wikipedia.org%2Fwiki%2F%E5%85%A8%E5%BD%A2%E5%92%8C%E5%8D%8A%E5%BD%A2">https://zh.wikipedia.org/wiki/%E5%85%A8%E5%BD%A2%E5%92%8C%E5%8D%8A%E5%BD%A2</a>
[2]. <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fwww.voidcn.com%2Farticle%2Fp-njiniuhl-nu.html">http://www.voidcn.com/article/p-njiniuhl-nu.html</a>
[3]. https://www.jianshu.com/p/152e081fec1b</p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>sklearn总结</title>
    <url>/2022/03/29/sklearn%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="model">model</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier <span class="comment"># 随机森林</span></span><br><span class="line"><span class="comment"># RandomForestClassifier随机森林：n_estimators 森林中树的数量(default=100), max_features 寻找最优划分时考虑的最大数目的特征,</span></span><br><span class="line">rf_clf_model = RandomForestClassifier(n_estimators=<span class="number">150</span>, max_depth=<span class="number">7</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>
<h1 id="metric">metric</h1>
<h2 id="accuracyprecisionrcall-f1_score">accuracy,precision,rcall,
f1_score</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, f1_score,precision_score,recall_score</span><br></pre></td></tr></table></figure>
<h1 id="model_selection">model_selection</h1>
<h2 id="交叉验证-cross_val_score">交叉验证 cross_val_score</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from sklearn.model_selection import cross_val_score</span><br></pre></td></tr></table></figure>
<p>cross_val_score交叉验证
首个参数为estimator(也就是要执行fit的model),cv
折数，默认是5折验证，返回的cv长度的score</p>
<p>参考 https://blog.csdn.net/qq_36523839/article/details/80707678</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets	<span class="comment">#自带数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split,cross_val_score	<span class="comment">#划分数据 交叉验证</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier  <span class="comment">#一个简单的模型，只有K一个参数，类似K-means</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">iris = datasets.load_iris()		<span class="comment">#加载sklearn自带的数据集</span></span><br><span class="line">X = iris.data 			<span class="comment">#这是数据</span></span><br><span class="line">y = iris.target 		<span class="comment">#这是每个数据所对应的标签</span></span><br><span class="line">train_X,test_X,train_y,test_y = train_test_split(X,y,test_size=<span class="number">1</span>/<span class="number">3</span>,random_state=<span class="number">3</span>)	<span class="comment">#这里划分数据以1/3的来划分 训练集训练结果 测试集测试结果</span></span><br><span class="line">k_range = <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">31</span>)</span><br><span class="line">cv_scores = []		<span class="comment">#用来放每个模型的结果值</span></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> k_range:</span><br><span class="line">    knn = KNeighborsClassifier(n)   <span class="comment">#knn模型，这里一个超参数可以做预测，当多个超参数时需要使用另一种方法GridSearchCV</span></span><br><span class="line">    scores = cross_val_score(knn,train_X,train_y,cv=<span class="number">10</span>,scoring=<span class="string">&#x27;accuracy&#x27;</span>)  <span class="comment">#cv：选择每次测试折数  accuracy：评价指标是准确度,可以省略使用默认值，具体使用参考下面。</span></span><br><span class="line">    cv_scores.append(scores.mean())</span><br><span class="line">plt.plot(k_range,cv_scores)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;K&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)		<span class="comment">#通过图像选择最好的参数</span></span><br><span class="line">plt.show()</span><br><span class="line">best_knn = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)	<span class="comment"># 选择最优的K=3传入模型</span></span><br><span class="line">best_knn.fit(train_X,train_y)			<span class="comment">#训练模型</span></span><br><span class="line"><span class="built_in">print</span>(best_knn.score(test_X,test_y))	<span class="comment">#看看评分</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>sklearn</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title>使用PaddleVideo实现在attention_lstm模型上训练youtube-8m数据</title>
    <url>/2021/11/25/%E4%BD%BF%E7%94%A8PaddleVideo%E5%AE%9E%E7%8E%B0%E5%9C%A8attention-lstm%E6%A8%A1%E5%9E%8B%E4%B8%8A%E8%AE%AD%E7%BB%83youtube-8m%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<h1 id="安装">安装</h1>
<p>参照
https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/install.md</p>
<p>注意这里创建虚拟环境的时候要使用python=3.7，因为3.7之后的版本，paddleVideo只支持Linux系统。</p>
<p>其他要求： <code>cuda &gt;= 10.1</code>,
<code>cuDNN &gt;= 7.6.4</code></p>
<p>安装完<code>paddlepaddle</code>之后，再安装<code>paddleVideo</code></p>
<h1 id="快速开始">快速开始</h1>
<p>这里官方教程写的不是很详细，安装完之后让直接用命令行方式启动程序，因为<code>paddleVideo</code>这个库里面有三个inference模型，分别是</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Inference models that Paddle provides are listed as follows:</span><br><span class="line"></span><br><span class="line">&#123;&#x27;TSN&#x27;, &#x27;ppTSM&#x27;, &#x27;TSM&#x27;&#125;</span><br></pre></td></tr></table></figure>
<p>这里安装好环境之后，直接命令行是会报错的，因为还没有下载模型参数以及label，会报<code>'If you want to use your own model, Please input model_file as model path!'</code>的错误。</p>
<p>这时候进入python环境，跑以下程序:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> ppvideo <span class="keyword">import</span> PaddleVideo</span><br><span class="line">clas = PaddleVideo(model_name=<span class="string">&#x27;ppTSM&#x27;</span>,use_gpu=<span class="literal">False</span>,use_tensorrt=<span class="literal">False</span>)</span><br><span class="line">video_file=<span class="string">&#x27;data/example.avi&#x27;</span></span><br><span class="line">result=clas.predict(video_file)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<p>当你enter第二句话的时候，程序就会开始下载<code>InferenceModel</code>，大概有90.6兆，下载完之后再执行下面几行的程序，其中<code>video_file</code>这里要注意一下，程序里是相对路径。</p>
<p>下载的模型会放在
<code>C:\Users\XXXX\.paddlevideo_inference\inference_model\ppTSM</code></p>
<p>下载的label的txt在<code>'D:\\XXXXX\\anaconda_envs\\paddleVideo\\lib\\site-packages\\ppvideo\\tools\\../data/k400/Kinetics-400_label_list.txt'</code>也就是虚拟环境那个ppvideo里</p>
<h1 id="训练attention-lstm模型">训练attention-lstm模型</h1>
<p>前两节安装完之后，快速开始是为了测试安装正确与否。paddleVideo只有attention-lstm模型，并没有提供在youtube-8m上训练后的参数和label。所以这部分我们使用paddle框架来自己训练。</p>
<h2 id="下载youtube-8m数据集并转换为paddlepaddle处理的格式">下载youtube-8m数据集并转换为paddlepaddle处理的格式</h2>
<p>整个数据集包含3844个训练数据文件和3844个验证数据文件（TFRecord格式）</p>
<p>在linux系统下用curl下载，在windows下可以利用git的bash命令行的方式下载：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl data.yt8m.org/download.py | partition=2/frame/train mirror=asia python</span><br><span class="line"></span><br><span class="line">curl data.yt8m.org/download.py | partition=2/frame/validate mirror=asia python</span><br><span class="line"></span><br><span class="line">curl data.yt8m.org/download.py | partition=2/frame/test mirror=asia python</span><br></pre></td></tr></table></figure>
<p>数据下载完成之后，因为paddlepaddle需要使用pickle的数据格式，所以需要用<code>https://github.com/PaddlePaddle/PaddleVideo/</code>该官方仓库里data/yt8m下的脚本<code>tf2pkl.py</code>脚本进行转换。<code>tf2pkl.py</code>文件运行时需要两个参数，分别是数据源tf文件存放路径和转化后的pkl文件存放路径。</p>
<blockquote>
<p>由于TFRecord文件的读取需要用到Tensorflow，用户要先安装Tensorflow，或者在安装有Tensorflow的环境中转化完数据，再拷贝到data/dataset/youtube8m/pkl目录下。为了避免和PaddlePaddle环境冲突，建议先在其他地方转化完成再将数据拷贝过来。</p>
</blockquote>
<p>上面在转换数据格式的时候还要注意，<code>tf2pkl.py</code>文件用的tensorflow是1.X的版本，用2.0之后版本的需要重新创建虚拟环境，如果不想麻烦，可以直接在这个paddleVideo的虚拟环境里面安装<code>tensorflow-gpu==1.14.0</code></p>
<p>这里如果安装的是gpu版本，需要有对应的cuda版本支持，1.14.0需要10.0的cuda，我的服务器是11.2的cuda，所以这里我就直接安装的是cpu版本的tensorflow。其他的版本支持请查阅：https://www.tensorflow.org/install/source#gpu</p>
<p><img src="/2021/11/25/%E4%BD%BF%E7%94%A8PaddleVideo%E5%AE%9E%E7%8E%B0%E5%9C%A8attention-lstm%E6%A8%A1%E5%9E%8B%E4%B8%8A%E8%AE%AD%E7%BB%83youtube-8m%E6%95%B0%E6%8D%AE/image-20211126133025867.png"></p>
<ul>
<li>在linux中命令：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python tf2pkl.py ./tf/train ./pkl/train</span><br><span class="line">python tf2pkl.py ./tf/val ./pkl/val</span><br></pre></td></tr></table></figure>
<ul>
<li>在windows中我的方案：</li>
</ul>
<p>我看到<code>tf2pkl.py</code>脚本里是用sys命令行的方式调用的，然后他整个脚本都是在linux的模式下的模式。其中有几个地方改动一下就可以适用于windows：</p>
<ol type="1">
<li>删掉以下几行：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># assert (len(sys.argv) == 3)</span><br><span class="line"># source_dir = sys.argv[1]</span><br><span class="line"># target_dir = sys.argv[2]</span><br></pre></td></tr></table></figure>
<p>禁用命令行调用的模式</p>
<ol start="2" type="1">
<li>将record_dir
变成你想转化的文件的文件夹的路径，比如<code>\data\dataset\youtube8m\tf</code></li>
<li>将main函数中将outputdir路径改成你要存储pickle文件的文件夹的路径。</li>
</ol>
<p>完成上述步骤之后，直接在IDE中运行<code>tf2pkl.py</code>就可以开始转化文件了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=1</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow中lstm,GRU</title>
    <url>/2023/01/04/tensorflow%E4%B8%ADlstm-GRU/</url>
    <content><![CDATA[<h1 id="gru">GRU</h1>
<figure>
<img src="/2023/01/04/tensorflow%E4%B8%ADlstm-GRU/LSTM3-var-GRU.png" alt="GRU">
<figcaption aria-hidden="true">GRU</figcaption>
</figure>
<p>其实单看输出，GRU的输出是和简单的RNN一样的，都只有一个hidden_state。所以在tensorflow中它的输出其实和RNN
layer一样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">inputs = tf.random.normal([<span class="number">32</span>, <span class="number">10</span>, <span class="number">8</span>])</span><br><span class="line">gru = tf.keras.layers.GRU(<span class="number">4</span>)</span><br><span class="line">output = gru(inputs)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>其中有两个可以传递给GRU的参数，一个是return_state，一个是return_sequence。两个值都是bool类型。如果单独传递return_sequence=True，那么输出将只有一个值，也就是每一个时间步的序列：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gru = tf.keras.layers.GRU(<span class="number">4</span>, return_sequences=<span class="literal">True</span>)</span><br><span class="line">output = gru(inputs)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">10</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>如果单独传递return_state=True，那么输出将会是两个值，可以仔细看官方文档中的说明是<code>Boolean. Whether to return the last state in addition to the output. Default:</code>False.`也就是output和最后的hidden_state会一起输出，并且output会等于final_state：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gru = tf.keras.layers.GRU(<span class="number">4</span>, return_state=<span class="literal">True</span>)</span><br><span class="line">output, final_state = gru(inputs)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"><span class="built_in">print</span>(final_state.shape) <span class="comment"># output=final_state</span></span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>如果单独传递return_sequences=True，LSTM将只返回整个序列！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lstm = tf.keras.layers.LSTM(<span class="number">4</span>,return_sequences=<span class="literal">True</span>)</span><br><span class="line">inputs = tf.random.normal([<span class="number">32</span>, <span class="number">10</span>, <span class="number">8</span>])</span><br><span class="line">whole_seq_output = lstm(inputs)</span><br><span class="line"><span class="built_in">print</span>(whole_seq_output.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">10</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>那如果两个值都设置成<code>True</code>呢？这将返回两个输出，第一个输出是整个序列，第二个输出是最终的state。注意这里并没有<code>output</code>了，因为<code>output</code>其实是<code>sequence</code>中最后一个序列<code>sequence[:,-1,:]</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gru = tf.keras.layers.GRU(<span class="number">4</span>, return_sequences=<span class="literal">True</span>, return_state=<span class="literal">True</span>)</span><br><span class="line">whole_sequence_output, final_state = gru(inputs)</span><br><span class="line"><span class="built_in">print</span>(whole_sequence_output.shape)</span><br><span class="line"><span class="built_in">print</span>(final_state.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">10</span>, <span class="number">4</span>)</span><br><span class="line">(<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h1 id="lstm">LSTM</h1>
<p>轮到LSTM，因为架构上跟GRU有点区别，所以在返回结果上就多了一个carry_state.</p>
<figure>
<img src="/2023/01/04/tensorflow%E4%B8%ADlstm-GRU/LSTM3-var-peepholes.png" alt="LSTM">
<figcaption aria-hidden="true">LSTM</figcaption>
</figure>
<p>想要了解LSTM的具体计算，参考<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">博客</a></p>
<p>在tensorflow中一样有return_state和return_sequences：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs = tf.random.normal([<span class="number">32</span>, <span class="number">10</span>, <span class="number">8</span>])</span><br><span class="line">lstm = tf.keras.layers.LSTM(<span class="number">4</span>)</span><br><span class="line">output = lstm(inputs)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>如果单独传递return_state，这里和GRU不一样的地方在于lstm有两个state，一个是memory_state一个是carry_state</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lstm = tf.keras.layers.LSTM(<span class="number">4</span>,return_state=<span class="literal">True</span>)</span><br><span class="line">output, final_memory_state, final_carry_state = lstm(inputs)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"><span class="built_in">print</span>(final_memory_state.shape) <span class="comment"># final_memory_state=output</span></span><br><span class="line"><span class="built_in">print</span>(final_carry_state.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>如果同时设置True</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lstm = tf.keras.layers.LSTM(<span class="number">4</span>,return_sequences=<span class="literal">True</span>,return_state=<span class="literal">True</span>)</span><br><span class="line">whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)</span><br><span class="line"><span class="built_in">print</span>(whole_seq_output.shape)</span><br><span class="line"><span class="built_in">print</span>(final_memory_state.shape) <span class="comment"># final_memory_state=whole_seq_output[:,-1,:]</span></span><br><span class="line"><span class="built_in">print</span>(final_carry_state.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">10</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h1 id="gru-vs-lstm">GRU vs LSTM</h1>
<p>至于我们在训练模型的时候选择哪一个cell作为RNN的cell，cs224n课程给出的答案是：</p>
<blockquote>
<p>Researchers have proposed many gated RNN variants, but LSTM and GRU
are the most widely-used.</p>
<p>Rule of thumb: LSTM is a good default choice (especially if your data
has particularly long dependencies, or you have lots of training data);
Switch to GRUs for speed and fewer parameters.</p>
</blockquote>
<p>LSTM doesn’t guarantee that there is no vanishing/exploding gradient,
but it does provide an easier way for the model to learn long-distance
dependencies.</p>
<p>在2023年的今天，lstm也不再是研究者青睐的对象，最火的模型变成了Transformer：</p>
<figure>
<img src="/2023/01/04/tensorflow%E4%B8%ADlstm-GRU/image-20230313095438649.png" alt="image-20230313095438649">
<figcaption aria-hidden="true">image-20230313095438649</figcaption>
</figure>
<p>这里也贴出2022年的最新WMT的<a href="https://www.statmt.org/wmt22/pdf/2022.wmt-1.1.pdf">结果</a></p>
]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>卷积和反卷积中的output shape计算</title>
    <url>/2022/08/29/%E5%8D%B7%E7%A7%AF%E5%92%8C%E5%8F%8D%E5%8D%B7%E7%A7%AF%E4%B8%AD%E7%9A%84output-shape%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<h1 id="conv2d">Conv2D</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_shape = (<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">3</span>)</span><br><span class="line">x = tf.random.normal(input_shape)</span><br><span class="line"></span><br><span class="line">y1 = tf.keras.layers.Conv2D(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&#x27;valid&#x27;</span>)(x) <span class="comment"># output_shape= (input_shape - filter_size + 1) * (input_shape - filter_size + 1)</span></span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">26</span>, <span class="number">26</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y2 = tf.keras.layers.Conv2D(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;same&quot;</span>)(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y3 = tf.keras.layers.Conv2D(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;valid&quot;</span>,strides=<span class="number">2</span>)(x) </span><br><span class="line"><span class="built_in">print</span>(y.shape) <span class="comment"># output_shape= [(input_shape - filter_size)/strides + 1] * [(input_shape - filter_size)/strides + 1]</span></span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">13</span>, <span class="number">13</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y4 = tf.keras.layers.Conv2D(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;same&quot;</span>,strides=<span class="number">2</span>)(x) </span><br><span class="line"><span class="built_in">print</span>(y.shape) <span class="comment"># output_shape= (input_shape - filter_size + 1) * (input_shape - filter_size + 1)</span></span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">14</span>, <span class="number">14</span>, <span class="number">2</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上例中输入是<code>(28,28,3)</code>的<code>shape</code>。
我们在推算<code>output shape</code>时要分两种情况：</p>
<ol type="1">
<li><code>padding="valid"</code>:
<code>output_shape= [(input_shape - filter_size)/strides + 1] * [(input_shape - filter_size)/strides + 1]</code>。如果<code>strides</code>除不尽，则向下取整（取比该数小的那个整数）。所以<code>y3</code>的<code>shape</code>是13</li>
<li><code>padding="same"</code>:
当<code>padding="same"</code>时计算很简单，得到的<code>output</code>的<code>shape</code>一定是<code>input_shape/strides</code>。所以当<code>strides=1</code>时，输入和输出的<code>shape</code>时相等的，比如上面的<code>y2</code>。</li>
</ol>
<p>如果需要更细节的原理可以参考<a href="https://towardsdatascience.com/understand-transposed-convolutions-and-build-your-own-transposed-convolution-layer-from-scratch-4f5d97b2967">博客</a>。这篇文章讲的特别好！重点推荐。</p>
<h1 id="conv2dtranspose">Conv2DTranspose</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y1 = tf.keras.layers.Conv2DTranspose(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;valid&quot;</span>)(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">30</span>, <span class="number">30</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y2 = tf.keras.layers.Conv2DTranspose(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;same&quot;</span>)(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y3 = tf.keras.layers.Conv2DTranspose(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;valid&quot;</span>,strides=<span class="number">2</span>)(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">57</span>, <span class="number">57</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y4 = tf.keras.layers.Conv2DTranspose(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;same&quot;</span>,strides=<span class="number">2</span>)(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">56</span>, <span class="number">56</span>, <span class="number">2</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>同样的总结下这个反卷积我们在推算<code>output shape</code>时也是分两种情况：
1. <code>padding="valid"</code>:
<code>output_length = input_length * stride + max(filter_size - stride, 0)</code>
2. <code>padding="same"</code>:
<code>output_shape=input_shape * strides</code>。当<code>strides=1</code>时，输出<code>shape</code>等于输入<code>shape</code></p>
<p>当<code>padding=valid</code>时的计算有点复杂。再细究一下的话，<code>tensorflow</code>中<code>Conv2DTranspose</code>还接受<code>output_padding</code>这个参数。如果有这个参数的话（默认是None），可以参考<code>tensorflow</code>官方文档给出的<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose">公式</a></p>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>在linux上配置GPU深度学习环境</title>
    <url>/2021/12/03/%E5%9C%A8linux%E4%B8%8A%E9%85%8D%E7%BD%AEGPU%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<h1 id="配置云服务器">配置云服务器</h1>
<p>系统 ： ubuntu 18.04</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 更新所有环境</span><br><span class="line">sudo apt update </span><br><span class="line"></span><br><span class="line"># 安装开发所需的基本包</span><br><span class="line">sudo apt install build-essential</span><br><span class="line"></span><br><span class="line"># 安装cuda.这里会连同显卡驱动一起安装</span><br><span class="line">wget https://developer.download.nvidia.com/compute/cuda/11.5.1/local_installers/cuda_11.5.1_495.29.05_linux.run</span><br><span class="line">sudo sh cuda_11.5.1_495.29.05_linux.run</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 去anaconda官方复制miniconda的下载链接</span><br><span class="line">wget https://repo.anaconda.com/miniconda/Miniconda3-py38_4.10.3-Linux-x86_64.sh</span><br><span class="line"></span><br><span class="line"># 进入conda环境</span><br><span class="line">bash</span><br><span class="line"></span><br><span class="line">之后就继续用pip安装所需要的包</span><br><span class="line">apt-get install python3.8</span><br></pre></td></tr></table></figure>
<p>这里需要知道，如果我们在云端配置的环境，需要将云端的jupyter
notebook的运行端口映射到本地来，可以这样做：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh -L8888:localhost:8888 ubuntu@100.20.65.33</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果使用aws配置远端服务器，在本地连接时使用:</p>
<p>$ chomd 400 Downloads/d2l.pem</p>
<p>$ ssh -i Downloads/d2l.pem ubuntu@54.245.23.40</p>
<p>以上两条命令中将密钥path和ip地址都替换成自己的。</p>
</blockquote>
<h1 id="docker中的tensorflow-gpu配置">docker中的tensorflow-gpu配置</h1>
<p>首先在宿主机（本机）安装好<code>NVIDIA GPU</code>的驱动程序，然后对于每一个容器都要各自安装对应版本的<code>cuda</code>和<code>cudnn</code>。</p>
<h2 id="下载tensorflow-docker镜像">下载TensorFlow Docker镜像</h2>
<p>官方 TensorFlow Docker 映像位于 <a href="https://hub.docker.com/r/tensorflow/tensorflow/">tensorflow/tensorflow</a>
Docker Hub 代码库中。映像版本按照以下格式进行<a href="https://hub.docker.com/r/tensorflow/tensorflow/tags/">标记</a>：</p>
<table>
<colgroup>
<col style="width: 15%">
<col style="width: 84%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">标记</th>
<th style="text-align: left;">说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>latest</code></td>
<td style="text-align: left;">TensorFlow CPU
二进制映像的最新版本。（默认版本）</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>nightly</code></td>
<td style="text-align: left;">TensorFlow 映像的每夜版。（不稳定）</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em><code>version</code></em></td>
<td style="text-align: left;">指定 TensorFlow
二进制映像的版本，例如：2.1.0</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>devel</code></td>
<td style="text-align: left;">TensorFlow <code>master</code>
开发环境的每夜版。包含 TensorFlow 源代码。</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>custom-op</code></td>
<td style="text-align: left;">用于开发 TF
自定义操作的特殊实验性映像。详见<a href="https://github.com/tensorflow/custom-op">此处</a>。</td>
</tr>
</tbody>
</table>
<p>每个基本标记都有会添加或更改功能的变体：</p>
<table>
<colgroup>
<col style="width: 17%">
<col style="width: 82%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">标记变体</th>
<th style="text-align: left;">说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>tag-gpu</code></td>
<td style="text-align: left;">支持 GPU 的指定标记版本。（<a href="https://www.tensorflow.org/install/docker?hl=zh_cn#gpu_support">详见下文</a>）</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>tag-jupyter</code></td>
<td style="text-align: left;">针对 Jupyter 的指定标记版本（包含
TensorFlow 教程笔记本）</td>
</tr>
</tbody>
</table>
<p>您可以一次使用多个变体。例如，以下命令会将 TensorFlow
版本映像下载到计算机上：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker pull tensorflow/tensorflow                     # latest stable release</span><br><span class="line">docker pull tensorflow/tensorflow:devel-gpu           # nightly dev release w/ GPU support</span><br><span class="line">docker pull tensorflow/tensorflow:latest-gpu-jupyter  # latest release w/ GPU support and Jupyter</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意如果要用gpu版本的tensorflow，需要pull带有gpu tag的镜像.
<code>docker pull tensorflow/tensorflow:latest-gpu</code>。如果不带gpu标签，会默认下载CPU版本的tensorflow。</p>
</blockquote>
<h3 id="验证tensorflow-gpu">验证tensorflow gpu</h3>
<p>查看是否有GPU：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">gpu_device_name = tf.test.gpu_device_name()</span><br><span class="line"><span class="built_in">print</span>(gpu_device_name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># GPU是否可用,返回True或者False</span></span><br><span class="line">tf.test.is_gpu_available()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.client <span class="keyword">import</span> device_lib</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出所有的本地机器设备</span></span><br><span class="line">local_device_protos = device_lib.list_local_devices()</span><br><span class="line"><span class="comment"># 打印</span></span><br><span class="line"><span class="comment">#     print(local_device_protos)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只打印GPU设备</span></span><br><span class="line">[<span class="built_in">print</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> local_device_protos <span class="keyword">if</span> x.device_type == <span class="string">&#x27;GPU&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h2 id="docker中安装多种cuda版本并切换">docker中安装多种cuda版本并切换</h2>
<p>去<code>cuda</code>官网下载所需版本，以<code>.run</code>结尾。楼主系统为<code>linux</code>。</p>
<p>进入到放置 <code>cuda_9.0.176_384.81_linux.run</code> 的目录：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo chmod +x cuda_9.0.176_384.81_linux.run # 为 cuda_9.0.176_384.81_linux.run 添加可执行权限</span><br><span class="line">./cuda_9.0.176_384.81_linux.run # 安装 cuda_9.0.176_384.81_linux.run</span><br></pre></td></tr></table></figure>
<p>在安装过程中截取其中比较重要的几个选择：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Do you accept the previously read EULA?</span><br><span class="line">accept/decline/quit: accept</span><br><span class="line"></span><br><span class="line">Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 384.81?</span><br><span class="line">(y)es/(n)o/(q)uit: n # 如果在这之前已经安装好更高版本的显卡驱动就不需要再重复安装，如果需要重复安装就选择 yes,此外还需要关闭图形界面。</span><br><span class="line"></span><br><span class="line">Install the CUDA 9.0 Toolkit?</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line"></span><br><span class="line">Enter Toolkit Location</span><br><span class="line"> [ default is /usr/local/cuda-9.0 ]: # 一般选择默认即可，也可以选择安装在其他目录，在需要用的时候指向该目录或者使用软连接 link 到 /usr/local/cuda。</span><br><span class="line"></span><br><span class="line">/usr/local/cuda-9.0 is not writable.</span><br><span class="line">Do you wish to run the installation with &#x27;sudo&#x27;?</span><br><span class="line">(y)es/(n)o: y</span><br><span class="line"></span><br><span class="line">Please enter your password: </span><br><span class="line">Do you want to install a symbolic link at /usr/local/cuda? # 是否将安装目录通过软连接的方式 link 到 /usr/local/cuda，yes or no 都可以，取决于你是否使用 /usr/local/cuda 为默认的 cuda 目录。</span><br><span class="line">(y)es/(n)o/(q)uit: n</span><br><span class="line"></span><br><span class="line">Install the CUDA 9.0 Samples?</span><br><span class="line">(y)es/(n)o/(q)uit: n</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>选择的汇总： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Driver:   Not Selected</span><br><span class="line">Toolkit:  Installed in /usr/local/cuda-9.0</span><br><span class="line">Samples:  Not Selected</span><br><span class="line"></span><br><span class="line">Please make sure that</span><br><span class="line"> -   PATH includes /usr/local/cuda-9.0/bin</span><br><span class="line"> -   LD_LIBRARY_PATH includes /usr/local/cuda-9.0/lib64, or, add /usr/local/cuda-9.0/lib64 to /etc/ld.so.conf and run ldconfig as root</span><br><span class="line"></span><br><span class="line">To uninstall the CUDA Toolkit, run the uninstall script in /usr/local/cuda-9.0/bin</span><br><span class="line"></span><br><span class="line">Please see CUDA_Installation_Guide_Linux.pdf in /usr/local/cuda-9.0/doc/pdf for detailed information on setting up CUDA.</span><br><span class="line"></span><br><span class="line">***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 384.00 is required for CUDA 9.0 functionality to work.</span><br><span class="line">To install the driver using this installer, run the following command, replacing &lt;CudaInstaller&gt; with the name of this run file:</span><br><span class="line">    sudo &lt;CudaInstaller&gt;.run -silent -driver</span><br></pre></td></tr></table></figure> 安装完成后可以在 <code>/usr/local</code>
目录下看到：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cuda-11.1 # 之前安装的cuda-11.1 </span><br><span class="line">cuda-9.0 # 刚刚安装的cuda-9.0 </span><br><span class="line">cuda # cuda-10.0 的软连接</span><br></pre></td></tr></table></figure>
<p>多版本切换：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#在切换cuda版本时</span><br><span class="line">rm -rf /usr/local/cuda#删除之前创建的软链接</span><br><span class="line">sudo ln -s /usr/local/cuda-8.0/ /usr/local/cuda/</span><br><span class="line">nvcc --version #查看当前 cuda 版本</span><br><span class="line"></span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2016 NVIDIA Corporation</span><br><span class="line">Built on Mon_Jan_23_12:24:11_CST_2017</span><br><span class="line">Cuda compilation tools, release 8.0, V8.0.62</span><br><span class="line"></span><br><span class="line">#cuda8.0 切换到 cuda9.0 </span><br><span class="line">rm -rf /usr/local/cuda</span><br><span class="line">sudo ln -s /usr/local/cuda-9.0/ /usr/local/cuda/</span><br><span class="line">nvcc --version</span><br></pre></td></tr></table></figure>
<p>上面的前提是<code>linux</code>系统的环境变量中<code>(~./bashrc文件)</code>，<code>cuda</code>的路径是<code>/usr/local/cuda</code></p>
<h2 id="tensorflow_decision_forests使用">tensorflow_decision_forests使用</h2>
<p>我一开始pull了tensorflow-gpu版本的docker环境，想用一下tensorflow的tensorflow_decision_forests库，该库是随机森林的集合库，内有很多算法可以用。官方使用document在[https://www.tensorflow.org/decision_forests/tutorials/beginner_colab?hl=zh_cn]。</p>
<p>我一开始没注意，后来发现我在docker环境中直接用pip安装该库时，帮我又安装了cpu版本的tensorflow，这样就和我的gpu版本冲突了，然后去网上搜了一下，发现该库现在还有很多使用限制：1.
仅仅支持linux，不支持windows和mac 2. 仅支持cpu
，还没有gpu版本，见[https://github.com/tensorflow/decision-forests/issues/38].作者的意思是用gpu训练会更复杂，更详细的我就没看了。</p>
<p>然后我的做法是在该tensorflow-gpu的docker环境中使用<strong>virtualenv</strong>创建一个tensorfow的cpu虚拟环境，然后再用pip安装TF-DF这个包。避免污染docker主环境中的tensorflow-gpu。</p>
<blockquote>
<p>之所以不用miniconda，是因为conda会默认替代掉我容器自带的python以及安装好的tensorflow-gpu。我只是想用一下TFDF这个库，不想太折腾。</p>
</blockquote>
<p>此处贴virtualenv的命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install virtualenv</span><br><span class="line"></span><br><span class="line">virtualenv venv # 在项目目录中执行，会创建一个名称为venv的虚拟环境，venv文件夹下包含python和pip</span><br><span class="line"></span><br><span class="line">source venv/bin/activate # 激活环境</span><br><span class="line"></span><br><span class="line">激活完了之后就pip安装tensorflow就行了</span><br></pre></td></tr></table></figure>
<h1 id="本地机器vscode配置使用远程服务器中运行的容器">本地机器vscode配置使用远程服务器中运行的容器</h1>
<p>参考 [https://zhuanlan.zhihu.com/p/80099904]</p>
<p>其中有几个地方需要注意一下的是：</p>
<ol type="1">
<li><p>如果遇到在本地机器vscode中使用插件remote
ssh连接不上容器的问题时，需要<code>vim /etc/ssh/sshd_config    ,将PermitRootLogin的值改为yes（去掉前面的#号）</code></p></li>
<li><p>远程容器里面还需要有python插件，才能在本地机器vscode中debug代码</p></li>
<li><p>上面博主服务器端口映射的是容器的22端口，我没有尝试过用其他端口映射。22端口是ssh登陆的默认端口。</p></li>
</ol>
<h1 id="cudnn深度学习库的安装和验证">cudnn深度学习库的安装和验证</h1>
<p>参考[https://blog.csdn.net/caicaiatnbu/article/details/87626491]</p>
<p>注意点：</p>
<ol type="1">
<li>下载对应的linux版本的cudnn</li>
</ol>
<h1 id="conda安装cuda和cudnn">conda安装cuda和cudnn</h1>
<p>如果使用的是conda的环境，可以单独使用conda install
cuda来在虚拟环境中安装不同于本机版本的cuda和cudnn，而不需要使用我上面提到的那种方式(每次换cuda版本需要更换/usr/local/cuda的软链接的方式)。</p>
<p>conda创建了虚拟环境后，激活进入虚拟环境</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda install cudatoolkit=10.2</span><br></pre></td></tr></table></figure>
<p>以上命令安装了10.2版本的cuda，然后可以使用<code>cuda search cudnn</code>找一下合适版本的cudnn，然后还是用<code>cuda install cudnn=版本号</code>来安装</p>
<blockquote>
<p>注意：这里用conda安装的cuda和cudnn，是无法像在本机使用nvcc
-V来检查版本的，所以即使你在虚拟环境激活的情况下使用 nvcc
-V的命令会返回无此命令或者是返回的还是本机cuda的版本号！</p>
</blockquote>
<p>那可能有人会问，如果想在conda虚拟环境下测试cuda和cudnn是否安装成功怎么办？</p>
<p>目前的办法是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># In[]:</span><br><span class="line">import torch</span><br><span class="line"># cpu</span><br><span class="line">print(torch.__version__)</span><br><span class="line"># gpu</span><br><span class="line">print(torch.cuda.is_available())</span><br><span class="line"></span><br><span class="line"># In[]:</span><br><span class="line">import tensorflow as tf</span><br><span class="line"># cpu</span><br><span class="line">print(tf.__version__)</span><br><span class="line"># v1 to test gpu</span><br><span class="line">print(tf.test.is_gpu_available())</span><br><span class="line"># v2 to test gpu</span><br><span class="line">print(tf.config.list_physical_devices(&#x27;GPU&#x27;))</span><br></pre></td></tr></table></figure>
<p>参考[https://blog.csdn.net/qq_37774098/article/details/109895048]</p>
<h1 id="docker拉取的pytorch-gpu版找不到cuda和cudnn的位置为何">docker拉取的pytorch-gpu版找不到cuda和cudnn的位置，为何？</h1>
<p>参考 https://blog.csdn.net/ljp1919/article/details/106209358</p>
<h1 id="tensorflow和pytorch验证gpu是否可用">tensorflow和pytorch验证GPU是否可用</h1>
<h2 id="tensorflow">tensorflow</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"> </span><br><span class="line">gpu_device_name = tf.test.gpu_device_name()</span><br><span class="line"><span class="built_in">print</span>(gpu_device_name)</span><br><span class="line"><span class="comment"># 查看是否可用</span></span><br><span class="line">tf.test.is_gpu_available()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.client <span class="keyword">import</span> device_lib</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 列出所有的本地机器设备</span></span><br><span class="line">local_device_protos = device_lib.list_local_devices()</span><br><span class="line"><span class="comment"># 打印</span></span><br><span class="line"><span class="comment"># print(local_device_protos)</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 只打印GPU设备</span></span><br><span class="line">[<span class="built_in">print</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> local_device_protos <span class="keyword">if</span> x.device_type == <span class="string">&#x27;GPU&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h2 id="pytorch">pytorch</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">flag = torch.cuda.is_available()</span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;CUDA可使用&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;CUDA不可用&quot;</span>)</span><br><span class="line"></span><br><span class="line">ngpu= <span class="number">1</span></span><br><span class="line"><span class="comment"># Decide which device we want to run on</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> (torch.cuda.is_available() <span class="keyword">and</span> ngpu &gt; <span class="number">0</span>) <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;驱动为：&quot;</span>,device)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;GPU型号： &quot;</span>,torch.cuda.get_device_name(<span class="number">0</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> 	torch</span><br><span class="line"><span class="keyword">import</span>  time</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br><span class="line"><span class="comment"># print(&#x27;hello, world.&#x27;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">10000</span>, <span class="number">1000</span>)</span><br><span class="line">b = torch.randn(<span class="number">1000</span>, <span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line">c = torch.matmul(a, b)</span><br><span class="line">t1 = time.time()</span><br><span class="line"><span class="built_in">print</span>(a.device, t1 - t0, c.norm(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">a = a.to(device)</span><br><span class="line">b = b.to(device)</span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line">c = torch.matmul(a, b)</span><br><span class="line">t2 = time.time()</span><br><span class="line"><span class="built_in">print</span>(a.device, t2 - t0, c.norm(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line">c = torch.matmul(a, b)</span><br><span class="line">t2 = time.time()</span><br><span class="line"><span class="built_in">print</span>(a.device, t2 - t0, c.norm(<span class="number">2</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="附录docker使用">附录：docker使用</h1>
<p>创建容器：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --gpus all -d -v /:/host -p 3333:22 tensorflow/tensorflow:2.4.0-gpu tail -f /var/log/dpkg.log # 创建容器并启动</span><br><span class="line">docker create --name postgres nginx:latest # 创建容器并不启动</span><br><span class="line"></span><br><span class="line">docker start # 启动一个或多个已经被停止的容器</span><br><span class="line"></span><br><span class="line">docker stop # 停止一个运行中的容器</span><br><span class="line"></span><br><span class="line">docker restart # 重启容器</span><br></pre></td></tr></table></figure>
<p>进入容器：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec -it [container_id] /bin/bash</span><br><span class="line"></span><br><span class="line">docker inpect # 获取容器/镜像的元数据</span><br></pre></td></tr></table></figure>
<p>停止容器运行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker kill -s KILL [container_id]</span><br><span class="line">或者 docker stop [container_id]</span><br></pre></td></tr></table></figure>
<p>删除一个或多个容器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker rm -f db01 db02 # 强制删除，容器可以是在运行着的状态</span><br><span class="line">或者 docker rm db01</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>docker save 与 docker export
区别参考:[https://jingsam.github.io/2017/08/26/docker-save-and-docker-export.html]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker save -o images.tar postgres:9.6 # 打包postgres镜像</span><br><span class="line">docker load -9 images.tar # 载入镜像</span><br></pre></td></tr></table></figure>
<p>docker
save的应用场景是，如果你的应用是使用docker-compose.yml编排的多个镜像组合，但你要部署的客户服务器并不能连外网。这时，你可以使用docker
save将用到的镜像打个包，然后拷贝到客户服务器上使用docker load载入。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker export -o postgres-export.tar postgres # 将容器postgres打包成一个tar</span><br><span class="line">docker import postgres-export.tar postgres:latest # 这是将tar包import成一个镜像，镜像和tag名字自定义</span><br></pre></td></tr></table></figure>
<p>docker
export的应用场景主要用来制作基础镜像，比如你从一个ubuntu镜像启动一个容器，然后安装一些软件和进行一些设置后，使用docker
export保存为一个基础镜像。然后，把这个镜像分发给其他人使用，比如作为基础的开发环境。</p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>命令行运行python脚本时传入参数的三种方式</title>
    <url>/2021/11/26/%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%BF%90%E8%A1%8Cpython%E8%84%9A%E6%9C%AC%E6%97%B6%E4%BC%A0%E5%85%A5%E5%8F%82%E6%95%B0%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<h1 id="sys.argv">sys.argv</h1>
<p>sys模块是很常用的模块，
它封装了与python解释器相关的数据，例如sys.modules里面有已经加载了的所有模块信息，sys.path里面是PYTHONPATH的内容，而sys.argv则封装了传入的参数数据。
使用sys.argv接收上面第一个命令中包含的参数方式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">print(sys.argv[0],sys.argv[1],sys.argv[2])</span><br></pre></td></tr></table></figure>
<p>其中<code>sys.argv[0]</code>是该脚本的名称，<code>sys.argv[1]</code>才是第一个参数，<code>sys.argv</code>是一个列表</p>
<p>用这种方式，命令行调用方式为：</p>
<p><code>python script.py parameter1 parameter2</code></p>
<h1 id="argparse">argparse</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;Process some integers.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;integers&#x27;</span>, metavar=<span class="string">&#x27;N&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, nargs=<span class="string">&#x27;+&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;an integer for the accumulator&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--sum&#x27;</span>, dest=<span class="string">&#x27;accumulate&#x27;</span>, action=<span class="string">&#x27;store_const&#x27;</span>,</span><br><span class="line">                    const=<span class="built_in">sum</span>, default=<span class="built_in">max</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;sum the integers (default: find the max)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"><span class="built_in">print</span>(args.accumulate(args.integers))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>如何对开源大语言模型微调？需要多少数据？</title>
    <url>/2023/06/08/%E5%A6%82%E4%BD%95%E5%AF%B9%E5%BC%80%E6%BA%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%EF%BC%9F%E9%9C%80%E8%A6%81%E5%A4%9A%E5%B0%91%E6%95%B0%E6%8D%AE%EF%BC%9F/</url>
    <content><![CDATA[<p>随着各大语言模型的开源，很多研究在没有计算资源的情况下，唯一可想的办法就是在各大开源模型上利用垂直领域的数据集来finetune开源大模型，充分利用pretrained模型的能力，而又能让它解决特定的任务。
斯坦福的羊驼alpaca模型的发布掀起了instruction
tuning的一阵狂潮，国内很多工作也在模仿Stanford这个工作方法做自己领域的大模型，其实在接触这些工作的时候我一直有一个疑问，就是什么时候我们该finetune？手里有多少数据的时候你可以finetune。</p>
<p>如果我们要开展微调，数据以及如何组织数据形式和微调的方式是两个主要问题。</p>
<h1 id="微调的方式">微调的方式</h1>
<p>现有的LLM规模太大，因此完全微调模型已并非易事。因此无论学界还是业界都需要一种高效的方法在下游任务数据上训练，这就为参数高效微调（Parameter-efficient
fine-tuning，PEFT）带来了研究空间。PEFT的目的是只训练一小部分参数（可以是大模型自身的，也可以是额外引入的）就能提升模型在下游任务的效果。</p>
<p><a href="http://arxiv.org/abs/2303.15647">Scaling Down to Scale Up: A
Guide to Parameter-Efficient Fine-Tuning</a>
一文总结了现有peft的主要方法，并做了分类：</p>
<figure>
<img src="/2023/06/08/%E5%A6%82%E4%BD%95%E5%AF%B9%E5%BC%80%E6%BA%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%EF%BC%9F%E9%9C%80%E8%A6%81%E5%A4%9A%E5%B0%91%E6%95%B0%E6%8D%AE%EF%BC%9F/image-20230608131344681.png" alt="PEFT methods分类">
<figcaption aria-hidden="true">PEFT methods分类</figcaption>
</figure>
<p>可以看到lora也就是我们现在经常用到的微调手段被分配到了reparametrization-based里面。并且在additive这个分类里，又分了两个子类：adapter-like和soft
prompts。对于每一个分类的解释可以看paper或者参考这篇<a href="https://zhuanlan.zhihu.com/p/619583361">知乎博客</a></p>
<p>我这里只做简单的对比：</p>
<ul>
<li>prefix tuning，prompt
tuning根本就是为了解决人工为每一个任务构造prompt太过于随机性，而且构造的模板有时候是离散化的；另外一个痛点就是如果对模型在下游任务上进行全参数调整，每一个任务得保存一份参数副本，对存储和训练都是考验。所以这一类方法就是让模型通过学习来自动寻找最合适的prompt，称之为soft
prompt.LLM那一部分的参数不做调整，只调整添加的这一部分参数。之后清华大学提出的ptuning和ptuning
v2版本，可以简单的将P-Tuning认为是针对Prompt Tuning的改进，P-Tuning
v2认为是针对Prefix Tuning的改进。</li>
<li>lora这一类方法（AdaLoRA，Qlora）基于的理论是：将LLM在下游任务进行微调时发现改变的参数都是哪些低秩矩阵，所以这些方法重构了一种低秩矩阵运算，这些矩阵里面的参数就可以代表下游任务的知识，这样将LLM的预训练参数和这部分参数进行合并之后就可以适配下游任务了。</li>
</ul>
<h1 id="微调实践applications">微调实践Applications</h1>
<p>这一章节主要介绍一些值得关注的微调实践工作，可以给我们在实际工作中提供一些微调的思路，比如看看别人数据集是如何组织的，有多少的数据量，针对什么任务有了performance的提高，还有做evaluation是咋做的。</p>
<h1 id="stanford-alpaca">Stanford Alpaca</h1>
<p>这是开创instruction
tuning工作的鼻祖，而且斯坦福的代码写的很优秀，特别是用GPT3生成instruction-input-output那一部分，把代码通读一遍可以加深self-instruct方法的理解。而且斯坦福的这个数据集只有52k条，但是有意思的是它在组织这52k条数据的时候是需要充分保持task的多样性的，毕竟我们知道instruction
tuning当时在 Finetuned Language Models Are Zero-Shot Learners
文中被提出的时候，其实是为了提高模型在unseen
task上的zero-shot能力，它有一个重要的前提是finetune的task要多样，这个非常重要！</p>
<blockquote>
<p>现在有一些国内的工作就是直接弄了一些数据就开始finetune，然后叫自己是instruction
tuning，我觉得不太合理。</p>
</blockquote>
<p>最近出了一个工作： LIMA: Less Is More for
Alignment，在LLaMa-65B基础上用1000条instruction数据训练的模型，在43%的情况下，LIMA可以超过或者和GPT4平齐，这真的很厉害了，毕竟只用了1000条数据，而且作者也用斯坦福的方法复刻了52k微调llama-65B的大羊驼，发现还是LIMA优秀一点，作者猜测是因为数据集质量，这1000条数据是精心策划的。</p>
<h1 id="chatglm-6b-p-tuning">Chatglm-6B p-tuning</h1>
<p>基于chatglm-6B的微调项目超级多，chatglm有天然的中文优势，所以国内好多语言模型都是基于清华的这个语言模型做的工作。chatglm-6B给出的官方github
repo中包含了p-tuning v2的<a href="https://github.com/THUDM/P-tuning-v2">代码</a>, p tuning
v2的原理就是将应该人工写的那一部分prompt用参数来学习，LLM预训练好的那一部分参数固定住，只更新添加的这部分参数。参考chatglm-6B自己给出的再ADGEN（广告生成的数据集）上finetuneg
chatglm6B的代码：
https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning，
这部分代码的数据组织部分蛮有意思的，数据集长这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;content&quot;</span>: <span class="string">&quot;类型#上衣*版型#宽松*版型#显瘦*图案#线条*衣样式#衬衫*衣袖型#泡泡袖*衣款式#抽绳&quot;</span>,</span><br><span class="line">    <span class="string">&quot;summary&quot;</span>: <span class="string">&quot;这件衬衫的款式非常的宽松，利落的线条可以很好的隐藏身材上的小缺点，穿在身上有着很好的显瘦效果。领口装饰了一个可爱的抽绳，漂亮的绳结展现出了十足的个性，配合时尚的泡泡袖型，尽显女性甜美可爱的气息。&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>也就是不像alpaca数据集有instruction了，只是一个映射关系，而且在main.py训练代码里也没有见到处理数据sample时要给每一个数据前加instruction，唯一加的就是在content前加了一个“问：”，在summary前加了一个“答：”。</p>
<h2 id="gorilla">Gorilla</h2>
<p>这是一个微调LLaMa-7B让LLM实现用语言方式让LLM返回API调用代码的工作。</p>
<p>参考：</p>
<ul>
<li>https://gorilla.cs.berkeley.edu/</li>
<li><a href="https://github.com/ShishirPatil/gorilla">Gorilla Github
Repo</a></li>
<li><a href="http://arxiv.org/abs/2305.15334">Gorilla: Large Language
Model Connected with Massive APIs</a></li>
</ul>
<figure>
<img src="/2023/06/08/%E5%A6%82%E4%BD%95%E5%AF%B9%E5%BC%80%E6%BA%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%EF%BC%9F%E9%9C%80%E8%A6%81%E5%A4%9A%E5%B0%91%E6%95%B0%E6%8D%AE%EF%BC%9F/image-20230608133459274.png" alt="image-20230608133459274">
<figcaption aria-hidden="true">image-20230608133459274</figcaption>
</figure>
<p>产生数据集的方式跟alpaca如出一辙，利用了self-instruct方式，让GPT-4来产生instruction-API的训练数据。而且它分了两种模式来训练，一种是有retriever的模式，在instruction中添加了数据库里关于这个API的帮助信息，一种就是instruction-API的格式。因为repo里并没有开源这部分的训练数据，所以我们也只能看论文来猜测数据是长这样的，等作者公布了数据可以再补充这部分数据到底长什么样子。</p>
<p>我们在使用这个model进行推理时，输入给他的就是一串我呢本，告诉它你想获得一个怎么样的API，比如“I
would like to identify the objects in an image”或者更模糊一点：“I am
going to the zoo, and would like to track
animals”。在zero-shot模式下这个instruction会直接给到gorilla，模型会给你返回一串API调用的代码，像这样：</p>
<figure>
<img src="/2023/06/08/%E5%A6%82%E4%BD%95%E5%AF%B9%E5%BC%80%E6%BA%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%EF%BC%9F%E9%9C%80%E8%A6%81%E5%A4%9A%E5%B0%91%E6%95%B0%E6%8D%AE%EF%BC%9F/image-20230608134439167.png" alt="image-20230608134439167">
<figcaption aria-hidden="true">image-20230608134439167</figcaption>
</figure>
<p>总体来说这个项目想法蛮有意思的，就是很多东西暂时还没开源，我们拭目以待吧，现在就用用就行。</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>生成模型</title>
    <url>/2024/08/13/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>构建和评估RAG应用</title>
    <url>/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<p>最近吴恩达出了一个小课程，传送门: <a href="https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/">Building
and Evaluating Advanced RAG Applications</a>。<a href="https://www.bilibili.com/video/BV1494y1E7H9?p=3&amp;vd_source=f998f640fc8575504e3e97753bf817f4">B站</a>也有人搬运了，有中英文字幕。最近也正好在做RAG相关的项目，看到这个课程里有一些新的东西，权当在这篇博客里总结记录。</p>
<p>另外还推荐阅读一篇综述<a href="https://arxiv.org/abs/2311.05876">Trends in Integration of
Knowledge and Large Language Models: A Survey and Taxonomy of Methods,
Benchmarks, and Applications</a>, 该综述的第三章详细介绍了retrieval
augmentation的方法。我这篇博客会首先理顺一些理论，然后再介绍吴恩达课程里的知识（个人认为吴大佬出的关于LLM的一系列shot
course可食用性不够高，比如上面说的这个RAG相关的课怎么看都觉得是在推广LlamaIndex这个框架，对于原理一句话带过，很多细节不清楚）。</p>
<p>3.2节提到的两个工作值得注意：</p>
<ol type="1">
<li>Query2doc</li>
</ol>
<blockquote>
<p>Query2doc prompts the LLMs to generate a pseudo-document by employing
a few-shot prompting paradigm. Subsequently, the original query is
expanded by incorporating the pseudo-document. The retriever module uses
this new query to retrieve a list of relevant documents.</p>
</blockquote>
<ol start="2" type="1">
<li>Rewrite-Retrieve-Read</li>
</ol>
<blockquote>
<p>Different with Query2doc,they adopt a trainable language model to
perform the rewriting step</p>
</blockquote>
<p>在抽取的context的使用上，我们一般的认知是加入到prompt里，告诉LLM根据这个context回答某个query，这篇综述在3.2节还概括介绍了另外两种使用knowledge的方式：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20231205145030752.png" alt="image-20231205145030752">
<figcaption aria-hidden="true">image-20231205145030752</figcaption>
</figure>
<p>我个人认为第二种方式实操性差一点，第三种和第一种应该是大家会普遍采取的方式，第二种需要更多精细的prompt设计。</p>
<hr>
<p>以下为课程相关的 ，传送门: <a href="https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/">Building
and Evaluating Advanced RAG Applications</a>. 课程笔记参考<a href="https://medium.com/@LakshmiNarayana_U/frameworks-in-focus-building-and-evaluating-advanced-rag-with-trulens-and-llamaindex-insights-19db95ffcf6e">Frameworks
in Focus: ‘Building and Evaluating Advanced RAG’ with TruLens and
LlamaIndex Insights</a></p>
<h1 id="构建-construction">构建 Construction</h1>
<p>简单的RAG构建的资料太多太多了，最简易的RAG构建可以参考<a href="https://huggingface.co/learn/cookbook/rag_zephyr_langchain">Simple
RAG for GitHub issues using Hugging Face Zephyr and LangChain</a>.</p>
<p>RAG中两个最核心的模块： Retrieval 和 Generation
(Read)，内部都有很多可以enhance的地方。这里列举一些可以查阅的资料，内整理了一些对于RAG的enhancement的点：</p>
<ul>
<li><a href="http://arxiv.org/abs/2402.19473">Retrieval-Augmented
Generation for AI-Generated Content: A Survey</a> Chapter 3</li>
<li><a href="https://medium.com/aimonks/retrieval-augmented-generation-rag-enhancement-for-llm-based-prediction-relp-59645a67dcdb">Retrieval
Augmented Generation (RAG) Enhancement for LLM-based Prediction —
RELP</a></li>
<li><a href="https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6">Advanced
RAG Techniques: an Illustrated Overview</a> 这个博客整理的挺全面的</li>
<li><a href="https://github.com/huggingface/cookbook/blob/main/notebooks/en/advanced_rag.ipynb">Advanced
RAG on Hugging Face documentation using LangChain</a></li>
</ul>
<figure>
<img src="https://camo.githubusercontent.com/738a616ea3fc69c8c0a0f26deae64b0f88e6e1d430db5c0454f1127b362b2e98/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f68756767696e67666163652f636f6f6b626f6f6b2d696d616765732f7265736f6c76652f6d61696e2f5241475f776f726b666c6f772e706e67" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>上面这张图来自于langchain的cookbook，蓝色部分是作者认为<code>all possibilities for system enhancement</code>。我这里只对一些我关注的技术做整理和探索。</p>
<h2 id="chunking">Chunking</h2>
<p>有一堆文档，如何将这些文档切分成“完美的”chunk。</p>
<p>我比较关注的是对PDF格式的文件的处理，比较有参考价值的资料：<a href="https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb">5
Levels Of Text
Splitting</a>，内介绍的level1和level2的切分方式都是现在比较常见的。</p>
<p>对于PDF中的图片，也有<a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Multi_modal_RAG.ipynb">博客</a>进行了探索</p>
<p><img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/下载-17152333305102.png"></p>
<p>对于PDF中table的处理，一个可行的方式是用Unstuctured这个library抽取出HTML格式的table，然后用LLM将其summary一下，那么对于retriveal的时候，是将summary的vector和query的vector去进行比对的，如果match上了，就会把原生的HTML的表格表示输入给LLM去生成最终的答案。做法详见<a href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb">Semi-structured
RAG</a>。<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/下载%20(1)-17152442960173.png"></p>
<p>对于PDF中图片的处理，也是对image先用LLM总结描述一下。其实<a href="https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb">5
Levels Of Text
Splitting</a>里面介绍的方法都是可以的，但我觉得实操会有一定的难度。因为PDF中的images是会被单独放到一个文件夹里的，前后夹的文本其实是丢失了，这样不可避免的就会丢失一定的语义信息。表格其实还好，但是很多时候贴了一张图片之后，后面的文字基本上是相关联的。这时候需要把图片的信息和后面的文字结合起来就需要知道每一个图片所在pdf的位置，我目前看到的资料还没有很好的解决这个问题。</p>
<h1 id="评估-evaluation">评估 Evaluation</h1>
<p>该课程建议从三个维度来评测一个RAG Application的好坏：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20231205105234142.png" alt="image-20231205105234142">
<figcaption aria-hidden="true">image-20231205105234142</figcaption>
</figure>
<ul>
<li>问题和回答的相关性</li>
<li>根据问题抽取出来的context和问题的相关性</li>
<li>回答和context的相关性</li>
</ul>
<p>该课程主要目的是宣传自己的框架Trulens(目前该框架在github有1.8k
star，热度不咋高)，如果想了解Evaluation的全景知识建议看一下<a href="http://arxiv.org/abs/2312.10997">Retrieval-Augmented Generation
for Large Language Models: A Survey</a></p>
<p>上面的review中很重要的两个总结：</p>
<ol type="1">
<li>上面所说的三个quality
score如何计算？可以看到仍然是我们熟悉的一些metrics</li>
</ol>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240228142327120.png" alt="image-20240228142327120">
<figcaption aria-hidden="true">image-20240228142327120</figcaption>
</figure>
<ol start="2" type="1">
<li>现有的可用评估框架有哪些？</li>
</ol>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240508133011839.png" alt="image-20240508133011839">
<figcaption aria-hidden="true">image-20240508133011839</figcaption>
</figure>
<p>我们上面提到的课程里使用的就是该表格中列出的TruLens.
上面这张表格总结的还不是特别全面，而且没有datasets的整理，24年新出的文章<a href="http://arxiv.org/abs/2401.17043">CRUD-RAG: A Comprehensive Chinese
Benchmark for Retrieval-Augmented Generation of Large Language
Models</a> 中对这部分做了整理：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240228152011016.png" alt="image-20240228152011016">
<figcaption aria-hidden="true">image-20240228152011016</figcaption>
</figure>
<blockquote>
<p>这里做一下update，在作者写这篇文章时，综述<a href="http://arxiv.org/abs/2312.10997">Retrieval-Augmented Generation
for Large Language Models: A
Survey</a>还未对评估的数据集做整理，但最近一期3月的论文更新中已经有了这部分的内容。主要增添了对于每一个评测任务的数据集的整理。</p>
</blockquote>
<p>其中[7]就是RGB，它数据的生成是利用一系列收集到的news
report，然后利用LLM来基于这些report生成relevant
events,questions和answers。[38]是ARES，利用flan-t5来生成的一系列合成query和answer。其中比较重要的一列是是否有金标准，也就是上图中的倒数第二列。
13,12以及38分别是TruLens-Eval，RAGAS和ARES，这三个是不需要金标准的，不过代价是需要用到Chatgpt来做自动评估呀，这些可都是白花花的银子。使用Trulens-Eval都是需要配置openai的API的。</p>
<h2 id="langchain-benchmark">LangChain Benchmark</h2>
<p>对于想要快速去搭建一个评估RAG的框架的人来说，最好是有现成的可以直接用的评估体系，省去自己搜集数据以及编写各种计算metrics的麻烦。langchain提供了这么一个benchmark包，<a href="https://langchain-ai.github.io/langchain-benchmarks/notebooks/retrieval/comparing_techniques.html">介绍传送门</a>,截止到24年3月，该库已经包含了三个开源数据集，两个是上面介绍的python文档和pdf的QA问答数据集，还有一个是正在开发中的基于PPT的问答数据集：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240314142718109.png" alt="image-20240314142718109">
<figcaption aria-hidden="true">image-20240314142718109</figcaption>
</figure>
<p>这份langchain官方教程里用了好多新的tool，其中一个就是Smith,
在notebook中clone的所有数据集都可以在这个平台上看到，有点像console。LangChain
Docs Q&amp;A的数据长这样：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;question&quot;</span>: <span class="string">&quot;How can I parallelize calls in LangChain?&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;answer&quot;</span>: <span class="string">&quot;To make parallel calls from a LangChain object, use the &#x27;batch()&#x27; (or asynchronous &#x27;abatch()&#x27;) method. You can also use a `RunnableParallel` object.&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>langchain-benchmark总体而言还处于初期，对于retrival的task也只有三个数据集做支撑，定制化的程度不是特别高。具体可以参考<a href="https://langchain-ai.github.io/langchain-benchmarks/">langchain-benchmark官方教程</a>。</p>
<p>今天在看huggingface官网文档的时候又看到官方出了新的evaluation的<a href="https://github.com/huggingface/cookbook/blob/main/notebooks/en/rag_evaluation.ipynb">guidebook</a>，这份代码里写的相当详细，不再是一个普通的RAG评估流程，还介绍了评估数据集的生成方式，最重要的是还做了数据集的filtering，这份教程对于企业内部生成自己的评估数据集是有很大的参考价值的。</p>
<h2 id="crud">CRUD</h2>
<p>现在我们花点篇幅来详细说一下CRUD这个中文评估benchmark。作者的出发点在于评估一个RAG的应用，要区别于评估一个LLM模型，下面这句话是作者从四个维度来评估RAG的出发点：</p>
<blockquote>
<p>Lewis et al. [25] argue that the core of RAG systems is their
interactive way of combining LLMs with external knowledge sources</p>
</blockquote>
<p>RAG和LLM的交互方式，也就是RAG帮助LLM做了哪些东西让LLM能更好的回答问题，作者觉得是这四个方面：Create，Read，Update和Delete.</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240314135617343.png" alt="image-20240314135617343">
<figcaption aria-hidden="true">image-20240314135617343</figcaption>
</figure>
<p>Read很常见，RAG会从知识库中搜集更多的信息来供LLM回答问题，Update主要是为了解决LLM无法回答具有时效性的问题，或者当时训练模型时没有加入的信息，Delete这点其实在我看来有点牵强。Read和Update这两点确实是评估一个RAG很关键的方面。</p>
<p>做RAG的评估，最重要的两点就是：</p>
<ol type="1">
<li>数据集的准备，作者打算从上面四个维度去衡量一个RAG的好坏，那就得准备相应的数据集，这部分的工作是我们平时自己做测评的重点</li>
<li>测评metrics的选择，除了我们熟知的BLEU，ROUGE，还有bert判分。其中还有作者基于QuestEval创造的RAGQuestEval评分。这个metrics还挺有意思的。这里放在这里详细介绍下：</li>
</ol>
<p>首先基于ground truth
sentence生成一系列的问题，生成问题的prompt设计是这样的：</p>
<blockquote>
<p>你是一位新闻编辑，现在，你被提供了一篇新闻，请先从新闻中抽取出你认为重要的所有关键信息（通常为一个关键词，包含文章中的所有实体和名词性短语），然后，根据关键信息设计几个问题，考验大家能否正确回答问题。用json的形式返回答案。以下是个例子。</p>
<p>新闻：2014年，全国新增并网光伏发电容量1060万千瓦，约占全球新增容量的四分之一。其中，全国新增光伏电站855万千瓦，分布式205万千瓦。据统计，2014年中国光伏发电量达到了250亿千瓦时，同比增⻓超过
200%。</p>
<p>{json_response}</p>
<p>现在你需要为这篇新闻设计问题，尽量涵盖大多数关键信息，请尽量让答案可以用两三个词回答，答案不能太长，key_info包含文章中的所有实体和名词性短语，question与key_info一一对应，数量一致，输出用json的格式：</p>
<p>{news}</p>
</blockquote>
<p>注意这里先让LLM抽取文章中的所有实体和名词性短语作为关键信息，question是根据这些关键信息生成的。问题生成完之后分别用reference
sentence和ground truth
sentence作为context，去让LLM回答上面生成的问题。如果遇到无法回答的问题就让LLM答“无法回答”.
最后一步针对回答的结果计算precision 和 recall。</p>
<p>该文章作者在数据的处理方面，选择去爬取网上最新的news，然后用这8000个新闻建立了三个task的数据集：open-domain
multi-document
summarization(考察RAG的delete能力)，text-continuation(考察RAG的Generation能力)，question-answering(read能力)和hallucination
modification(考察RAG的Update能力)。</p>
<p>其实仔细看上面review中的总结，CRUD这篇文章里提到的应该考察RAG的“哪些能力”还是不够全面的，而且我个人认为CRUD里面仅仅是以end-to-end的方式计算generated
anwser和gound
truth之间的差距也是不太可取的，它没有涉及到RAG里面很重要的一个环节：retrieval。更全面的方式应该是计算三种quality
scores（具体参考review的介绍）：</p>
<ul>
<li>context relevance： query 和 context 的关系</li>
<li>faithfulness(groundness)：answer 和 context 的关系</li>
</ul>
<p><strong>This measures the factual consistency of the generated answer
againest the given context</strong></p>
<p>主要用于检测LLM的幻觉。这里<a href="https://aws.amazon.com/cn/blogs/china/automated-rag-project-assessment-testing-using-trulens/">博客</a>
对trulens的计算方式做了详细介绍，注意它里面的prompt的设计。Ragas框架对于faithfulness的计算查看<a href="https://docs.ragas.io/en/latest/concepts/metrics/faithfulness.html">Faithfulness</a>，也是用chatgpt来把answer中的statement拆开然后分别去与召回的context做对照，可以查看ragas框架计算faithfulness的<a href="https://github.com/explodinggradients/ragas/blob/c5eac536000fcbc3d9fb9a741dfe10163cdc3cce/src/ragas/metrics/_faithfulness.py#L108">代码</a>.</p>
<p><em>My spicy comment:
trulens和ragas两者还挺类似的，就是ragas除了计算faithfulness，还多了好几个metrics，如context
precision, context recall, context entity recall。其实就是把context
relevance这个metric拆分地更细了。不仅如此，ragas把answer
relevance也拆的更细了，它包含了answer correctness, answer
relevance和answer similarity.
相比较而言，ragas在笔者写这篇文章的时候，star数是要比trulens多的，前者4.8k，后者1.8k。而且issues明显要多于trulens，直觉上看应该是ragas用的人比较多。</em></p>
<p>在整理这部分metrics的时候，也搜了一下大家都在用什么样的框架来评估自己的RAG，看到reddit上也有人有这样的疑问<a href="https://www.reddit.com/r/LangChain/comments/1bijg75/why_is_everyone_using_ragas_for_rag_evaluation/">Why
is everyone using RAGAS for RAG evaluation? For me it looks very
unreliable</a>,
我觉得其中一个回答比较贴合当下对于RAG评估的一个现状：</p>
<blockquote>
<p>There is no proper techincal report, paper, or any experiment that
ragas metric is useful and effective to evaluate LLM performance. That's
why I do not choose ragas at my <a href="https://github.com/Marker-Inc-Korea/AutoRAG">AutoRAG</a> tool. I
use metrics like G-eval or sem score that has proper experiment and
result that shows such metrics are effective. I think evaluating LLM
generation performance is not easy problem and do not have silver
bullet. All we can do is doing lots of experiment and mixing various
metrics for reliable result. In this term, ragas can be a opiton... (If
i am missing ragas experiment or benchmark result, let me know)</p>
<p>https://www.reddit.com/r/LangChain/comments/1bijg75/comment/kvoj1q8/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button</p>
</blockquote>
<ul>
<li>answer relevance： answer 和 query 的关系</li>
</ul>
<p>至于计算出上面这三个方面的数值，有多种方式。有用LLM的，比如Trulens就是用的chatgpt，也可以用claude，参考见<a href="https://aws.amazon.com/cn/blogs/china/automated-rag-project-assessment-testing-using-trulens/">基于大语言模型知识问答应用落地实践
– 使用 TruLens 做自动化 RAG
项目评估测试</a>。也有直接计算相似度的，比如我们熟悉的bert
score，rouge-L。review在这里也进行了整理：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240508132450374.png" alt="image-20240508132450374">
<figcaption aria-hidden="true">image-20240508132450374</figcaption>
</figure>
<h2 id="customization模式">Customization模式</h2>
<p>其实在具体的业务场景下，如果已经搭建了一套RAG系统，如何来评估这个RAG系统的好坏，更合理的方式还是需要用自己的数据来测评，如果只是用一些公开的benchmark，如上面提到的langchain
benchmark，还是CRUD提出的以新闻为数据的benchmark，都有一点不那么让人信服，毕竟你费劲巴拉地搭建一个RAG的chatbot，还是要在自己的具体的业务场景表现好，客户才会买账吧。</p>
<p>但更多情况下，业务场景下往往是缺少金标数据集的，这时候就需要去针对自己的业务场景去生成一些“合成”数据集。我们可能基于的就是一堆的业务文档，这些文档有的是PDF，有的可能是word，也会有PPT，如果根据这些文档去生成自己的评测数据集，这样基于这个评测数据集我们再去“调整”我们RAG中的各个能影响RAG
performance的环节：embedding模型选择哪个，LLM选择哪个？chunking应该如何优化等等？加了rewrite和rerank等techniques之后有没有让RAG的效果变好，这里的变好仅仅是指在我们自己的业务数据上变好，而不是在其他开源的benchmark上，这样才具有一定的说服力。</p>
<p>参考博客<a href="https://huggingface.co/learn/cookbook/rag_evaluation">RAG
Evaluation</a>, 文章介绍了一种根据documents生成synthetic evaluation
dataset的办法，里面还加了一些tricks：如何用一个critique
agents去筛选QA。不过该篇文章evaluation环节仅仅计算了answer和query的关系（faithfulness），它给出的理由是：</p>
<blockquote>
<p>Out of <a href="https://docs.ragas.io/en/latest/concepts/metrics/index.html">the
different RAG evaluation metrics</a>, we choose to focus only on
faithfulness since it the best end-to-end metric of our system’s
performance.</p>
</blockquote>
<h1 id="rag-中的painpoints">RAG 中的PainPoints</h1>
<p>参考：</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=EBpT_cscTis">LLamaindex出品的视频</a></li>
<li><a href="https://towardsdatascience.com/12-rag-pain-points-and-proposed-solutions-43709939a28c#cea4">12
RAG Pain Points and Proposed Solutions</a></li>
</ul>
<p>上面视频对应的博客</p>
<ul>
<li><a href="http://arxiv.org/abs/2401.05856">Seven Failure Points When
Engineering a Retrieval Augmented Generation System</a></li>
</ul>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240725140423246-17218874651211.png" alt="image-20240725140423246">
<figcaption aria-hidden="true">image-20240725140423246</figcaption>
</figure>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240725140443402.png" alt="image-20240725140443402">
<figcaption aria-hidden="true">image-20240725140443402</figcaption>
</figure>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>图像的读取方式</title>
    <url>/2022/06/21/%E5%9B%BE%E5%83%8F%E7%9A%84%E8%AF%BB%E5%8F%96%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<h1 id="pil">PIL</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&quot;1.jpg&quot;</span>)</span><br><span class="line">img.load()</span><br><span class="line"></span><br><span class="line">array = np.asarrary(img)</span><br><span class="line"></span><br><span class="line">img.show() <span class="comment"># view the picture</span></span><br><span class="line"></span><br><span class="line">img.save(<span class="string">&quot;./new_img.jpg&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>PIL.Image包有很多其他的功能，比如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从array生成图片convert it to a Pillow image</span></span><br><span class="line">Image.fromarray(data,<span class="string">&#x27;RGB&#x27;</span>) <span class="comment"># 如果mode=&#x27;L&#x27;,那么只有一个通道</span></span><br></pre></td></tr></table></figure>
<h1 id="cv2">cv2</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">&quot;./1.jpg&quot;</span>) <span class="comment"># 返回的img的channel顺序是BGR</span></span><br><span class="line"></span><br><span class="line">grey_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) <span class="comment"># 转化为灰度图的模式</span></span><br><span class="line">rgb_img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) <span class="comment"># 转化为RGB的模式</span></span><br></pre></td></tr></table></figure>
<p><code>cv2</code>一个有用的method是resize</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv2.resize(raw_img,(width,height))</span><br></pre></td></tr></table></figure>
<h1 id="tensorflow">tensorflow</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">img = tf.keras.preprocessing.image.load_img(<span class="string">&quot;./i.jpg&quot;</span>,target_size=(<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>)) <span class="comment"># Loads an image into PIL format</span></span><br><span class="line">img = tf.keras.preprocessing.image.img_to_array(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从array转化成PIL image 实例</span></span><br><span class="line">img = tf.keras.preprocessing.image.array_to_img(array) <span class="comment"># array是3D numpy array</span></span><br><span class="line"></span><br><span class="line">tf.keras.utils.save_img(path,array)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>训练一个information retrival</title>
    <url>/2023/06/30/%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AAinformation-retrival/</url>
    <content><![CDATA[<p>近期中文大语言模型出现了好多产品，独领风骚的chatglm-6B确实表现很不错，所以大家就纷纷下场想做一个自己领域的大语言模型，最近看到一篇<a href="http://arxiv.org/abs/2305.15062">Lawyer LLaMA Technical
Report</a>，作者基于llama模型做了一个法律的语言模型。我觉得这篇文章值得想在垂直领域做语言模型的研究者参考，特别是它所采取的路径：</p>
<figure>
<img src="/2023/06/30/%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AAinformation-retrival/image-20230630105909352.png" alt="training process of lawyer LLaMA">
<figcaption aria-hidden="true">training process of lawyer
LLaMA</figcaption>
</figure>
<p>也有<a href="https://mp.weixin.qq.com/s/XYJMDND4Od41WECqlxt3Fw">博客</a>粗略介绍了这篇文章。</p>
<p>我重点关注的是这个工作里的信息抽取：</p>
<figure>
<img src="/2023/06/30/%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AAinformation-retrival/image-20230630110134378.png" alt="image-20230630110134378">
<figcaption aria-hidden="true">image-20230630110134378</figcaption>
</figure>
<p>思路大概就是和斯坦福的dsp一个思路，用户问一个问题，第一种方式就是直接把这个问题抛给llm，让它去回答，第二种就是在把问题抛给llm之前先过一道retrival模型，这个模型会抽取一些和回答这个问题相关的一些context，将这些context加入prompt中，然后再抛给llm。这有一个专门的名词，在Demonstrate-Search-Predict:
Composing retrieval and language models for knowledge-intensive NLP
中有所介绍：
<em>retrieve-then-read</em>。有兴趣的可以再看一下斯坦福在做这部分工作时出的<a href="https://github.com/stanfordnlp/dsp/blob/main/intro.ipynb">notebook</a>介绍，看完你就知道为啥要retrieve一些context加入到prompt中去。</p>
<p>可惜的是laywer
llama这篇文章并没有详细的介绍它的retrieve咋做的，文章中也是一笔带过，数据规模包括组织形式一概没说。我主要是参考斯坦福的dsp这框架中retrieve的实现，它是基于ColBERTv2做的抽取模块，但其实colbert预训练的模型是在微软的msmarco数据集上训练的，也就是没有中文，那必然对中文的适配度应该就不太行，所以我就想找找有咩有好心的博主写过自己用colbert在自己组织的语料库上训练的过程，发现没有，所以这就是我写这篇文章的动机啦，希望能给后面想用自己组建的数据集训练一个colbert做信息抽取的童鞋一点参考。</p>
<p>以下的内容参考<a href="https://github.com/stanford-futuredata/ColBERT">colbert
github</a></p>
<h1 id="数据组织">数据组织</h1>
<h1 id="模型使用">模型使用</h1>
<h2 id="用官方训练好的checkpoints">用官方训练好的checkpoints</h2>
<p>参考https://github.com/stanford-futuredata/ColBERT/blob/main/docs/intro.ipynb</p>
<p>里面的输就LoTTE的url已经失效了，包括checkpoints的下载地址。可以到hugging
face的仓库找</p>
<ul>
<li>colbertv2.0 checkpoints :
https://huggingface.co/colbert-ir/colbertv2.0/tree/main</li>
<li>lotte 数据集地址：
https://huggingface.co/datasets/colbertv2/lotte/tree/main</li>
</ul>
<p>注意上面这个lotte的数据集的组织方式和colbert官方github仓库里的介绍不一样，这一点上colbert2这个代码仓库的维护做的蛮糟糕的，比羊驼可差远了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.insert(<span class="number">0</span>, <span class="string">&#x27;../&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面这些包是colbert写好的类，可以导入预训练的checkpoints和你自己的querys，documents</span></span><br><span class="line"><span class="keyword">from</span> colbert.infra <span class="keyword">import</span> Run, RunConfig, ColBERTConfig</span><br><span class="line"><span class="keyword">from</span> colbert.data <span class="keyword">import</span> Queries, Collection</span><br><span class="line"><span class="keyword">from</span> colbert <span class="keyword">import</span> Indexer, Searcher</span><br><span class="line"></span><br><span class="line">dataroot = <span class="string">&#x27;downloads/lotte&#x27;</span> <span class="comment"># 假设你已经把lotte放到downloads文件夹下</span></span><br><span class="line">dataset = <span class="string">&#x27;lifestyle&#x27;</span></span><br><span class="line">datasplit = <span class="string">&#x27;dev&#x27;</span></span><br><span class="line"></span><br><span class="line">queries = os.path.join(dataroot, dataset, datasplit, <span class="string">&#x27;questions.search.tsv&#x27;</span>) <span class="comment"># questions.search.txv没找到</span></span><br><span class="line">collection = os.path.join(dataroot, dataset, datasplit, <span class="string">&#x27;collection.tsv&#x27;</span>) <span class="comment"># collections.tsv也没找到</span></span><br><span class="line"></span><br><span class="line">queries = Queries(path=queries)</span><br><span class="line">collection = Collection(path=collection)</span><br><span class="line"></span><br><span class="line"><span class="string">f&#x27;Loaded <span class="subst">&#123;<span class="built_in">len</span>(queries)&#125;</span> queries and <span class="subst">&#123;<span class="built_in">len</span>(collection):,&#125;</span> passages&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># indexing 主要用于对所有的document进行index</span></span><br><span class="line">nbits = <span class="number">2</span>   <span class="comment"># encode each dimension with 2 bits</span></span><br><span class="line">doc_maxlen = <span class="number">300</span>   <span class="comment"># truncate passages at 300 tokens</span></span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&#x27;downloads/colbertv2.0&#x27;</span></span><br><span class="line">index_name = <span class="string">f&#x27;<span class="subst">&#123;dataset&#125;</span>.<span class="subst">&#123;datasplit&#125;</span>.<span class="subst">&#123;nbits&#125;</span>bits&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Run().context(RunConfig(nranks=<span class="number">4</span>, experiment=<span class="string">&#x27;notebook&#x27;</span>)):  <span class="comment"># nranks specifies the number of GPUs to use.</span></span><br><span class="line">    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits)</span><br><span class="line"></span><br><span class="line">    indexer = Indexer(checkpoint=checkpoint, config=config)</span><br><span class="line">    indexer.index(name=index_name, collection=collection, overwrite=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># search </span></span><br><span class="line"><span class="keyword">with</span> Run().context(RunConfig(experiment=<span class="string">&#x27;notebook&#x27;</span>)):</span><br><span class="line">    searcher = Searcher(index=index_name)</span><br><span class="line">query = queries[<span class="number">37</span>]   <span class="comment"># or supply your own query</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;#&gt; <span class="subst">&#123;query&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find the top-3 passages for this query</span></span><br><span class="line">results = searcher.search(query, k=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the top-k retrieved passages</span></span><br><span class="line"><span class="keyword">for</span> passage_id, passage_rank, passage_score <span class="keyword">in</span> <span class="built_in">zip</span>(*results):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\t [<span class="subst">&#123;passage_rank&#125;</span>] \t\t <span class="subst">&#123;passage_score:<span class="number">.1</span>f&#125;</span> \t\t <span class="subst">&#123;searcher.collection[passage_id]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>ok
偶然看到官方仓库昨天更新了intro的notebook，我看到里面下载数据集的仓库变化了，然后导入包的时候也写得更清晰了，加了条件：https://github.com/stanford-futuredata/ColBERT/blob/main/docs/intro2new.ipynb</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>浅析stanford alpaca羊驼代码</title>
    <url>/2023/06/20/%E6%B5%85%E6%9E%90stanford-alpaca%E7%BE%8A%E9%A9%BC%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<p>斯坦福的羊驼alpaca开启了大家微调大语言模型的先河，现在很多国内的工作都是基于斯坦福的羊驼模型的范式来微调chatglm-6B,alpaca的<a href="https://github.com/tatsu-lab/stanford_alpaca">github</a>.</p>
<p>之前我一个很厉害的师兄说过，要想写代码写的厉害或者有所提高，最重要的就是要多读别人优秀的代码，多思考别人为什么这么写，如果让自己写的话是不是可以做到如此高效。这就是我想写这篇博客的原因，我是最近几个月才接触的huggingface的transformer库，发现很多API虽然设计的很简单，但里面功能丰富，不可能一下子就掌握住，所以我的办法是多看别人是如何写的，我的主要参考repo就是alpaca和chatglm-6B官方repo给出的那些finetune
LLM的代码。</p>
<p>回到羊驼alpca这份代码，不得不说斯坦福的这份代码写的真的很优秀，值得一句一句去debug。</p>
<h1 id="数据准备部分">数据准备部分</h1>
<p>代码在<code>generate_instruction.py</code>内。</p>
<p>这部分代码主要功能是实现由seed_tasks.jsonl作为模板，让GPT3.5来根据两个seed生成一些instruction和input，output。思想基于self-instruct的理念，在我另外一篇博客instrcution
tuning中有所详细介绍。</p>
<p>这里的启动函数是<code>def generate_instruction_following_data()</code>,
首先会将seed_tasks读取进来，然后从中随机选取num_prompt_instructions个数据，默认是三个：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)</span><br></pre></td></tr></table></figure>
<p>注意这里羊驼采用了向chatgpt传输batch请求的方式，也就是一次性向chatgpt传输多个prompt，程序里默认是5个prompt一起传输给gpt，然后每一个prompt长什么样子呢？</p>
<p>举一个简单的例子, 下面这是一个seed_task</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;&quot;id&quot;: &quot;seed_task_1&quot;, &quot;name&quot;: &quot;antonym_relation&quot;, &quot;instruction&quot;: &quot;What is the relation between the given pairs?&quot;, &quot;instances&quot;: [&#123;&quot;input&quot;: &quot;Night : Day :: Right : Left&quot;, &quot;output&quot;: &quot;The relation between the given pairs is that they are opposites.&quot;&#125;], &quot;is_classification&quot;: false&#125;</span><br></pre></td></tr></table></figure>
<p>作者将三个seed_task拼接在一起，然后前面加上事先定义好的prompt：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_prompt</span>(<span class="params">prompt_instructions</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Encode multiple prompt instructions into a single string.&quot;&quot;&quot;</span></span><br><span class="line">    prompt = <span class="built_in">open</span>(<span class="string">&quot;./prompt.txt&quot;</span>).read() + <span class="string">&quot;\n&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, task_dict <span class="keyword">in</span> <span class="built_in">enumerate</span>(prompt_instructions):</span><br><span class="line">        (instruction, <span class="built_in">input</span>, output) = task_dict[<span class="string">&quot;instruction&quot;</span>], task_dict[<span class="string">&quot;input&quot;</span>], task_dict[<span class="string">&quot;output&quot;</span>]</span><br><span class="line">        instruction = re.sub(<span class="string">r&quot;\s+&quot;</span>, <span class="string">&quot; &quot;</span>, instruction).strip().rstrip(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">        <span class="built_in">input</span> = <span class="string">&quot;&lt;noinput&gt;&quot;</span> <span class="keyword">if</span> <span class="built_in">input</span>.lower() == <span class="string">&quot;&quot;</span> <span class="keyword">else</span> <span class="built_in">input</span></span><br><span class="line">        prompt += <span class="string">f&quot;###\n&quot;</span></span><br><span class="line">        prompt += <span class="string">f&quot;<span class="subst">&#123;idx + <span class="number">1</span>&#125;</span>. Instruction: <span class="subst">&#123;instruction&#125;</span>\n&quot;</span></span><br><span class="line">        prompt += <span class="string">f&quot;<span class="subst">&#123;idx + <span class="number">1</span>&#125;</span>. Input:\n<span class="subst">&#123;<span class="built_in">input</span>&#125;</span>\n&quot;</span></span><br><span class="line">        prompt += <span class="string">f&quot;<span class="subst">&#123;idx + <span class="number">1</span>&#125;</span>. Output:\n<span class="subst">&#123;output&#125;</span>\n&quot;</span></span><br><span class="line">    prompt += <span class="string">f&quot;###\n&quot;</span></span><br><span class="line">    prompt += <span class="string">f&quot;<span class="subst">&#123;idx + <span class="number">2</span>&#125;</span>. Instruction:&quot;</span></span><br><span class="line">    <span class="keyword">return</span> prompt</span><br></pre></td></tr></table></figure>
<p>事先定义好的prompt长这样：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">You are asked to come up with a set of 20 diverse task instructions. These task instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions.</span><br><span class="line"></span><br><span class="line">Here are the requirements:</span><br><span class="line">1. Try not to repeat the verb for each instruction to maximize diversity.</span><br><span class="line">2. The language used for the instruction also should be diverse. For example, you should combine questions with imperative instrucitons.</span><br><span class="line">3. The type of instructions should be diverse. The list should include diverse types of tasks like open-ended generation, classification, editing, etc.</span><br><span class="line">4. A GPT language model should be able to complete the instruction. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a reminder because it cannot perform any action.</span><br><span class="line">5. The instructions should be in English.</span><br><span class="line">6. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a question is permitted.</span><br><span class="line">7. You should generate an appropriate input to the instruction. The input field should contain a specific example provided for the instruction. It should involve realistic data and should not contain simple placeholders. The input should provide substantial content to make the instruction challenging but should ideally not exceed 100 words.</span><br><span class="line">8. Not all instructions require input. For example, when a instruction asks about some general information, &quot;what is the highest peak in the world&quot;, it is not necssary to provide a specific context. In this case, we simply put &quot;&lt;noinput&gt;&quot; in the input field.</span><br><span class="line">9. The output should be an appropriate response to the instruction and the input. Make sure the output is less than 100 words.</span><br><span class="line"></span><br><span class="line">List of 20 tasks:</span><br></pre></td></tr></table></figure>
<p>理解起来就是作者在list of 20 tasks后面跟上了三个seed
tasks，也就是给gpt打个样，让它知道按这个模式去生成。这里有个地方值得参考的：</p>
<blockquote>
<p>用模板的时候给每一个example配上分隔符，这里作者采用了###,
不仅如此，作者还采用了序号的方式，这些都是为了方便后面对gpt返回的text进行处理</p>
</blockquote>
<p>这里还有一个值得学习的地方在utils.py内，我们在使用openai的api获取回复时，有时候会遇到prompt过长的问题，羊驼catch了这个报错，将prompt的长度变为原来的80%，然后再向gpt发送请求，正是由于这份耐心，这个代码的耦合性就没那么高，所以易用性非常强，非常值得野生程序员学习。</p>
<hr>
<p>gpt返回的文本羊驼模型还做了similarity的计算，将相似度超过一定阈值的instrcution剔除掉。这部分代码可以直接复用。</p>
<h1 id="train中的数据处理">train中的数据处理</h1>
<p>羊驼模型采用了一个函数生成transformers库所需要的参数：
<code>make_supervised_data_module</code>，该函数返回一个dict，其中字典的key就是我们初始化Trainer类所需要的train_dataset,eval_dataset和data_collator。这个函数里首先是构建数据集的类<code>SupervisedDataset</code>，继承自Dataset.
注意pytorch里规定如果你想要创建一个自建的Dataset，这个继承自Dataset的子类必须重写<code>__len__</code>和<code>__getitem__</code>两个方法，羊驼这里写的是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SupervisedDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Dataset for supervised fine-tuning.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data_path: <span class="built_in">str</span>, tokenizer: transformers.PreTrainedTokenizer</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SupervisedDataset, self).__init__()</span><br><span class="line">        logging.warning(<span class="string">&quot;Loading data...&quot;</span>)</span><br><span class="line">        list_data_dict = utils.jload(data_path)</span><br><span class="line"></span><br><span class="line">        logging.warning(<span class="string">&quot;Formatting inputs...&quot;</span>)</span><br><span class="line">        prompt_input, prompt_no_input = PROMPT_DICT[<span class="string">&quot;prompt_input&quot;</span>], PROMPT_DICT[<span class="string">&quot;prompt_no_input&quot;</span>]</span><br><span class="line">        sources = [</span><br><span class="line">            prompt_input.format_map(example) <span class="keyword">if</span> example.get(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;&quot;</span>) != <span class="string">&quot;&quot;</span> <span class="keyword">else</span> prompt_no_input.format_map(example)</span><br><span class="line">            <span class="keyword">for</span> example <span class="keyword">in</span> list_data_dict</span><br><span class="line">        ]</span><br><span class="line">        targets = [<span class="string">f&quot;<span class="subst">&#123;example[<span class="string">&#x27;output&#x27;</span>]&#125;</span><span class="subst">&#123;tokenizer.eos_token&#125;</span>&quot;</span> <span class="keyword">for</span> example <span class="keyword">in</span> list_data_dict]</span><br><span class="line"></span><br><span class="line">        logging.warning(<span class="string">&quot;Tokenizing inputs... This may take some time...&quot;</span>)</span><br><span class="line">        data_dict = preprocess(sources, targets, tokenizer)</span><br><span class="line"></span><br><span class="line">        self.input_ids = data_dict[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">        self.labels = data_dict[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.input_ids)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, i</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(input_ids=self.input_ids[i], labels=self.labels[i]) <span class="comment"># 这里key必须是input_ids和labels,这是由于是llama模型的规定。</span></span><br></pre></td></tr></table></figure>
<p>羊驼模型用的DataCollator是自定义的，首先解释下datacollator是什么东西，transformers的官方文档的解释是：</p>
<blockquote>
<p>Data collators are objects that will form a batch by using a list of
dataset elements as input. These elements are of the same type as the
elements of <code>train_dataset</code> or <code>eval_dataset</code>.</p>
</blockquote>
<p>也就是你把所有的文本用tokenizer转化成input_ids和labels之后，要把他们组织成batch的形式，不仅如此，collator还能做一些数据处理的工作。它的输入就是我们之前的数据集，注意我们数据集的组织形式每一个数据sample它是一个字典，字典有两个key。所以羊驼这里首先将其拆分,
一句话解决，非常善于利用[ for
句式]，让我写的话应该是写成非常冗余的两个for循环。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_ids, labels = <span class="built_in">tuple</span>([instance[key] <span class="keyword">for</span> instance <span class="keyword">in</span> instances] <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">&quot;input_ids&quot;</span>, <span class="string">&quot;labels&quot;</span>))</span><br></pre></td></tr></table></figure>
<p>后面就是很简单的train了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.train()</span><br><span class="line">trainer.save_state()</span><br><span class="line">trainer.save_model(output_dir=training_args.output_dir)</span><br></pre></td></tr></table></figure>
<p>我当时看到这里的时候有点奇怪，因为我之前的习惯还是tensorflow那一套，基本上你把数据处理完之后还要prefetech，batch等，但是这里感觉transformer全部做了集成，可以仔细看羊驼模型的训练启动命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">torchrun --nproc_per_node=4 --master_port=&lt;your_random_port&gt; train.py \</span><br><span class="line">    --model_name_or_path <span class="string">&quot;facebook/opt-6.7b&quot;</span> \</span><br><span class="line">    --data_path ./alpaca_data.json \</span><br><span class="line">    --bf16 True \</span><br><span class="line">    --output_dir &lt;your_output_dir&gt; \</span><br><span class="line">    --num_train_epochs 3 \</span><br><span class="line">    --per_device_train_batch_size 4 \</span><br><span class="line">    --per_device_eval_batch_size 4 \</span><br><span class="line">    --gradient_accumulation_steps 8 \</span><br><span class="line">    --evaluation_strategy <span class="string">&quot;no&quot;</span> \</span><br><span class="line">    --save_strategy <span class="string">&quot;steps&quot;</span> \</span><br><span class="line">    --save_steps 2000 \</span><br><span class="line">    --save_total_limit 1 \</span><br><span class="line">    --learning_rate 2e-5 \</span><br><span class="line">    --weight_decay 0. \</span><br><span class="line">    --warmup_ratio 0.03 \</span><br><span class="line">    --lr_scheduler_type <span class="string">&quot;cosine&quot;</span> \</span><br><span class="line">    --logging_steps 1 \</span><br><span class="line">    --fsdp <span class="string">&quot;full_shard auto_wrap&quot;</span> \</span><br><span class="line">    --fsdp_transformer_layer_cls_to_wrap <span class="string">&#x27;OPTDecoderLayer&#x27;</span> \</span><br><span class="line">    --tf32 True</span><br></pre></td></tr></table></figure>
<p>其中的per_device_train_batch_size就指定了一个device上弄几个数据，也就是batch_size是多少，另外loss计算这些都是pretrained模型定好的，所以不用管，包括optimizer，所以我们在训练的时候只需要指定训练过程中用到的参数，比如保存步数，学习率，训练多少个epoch等。这是finetune大语言模型和之前做深度学习模型不一样的地方，技术往往更新的太快，都快看不懂大家写的代码了，怎么咔咔一两句就开始训练了，所以函数集成太厉害也不是只有好处。</p>
<h1 id="推荐阅读">推荐阅读</h1>
<ul>
<li>https://mp.weixin.qq.com/s/ehEM04xmeJyqB4z7rmLKBQ
讲解self-instruct方法</li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
</search>
