<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>machine translation相关论文阅读</title>
    <url>/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<p>machine translation 这个任务一般是作为language
modeling的紧接一个话题。它的前身（2010年之前）是statistical machine
translation，但自从Neural machine
translation出来之后，用statistical的方式来做translation就少了很多。有兴趣的可以了解下statistical
machine translation的<a href="https://www.cs.upc.edu/~ageno/anlp/classeMT.pdf">具体细节</a>.
本博客主要记录NMT的主要论文和研究。NMT的架构主要是encoder-decoder架构，它其实是一个很典型的seq-to-seq的模型,
关于它的定义：</p>
<blockquote>
<p>Neural Machine Translation (NMT) is a way to do Machine Translation
with a single end-to-end neural network</p>
</blockquote>
<p>它的一般架构是这样的:</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230313132157791-16786849189061.png" alt="Seq2Seq">
<figcaption aria-hidden="true">Seq2Seq</figcaption>
</figure>
<p>NMT所有的模型都基于一个统一的数学公式：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230313134416540.png" alt="数学公式">
<figcaption aria-hidden="true">数学公式</figcaption>
</figure>
<p>注意这里和statistical machine translation的公式是不一样的：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230313134658535.png" alt="statistical machine translation">
<figcaption aria-hidden="true">statistical machine
translation</figcaption>
</figure>
<p>用统计翻译模型做的时候是分别解决translation model以及language
model的问题，涉及很多特征工程的问题，很复杂。</p>
<p>在machine
translation领域，encoder-decoder架构的模型经历了好几次演变，最终才转化成加入了attention机制，模型架构的整理可以参考<a href="http://arxiv.org/abs/1912.02047">Neural Machine Translation: A
Review and
Survey</a>。文章的第五章介绍了将encoder编码为固定长度的向量的用法。其中有两种使用这个<code>C</code>的用法，1.
作为decoder的初始化state 2.
作为decoder每一个时间步的固定输入和input一起去计算hidden state：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20221214151254068.png" alt="Encoder-decoder architectures with fixed-length sentence encodings">
<figcaption aria-hidden="true">Encoder-decoder architectures with
fixed-length sentence encodings</figcaption>
</figure>
<p>这些文章从<a href="https://arxiv.org/abs/1409.3215">Sequence to
Sequence Learning with Neural Networks</a>，再到<a href="http://arxiv.org/abs/1406.1078">Learning Phrase Representations
using RNN Encoder-Decoder for Statistical Machine Translation</a>.
然后就过度到attention时代了，所以作者在这篇review中只花了很少的第五章节就结束了。第六章就开始讲attentional
encoder-decoder networks。</p>
<blockquote>
<p>The concept of attention is no longer just a technique to improve
sentence lengths in NMT. Since its introduction by Bahdanau et al.
(2015) it has become a vital part of various NMT architectures,
culminating in the Transformer architecture</p>
</blockquote>
<p>这句话是6.1的精髓，attention的概念不再是我们上文所说的那些用于初始化呀，还是用作duplicate
context。Bahdanau 2015年的这篇文章，也就是引入multi-head
attention的这篇文章彻底打破了这个convention。因为我们可以看到transformer的架构中都没有RNN的身影，有的只是attention
weights的计算。</p>
<h1 id="learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation-2014">Learning
Phrase Representations using RNN Encoder-Decoder for Statistical Machine
Translation 2014</h1>
<p>这是在机器翻译领域encoder-decoder架构，在attention
机制提出之前表现最好的RNN模型。其实模型挺简单的，encoder负责将input
sequence编码成了一个固定的向量Context，然后基于这个向量，decoder每一个时间步产生一个单词。在decoder的每一个时间步进行的运算是：</p>
<p><img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20221212164533545.png" alt="image-20221212164533545"> <code>y_t</code>是由s_t得到的。</p>
<p>同样的，这篇文章可以结合<a href="https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb">代码</a>来看，轻易理解。该代码是用pytorch实现的。这个pytorch的实现是从<a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning
with Neural Networks</a>开始讲解的，Learning Phrase Representations
using RNN Encoder-Decoder for Statistical Machine
Translation这篇文章进步在</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20221216102034819.png" alt="image-20221216102034819">
<figcaption aria-hidden="true">image-20221216102034819</figcaption>
</figure>
<p>可以看到该篇文章介绍的模型优势在于预测y的时候加入了context以及<span class="math inline">\(y_{t-1}\)</span>,而不是仅仅依赖于<span class="math inline">\(s_t\)</span></p>
<p>以上的文章都是将input
sentence编码成一个fixed-length的vector，从下面这篇2015年Bahdanau的文章开始，attention就开始用于NMT。为了解决fixed-length
vector的问题，这样我们就不必要将input
sentence的所有信息都编码到一个固定长度的向量里。</p>
<h1 id="neural-machine-translation-by-jointly-learning-to-align-and-translate-2015">Neural
Machine Translation by Jointly Learning to Align and Translate 2015</h1>
<p>从这篇文章开始，attention的机制开始使用在翻译中。</p>
<p>在Introduction章节，最重要的一句话：</p>
<blockquote>
<p>The most important distinguishing feature of this approach from the
basic encoder–decoder is that it does not attempt to encode a whole
input sentence into a single fixed-length vector. Instead, it encodes
the input sentence into a sequence of vectors and chooses a subset of
these vectors adaptively while decoding the translation</p>
</blockquote>
<p>意即跟以往那种encoder-decoder的网络来做translation的model不同，虽然提出的模型也属于encoder-decoder架构，但不是将input
sentence编码成一个固定长度的向量，而是将input
sentence编码成一系列的向量并自适应的从中选择一个小子集的向量用来做decode。</p>
<p>截至文章发表，现有做机器翻译的模型中，表现最好的模型是RNN，内units用lstm。可以称之为RNN
Encoder-Decoder。</p>
<p>还有一个发现是，这些encoder和decoder block，里面基本上是stacked
rnns结构，也就是堆了好几层rnn。这个发现可以追溯到<a href="https://arxiv.org/pdf/1703.03906.pdf">paper</a>.
该作者发现在NMT任务上，high-performing rnns are usually multi-layer,
不仅如此，对于encoder rnn，2到4层是最好的，对于decoder
rnn，4层是最好的。通常情况下，2层堆叠的RNN比一层RNN要lot better;
为了解决long dependency的问题，用lstm
cell是必要的，但这也不够，需要使用一些其他的技术，比如skip-connection，dense-connections。</p>
<hr>
<p>这里值得一提的是，虽然Bahdanau
2015年出的这篇文章很火。但是后来通过学习cs224n和观察tensorflow的文档：<a href="https://www.tensorflow.org/text/tutorials/nmt_with_attention">Neural
machine translation with attention</a>,发现<a href="https://arxiv.org/abs/1508.04025v5">luong
2015</a>的这篇文章中的架构使用的更多，它的计算公式和Bahdanau介绍的有一点点不一样，再luong的文章中我们也可以看到它自己说的和Bahdanau不一样的地方：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230313171420621.png" alt="image-20230313171420621">
<figcaption aria-hidden="true">image-20230313171420621</figcaption>
</figure>
<blockquote>
<p>Comparison to (Bahdanau et al., 2015) – While our global attention
approach is similar in spirit to the model proposed by Bahdanau et al.
(2015), there are several key differences which reflect how we have both
simplified and generalized from the original model. First, we simply use
hidden states at the top LSTM layers in both the encoder and decoder as
illustrated in Figure 2. Bahdanau et al. (2015), on the other hand, use
the concatenation of the forward and backward source hidden states in
the bi-directional encoder and target hidden states in their
non-stacking unidirectional decoder. Second, our computation path is
simpler; we go from ht → at → ct → ̃ ht then make a prediction as
detailed in Eq. (5), Eq. (6), and Figure 2. On the other hand, at any
time t, Bahdanau et al. (2015) build from the previous hidden state ht−1
→ at → ct → ht, which, in turn, goes through a deep-output and a maxout
layer before making predictions.7 Lastly, Bahdanau et al. (2015) only
experimented with one alignment function, the concat product; whereas we
show later that the other alternatives are better.</p>
</blockquote>
<p>所以关于用attention来做machine
translation的模型，我们只需要记住下面的计算过程就行，因为它也不是现在流行的machine
translation的方法（毕竟2015年的时候transformer还没出来）：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230313171823584.png" alt="attention in equations">
<figcaption aria-hidden="true">attention in equations</figcaption>
</figure>
<h1 id="attention-is-all-you-need-2017">Attention is all you need
2017</h1>
<p>在transformer的<a href="https://arxiv.org/abs/1706.03762">paper</a>中，作者首先介绍本文：主流的sequence
tranduction模型主要基于复杂的RNN或者CNN模型，它们包含encoder和decoder两部分，其中表现最好的模型在encoder和decoder之间增加了attention
mechanism。本文提出了一个新的简单的网络结构名叫<em>transformer</em>，也是完全基于attention机制，"dispensing
with recurrence and convolutions entirely"!
根本无需循环和卷积！了不起的Network~</p>
<p>在阅读这篇文章之前需要提前了解我在另外一篇博客 Attention and
transformer
model中的知识，在translation领域我们的科学家们是如何从RNN循环神经网络过渡到CNN，然后最终是transformer的天下的状态。技术经过了一轮轮的迭代，每一种基础模型架构提出后，会不断的有文章提出新的改进，文章千千万，不可能全部读完，就精读一些经典文章就好，Vaswani这篇文章是NMT领域必读paper，文章不长，加上参考文献才12页，介绍部分非常简单，导致这篇文章的入门门槛很高（个人感觉）。我一开始先读的这篇文章，发现啃不下去，又去找了很多资料来看，其中对我非常帮助的有很多：</p>
<ul>
<li><a href="http://jalammar.github.io/illustrated-transformer/">非常通俗易懂的blog</a>
有中文版本的翻译</li>
<li><a href="http://arxiv.org/abs/1912.02047">Neural Machine
Translation: A Review and Survey</a>
虽然这篇paper很长，90+页。前六章可以作为参照，不多25页左右，写的非常好</li>
<li><a href="http://cs231n.stanford.edu/slides/2022/lecture_11_ruohan.pdf">stanford
cs231n课程的ppt</a>
斯坦福这个课程真的很棒，youtube上可以找到17年的视频，17年的课程中没有attention的内容，所以就姑且看看ppt吧，希望斯坦福有朝一日能将最新的课程分享出来，也算是做贡献了</li>
<li>cs231n推荐的阅读<a href="https://lilianweng.github.io/posts/2018-06-24-attention/">博客</a>
非常全面的整理，强烈建议食用.
这位作者也附上了自己的transformer实现，在它参考的那些github实现里，哈佛大学的<a href="http://nlp.seas.harvard.edu/2018/04/01/attention.html">pytorch实现</a>也值得借鉴。</li>
</ul>
<p>Transformer这篇文章有几个主要的创新点：</p>
<ol type="1">
<li>使用self-attention机制，并首次提出使用multi-head attention</li>
</ol>
<p>该机制作用是在编码当前word的时候，这个self-attention就会告诉我们编码这个词语我们应该放多少注意力在这个句子中其他的词语身上，说白了其实就是计算当前词语和其他词语的关系。这也是CNN用于解决NMT问题时用不同width的kernel来扫input
metric的原因。</p>
<p>multi-head的意思是我使用多个不同的self-attention
layer来处理我们的输入，直观感觉是训练的参数更多了，模型的表现力自然要好一点。</p>
<ol start="2" type="1">
<li>Positional embeddings</li>
</ol>
<p>前一个创新点解决了dependence的问题，那如何解决位置的问题呢？也就是我这个词在编码的时候或者解码的时候应该放置在句子的哪个位置上。文章就用pisitional
embedding来解决这个问题。这个positional embedding和input
embedding拥有相同的shape，所以两者可以直接相加。transformer这篇文章提供了两种encoding方式：</p>
<p>1） sunusoidal positional encoding</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230302133247664.png" alt="image-20230302133247664">
<figcaption aria-hidden="true">image-20230302133247664</figcaption>
</figure>
<p>其中，pos=1,...,L(L是input句子的长度)，i是某一个PE中的一个维度，取值范围是1到dmodel。python实现为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span>(<span class="params">length, depth</span>):</span></span><br><span class="line">  depth = depth/<span class="number">2</span></span><br><span class="line"></span><br><span class="line">  positions = np.arange(length)[:, np.newaxis]     <span class="comment"># (seq, 1)</span></span><br><span class="line">  depths = np.arange(depth)[np.newaxis, :]/depth   <span class="comment"># (1, depth)</span></span><br><span class="line"></span><br><span class="line">  angle_rates = <span class="number">1</span> / (<span class="number">10000</span>**depths)         <span class="comment"># (1, depth)</span></span><br><span class="line">  angle_rads = positions * angle_rates      <span class="comment"># (pos, depth)</span></span><br><span class="line"></span><br><span class="line">  pos_encoding = np.concatenate(</span><br><span class="line">      [np.sin(angle_rads), np.cos(angle_rads)],</span><br><span class="line">      axis=-<span class="number">1</span>) </span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> tf.cast(pos_encoding, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">pos_encoding = positional_encoding(length=<span class="number">2048</span>, depth=<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the shape.</span></span><br><span class="line"><span class="built_in">print</span>(pos_encoding.shape) <span class="comment"># (2014,512)</span></span><br></pre></td></tr></table></figure>
<p>2） learned positional encoding</p>
<p>整体上看，这篇文章提出的transformer模型在做translation的任务时，架构是这样的：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230301133249379.png" alt="image-20230301133249379">
<figcaption aria-hidden="true">image-20230301133249379</figcaption>
</figure>
<p>其中encoders部分包含了6个encoders的block，decoders部分也包含了6个decoders的block，将encoders的每一个block拆开来看，有两个sub
layer：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230301133424302.png" alt="image-20230301133424302">
<figcaption aria-hidden="true">image-20230301133424302</figcaption>
</figure>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230301133440152.png" alt="image-20230301133440152">
<figcaption aria-hidden="true">image-20230301133440152</figcaption>
</figure>
<p>其中decoder部分的block比encoder部分的block多了一个sub
layer，其中self-attention和encoder-decoder attention都是multi-head
attention layer，只不过decoder部分的第一个multi-head attention
layer是一个masked multi-head
attention，为了防止未来的信息泄露给当下（prevent positions from
attending to the future）.</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230301133608484.png" alt="image-20230301133608484">
<figcaption aria-hidden="true">image-20230301133608484</figcaption>
</figure>
<p>在transformer模型中，作者还使用了residual
connection，所以在encoder的每一个block中，数据的flow是:</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230307162830473-16781777124571.png" alt="transformer架构">
<figcaption aria-hidden="true">transformer架构</figcaption>
</figure>
<p>其中self-attention中涉及的运算details是：</p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230301134258180.png" alt="image-20230301134258180">
<figcaption aria-hidden="true">image-20230301134258180</figcaption>
</figure>
<p>可以发现其中涉及的运算都是矩阵的点乘，并没有RNN中那种时间步的概念，所以所有运算都是可以parallelizable，这就能使得模型的推理和训练更加的efficient。并且！Transformers也可以抓住distant的依赖，而不是像rnn那样对于长依赖并不是很擅长，因为它前面的信息如果像传递到很后面的单词推理上，需要经历很多时间步的计算，而transformer在推理每一个单词的时候都可以access到input句子中的每一个单词（毕竟我们的Z中包含了每一个单词跟其他单词的关系)。</p>
<p>其中positional encoding现在可以简单的理解成在我们编码的word
embedding上我们又加了一个positional
encoding，维度和我们的embedding一模一样。</p>
<blockquote>
<p>在tensorflow中有一个layer是<code>MultiHeadAttention</code>,如果我们想实现transformer里的这个self-attention，那就是query，key，value其实都是由input
vector计算来的。</p>
</blockquote>
<p>以上的理论计算看起来可能会有点模糊，可以同步参照<a href="https://lilianweng.github.io/posts/2018-06-24-attention/">博客</a>
参考 <a href="http://jalammar.github.io/illustrated-transformer/">illustrated
transformer</a>介绍的详细细节，基于tensorflow框架实现的<a href="https://github.com/lilianweng/transformer-tensorflow/blob/master/transformer.py">transformer</a>来帮助自己理解transformer模型。</p>
<h2 id="encoder部分">encoder部分</h2>
<p>encoder的每一个block由两个sub-layer组成，中间穿插resnet
connection。</p>
<p><img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/transformer_encoder_block.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multihead_attention</span>(<span class="params">self, query, memory=<span class="literal">None</span>, mask=<span class="literal">None</span>, scope=<span class="string">&#x27;attn&#x27;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            query (tf.tensor): of shape (batch, q_size, d_model)</span></span><br><span class="line"><span class="string">            memory (tf.tensor): of shape (batch, m_size, d_model)</span></span><br><span class="line"><span class="string">            mask (tf.tensor): shape (batch, q_size, k_size)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:h</span></span><br><span class="line"><span class="string">            a tensor of shape (bs, q_size, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> memory <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment"># 如果memory是None，那么就是一个典型的self-attention layer</span></span><br><span class="line">            memory = query</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># Linear project to d_model dimension: [batch, q_size/k_size, d_model]</span></span><br><span class="line">            Q = tf.layers.dense(query, self.d_model, activation=tf.nn.relu)</span><br><span class="line">            K = tf.layers.dense(memory, self.d_model, activation=tf.nn.relu)</span><br><span class="line">            V = tf.layers.dense(memory, self.d_model, activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Split the matrix to multiple heads and then concatenate to have a larger</span></span><br><span class="line">            <span class="comment"># batch size: [h*batch, q_size/k_size, d_model/num_heads]</span></span><br><span class="line">            Q_split = tf.concat(tf.split(Q, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            K_split = tf.concat(tf.split(K, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            V_split = tf.concat(tf.split(V, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            mask_split = tf.tile(mask, [self.h, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Apply scaled dot product attention</span></span><br><span class="line">            out = self.scaled_dot_product_attention(Q_split, K_split, V_split, mask=mask_split)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Merge the multi-head back to the original shape</span></span><br><span class="line">            out = tf.concat(tf.split(out, self.h, axis=<span class="number">0</span>), axis=<span class="number">2</span>)  <span class="comment"># [bs, q_size, d_model]</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># The final linear layer and dropout.</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_model)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dropout(out, rate=self.drop_rate, training=self._is_training)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_forwad</span>(<span class="params">self, inp, scope=<span class="string">&#x27;ff&#x27;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Position-wise fully connected feed-forward network, applied to each position</span></span><br><span class="line"><span class="string">        separately and identically. It can be implemented as (linear + ReLU + linear) or</span></span><br><span class="line"><span class="string">        (conv1d + ReLU + conv1d).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            inp (tf.tensor): shape [batch, length, d_model]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        out = inp</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_ff, activation=tf.nn.relu)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dropout(out, rate=self.drop_rate, training=self._is_training)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_model, activation=None)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># by default, use_bias=True</span></span><br><span class="line">            out = tf.layers.conv1d(out, filters=self.d_ff, kernel_size=<span class="number">1</span>, activation=tf.nn.relu)</span><br><span class="line">            out = tf.layers.conv1d(out, filters=self.d_model, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out  </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encoder_layer</span>(<span class="params">self, inp, input_mask, scope</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            inp: tf.tensor of shape (batch, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            input_mask: tf.tensor of shape (batch, seq_len, seq_len)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        out = inp</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># One multi-head attention + one feed-forword</span></span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(out, mask=input_mask))</span><br><span class="line">            out = self.layer_norm(out + self.feed_forwad(out))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h2 id="decoder部分">decoder部分</h2>
<p><img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/encoder-decoder.png"></p>
<p>在decoder部分，我们可以看到每一个decoder
block的输入有两个：<em>整个encoder部分的输出</em>以及<em>上一个decoder
block的输出（第一个decoder
block是词向量的输入）</em>，而encoder部分的输出是接到每一个decoder
block的第二个sublayer的。正如刚刚提到了，decoder部分的每一个block跟encoder部分的block有一个不一样的地方，那就是多了一个sublayer：
encoder-decoder
attention。至于encoder部分和decoder部分是如何connect的，</p>
<blockquote>
<p>The encoder start by processing the input sequence. The output of the
top encoder is then transformed into a set of attention vectors K and V.
These are to be used by each decoder in its “encoder-decoder attention”
layer which helps the decoder focus on appropriate places in the input
sequence</p>
</blockquote>
<p>也就是我们得到了encoder部分top layer（最后一个encoder
layer）的输出之后，我们将输出转化成K和V.
我们可以看到在<code>multihead_attention</code>里，memory是<code>enc_out</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder_layer</span>(<span class="params">self, target, enc_out, input_mask, target_mask, scope</span>):</span></span><br><span class="line">        out = target</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(</span><br><span class="line">                out, mask=target_mask, scope=<span class="string">&#x27;self_attn&#x27;</span>))</span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(</span><br><span class="line">                out, memory=enc_out, mask=input_mask)) <span class="comment"># 将encoder部分的输出结果作为输入</span></span><br><span class="line">            out = self.layer_norm(out + self.feed_forwad(out))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder</span>(<span class="params">self, target, enc_out, input_mask, target_mask, scope=<span class="string">&#x27;decoder&#x27;</span></span>):</span></span><br><span class="line">        out = target</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_enc_layers):</span><br><span class="line">                out = self.decoder_layer(out, enc_out, input_mask, target_mask, <span class="string">f&#x27;dec_<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>以上实现的transformer其实我觉得还是有一点点复杂，毕竟在tensorflow2.0+版本中已经有了官方实现好的<code>layers.MultiHeadAttention</code>可以使用，应该可以大大简化我们实现步骤，特别是上面的<code>def multihead_attention(self, query, memory=None, mask=None, scope='attn'):</code>。从刚刚的实现里我们可以发现，除了decoder部分每一个block的第二个sublayer的attention计算有一点不一样之外，其他的attention计算都是一模一样的。我在github上找了不少用TF2.0实现的transformer（最标准的也是Attention
is all you
need的模型），发现很多都都写得一般般，最终发现还是tensorflow官方文档写的<a href="https://www.tensorflow.org/text/tutorials/transformer#add_and_normalize">tutotial</a>写的最好.</p>
<p>现在对照tensorflow的tutorial以及上面transformer的计算过程，拆解一下官方给的代码。</p>
<p>首先定义一个baseAttention类，然后在此基础上我们再定义encoder和decoder中的attention：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseAttention</span>(<span class="params">tf.keras.layers.Layer</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)</span><br><span class="line">    self.layernorm = tf.keras.layers.LayerNormalization()</span><br><span class="line">    self.add = tf.keras.layers.Add()</span><br></pre></td></tr></table></figure>
<p>那么针对encoder结果输入到decoder的cross attention
layer怎么处理呢？这时候我们使用MultiHeadAttention时就需要将target
sequence x当作是query，将encoder输出当作是context
sequence也就是key/value。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossAttention</span>(<span class="params">BaseAttention</span>):</span> <span class="comment"># encoder结果输入到decoder的层</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x, context</span>):</span> <span class="comment"># 这里的x是target sequence,context是encoder的输出结果</span></span><br><span class="line">    attn_output, attn_scores = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        key=context,</span><br><span class="line">        value=context,</span><br><span class="line">        return_attention_scores=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Cache the attention scores for plotting later.</span></span><br><span class="line">    self.last_attn_scores = attn_scores</span><br><span class="line"></span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>然后我们再定义global attention，global
attention就是没有任何特殊操作的（比如上面的attention计算它有特别的context），而在transformor中更多的是self-attention，也就是我们传递给MultiHeadAttention的query,key,value都是同一个值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GlobalSelfAttention</span>(<span class="params">BaseAttention</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    attn_output = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        value=x,</span><br><span class="line">        key=x)</span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>最后我们定义causal self attention
layer，这个是在decoder的每一个block的第一个sublayer：self-attention
layer.其实这个layer是和global attention
layer差不多的，但还是有一点微小的差别。为什么呢？因为我们在decoder阶段，我们是一个词语一个词语的预测的，这其实包含了一层因果关系，我们在预测一个词语的时候，我们应该已知它前面一个词语是什么，RNN中的hidden
state传递到下一个时间步就是这个因果关系的传递。那么如果我们使用刚刚我们实现的global
attention layer来实现这个self
attention，并没有包含这个因果关系，不仅如此，如果我们使用常规的self
attention的计算，将target
sequence全部当作输入输入到decoder中的第一个block中，会有未来的数据提前被当前时刻看到的风险，所以在Transformer这篇文章中，作者提出使用mask的技术来避免这个问题。</p>
<p>在tensorflow中实现很简单，就只需要给MultiHeadAttention传递一个use_causal_mask
= True的参数即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CausalSelfAttention</span>(<span class="params">BaseAttention</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    attn_output = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        value=x,</span><br><span class="line">        key=x,</span><br><span class="line">        use_causal_mask = <span class="literal">True</span>) <span class="comment"># The causal mask ensures that each location only has access to the locations that come before it</span></span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这样就可以保证先前的sequence并不依赖于之后的elements。这里我本来有一个疑问是，这样一来这个causal
layer并不能实现bi-rnn的能力？但后来一想并不是，因为双向的RNN的后向是指后面的词语先输入，其实就是从后往前输入，这样就可以知道一个sequence当前词语依赖于后面的词语的权重。</p>
<h2 id="补充介绍">补充介绍</h2>
<h3 id="tf.keras.layers.multiheadattention">tf.keras.layers.MultiHeadAttention</h3>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention">doc</a></p>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230309150541287.png" alt="image-20230309150541287">
<figcaption aria-hidden="true">image-20230309150541287</figcaption>
</figure>
<figure>
<img src="/2023/03/13/machine-translation%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20230309150600806.png" alt="image-20230309150600806">
<figcaption aria-hidden="true">image-20230309150600806</figcaption>
</figure>
<p>注意，return的结果包含两个，其中attention_output的shape的第二维是和target
sequence的长度是一致的，并且E是和query的最后一维是一致的。</p>
<h1 id="attention-family">Attention Family</h1>
<p>这个章节整理于<a href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/">blog</a>，这个作者之前写了一篇介绍attention的文章，后面在2023年一月的时候又更新了两篇博客，详细介绍了从2020年以来出现的新的Transformer
models。权当自己学习记录一些我还需要补充的知识。</p>
<blockquote>
<p>The <strong>Transformer</strong> (which will be referred to as
“vanilla Transformer” to distinguish it from other enhanced versions; <a href="https://arxiv.org/abs/1706.03762">Vaswani, et al., 2017</a>) model
has an encoder-decoder architecture, as commonly used in many <a href="https://lilianweng.github.io/posts/2018-06-24-attention/#born-for-translation">NMT</a>
models. Later simplified Transformer was shown to achieve great
performance in language modeling tasks, like in encoder-only <a href="https://lilianweng.github.io/posts/2019-01-31-lm/#bert">BERT</a>
or decoder-only <a href="https://lilianweng.github.io/posts/2019-01-31-lm/#openai-gpt">GPT</a>.</p>
</blockquote>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>one paper a week计划reading notes</title>
    <url>/2023/03/06/one-paper-a-week%E8%AE%A1%E5%88%92reading-notes/</url>
    <content><![CDATA[<p>该博客为了记录自己一周一篇论文的计划而做的阅读笔记。任何事情开始时总是踌躇满志，有一群志同道合的小伙伴。我希望我是坚持到最后的那一个。</p>
<h1 id="week1-a-survey-of-the-usages-of-deep-learning-for-natural-language-processing---2021">week1:
A Survey of the Usages of Deep Learning for Natural Language Processing
- 2021</h1>
<p>第二章节介绍 nlp和ai的基本概念；第三章节介绍deep
learning如何解决nlp问题；第四章节介绍了applications，具体有information
retrival，information extraction，text classification，text
generation，summarization，question answering，machine
translation。第五章节是conclusion。</p>
<h2 id="chapter3.-nlp-中core-areas">chapter3. Nlp 中core areas</h2>
<p>一共四个core area： language
modeling，morphology，parsing，semantics。</p>
<blockquote>
<p>Morphology is <strong>the study of the internal structure of
words</strong> and forms a core part of linguistic study today. The term
morphology is Greek and is a makeup of morph- meaning 'shape, form', and
-ology which means 'the study of something'</p>
</blockquote>
<ul>
<li>language modeling is the process of creating a model to predict
workds or simply linguistic omponenets given previous words or
components.</li>
</ul>
<p>关于language modeling的总结见 <a href="https://www.sciencedirect.com/science/article/pii/S088523081400093X">A
survey on the application of recurrent neural networks to statistical
language modeling</a>,关于 评估一个lanuage model的metric总结见 <a href="https://arxiv.org/pdf/1312.3005.pdf">One Billion Word Benchmark
for Measuring Progress in Statistical Language Modeling</a>.
另外作者还介绍了两个benchmark数据集分别叫 Penn Treebank(PTB)和Billion
Word Benchmark.</p>
<ul>
<li>Morphology is concerned with finding segments within single words,
including roots and stems, prefixes, suffixes, and—in some
languages—infixes. Affixes (prefixes, suffixes, or infixes) are used to
overtly modify stems for gender, number, person, et cetera.</li>
<li>Parsing examines how different words and phrases relate to each
other within a sentence.</li>
<li>Semantic processing involves understanding the meaning of words,
phrases, sentences, or documents at some level</li>
</ul>
<h2 id="chapter4.深度学习在nlp中的applications">chapter4.深度学习在nlp中的applications</h2>
<p>这篇文章一共介绍了7个nlp的应用，包括information retrieval,
information extraction,text classification, text generation,
summarization, question ansering, machine
translation。基本上都是我们熟知的现在nlp领域比较火的几个小方向。</p>
<h3 id="信息检索-information-retrieval">信息检索 information
retrieval</h3>
<p>信息检索这个任务最主要的一个问题时ranking documents with respect to
the query string.</p>
<p>作者将信息检索的solution分为两种，第一种是representation-focused
approaches，对query和document分别建立一个representation，然后直接匹配两种representation。第二种是interaction-docused
approaches,首先build local interactions，然后用深度学习的技术去learn how
the two pieces of text match。</p>
<p>SOA：Cedr: Contextualized embeddings for document ranking
截至2019年。</p>
<h3 id="信息抽取-information-extraction">信息抽取 information
extraction</h3>
<ol type="1">
<li>name entity recognition</li>
</ol>
<p>paper： Contextual string embeddings for sequence labeling
取得了SOA的结果。</p>
<ol start="2" type="1">
<li>event extraction</li>
<li>relationship extraction</li>
</ol>
<blockquote>
<p>These may be possessive, antonymous or synonymous relationships, or
more natural, familial or geographic, relationships</p>
</blockquote>
<h3 id="文本分类-text-classification">文本分类 text classification</h3>
<p>值得注意的是深度学习的深度网络在这一个长文本上的表现其实并不是特别必要</p>
<blockquote>
<p>Worsham and Kalita [198] found that for the task of classifying long
full-length books by genre, gradient boosting trees are superior to
neural networks, including both CNNs and LSTMs</p>
</blockquote>
<h3 id="文本生成-text-generation">文本生成 text generation</h3>
<ol type="1">
<li>poetry generation 文本生成领域最难的子问题</li>
</ol>
<p>到2019年底，GPT-2是表现最好的生成器</p>
<ol start="2" type="1">
<li>joke and pun generation</li>
<li>story generation</li>
<li>text generation with GANs</li>
<li>text generation with VAEs</li>
</ol>
<p>这个子任务面临的挑战：1. lack of creativity and coherence 2. lack of
appropriate metrics to measure the results(always using human
evaluations)</p>
<p>值得关注：image captioning</p>
<h3 id="文本总结-summarization">文本总结 summarization</h3>
<p>包含两种类型的总结： extractive和abastractive</p>
<p>SOA: Pretraining-based natural language generation for text
summarization 2019</p>
<h3 id="问答question-answering">问答question answering</h3>
<p><a href="https://arxiv.org/abs/1810.04805">Bert</a>在这个任务上取得了SOA</p>
<h3 id="机器翻译-machine-translation">机器翻译 Machine Translation</h3>
<p>两篇值得关注的综述</p>
<h1 id="week2-attention-is-all-you-need">week2: Attention is all you
need</h1>
<p>这篇文章早在几个月前看过，也做过记录，这边就不多做记录了。</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>阅读论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>machine learning专项课程notes</title>
    <url>/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/</url>
    <content><![CDATA[<p>本文章整理吴恩达老师在coursera上的machine
learning专项课程的知识点，另外补充记录课堂上没有cover到的一些知识，参考西瓜书以及deep
learning花书。</p>
<h1 id="class-1-supervised-regression-and-classification">class 1
supervised regression and classification</h1>
<h2 id="picking-the-appropriate-learning-rate">picking the appropriate
learning rate:</h2>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230223103443797-16771196850861.png" alt="pick value">
<figcaption aria-hidden="true">pick value</figcaption>
</figure>
<h2 id="logistic-function的loss-function">logistic function的loss
function：</h2>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230223112138369-16771225003742.png" alt="image-20230223112138369">
<figcaption aria-hidden="true">image-20230223112138369</figcaption>
</figure>
<h2 id="bias和variance">bias和variance：</h2>
<p>参考<a href="https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229">博客</a></p>
<p><strong>What is bias?</strong></p>
<p>Bias is the difference between the average prediction of our model
and the correct value which we are trying to predict. Model with high
bias pays very little attention to the training data and oversimplifies
the model. It always leads to high error on training and test data.</p>
<p><strong>What is variance?</strong></p>
<p>Variance is the variability of model prediction for a given data
point or a value which tells us spread of our data. Model with high
variance pays a lot of attention to training data and does not
generalize on the data which it hasn’t seen before. As a result, such
models perform very well on training data but has high error rates on
test data.</p>
<p>总结来说就说，一个模型的bias如果很大的话，说明该模型是underfit，并没有很好的拟合训练数据，也就是training
error比较大；而variance比较大的话，说明该模型把更多的关注力放在训练数据上，但是在它没见过的数据集上表现很差，也就是train_error小，而dev_error大，泛化能力差。</p>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230223115103788-16771242656113.png" alt="bias and variance">
<figcaption aria-hidden="true">bias and variance</figcaption>
</figure>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230223134302857.png" alt="image-20230223134302857">
<figcaption aria-hidden="true">image-20230223134302857</figcaption>
</figure>
<h1 id="class-2-advanced-learning-algorithms">class 2 advanced learning
algorithms</h1>
<h2 id="optimizer">optimizer</h2>
<h3 id="gradient-descent-with-momentum">gradient descent with
momentum</h3>
<p>动量梯度下降法，运行速度要快于标准的梯度下降。基本的想法就是计算梯度的指数加权平均数，并利用该梯度来更新。<a href="https://towardsdatascience.com/gradient-descent-with-momentum-59420f626c8f">博客</a>中阐述这个问题的方式是：</p>
<p>标准的GD只是考虑当下的learning
rate和当下的gradient，并没有考虑前面batch的gradient。这就带来几个问题：</p>
<ol type="1">
<li>在固定learning
rate下，J在一些点上变化缓慢，基本上是处于一个平原区，其实这时候我们希望它的步子要迈的大一点</li>
<li>在一些J变化比较多的地方，我们其实希望步子要小一点</li>
</ol>
<p>那么Momemtum解决了什么问题呢？它可以在J下降不动的时候，让gradient比原来的标准值要大一点，也就是我们间接的把步子迈大了。具体需要用指数加权平均数来实现，因为指数加权平均数很好的考虑了要给予多大的权重给过去的gradient，当然给予最大权重是当前的gradient。</p>
<p><img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/gd%20with%20momentum.png"></p>
<p>以上这个公式就是gradient加了动量momentum的公式，具体的v(t-1)是上一步的weight，而delta(t)是用当前batch计算的梯度。beta是常数，通常取值为0.9</p>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230227134323054-16774766052811.png" alt="image-20230227134323054">
<figcaption aria-hidden="true">image-20230227134323054</figcaption>
</figure>
<h3 id="rmsprop">RMSprop</h3>
<p>rmsprop也可以加速训练过程，而且会降低训练过程中J下降的抖动：</p>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230227141934848-16774787770602.png" alt="image-20230227141934848">
<figcaption aria-hidden="true">image-20230227141934848</figcaption>
</figure>
<h3 id="adam">Adam</h3>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230227143504471-16774797058863.png" alt="image-20230227143504471">
<figcaption aria-hidden="true">image-20230227143504471</figcaption>
</figure>
<p>可以发现Adam其实是结合了RMSprop和momentum，其中beta1缺省值是0.9，beta2的缺省值是0.999.</p>
<p>更深入一点我们还有AdamW，是为了解决adam优化器训练出来的模型比SDG训练出来的模型泛化能力差，这个泛化能力差主要是由于L2正则化也就是weight
decay并没有在Adam中很好的体现，详细可以参考AdamW的<a href="https://arxiv.org/abs/1711.05101">paper：Fixing Weight Decay
Regularization in Adam</a>和<a href="https://towardsdatascience.com/why-adamw-matters-736223f31b5d">Why
AdamW matters</a></p>
<h2 id="metric">metric</h2>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230227155504810-16774845099534.png" alt="precision and recall">
<figcaption aria-hidden="true">precision and recall</figcaption>
</figure>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230227155742925.png" alt="image-20230227155742925">
<figcaption aria-hidden="true">image-20230227155742925</figcaption>
</figure>
<p>如果我们根据sigmoid之后的概率值来进行分类，那么如果我们设定一个很大的概率值，比如说图上的0.7，也就表示说我们只在我们非常自信的时候才会预测一个人得了病，那么必然#total
predicted
positive会变小，所以precision会是一个较大的值；而如果我们采用一个比较小的概率值，比如0.3，也就表示说我们把更多的sample预测成了阳性，那么就代表，我们并不想错过任何一个病历。</p>
<p>我们拥有另一个metric叫做F1
score来衡量一个模型在precision和recall上的共同特征。</p>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230227160513093-16774851219125.png" alt="F1 score">
<figcaption aria-hidden="true">F1 score</figcaption>
</figure>
<h2 id="decision-tree">decision tree</h2>
<p>information
entropy即信息熵是度量样本集合纯度purity最常用的一种指标，决策树就是基于将每一次分类的集合往纯度越高的方向去提升，从而达到分类数据集的目的。</p>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230228101129354-16775503015086.png" alt="信息熵的数学计算">
<figcaption aria-hidden="true">信息熵的数学计算</figcaption>
</figure>
<p>信息熵越大，D的纯度越小，说明数据很混乱。</p>
<p>我们可以用信息增益来进行决策树的划分属性选择，每一次都选择使得划分的信息增益最大。</p>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230228102027466-16775508287787.png" alt="image-20230228102027466">
<figcaption aria-hidden="true">image-20230228102027466</figcaption>
</figure>
<h3 id="decision-tree-ensembles">decision tree ensembles</h3>
<p>解决问题： trees are highly sensitive to small changes of the
data</p>
<p>解决办法： combines several decision trees to produce better
predictive performance than utilizing a single decision tree</p>
<p>有两种技术可以用来实现ensemble decision trees: bagging和boosting</p>
<ol type="1">
<li>bagging(bootstrap aggregation)</li>
</ol>
<blockquote>
<p><em><strong>Bagging*</strong> (Bootstrap Aggregation) is used when
our goal is to reduce the variance of a decision tree. </em>Here idea is
to create several subsets of data from training sample chosen randomly*
*with replacement**. Now, each collection of subset data is used to
train their decision trees. As a result, we end up with an ensemble of
different models. Average of all the predictions from different trees
are used which is more robust than a single decision tree.*</p>
</blockquote>
<p><em>Random Forest</em> is an extension over bagging. It takes one
extra step where in addition to taking the random subset of data, it
also takes the random selection of features rather than using all
features to grow trees. When you have many random trees. It’s called
Random Forest</p>
<ol start="2" type="1">
<li>Boosting</li>
</ol>
<blockquote>
<p><strong>Boosting</strong> <em>is another ensemble technique to create
a collection of predictors. In this technique, learners are learned
sequentially with early learners fitting simple models to the data and
then analyzing data for errors. In other words, we fit consecutive trees
(random sample) and at every step, the goal is to solve for net error
from the prior tree.</em></p>
</blockquote>
<p>参考资料： <a href="https://towardsdatascience.com/decision-tree-ensembles-bagging-and-boosting-266a8ba60fd9">博客</a></p>
<h2 id="conclusion">conclusion</h2>
<p>when to use decision trees?</p>
<figure>
<img src="/2023/02/23/machine-learning%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8Bnotes/image-20230228131322629.png" alt="decision trees vs NN">
<figcaption aria-hidden="true">decision trees vs NN</figcaption>
</figure>
<h1 id="class-3-unsupervised-learning-recommenders-reinforcement-learning">class
3 unsupervised learning, recommenders, reinforcement learning</h1>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>TF2中的custom layer&amp;model&amp;training</title>
    <url>/2023/02/13/TF2%E4%B8%AD%E7%9A%84custom-layer-model-training/</url>
    <content><![CDATA[<p>在上Coursera上关于<a href="https://www.coursera.org/specializations/tensorflow-advanced-techniques">Tensorflow的高级用法课程</a>时，老师简略介绍了custom
layer和custom
model的用法，但后来看到其实课程覆盖的内容比较简单，除了介绍了__init__和call两个可override的function外没有介绍其他的。偶然看到一篇博客详细介绍了在tensorflow中如何使用sub
classing来搭建模型，写的非常好，这里贴上<a href="https://towardsdatascience.com/model-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e">链接</a></p>
<p>我们知道在tensorflow中有三种搭建模型的方式： 1) sequential API
也就是想创建一个Sequential实例，然后通过add的方式把一个layer加到模型中去，如：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># declare input shape </span></span><br><span class="line">seq_model = tf.keras.Sequential()</span><br><span class="line">seq_model.add(tf.keras.Input(shape=imput_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block 1</span></span><br><span class="line">seq_model.add(tf.keras.layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, strides=<span class="number">2</span>, activation=<span class="string">&quot;relu&quot;</span>))</span><br><span class="line">seq_model.add(tf.keras.layers.MaxPooling2D(<span class="number">3</span>))</span><br><span class="line">seq_model.add(tf.keras.layers.BatchNormalization())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block 2</span></span><br><span class="line">seq_model.add(tf.keras.layers.Conv2D(<span class="number">64</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>))</span><br><span class="line">seq_model.add(tf.keras.layers.BatchNormalization())</span><br><span class="line">seq_model.add(tf.keras.layers.Dropout(<span class="number">0.3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now that we apply global max pooling.</span></span><br><span class="line">seq_model.add(tf.keras.layers.GlobalMaxPooling2D())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Finally, we add a classification layer.</span></span><br><span class="line">seq_model.add(tf.keras.layers.Dense(output_dim))</span><br></pre></td></tr></table></figure>
sequential的方式在researcher中用的不多，随着模型变得越来越复杂，可以看到tensorflow的application模块实现的官方模型代码中，已经见不到这种形式了。
2) Functional API 正如其名，就是用函数调用的方式来搭建模型：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># declare input shape </span></span><br><span class="line"><span class="built_in">input</span> = tf.keras.Input(shape=(imput_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block 1</span></span><br><span class="line">x = tf.keras.layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, strides=<span class="number">2</span>, activation=<span class="string">&quot;relu&quot;</span>)(<span class="built_in">input</span>)</span><br><span class="line">x = tf.keras.layers.MaxPooling2D(<span class="number">3</span>)(x)</span><br><span class="line">x = tf.keras.layers.BatchNormalization()(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block 2</span></span><br><span class="line">x = tf.keras.layers.Conv2D(<span class="number">64</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">x = tf.keras.layers.BatchNormalization()(x)</span><br><span class="line">x = tf.keras.layers.Dropout(<span class="number">0.3</span>)(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now that we apply global max pooling.</span></span><br><span class="line">gap = tf.keras.layers.GlobalMaxPooling2D()(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Finally, we add a classification layer.</span></span><br><span class="line">output = tf.keras.layers.Dense(output_dim)(gap)</span><br><span class="line"></span><br><span class="line"><span class="comment"># bind all</span></span><br><span class="line">func_model = tf.keras.Model(<span class="built_in">input</span>, output)</span><br></pre></td></tr></table></figure>
注意：这种方式最终要使用<code>tf.keras.Model()</code>来将inputs和outputs接起来。</p>
<ol start="3" type="1">
<li>Model sub-classing API 第三种方式是现在用的最多的方式。
之前我没理解layer和model两种调用方式的区别，我觉得就是一系列运算，我们把输入输进来，return
output结果的一个过程。但如果一个类它是Layer的子类，它比model的子类多了一个功能，它有state属性，也就是我们熟悉的weights。比如Dense
layer，我们知道它做了线性运算+激活函数，其中的weights就是我们assign给每一个feature的权重，但其实我们并不只是想要这一类别的运算，比如下面的：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleQuadratic</span>(<span class="params">Layer</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, units=<span class="number">32</span>, activation=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;Initializes the class and sets up the internal variables&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        <span class="built_in">super</span>(SimpleQuadratic, self).__init__()</span><br><span class="line">        self.units = units</span><br><span class="line">        self.activation = tf.keras.activations.get(activation)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, input_shape</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;Create the state of the layer (weights)&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># a and b should be initialized with random normal, c (or the bias) with zeros.</span></span><br><span class="line">        <span class="comment"># remember to set these as trainable.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        a_init = tf.random_normal_initializer()</span><br><span class="line">        b_init = tf.random_normal_initializer()</span><br><span class="line">        c_init = tf.zeros_initializer()</span><br><span class="line">        </span><br><span class="line">        self.a = tf.Variable(name = <span class="string">&quot;kernel&quot;</span>, initial_value = a_init(shape= (input_shape[-<span class="number">1</span>], self.units), </span><br><span class="line">                                                                    dtype= <span class="string">&quot;float32&quot;</span>), trainable = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        self.b = tf.Variable(name = <span class="string">&quot;kernel&quot;</span>, initial_value = b_init(shape= (input_shape[-<span class="number">1</span>], self.units), </span><br><span class="line">                                                                    dtype= <span class="string">&quot;float32&quot;</span>), trainable = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        self.c = tf.Variable(name = <span class="string">&quot;bias&quot;</span>, initial_value = c_init(shape= (self.units,), </span><br><span class="line">                                                                    dtype= <span class="string">&quot;float32&quot;</span>), trainable = <span class="literal">True</span>)</span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span> </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;Defines the computation from inputs to outputs&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        result = tf.matmul(tf.math.square(inputs), self.a) + tf.matmul(inputs, self.b) + self.c</span><br><span class="line">        <span class="keyword">return</span> self.activation(result)</span><br></pre></td></tr></table></figure>
上面的代码将inputs平方之后和a做乘积，之后再加上inputs和b的乘积，最终返回的是和。这样的运算是tf.keras.layer中没有的。这个时候我们自己customize
layer就很方便。还有一个很方便的地方在于很多模型其实是按模块来的，模块内部的layer很类似。这个时候我们就可以把这些模型内的layer包起来变成一个layer的子类（Module），再定义完这些module之后我们使用Model把这些module再包起来，这就是我们最终的model。这时候我们就可以看到Model和Layer子类的区别了，虽然两者都可以实现输入进来之后实现一系列运算返回运算结果，但后者可以实现更灵活的运算，而前者往往是在把每一个模块定义好之后最终定义我们训练模型的类。
&gt; In general, we use the Layer class to define the inner computation
blocks and will use the Model class to define the outer model,
practically the object that we will train. ---粘贴自博客</li>
</ol>
<p>我们以sub-classing的方式定义的model是没有办法调用summary来看模型架构的，作者也给出了解决方案：<a href="https://github.com/tensorflow/tensorflow/issues/31647#issuecomment-692586409">github
comments</a></p>
<p>方法就是在Model的子类中添加build_graph方法： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_graph</span>(<span class="params">self, raw_shape</span>):</span></span><br><span class="line">        x = tf.keras.layers.Input(shape=raw_shape)</span><br><span class="line">        <span class="keyword">return</span> Model(inputs=[x], outputs=self.call(x))</span><br></pre></td></tr></table></figure>
这样我们就可以正常调用summary() <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cm.build_graph(raw_input).summary()</span><br><span class="line"><span class="comment"># 不仅如此还能使用tf.keras.utils.plot_model来生成png</span></span><br><span class="line">tf.keras.utils.plot_model(</span><br><span class="line">    model.build_graph(raw_input),                      <span class="comment"># here is the trick (for now)</span></span><br><span class="line">    to_file=<span class="string">&#x27;model.png&#x27;</span>, dpi=<span class="number">96</span>,              <span class="comment"># saving  </span></span><br><span class="line">    show_shapes=<span class="literal">True</span>, show_layer_names=<span class="literal">True</span>,  <span class="comment"># show shapes and layer name</span></span><br><span class="line">    expand_nested=<span class="literal">False</span>                       <span class="comment"># will show nested block</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>作者同样推荐了一篇博客讲tensorflow中保存模型的各种方式：<a href="https://colab.research.google.com/drive/172D4jishSgE3N7AO6U2OKAA_0wNnrMOq#scrollTo=vnTvqAgfspGJ">博客地址</a>.非常推荐阅读</p>
<p>总结一下就是：</p>
<ol type="1">
<li>对于Functional
API创建的模型，最好的保存模型和导入模型的方式是：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(<span class="string">&#x27;path_to_my_model.h5&#x27;</span>)</span><br><span class="line"><span class="keyword">del</span> model</span><br><span class="line">model = keras.models.load_model(<span class="string">&#x27;path_to_my_model.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>以上方式会将模型的架构，weights以及训练过程中的设定（也就是model.compile()）的内容全部保存。</p>
<ol start="2" type="1">
<li>对于sub class创建的模型，推荐的方式是用save_weights</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save_weights(<span class="string">&#x27;path_to_my_weights&#x27;</span>, save_format=<span class="string">&#x27;tf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>如果想要加载weights，必须要知道原来用sub
class建立模型的code。不仅如此，还需要用原来的code先build起模型，让模型知道输入tensor的shape以及dtype，如果没有build这一步程序将会报错。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_model = MiniInception()</span><br><span class="line">new_model.build((<span class="literal">None</span>, x_train.shape[<span class="number">1</span>:])) <span class="comment"># or .build((x_train.shape))</span></span><br><span class="line">new_model.load_weights(<span class="string">&#x27;net.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="tf.function">tf.function</h1>
<p>在我们定义custum training
过程中时我们经常会用到这个装饰器@tf.function</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span>(<span class="params">step, x, y</span>):</span></span><br><span class="line">   <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">   input: x, y &lt;- typically batches </span></span><br><span class="line"><span class="string">   input: step &lt;- batch step</span></span><br><span class="line"><span class="string">   return: loss value</span></span><br><span class="line"><span class="string">   &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># start the scope of gradient </span></span><br><span class="line">   <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">      logits = model(x, training=<span class="literal">True</span>) <span class="comment"># forward pass</span></span><br><span class="line">      train_loss_value = loss_fn(y, logits) <span class="comment"># compute loss </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute gradient </span></span><br><span class="line">   grads = tape.gradient(train_loss_value, model.trainable_weights)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update weights</span></span><br><span class="line">   optimizer.apply_gradients(<span class="built_in">zip</span>(grads, model.trainable_weights))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update metrics</span></span><br><span class="line">   train_acc_metric.update_state(y, logits)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># write training loss and accuracy to the tensorboard</span></span><br><span class="line">   <span class="keyword">with</span> train_writer.as_default():</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;loss&#x27;</span>, train_loss_value, step=step)</span><br><span class="line">        tf.summary.scalar(</span><br><span class="line">            <span class="string">&#x27;accuracy&#x27;</span>, train_acc_metric.result(), step=step</span><br><span class="line">        ) </span><br><span class="line">   <span class="keyword">return</span> train_loss_value</span><br></pre></td></tr></table></figure>
<p>先看如果一个函数不加这个装饰器会如何：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Traced with&quot;</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    f(<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">f(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Traced with 2</span><br><span class="line">Traced with 2</span><br><span class="line">Traced with 2</span><br><span class="line">Traced with 2</span><br><span class="line">Traced with 2</span><br><span class="line">Traced with 3</span><br></pre></td></tr></table></figure>
<p>加上装饰器：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Traced with&quot;</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    f(<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">f(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Traced <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Traced <span class="keyword">with</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p>可以看到第二种加了装饰器的方式，即便是循环了5遍，我们仍然只有一行打印了2.</p>
<p>如果我们在上面的代码中print之前加上一行：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Traced with&quot;</span>, x)</span><br><span class="line">    <span class="comment"># add tf.print</span></span><br><span class="line">    tf.<span class="built_in">print</span>(<span class="string">&quot;Executed with&quot;</span>, x)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    f(<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">f(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>程序的输出就变成了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Traced <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">2</span></span><br><span class="line">Traced <span class="keyword">with</span> <span class="number">3</span></span><br><span class="line">Executed <span class="keyword">with</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p>可以看到<code>tf.print</code>就可以正常按<code>loop</code>运行。注意一点:
被<code>tf.function</code>装饰的函数只能包含<code>operations</code>而不能定义<code>variable</code>比如<code>tf.Variable()</code></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>image classification models总结</title>
    <url>/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>本博客旨在记录自己在了解<code>image classification</code>这个术语<code>computer vision</code>的一个子任务中常见的模型。耳熟能详的就是<code>ResNet</code>,<code>VGG</code>,<code>Inception</code>,<code>MobileNet</code>,
<code>Efficientnet</code>。每一个模型之间有什么区别，他们自身又有哪些变种，比如<code>VGG</code>，它拥有<code>VGG16,VGG19</code>等，<code>ResNet</code>又有很多，单是查看<code>Tensorflow</code>的官方文档就会发现在<code>tf.keras.applications</code>模块下，就有很多模型架构可选（也都有预训练参数）。整理这个博客的目的在于让自己对这些模型之间的差别有所了解，这样在不同的任务中才会知道使用什么样的模型架构来handle自己的数据。</p>
<p>在整理这篇博客的过程中，我也去搜了有没有<code>image classification</code>这个单任务上的<code>review</code>文章，文章都挺多的，筛选之后推荐这篇<a href="https://www.mdpi.com/2072-4292/13/22/4712">Review of Image
Classification Algorithms Based on Convolutional Neural
Networks</a>.这篇文章主要是介绍基于CNN的一些模型，共有三个章节。重点是第二章节梳理了<code>CNN-based</code>的一些模型，包括本文想要<code>cover</code>的<code>VGG</code>，<code>inception</code>，<code>resnet</code>，<code>mobilenet</code>。重点关注图像分类算法的小伙伴可以通读一下这篇文章。</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/image-20230207094857739-16757345394748.png" alt="Review of Image Classification Algorithms Based on Convolutional Neural Networks第二章节目录">
<figcaption aria-hidden="true">Review of Image Classification Algorithms
Based on Convolutional Neural Networks第二章节目录</figcaption>
</figure>
<h1 id="vgg">VGG</h1>
<p>首次提出在2014年的<a href="https://arxiv.org/abs/1409.1556">paper</a></p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/vgg16.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>上图中包含13个卷积层和3个全连接层，是<code>VGG16</code>的结构。而<code>VGG19</code>包含了16个卷积层和3个全连接层：</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/1dNYBNBDP7ZvckfOSYzHxIw.jpeg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><code>VGG</code>系列就是<code>VGG16</code>和<code>VGG19</code>，两者的区别在于19用了更多的卷积层。<code>Tensorflow</code>也提供了这两个模型的黑盒子实现供大家使用。</p>
<h1 id="resnet">ResNet</h1>
<p>首次提出在2016年的<a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">paper</a>，其中最重要的就是网络中的<code>residual block</code>:</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/1_nmPcwwnsHE-AC69ASkj9w.jpeg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>在作者的原文中我们可以发现，文章中提出的Resnet是34层，也就是ResNet34。在具体实现的时候，作者在每一个卷积操作之后（激活函数之前）加上了batch
Normalization。在<code>Module: tf.keras.applications.resnet</code><a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet">Tensorflow
applications
resnet</a>中现在只有ResNet101，152，50三个版本，其中ResNet50和ResNet34的区别在于：前者使用三个卷积一个block，后者是2个卷积一个block.
ResNet50表现更优异。ResNet101和ResNet152在一个block内使用了更多的卷积layer。</p>
<p>以上所说的都是resnet v1，后来同一个作者又发表了<a href="https://arxiv.org/pdf/1603.05027.pdf">Identity Mappings in Deep
Residual Networks</a>,提出了ResNet
v2。同样我们在tensorflow中也可以看到模块<code>Module: tf.keras.applications.resnet_v2</code>，同样的也有50，101，152三个版本的model。</p>
<p>v1和v2的区别在于：</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/image-20230206150618805-16756671800395.png" alt="ResNet v1 and ResNet v2">
<figcaption aria-hidden="true">ResNet v1 and ResNet v2</figcaption>
</figure>
<p>以上只是概念上的解释，看代码会更合适一点，其中<code>Deep Residual Learning for Image Recognition</code>文章中也给出了34，50，101，152等几个模型在实现中注意的细节：</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/image-20230206153053177-16756686549546.png" alt="image-20230206153053177">
<figcaption aria-hidden="true">image-20230206153053177</figcaption>
</figure>
<p>ResNet34 V1中，一个resnet
block是由两个卷积layer组成的，同时它和V2的一个区别就在于X进来后就先进行卷积运算，也就是上图中的weight</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras.activations <span class="keyword">import</span> relu</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers <span class="keyword">as</span> Layers</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResBlock</span>(<span class="params">Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, channels, stride=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ResBlock, self).__init__(name=<span class="string">&#x27;ResBlock&#x27;</span>)</span><br><span class="line">        self.flag = (stride != <span class="number">1</span>)</span><br><span class="line">        self.conv1 = Conv2D(channels, <span class="number">3</span>, stride, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.bn1 = BatchNormalization()</span><br><span class="line">        self.conv2 = Conv2D(channels, <span class="number">3</span>, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.bn2 = BatchNormalization()</span><br><span class="line">        self.relu = ReLU()</span><br><span class="line">        <span class="keyword">if</span> self.flag:</span><br><span class="line">            self.bn3 = BatchNormalization()</span><br><span class="line">            self.conv3 = Conv2D(channels, <span class="number">1</span>, stride)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x1 = self.conv1(x)</span><br><span class="line">        x1 = self.bn1(x1)</span><br><span class="line">        x1 = self.relu(x1)</span><br><span class="line">        x1 = self.conv2(x1)</span><br><span class="line">        x1 = self.bn2(x1)</span><br><span class="line">        <span class="keyword">if</span> self.flag:</span><br><span class="line">            x = self.conv3(x)</span><br><span class="line">            x = self.bn3(x)</span><br><span class="line">        x1 = Layers.add([x, x1])</span><br><span class="line">        x1 = self.relu(x1)</span><br><span class="line">        <span class="keyword">return</span> x1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet34</span>(<span class="params">Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ResNet34, self).__init__(name=<span class="string">&#x27;ResNet34&#x27;</span>)</span><br><span class="line">        self.conv1 = Conv2D(<span class="number">64</span>, <span class="number">7</span>, <span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.bn = BatchNormalization()</span><br><span class="line">        self.relu = ReLU()</span><br><span class="line">        self.mp1 = MaxPooling2D(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.conv2_1 = ResBlock(<span class="number">64</span>)</span><br><span class="line">        self.conv2_2 = ResBlock(<span class="number">64</span>)</span><br><span class="line">        self.conv2_3 = ResBlock(<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">        self.conv3_1 = ResBlock(<span class="number">128</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv3_2 = ResBlock(<span class="number">128</span>)</span><br><span class="line">        self.conv3_3 = ResBlock(<span class="number">128</span>)</span><br><span class="line">        self.conv3_4 = ResBlock(<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">        self.conv4_1 = ResBlock(<span class="number">256</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv4_2 = ResBlock(<span class="number">256</span>)</span><br><span class="line">        self.conv4_3 = ResBlock(<span class="number">256</span>)</span><br><span class="line">        self.conv4_4 = ResBlock(<span class="number">256</span>)</span><br><span class="line">        self.conv4_5 = ResBlock(<span class="number">256</span>)</span><br><span class="line">        self.conv4_6 = ResBlock(<span class="number">256</span>)</span><br><span class="line"></span><br><span class="line">        self.conv5_1 = ResBlock(<span class="number">512</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv5_2 = ResBlock(<span class="number">512</span>)</span><br><span class="line">        self.conv5_3 = ResBlock(<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">        self.pool = GlobalAveragePooling2D()</span><br><span class="line">        self.fc1 = Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.dp1 = Dropout(<span class="number">0.5</span>)</span><br><span class="line">        self.fc2 = Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.dp2 = Dropout(<span class="number">0.5</span>)</span><br><span class="line">        self.fc3 = Dense(<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.mp1(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2_1(x)</span><br><span class="line">        x = self.conv2_2(x)</span><br><span class="line">        x = self.conv2_3(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv3_1(x)</span><br><span class="line">        x = self.conv3_2(x)</span><br><span class="line">        x = self.conv3_3(x)</span><br><span class="line">        x = self.conv3_4(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv4_1(x)</span><br><span class="line">        x = self.conv4_2(x)</span><br><span class="line">        x = self.conv4_3(x)</span><br><span class="line">        x = self.conv4_4(x)</span><br><span class="line">        x = self.conv4_5(x)</span><br><span class="line">        x = self.conv4_6(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv5_1(x)</span><br><span class="line">        x = self.conv5_2(x)</span><br><span class="line">        x = self.conv5_3(x)</span><br><span class="line"></span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.dp1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.dp2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = ResNet34()</span><br><span class="line">model.build(input_shape=(<span class="number">1</span>, <span class="number">480</span>, <span class="number">480</span>, <span class="number">3</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<h1 id="inception">Inception</h1>
<p>已经有不少博客在科普Inception系列模型的区别<a href="https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202">A
Simple Guide to the Versions of the Inception Network</a></p>
<p>首先提出该模型的是2014年的<a href="https://arxiv.org/pdf/1409.4842v1.pdf">Going deeper with
convolutions</a> Inception
V1（GoogleNet）,然后又分别有了好几个变体：<code>Inception V2，Inception V3，Inception V4</code>，<a href="http://arxiv.org/abs/1602.07261">Inception-ResNet-v2</a>。和<code>ResNet</code>一样，<code>Inception</code>网络中一个重要的<code>module</code>是<code>Inception Module</code>：</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/image-20230206161239990-16756711610527.png" alt="Inception Module">
<figcaption aria-hidden="true">Inception Module</figcaption>
</figure>
<p>其中这些Network中被广泛使用的是<a href="https://arxiv.org/pdf/1512.00567v3.pdf">Inception_v3</a>和<a href="https://arxiv.org/pdf/1602.07261.pdf">Inception-ResNet-v2</a>.</p>
<h1 id="mobilenet">MobileNet</h1>
<blockquote>
<p>The idea behind MobileNet is to use depthwise separable convolutions
to build loghter deep neural networks. In regular convolutional layer,
the convolution kernel or filter is applied to all of the channels of
the input image, by doing weighted sum of the input pixels with the
filter and then slides to the next input pixels across the
images.MobileNet uses this regular convolution only in the first layer.
The next layers are the depthwise separable convolutions which are the
combination of the depthwise and pointwise convolution. The depthwise
convolution does the convolution on each channel separately. If the
image has three channels, therefore, the output image also has three
channels. This depthwise convolution is used to filter the input
channels. The next step is the pointwise convolution, which is similar
to the regular convolution but with a 1x1 filter. The purpose of
pointwise convolution is to merge the output channels of the depthwise
convolution to create new features. By doing so, the computational work
needed to be done is less than the regular convolutional networks.</p>
<p>引用自<a href="https://medium.com/@fransiska26/the-differences-between-inception-resnet-and-mobilenet-e97736a709b0">the-differences-between-inception-resnet-and-mobilenet</a></p>
</blockquote>
<p><code>MobileNet</code>使用了两种卷积形式，<code>depthwise</code>和<code>pointwise</code>，后者就是我们常见的卷积操作，只是使用的是1✖1的卷积核，input
image有多少个<code>channel</code>，<code>filter</code>就会延展为几个<code>channel</code>，比如输入进来的<code>channel</code>数是3，那么一个<code>3*3</code>大小的filter就会extend成3✖3✖3的一个立方体，然后这27个数分别禹输入image对应的区域做乘积之后相加取和。但是<code>depthwise</code>卷积是对每一个<code>channel</code>分别做卷积，如果输入图片有三个<code>channel</code>，那么输出的也会是三个<code>channel</code>。如图：</p>
<figure>
<img src="/2023/02/06/image-classification-models%E6%80%BB%E7%BB%93/image-20230207132331415.png" alt="depthwise convolution">
<figcaption aria-hidden="true">depthwise convolution</figcaption>
</figure>
<p><code>tensorflow</code>中有<code>DepthwiseConv2D</code>这个<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D">layer</a>,它对于<code>depthwise convolution</code>的解释是：</p>
<blockquote>
<p>Depthwise convolution is a type of convolution in which each input
channel is convolved with a different kernel (called a depthwise
kernel). You can understand depthwise convolution as the first step in a
depthwise separable convolution.</p>
</blockquote>
<p><code>MobileNetV2</code>主要引进了<code>Inverted residuals</code>和<code>linear bottlenecks</code>去解决在<code>depthwise</code>卷积操作中卷积核的参数往往是0的问题</p>
<h1 id="other-topics">other topics</h1>
<p>在阅读<a href="https://www.mdpi.com/2072-4292/13/22/4712">Review of
Image Classification Algorithms Based on Convolutional Neural
Networks</a>的最后一章节时，作者不仅介绍了现在research和industry领域用的比较多的image
classification的模型，也给出了各个模型在image-net数据集上的accuracy。在总结章，作者还提出了一些结论性的发现，我觉得蛮收益的，将文章的观点整理在这里。</p>
<ol type="1">
<li>2012年到2017年主要提供了日后用于分类的basic
CNN模型架构，这期间的模型架构有2012的alexnet，2014年的vgg，2014年的inception，2015年的resnet，2017年提出了attention加cnn的架构</li>
<li>attention加入到cnn之后形成了新的模型，也因此提高了模型的performance。现在很多模型会将SE
block嵌入到模型架构中去，我查了下这个SE
block是SEnet中的一个block，squeeze and excitation block。<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf">SEnet</a>是在2017年提出的，这个知识点待补充</li>
<li>超参数的选择对于CNN网络的performance影响很大，很多的工作在着力于减少超参数的个数以及replace
them with other composite coefficients。</li>
<li>手动设计一个performance很好的网络往往需要很多effort，<a href="https://en.wikipedia.org/wiki/Neural_architecture_search">NAS
search</a> （neural architecture search）可以让这个过程变得更简单</li>
<li>想要提升模型的performance，不仅仅需要将关注力放在模型架构的设计上，data
augmentation,transfer learning,training
strategies也可以帮助我们提高模型的准确度。在transfer learning上，paper：
Large Scale Learning of General Visual Representations for Transfer
总结了一些在不同的task上如何利用transfer
learning取得很好的performance的办法。</li>
</ol>
<hr>
<p>CNN model 还面临的挑战：</p>
<ol type="1">
<li>lightweight
models比如mobileNet系列的轻量级模型往往需要牺牲accuracy来提高efficiency。未来在embedded系统上，CNN的运行效率值得去explore</li>
<li>cnn模型在semi-supervised和unsupervised上的发挥不如NLP领域。</li>
</ol>
<hr>
<p>future directions:</p>
<ol type="1">
<li>重视vision transformer.
如何将卷积和transformer有效结合起来是当前的一个热点。目前的SOTA
network是 <a href="https://arxiv.org/abs/2106.04803">CoAtNet</a>，在image
net数据集上的accuracy是90.88，确实是目前在image
net数据集上performance最高的模型架构。值得读一下，mark！</li>
<li>有一些关于CNN的传统技术可能会成为阻碍CNN发展的重要因素，诸如：activation
function的选择，dropout，batch normalization。</li>
</ol>
<h1 id="senet-2017">SENet 2017</h1>
<p>原文 <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf">SEnet</a>，是由自动驾驶公司Momenta在2017年公布的一种全新的图像识别结构，它通过对特征通道间的相关性进行建模，把重要的特征进行强化来提升准确率。这个结构是2017
ILSVR竞赛的冠军，top5的错误率达到了2.251%，比2016年的第一名还要低25%，可谓提升巨大。</p>
<h1 id="coatnet">CoAtNet</h1>
<h1 id="reference">Reference</h1>
<ol type="1">
<li><a href="https://towardsdatascience.com/architecture-comparison-of-alexnet-vggnet-resnet-inception-densenet-beb8b116866d">Architecture
comparison of AlexNet, VGGNet, ResNet, Inception, DenseNet</a></li>
<li><a href="https://medium.com/analytics-vidhya/vggnet-architecture-explained-e5c7318aa5b6">VGGNet
Architecture Explained</a></li>
<li><a href="https://viso.ai/deep-learning/resnet-residual-neural-network/">resnet-residual-neural-network
Resnet系列网络架构的区别</a></li>
</ol>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow中lstm,GRU</title>
    <url>/2023/01/04/tensorflow%E4%B8%ADlstm-GRU/</url>
    <content><![CDATA[<h1 id="gru">GRU</h1>
<figure>
<img src="/2023/01/04/tensorflow%E4%B8%ADlstm-GRU/LSTM3-var-GRU.png" alt="GRU">
<figcaption aria-hidden="true">GRU</figcaption>
</figure>
<p>其实单看输出，GRU的输出是和简单的RNN一样的，都只有一个hidden_state。所以在tensorflow中它的输出其实和RNN
layer一样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">inputs = tf.random.normal([<span class="number">32</span>, <span class="number">10</span>, <span class="number">8</span>])</span><br><span class="line">gru = tf.keras.layers.GRU(<span class="number">4</span>)</span><br><span class="line">output = gru(inputs)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>其中有两个可以传递给GRU的参数，一个是return_state，一个是return_sequence。两个值都是bool类型。如果单独传递return_sequence=True，那么输出将只有一个值，也就是每一个时间步的序列：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gru = tf.keras.layers.GRU(<span class="number">4</span>, return_sequences=<span class="literal">True</span>)</span><br><span class="line">output = gru(inputs)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">10</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>如果单独传递return_state=True，那么输出将会是两个值，可以仔细看官方文档中的说明是<code>Boolean. Whether to return the last state in addition to the output. Default:</code>False.`也就是output和最后的hidden_state会一起输出，并且output会等于final_state：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gru = tf.keras.layers.GRU(<span class="number">4</span>, return_state=<span class="literal">True</span>)</span><br><span class="line">output, final_state = gru(inputs)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"><span class="built_in">print</span>(final_state.shape) <span class="comment"># output=final_state</span></span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>如果单独传递return_sequences=True，LSTM将只返回整个序列！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lstm = tf.keras.layers.LSTM(<span class="number">4</span>,return_sequences=<span class="literal">True</span>)</span><br><span class="line">inputs = tf.random.normal([<span class="number">32</span>, <span class="number">10</span>, <span class="number">8</span>])</span><br><span class="line">whole_seq_output = lstm(inputs)</span><br><span class="line"><span class="built_in">print</span>(whole_seq_output.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">10</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>那如果两个值都设置成<code>True</code>呢？这将返回两个输出，第一个输出是整个序列，第二个输出是最终的state。注意这里并没有<code>output</code>了，因为<code>output</code>其实是<code>sequence</code>中最后一个序列<code>sequence[:,-1,:]</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gru = tf.keras.layers.GRU(<span class="number">4</span>, return_sequences=<span class="literal">True</span>, return_state=<span class="literal">True</span>)</span><br><span class="line">whole_sequence_output, final_state = gru(inputs)</span><br><span class="line"><span class="built_in">print</span>(whole_sequence_output.shape)</span><br><span class="line"><span class="built_in">print</span>(final_state.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">10</span>, <span class="number">4</span>)</span><br><span class="line">(<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h1 id="lstm">LSTM</h1>
<p>轮到LSTM，因为架构上跟GRU有点区别，所以在返回结果上就多了一个carry_state.</p>
<figure>
<img src="/2023/01/04/tensorflow%E4%B8%ADlstm-GRU/LSTM3-var-peepholes.png" alt="LSTM">
<figcaption aria-hidden="true">LSTM</figcaption>
</figure>
<p>想要了解LSTM的具体计算，参考<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">博客</a></p>
<p>在tensorflow中一样有return_state和return_sequences：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs = tf.random.normal([<span class="number">32</span>, <span class="number">10</span>, <span class="number">8</span>])</span><br><span class="line">lstm = tf.keras.layers.LSTM(<span class="number">4</span>)</span><br><span class="line">output = lstm(inputs)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>如果单独传递return_state，这里和GRU不一样的地方在于lstm有两个state，一个是memory_state一个是carry_state</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lstm = tf.keras.layers.LSTM(<span class="number">4</span>,return_state=<span class="literal">True</span>)</span><br><span class="line">output, final_memory_state, final_carry_state = lstm(inputs)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"><span class="built_in">print</span>(final_memory_state.shape) <span class="comment"># final_memory_state=output</span></span><br><span class="line"><span class="built_in">print</span>(final_carry_state.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>如果同时设置True</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lstm = tf.keras.layers.LSTM(<span class="number">4</span>,return_sequences=<span class="literal">True</span>,return_state=<span class="literal">True</span>)</span><br><span class="line">whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)</span><br><span class="line"><span class="built_in">print</span>(whole_seq_output.shape)</span><br><span class="line"><span class="built_in">print</span>(final_memory_state.shape) <span class="comment"># final_memory_state=whole_seq_output[:,-1,:]</span></span><br><span class="line"><span class="built_in">print</span>(final_carry_state.shape)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">10</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; (<span class="number">32</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h1 id="gru-vs-lstm">GRU vs LSTM</h1>
<p>至于我们在训练模型的时候选择哪一个cell作为RNN的cell，cs224n课程给出的答案是：</p>
<blockquote>
<p>Researchers have proposed many gated RNN variants, but LSTM and GRU
are the most widely-used.</p>
<p>Rule of thumb: LSTM is a good default choice (especially if your data
has particularly long dependencies, or you have lots of training data);
Switch to GRUs for speed and fewer parameters.</p>
</blockquote>
<p>LSTM doesn’t guarantee that there is no vanishing/exploding gradient,
but it does provide an easier way for the model to learn long-distance
dependencies.</p>
<p>在2023年的今天，lstm也不再是研究者青睐的对象，最火的模型变成了Transformer：</p>
<figure>
<img src="/2023/01/04/tensorflow%E4%B8%ADlstm-GRU/image-20230313095438649.png" alt="image-20230313095438649">
<figcaption aria-hidden="true">image-20230313095438649</figcaption>
</figure>
<p>这里也贴出2022年的最新WMT的<a href="https://www.statmt.org/wmt22/pdf/2022.wmt-1.1.pdf">结果</a></p>
]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>R-CNN vs SPP vs Fast R-CNN vs Faster R-CNN</title>
    <url>/2022/11/02/R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/</url>
    <content><![CDATA[<p>最近在object
detection任务上读这几篇文章，见识到神仙打架。一开始我只是关注image
segmentation的任务，其中instance
segmentation任务中Mask-RCNN是其中比较火的一个model，所以就把跟这个模型相关的几个模型都找出来看了看。这里想记录下这几天看这几篇论文的心得体会，如果有写的不正确的地方，欢迎批评指正。</p>
<p>其实去仔细看这几篇论文很有意思，梳理一下时间线就是：</p>
<ol type="1">
<li><p><strong>2014</strong>年Girshick提出了RCNN，用于解决accurate
object detection 和 semantic
segmentation。该模型有一个drawbacks是每次一张图片输入进来，需要产生~2000个region
proposals，这些region的大小都是不一致的，但我们对图片进行分类的下游网络都是需要fixed
size的图片，那怎么办呢？作者提出使用wraped方法，具体可以参考作者的论文。总之最终我们输入到SVM也就是分类器的region图片大小都是一致的。</p></li>
<li><p>为了解决每次输入网络的图片大小怎么样才能变成fixed
size的vector，<strong>2015</strong>年he kaiming提出了SPP（spatial
pyramid pooling），跟前者RCNN不一样的地方在于：1) 将region
proposal的方法用在了图片输入cnn网络得到的feature map上，2)从feature
map选择出来的region
proposal不还是不一样大小么？作者没有使用wrap的方法，而是提出了一个SPP
layer,这个layer可以接受任何大小的图片，最终都会转化成一个fixed
size的向量，这样就可以轻松输入进SVM或者Dense layer进行分类了。</p></li>
<li><p>收到SPP的启发，Girshick在<strong>2015</strong>年提出了Fast-RCNN，将SPP
layer重新替换成ROI Pooling，经过ROI pooling，输出的并不是SPP
layer输出的金字塔式的向量了，而是只有一个。 <a href="https://analyticsindiamag.com/r-cnn-vs-fast-r-cnn-vs-faster-r-cnn-a-comparative-guide/">参考博客</a></p></li>
<li><p>经过前一轮的battle，虽然各自的模型都提出了自己的独特方法，但是无论是SPP
Net还是Fast-RCNN都没有提出在选择ROI(region of
interest)的方法。<strong>2016</strong>年He
Kaiming再次强势入场，提出了产生region
proposal的方法，它使用了一个单独的CNN网络来获取region
proposal.得到了这些proposal之后再将他们传递给Roi Pooling
layer，后面的过程和fast RCNN一致。 这篇Faster-RCNN的方法作者中有he
kaiming和Girshick，这里致敬下sun jian，感谢为computer
vision领域贡献的灵感和创造。</p></li>
</ol>
<h1 id="mask-rcnn">Mask-RCNN</h1>
<p>Mask-RCNN是Region-based CNN系列中的一个算法，用于解决instance
segmentation的问题，instance segmentation的难点在于我们不仅要做object
detection，而且需要将object的准确轮廓给识别出来，同时做出分类这是什么object。在<a href="https://arxiv.org/abs/1703.06870">Mask-RCNN</a></p>
<p>中，related
work一章节对RCNN这一系列的模型做了准确概括，建议大家读原文：</p>
<blockquote>
<p>The Region-based CNN (R-CNN) approach [13] to bounding-box object
detection is to attend to a manageable number of candidate object
regions [42, 20] and evaluate convolutional networks [25, 24]
independently on each RoI. R-CNN was extended [18, 12] to allow
attending to RoIs on feature maps using RoIPool, leading to fast speed
and better accuracy. Faster R-CNN [36] advanced this stream by learning
the attention mechanism with a Region Proposal Network (RPN). Faster
R-CNN is flexible and robust to many follow-up improvements (e.g., [38,
27, 21]), and is the current leading framework in several
benchmarks.</p>
</blockquote>
<p>在Mask-RCNN的文章中提出了一种新的ROIAlign
Layer，主要是为了解决Faster-Rcnn的网络中ROI pooling
layer的问题。在此补充下ROI pooling是怎么将不同size的ROI（region of
interest）都变成fixed-size的feature map的：</p>
<figure>
<img src="/2022/11/02/R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/image-20221128165901678-16696259446601.png" alt="ROI Pooling Layer">
<figcaption aria-hidden="true">ROI Pooling Layer</figcaption>
</figure>
<p>上图是将5*4大小的ROI变成了2✖2大小的feature
map。这种方式带来的影响就是可能在提取的extracted
features和ROI之间造成misalignments。但是这种misalignments并不会在faster-rcnn中对分类造成很大的影响，但如果要用这个ROI做segmentation的话就可能会造成巨大的影响，因此作者提出了一个ROIAlign
Layer。</p>
<blockquote>
<p>如果有小伙伴想对照code看这篇paper，可以参考<a href="https://github.com/matterport/Mask_RCNN/blob/3deaec5d902d16e1daf56b62d5971d428dc920bc/mrcnn/model.py#L486">tensorflow实现</a>。如果你对pytorch更熟悉可以参考paper中给出的官方github地址的实现。有一篇博客详细介绍了tensorlfow的实现，参见<a href="https://blog.paperspace.com/mask-r-cnn-in-tensorflow-2-0/">blog</a></p>
</blockquote>
<p>在MaskRCNN中使用了faster-rcnn中提出的RPN来产生ROI，然后才使用上面提到的ROI
Algin。RPN的具体细节参见fatser-rcnn原文和本博客的<a href="#RPN(Region%20Proposal%20Network)">第二章节</a></p>
<h1 id="rpnregion-proposal-network">RPN(Region Proposal Network)</h1>
<p>RPN
是在faster-rcnn中提出来的网络，主要是为了解决在rcnn和fast-rcnn两个前置模型中产生ROI的耗时问题，之前产生ROI主要是依靠Selected
Search。在读mask-rcnn的paper时发现这个网络的细节不甚了解，这里补充记录一下。感兴趣的朋友可以阅读<a href="http://arxiv.org/abs/1506.01497">paper</a>的第3.2节。</p>
<p>RPN在output长方体的ROI的同时，也会给每一个ROI产生一个Objectness
score。看原文的时候作者是以sliding
window的方式来讲解的，一开始看的有点懵。但其实就是卷积层的计算过程的拆解，我们先按照作者的思路来看RPN做了什么。</p>
<figure>
<img src="/2022/11/02/R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/image-20221129160354668-16697090363711.png" alt="faster-Rcnn">
<figcaption aria-hidden="true">faster-Rcnn</figcaption>
</figure>
<p>RPN的输入是经过一系列卷积层之后的feature
map，在这个map上，我们在上面再做一些运算：</p>
<figure>
<img src="/2022/11/02/R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/image-20221129160504387-16697091055822.png" alt="RPN network">
<figcaption aria-hidden="true">RPN network</figcaption>
</figure>
<p>针对一个window（n✖n），RPN做的就是将这个window映射到一个低维度的feature上，图上是256维的向量，然后我们再接两个dense
layer，一个用于预测box，一个用于做分类。k的意思是在每一个sliding
window上我们都维护了k个anchor，这个概念和yolo里一致。所以在每一个sliding-window上我们都可以得出来4k个box和2k个分类结果（是否有object的概率），这个anchor
box是和sliding-window的中心点绑定的，所以如果RPN的输入是一个W×H的feature
map，那么我们就会有W×H×k个anchors。</p>
<p>那么我们知道RPN是怎么计算的了，然后在训练阶段还有一些tricks。作者对每一个anchor都赋予了一个class
label，赋予positive的anchor为 1) anchor和 groud truth的box有最高的IOU 2)
如果anchor与groud
truth的box的IOU大于0.7。这两种anchor都会被赋予positive的标签，也就是代表它里面有object。对于哪些和任何groud
truth
box的IOU都小于0.3的anchor，赋予negtive的标签。在为RPN产生训练数据时，对于所有的anchors都有一个class
label，也就是它里面是否包含object。对于box的训练数据的处理有一点不一样的地方。可以参考<a href="https://github.com/matterport/Mask_RCNN/blob/3deaec5d902d16e1daf56b62d5971d428dc920bc/mrcnn/model.py#L486">tensorflow实现</a>，在<code>mrcnn/model.py</code>的<code>build_rpn_target</code>函数中：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># RPN Match: 1 = positive anchor, -1 = negative anchor, 0 = neutral</span></span><br><span class="line">rpn_match = np.zeros([anchors.shape[<span class="number">0</span>]], dtype=np.int32)</span><br><span class="line"><span class="comment"># RPN bounding boxes: [max anchors per image, (dy, dx, log(dh), log(dw))]</span></span><br><span class="line">rpn_bbox = np.zeros((config.RPN_TRAIN_ANCHORS_PER_IMAGE, <span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<p>rpn_bbox的数量是提前设定好的，也就是不是对每一个anchor都会有一个box来对应，对于那些标记为positive的anchor
box才会有bbox的target。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Generate RPN trainig targets</span></span><br><span class="line"><span class="comment"># target_rpn_match is 1 for positive anchors, -1 for negative anchors</span></span><br><span class="line"><span class="comment"># and 0 for neutral anchors.</span></span><br><span class="line">target_rpn_match, target_rpn_bbox = modellib.build_rpn_targets(</span><br><span class="line">    image.shape, model.anchors, gt_class_id, gt_bbox, model.config)</span><br><span class="line">log(<span class="string">&quot;target_rpn_match&quot;</span>, target_rpn_match)</span><br><span class="line">log(<span class="string">&quot;target_rpn_bbox&quot;</span>, target_rpn_bbox)</span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">target_rpn_match         shape: (65472,)              min:   -1.00000  max:    1.00000</span><br><span class="line">target_rpn_bbox          shape: (256, 4)              min:   -5.19860  max:    2.59641</span><br></pre></td></tr></table></figure>
<p>从上面可以看出，在全局变量设置中设置的是每一张图片最多有256个anchors，所以产生的rpn_bbox
shape就是(256,4),而用于RPN训练的class
label是全部的anchors的分类label。进一步的我们可以查看其中一张图片的anchors：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">positive_anchor_ix = np.where(target_rpn_match[:] == <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">negative_anchor_ix = np.where(target_rpn_match[:] == -<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">neutral_anchor_ix = np.where(target_rpn_match[:] == <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">positive_anchors = model.anchors[positive_anchor_ix]</span><br><span class="line">negative_anchors = model.anchors[negative_anchor_ix]</span><br><span class="line">neutral_anchors = model.anchors[neutral_anchor_ix]</span><br><span class="line">log(<span class="string">&quot;positive_anchors&quot;</span>, positive_anchors)</span><br><span class="line">log(<span class="string">&quot;negative_anchors&quot;</span>, negative_anchors)</span><br><span class="line">log(<span class="string">&quot;neutral anchors&quot;</span>, neutral_anchors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply refinement deltas to positive anchors</span></span><br><span class="line">refined_anchors = utils.apply_box_deltas(</span><br><span class="line">    positive_anchors,</span><br><span class="line">    target_rpn_bbox[:positive_anchors.shape[<span class="number">0</span>]] * model.config.RPN_BBOX_STD_DEV)</span><br><span class="line">log(<span class="string">&quot;refined_anchors&quot;</span>, refined_anchors, )</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">positive_anchors         shape: (14, 4)               min:    5.49033  max:  973.25483</span><br><span class="line">negative_anchors         shape: (242, 4)              min:  -22.62742  max: 1038.62742</span><br><span class="line">neutral anchors          shape: (65216, 4)            min: -362.03867  max: 1258.03867</span><br><span class="line">refined_anchors          shape: (14, 4)               min:    0.00000  max: 1023.99994</span><br></pre></td></tr></table></figure>
<p>即便是有65472个anchors，但其实正负anchor所占比重很小，大多数是neutral的anchor。其中对于positive的anchor，我们拥有在他们的bbox上refine过的box。</p>
<figure>
<img src="/2022/11/02/R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/positive_refined_box.png" alt="positive anchor">
<figcaption aria-hidden="true">positive anchor</figcaption>
</figure>
<p>上图中虚线画出来的是positive
anchor，实线框出来的是在这些anchor上refine过的box。</p>
<p>在训练阶段，计算loss时，对于regression loss，模型只会计算postive
anchors的regression
loss，也就是只计算那些被打上positive标签的anchor预测的box与groud-truth
box的回归loss。如果一个anchor，它的class
label在anatation阶段就是negtive的，模型并不会将它预测出来的box和ground-truth
box进行比较，他们的loss不会被计算进总的Loss。</p>
<p>关于如何训练的问题，在Faster-Rcnn的文章中给出了三种训练的算法，第一种也是文中采用的方法就是：1.
alternating training :
先把RPN单独训练，然后使用RPN产生的proposals去训练fast r-cnn.
然后将fine-tune过的RPN作为初始参数，然后再去产生proposal，再去训练fast
rcnn. 具体来说就是4步：</p>
<ol type="1">
<li>单独train RPN ： 用ImageNet pre-trained 参数做初始化，然后在region
proposal这个task上fine tune</li>
<li>利用第一步产生的proposal训练Fast-Rcnn，也就是<a href="R-CNN-vs-SPP-vs-Fast-R-CNN-vs-Faster-R-CNN/image-20221129160354668-16697090363711.png">架构图</a>中的最上面一部分，该网络也会使用ImageNet
pre-trained
参数做初始化。注意一直到这一步，两个网络都没有share任何卷积layers</li>
<li>第三步我们使用detector的network去初始化RPN，同时fix住最下面的卷积层，也就是两个网络共享的那些卷积层。这一步骤单独fine-tune
RPN的layers。</li>
<li>最后一步，fine-tune Fast-RCNN的unique的layers。</li>
</ol>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention and Transformer model</title>
    <url>/2022/10/27/Attention-and-Transformer-model/</url>
    <content><![CDATA[<p>斯坦福cs231n最新的课程中包含了attention的模型讲解，但是很可惜我们现在只能看到17年的老课程，在youtube上可以找到，课程主页是<a href="http://cs231n.stanford.edu/schedule.html">cs231n</a>。可以在课程主页中下载对应的slides和查看推荐的blog，都是学习attention机制的好教材。另外我在学习cs231n课程过程中，也参考了吴恩达对于sequence
model的讲解，它课程中也涉及到了attention机制，课后作业也包含了简单的attention机制的实现，可以作为辅助理解来看。这篇博客权当自己学习attention以及由此创造的attention系列模型比如transformer的记录。cs231n推荐的<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">博客</a>内容也是很通俗易懂，英文不好的同学有中文翻译可以参考。</p>
<h1 id="general-attention-model">general attention model</h1>
<p>RNN有多种类型的网络：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221027100649410.png" alt="image-20221027100649410">
<figcaption aria-hidden="true">image-20221027100649410</figcaption>
</figure>
<p>对于"many to
many"类型的网络，有可能输入的长度不等于输出的长度，在机器翻译的任务中很常见。这种网络也叫Sequence
to
Sequence，首先该网络会经由encoder对输入进行编码，然后再有decoder进行sequence的生成。但是这种网络在长句子中表现很差，如果输入句子的长度很长，encoder网络就很难记忆住所有信息，从而在decoder中翻译出准确的词语。由此，需要用到attention
model。从计算角度来说就是encoder每次都会产生一个固定长度的vector，这对于长句子来说fixed
length的向量很难记住很早之前的信息：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221027104026997.png" alt="image-20221027104026997">
<figcaption aria-hidden="true">image-20221027104026997</figcaption>
</figure>
<p>那为了解决一个fixed
length的vector很难记忆前序信息的缺陷，所以诞生了attention
机制！具体的就是在decoder阶段的每一个时间步利用都产生不同的context，这个context产生的过程就是attention计算的过程。主要思想是在产生y之前做一个attention的权重计算，这个权重指的是在计算某个时间步的y值时，我们应该对输入句子的每一个词给予多少关注，给予的关注多，权重就大。所以这里我们会基于initial
decoder state（previous hidden state of the (post-attention)
LSTM）和encoder网络的输出值计算权重，计算过程采用dense
layer.这些权重值的和是1。</p>
<p>对于很长输入的句子，encoder不再是输出一个固定的context。如下：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221027103845891.png" alt="image-20221027103845891">
<figcaption aria-hidden="true">image-20221027103845891</figcaption>
</figure>
<p>context的详细计算如下：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221027101307861.png" alt="吴恩达课程attention">
<figcaption aria-hidden="true">吴恩达课程attention</figcaption>
</figure>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221027101324129.png" alt="吴恩达课程attention">
<figcaption aria-hidden="true">吴恩达课程attention</figcaption>
</figure>
<p>上面两张图是吴恩达在深度学习专项课程中的讲解细节，在cs231n的图中可以看出来它将dense
layer的输出单独列出来了，也就是下图中alignment
scores（<code>e</code>），在e的基础上再计算attention
weights（<code>a</code>），所有的attention weights总和为1 <img src="/2022/10/27/Attention-and-Transformer-model/image-20221206141252321.png" alt="cs231n_attention"></p>
<p>但是总体来说两个的讲解方式是一致的，cs231n这门课为什么把其中的<code>e</code>单独拎出来也是有它的用意，主要为了后面讲解self-attention。这里我琢磨了好一会儿才明白。还有一点值得提一下，吴恩达的图里那个与hidden
state 一同输入进dense
layer的repeateVector不要搞混淆，其实这里每次在decoder的一个时间步计算context时用到的s都不一样，比如在上面的里，计算<code>c1</code>用的是encoder的最后一个hidden
state，而计算<code>C2</code>的时候我们需要用decoder的第一个时间步的hidden
state来计算：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221206142430931-16703078727111.png" alt="decoder第二个时间步">
<figcaption aria-hidden="true">decoder第二个时间步</figcaption>
</figure>
<p>这种attention计算机制如果用到image
caption上面如何做呢？获取图片的特征我们使用CNN来抓取，得到的feature
map用来当作rnn中的hidden state计算：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221206145117196-16703094790462.png" alt="image caption">
<figcaption aria-hidden="true">image caption</figcaption>
</figure>
<p>理解以上的原理很重要，再把上面这个图改写一下，将h变成decoder的query：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221206145317028-16703095989033.png" alt="query">
<figcaption aria-hidden="true">query</figcaption>
</figure>
<p>以上都是简单的attention机制，随后的transformer模型真正的将attention机制推广开来，见transformer的<a href="https://arxiv.org/abs/1706.03762">paper</a></p>
<h1 id="attention-is-all-you-need-2017">Attention is all you need
2017</h1>
<p>在transformer的<a href="https://arxiv.org/abs/1706.03762">paper</a>中，作者首先介绍本文：主流的sequence
tranduction模型主要基于复杂的RNN或者CNN模型，它们包含encoder和decoder两部分，其中表现最好的模型在encoder和decoder之间增加了attention
mechanism。本文提出了一个新的简单的网络结构名叫<em>transformer</em>，也是完全基于attention机制，"dispensing
with recurrence and convolutions entirely"!
根本无需循环和卷积！了不起的Network~</p>
<p>在阅读这篇文章之前需要提前了解我在另外一篇博客 Attention and
transformer
model中的知识，在translation领域我们的科学家们是如何从RNN循环神经网络过渡到CNN，然后最终是transformer的天下的状态。技术经过了一轮轮的迭代，每一种基础模型架构提出后，会不断的有文章提出新的改进，文章千千万，不可能全部读完，就精读一些经典文章就好，Vaswani这篇文章是NMT领域必读paper，文章不长，加上参考文献才12页，介绍部分非常简单，导致这篇文章的入门门槛很高（个人感觉）。我一开始先读的这篇文章，发现啃不下去，又去找了很多资料来看，其中对我非常帮助的有很多：</p>
<ul>
<li><a href="http://jalammar.github.io/illustrated-transformer/">非常通俗易懂的blog</a>
有中文版本的翻译</li>
<li><a href="http://arxiv.org/abs/1912.02047">Neural Machine
Translation: A Review and Survey</a>
虽然这篇paper很长，90+页。前六章可以作为参照，不多25页左右，写的非常好</li>
<li><a href="http://cs231n.stanford.edu/slides/2022/lecture_11_ruohan.pdf">stanford
cs231n课程的ppt</a>
斯坦福这个课程真的很棒，youtube上可以找到17年的视频，17年的课程中没有attention的内容，所以就姑且看看ppt吧，希望斯坦福有朝一日能将最新的课程分享出来，也算是做贡献了</li>
<li>cs231n推荐的阅读<a href="https://lilianweng.github.io/posts/2018-06-24-attention/">博客</a>
非常全面的整理，强烈建议食用.
这位作者也附上了自己的transformer实现，在它参考的那些github实现里，哈佛大学的<a href="http://nlp.seas.harvard.edu/2018/04/01/attention.html">pytorch实现</a>也值得借鉴。</li>
</ul>
<p>Transformer这篇文章有几个主要的创新点：</p>
<ol type="1">
<li>使用self-attention机制，并首次提出使用multi-head attention</li>
</ol>
<p>该机制作用是在编码当前word的时候，这个self-attention就会告诉我们编码这个词语我们应该放多少注意力在这个句子中其他的词语身上，说白了其实就是计算当前词语和其他词语的关系。这也是CNN用于解决NMT问题时用不同width的kernel来扫input
metric的原因。</p>
<p>multi-head的意思是我使用多个不同的self-attention
layer来处理我们的输入，直观感觉是训练的参数更多了，模型的表现力自然要好一点。</p>
<ol start="2" type="1">
<li>Positional embeddings</li>
</ol>
<p>前一个创新点解决了dependence的问题，那如何解决位置的问题呢？也就是我这个词在编码的时候或者解码的时候应该放置在句子的哪个位置上。文章就用pisitional
embedding来解决这个问题。这个positional embedding和input
embedding拥有相同的shape，所以两者可以直接相加。transformer这篇文章提供了两种encoding方式：</p>
<p>1） sunusoidal positional encoding</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230302133247664.png" alt="image-20230302133247664">
<figcaption aria-hidden="true">image-20230302133247664</figcaption>
</figure>
<p>其中，pos=1,...,L(L是input句子的长度)，i是某一个PE中的一个维度，取值范围是1到dmodel。python实现为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span>(<span class="params">length, depth</span>):</span></span><br><span class="line">  depth = depth/<span class="number">2</span></span><br><span class="line"></span><br><span class="line">  positions = np.arange(length)[:, np.newaxis]     <span class="comment"># (seq, 1)</span></span><br><span class="line">  depths = np.arange(depth)[np.newaxis, :]/depth   <span class="comment"># (1, depth)</span></span><br><span class="line"></span><br><span class="line">  angle_rates = <span class="number">1</span> / (<span class="number">10000</span>**depths)         <span class="comment"># (1, depth)</span></span><br><span class="line">  angle_rads = positions * angle_rates      <span class="comment"># (pos, depth)</span></span><br><span class="line"></span><br><span class="line">  pos_encoding = np.concatenate(</span><br><span class="line">      [np.sin(angle_rads), np.cos(angle_rads)],</span><br><span class="line">      axis=-<span class="number">1</span>) </span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> tf.cast(pos_encoding, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">pos_encoding = positional_encoding(length=<span class="number">2048</span>, depth=<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the shape.</span></span><br><span class="line"><span class="built_in">print</span>(pos_encoding.shape) <span class="comment"># (2014,512)</span></span><br></pre></td></tr></table></figure>
<p>2） learned positional encoding</p>
<p>整体上看，这篇文章提出的transformer模型在做translation的任务时，架构是这样的：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230301133249379.png" alt="image-20230301133249379">
<figcaption aria-hidden="true">image-20230301133249379</figcaption>
</figure>
<p>其中encoders部分包含了6个encoders的block，decoders部分也包含了6个decoders的block，将encoders的每一个block拆开来看，有两个sub
layer：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230301133424302.png" alt="image-20230301133424302">
<figcaption aria-hidden="true">image-20230301133424302</figcaption>
</figure>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230301133440152.png" alt="image-20230301133440152">
<figcaption aria-hidden="true">image-20230301133440152</figcaption>
</figure>
<p>其中decoder部分的block比encoder部分的block多了一个sub
layer，其中self-attention和encoder-decoder attention都是multi-head
attention layer，只不过decoder部分的第一个multi-head attention
layer是一个masked multi-head
attention，为了防止未来的信息泄露给当下（prevent positions from
attending to the future）.</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230301133608484.png" alt="image-20230301133608484">
<figcaption aria-hidden="true">image-20230301133608484</figcaption>
</figure>
<p>在transformer模型中，作者还使用了residual
connection，所以在encoder的每一个block中，数据的flow是:</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230307162830473-16781777124571.png" alt="transformer架构">
<figcaption aria-hidden="true">transformer架构</figcaption>
</figure>
<p>其中self-attention中涉及的运算details是：</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230301134258180.png" alt="image-20230301134258180">
<figcaption aria-hidden="true">image-20230301134258180</figcaption>
</figure>
<p>可以发现其中涉及的运算都是矩阵的点乘，并没有RNN中那种时间步的概念，所以所有运算都是可以parallelizable，这就能使得模型的推理和训练更加的efficient。并且！Transformers也可以抓住distant的依赖，而不是像rnn那样对于长依赖并不是很擅长，因为它前面的信息如果像传递到很后面的单词推理上，需要经历很多时间步的计算，而transformer在推理每一个单词的时候都可以access到input句子中的每一个单词（毕竟我们的Z中包含了每一个单词跟其他单词的关系)。</p>
<p>其中positional encoding现在可以简单的理解成在我们编码的word
embedding上我们又加了一个positional
encoding，维度和我们的embedding一模一样。</p>
<blockquote>
<p>在tensorflow中有一个layer是<code>MultiHeadAttention</code>,如果我们想实现transformer里的这个self-attention，那就是query，key，value其实都是由input
vector计算来的。</p>
</blockquote>
<p>以上的理论计算看起来可能会有点模糊，可以同步参照<a href="https://lilianweng.github.io/posts/2018-06-24-attention/">博客</a>
参考 <a href="http://jalammar.github.io/illustrated-transformer/">illustrated
transformer</a>介绍的详细细节，基于tensorflow框架实现的<a href="https://github.com/lilianweng/transformer-tensorflow/blob/master/transformer.py">transformer</a>来帮助自己理解transformer模型。</p>
<h2 id="encoder部分">encoder部分</h2>
<p>encoder的每一个block由两个sub-layer组成，中间穿插resnet
connection。</p>
<p><img src="/2022/10/27/Attention-and-Transformer-model/transformer_encoder_block.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multihead_attention</span>(<span class="params">self, query, memory=<span class="literal">None</span>, mask=<span class="literal">None</span>, scope=<span class="string">&#x27;attn&#x27;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            query (tf.tensor): of shape (batch, q_size, d_model)</span></span><br><span class="line"><span class="string">            memory (tf.tensor): of shape (batch, m_size, d_model)</span></span><br><span class="line"><span class="string">            mask (tf.tensor): shape (batch, q_size, k_size)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:h</span></span><br><span class="line"><span class="string">            a tensor of shape (bs, q_size, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> memory <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment"># 如果memory是None，那么就是一个典型的self-attention layer</span></span><br><span class="line">            memory = query</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># Linear project to d_model dimension: [batch, q_size/k_size, d_model]</span></span><br><span class="line">            Q = tf.layers.dense(query, self.d_model, activation=tf.nn.relu)</span><br><span class="line">            K = tf.layers.dense(memory, self.d_model, activation=tf.nn.relu)</span><br><span class="line">            V = tf.layers.dense(memory, self.d_model, activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Split the matrix to multiple heads and then concatenate to have a larger</span></span><br><span class="line">            <span class="comment"># batch size: [h*batch, q_size/k_size, d_model/num_heads]</span></span><br><span class="line">            Q_split = tf.concat(tf.split(Q, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            K_split = tf.concat(tf.split(K, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            V_split = tf.concat(tf.split(V, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            mask_split = tf.tile(mask, [self.h, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Apply scaled dot product attention</span></span><br><span class="line">            out = self.scaled_dot_product_attention(Q_split, K_split, V_split, mask=mask_split)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Merge the multi-head back to the original shape</span></span><br><span class="line">            out = tf.concat(tf.split(out, self.h, axis=<span class="number">0</span>), axis=<span class="number">2</span>)  <span class="comment"># [bs, q_size, d_model]</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># The final linear layer and dropout.</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_model)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dropout(out, rate=self.drop_rate, training=self._is_training)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_forwad</span>(<span class="params">self, inp, scope=<span class="string">&#x27;ff&#x27;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Position-wise fully connected feed-forward network, applied to each position</span></span><br><span class="line"><span class="string">        separately and identically. It can be implemented as (linear + ReLU + linear) or</span></span><br><span class="line"><span class="string">        (conv1d + ReLU + conv1d).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            inp (tf.tensor): shape [batch, length, d_model]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        out = inp</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_ff, activation=tf.nn.relu)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dropout(out, rate=self.drop_rate, training=self._is_training)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_model, activation=None)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># by default, use_bias=True</span></span><br><span class="line">            out = tf.layers.conv1d(out, filters=self.d_ff, kernel_size=<span class="number">1</span>, activation=tf.nn.relu)</span><br><span class="line">            out = tf.layers.conv1d(out, filters=self.d_model, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out  </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encoder_layer</span>(<span class="params">self, inp, input_mask, scope</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            inp: tf.tensor of shape (batch, seq_len, embed_size)</span></span><br><span class="line"><span class="string">            input_mask: tf.tensor of shape (batch, seq_len, seq_len)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        out = inp</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># One multi-head attention + one feed-forword</span></span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(out, mask=input_mask))</span><br><span class="line">            out = self.layer_norm(out + self.feed_forwad(out))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h2 id="decoder部分">decoder部分</h2>
<p><img src="/2022/10/27/Attention-and-Transformer-model/encoder-decoder.png"></p>
<p>在decoder部分，我们可以看到每一个decoder
block的输入有两个：<em>整个encoder部分的输出</em>以及<em>上一个decoder
block的输出（第一个decoder
block是词向量的输入）</em>，而encoder部分的输出是接到每一个decoder
block的第二个sublayer的。正如刚刚提到了，decoder部分的每一个block跟encoder部分的block有一个不一样的地方，那就是多了一个sublayer：
encoder-decoder
attention。至于encoder部分和decoder部分是如何connect的，</p>
<blockquote>
<p>The encoder start by processing the input sequence. The output of the
top encoder is then transformed into a set of attention vectors K and V.
These are to be used by each decoder in its “encoder-decoder attention”
layer which helps the decoder focus on appropriate places in the input
sequence</p>
</blockquote>
<p>也就是我们得到了encoder部分top layer（最后一个encoder
layer）的输出之后，我们将输出转化成K和V.
我们可以看到在<code>multihead_attention</code>里，memory是<code>enc_out</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder_layer</span>(<span class="params">self, target, enc_out, input_mask, target_mask, scope</span>):</span></span><br><span class="line">        out = target</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(</span><br><span class="line">                out, mask=target_mask, scope=<span class="string">&#x27;self_attn&#x27;</span>))</span><br><span class="line">            out = self.layer_norm(out + self.multihead_attention(</span><br><span class="line">                out, memory=enc_out, mask=input_mask)) <span class="comment"># 将encoder部分的输出结果作为输入</span></span><br><span class="line">            out = self.layer_norm(out + self.feed_forwad(out))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder</span>(<span class="params">self, target, enc_out, input_mask, target_mask, scope=<span class="string">&#x27;decoder&#x27;</span></span>):</span></span><br><span class="line">        out = target</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_enc_layers):</span><br><span class="line">                out = self.decoder_layer(out, enc_out, input_mask, target_mask, <span class="string">f&#x27;dec_<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>以上实现的transformer其实我觉得还是有一点点复杂，毕竟在tensorflow2.0+版本中已经有了官方实现好的<code>layers.MultiHeadAttention</code>可以使用，应该可以大大简化我们实现步骤，特别是上面的<code>def multihead_attention(self, query, memory=None, mask=None, scope='attn'):</code>。从刚刚的实现里我们可以发现，除了decoder部分每一个block的第二个sublayer的attention计算有一点不一样之外，其他的attention计算都是一模一样的。我在github上找了不少用TF2.0实现的transformer（最标准的也是Attention
is all you
need的模型），发现很多都都写得一般般，最终发现还是tensorflow官方文档写的<a href="https://www.tensorflow.org/text/tutorials/transformer#add_and_normalize">tutotial</a>写的最好.</p>
<p>现在对照tensorflow的tutorial以及上面transformer的计算过程，拆解一下官方给的代码。</p>
<p>首先定义一个baseAttention类，然后在此基础上我们再定义encoder和decoder中的attention：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseAttention</span>(<span class="params">tf.keras.layers.Layer</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)</span><br><span class="line">    self.layernorm = tf.keras.layers.LayerNormalization()</span><br><span class="line">    self.add = tf.keras.layers.Add()</span><br></pre></td></tr></table></figure>
<p>那么针对encoder结果输入到decoder的cross attention
layer怎么处理呢？这时候我们使用MultiHeadAttention时就需要将target
sequence x当作是query，将encoder输出当作是context
sequence也就是key/value。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossAttention</span>(<span class="params">BaseAttention</span>):</span> <span class="comment"># encoder结果输入到decoder的层</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x, context</span>):</span> <span class="comment"># 这里的x是target sequence,context是encoder的输出结果</span></span><br><span class="line">    attn_output, attn_scores = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        key=context,</span><br><span class="line">        value=context,</span><br><span class="line">        return_attention_scores=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Cache the attention scores for plotting later.</span></span><br><span class="line">    self.last_attn_scores = attn_scores</span><br><span class="line"></span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>然后我们再定义global attention，global
attention就是没有任何特殊操作的（比如上面的attention计算它有特别的context），而在transformor中更多的是self-attention，也就是我们传递给MultiHeadAttention的query,key,value都是同一个值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GlobalSelfAttention</span>(<span class="params">BaseAttention</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    attn_output = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        value=x,</span><br><span class="line">        key=x)</span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>最后我们定义causal self attention
layer，这个是在decoder的每一个block的第一个sublayer：self-attention
layer.其实这个layer是和global attention
layer差不多的，但还是有一点微小的差别。为什么呢？因为我们在decoder阶段，我们是一个词语一个词语的预测的，这其实包含了一层因果关系，我们在预测一个词语的时候，我们应该已知它前面一个词语是什么，RNN中的hidden
state传递到下一个时间步就是这个因果关系的传递。那么如果我们使用刚刚我们实现的global
attention layer来实现这个self
attention，并没有包含这个因果关系，不仅如此，如果我们使用常规的self
attention的计算，将target
sequence全部当作输入输入到decoder中的第一个block中，会有未来的数据提前被当前时刻看到的风险，所以在Transformer这篇文章中，作者提出使用mask的技术来避免这个问题。</p>
<p>在tensorflow中实现很简单，就只需要给MultiHeadAttention传递一个use_causal_mask
= True的参数即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CausalSelfAttention</span>(<span class="params">BaseAttention</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    attn_output = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        value=x,</span><br><span class="line">        key=x,</span><br><span class="line">        use_causal_mask = <span class="literal">True</span>) <span class="comment"># The causal mask ensures that each location only has access to the locations that come before it</span></span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这样就可以保证先前的sequence并不依赖于之后的elements。这里我本来有一个疑问是，这样一来这个causal
layer并不能实现bi-rnn的能力？但后来一想并不是，因为双向的RNN的后向是指后面的词语先输入，其实就是从后往前输入，这样就可以知道一个sequence当前词语依赖于后面的词语的权重。</p>
<h2 id="why-transformer">why transformer?</h2>
<p>这里想补充一个东西，在从encoder-decoder过渡到transformer的时候，我一直的疑问是为什么要用transformer呢？为什么Effective
Approaches to Attention-based Neural Machine
Translation这篇paper介绍的方法就渐渐不被人所用了呢？一开始我去看了下transformer的原文，发现paper介绍的非常简单，所以就去找了下博客，找到一篇解释为什么transformer比LSTM快的<a href="https://voidful.medium.com/why-transformer-faster-then-lstm-on-generation-c3f30977d747">博客</a></p>
<p>文章说在传统的也就是paper：Neural Machine Translation by Jointly
Learning to Align and
Translate中介绍的用LSTM的encoder-decoder架构来做机器翻译的问题，一个问题在于：在RNN模型中，我们在计算当前时间步时，需要使用到前一个时间步的hidden_state，这就造成一个问题是无法并行训练，你必须要等到前面的东西都输完了你才能计算当前时间步的结果。transformer就可以解决这个问题，它完全摒弃了RNN的结构，基本上是一个FCNN。首先我们都知道输入到模型中来的是一个序列，序列中的每一个单词我们都转化为了词向量。如果是传统的RNN模型，这时候就要一个vector一个vector的往RNN里输入了，但transformer不是，它是将这个embedding变幻成了三个向量空间，也就是我们后面看到的Q,K,V.</p>
<p>后面又去谷歌了一番，找到一个<a href="https://ai.stackexchange.com/questions/20075/why-does-the-transformer-do-better-than-rnn-and-lstm-in-long-range-context-depen">stack</a>上的问题，也有人有这个疑问，被采取的回答总结起来就是：</p>
<ol type="1">
<li>transformer避免了recursion，从而可以方便并行运算（减少了训练时间），这个说法其实跟上面博客一个意思</li>
<li>同时transformer在长句子的dependency上提高了performance</li>
</ol>
<blockquote>
<ul>
<li><strong>Non sequential</strong>: sentences are processed as a whole
rather than word by word.</li>
<li><strong>Self Attention</strong>: this is the newly introduced 'unit'
used to compute similarity scores between words in a sentence.</li>
<li><strong>Positional embeddings</strong>: another innovation
introduced to replace recurrence. The idea is to use fixed or learned
weights which encode information related to a specific position of a
token in a sentence.</li>
</ul>
</blockquote>
<p>transformer并没有long
dependency的问题，为什么？我们知道transformer的做法是将整个sequence作为一个整体输入到模型。也就是它在预测当前时间步的word时并不依赖于上一个时间步的状态，模型看到的是整个序列，也许有人会质疑双向RNN不是也可以解决这个问题吗，但是这个作者说双向RNN仍然不能彻底解决长句子的依赖问题。</p>
<p>其实在机器翻译领域，为了解决长句子的依赖问题，CNN曾经也被广泛用于解决这个问题，不仅如此，CNN还有共享参数的优点，也就是可以在GPU上并行计算。如何用CNN处理句子可以参考<a href="Convolutional%20Neural%20Networks%20for%20Sentence%20Classification">paper</a>,CNN解决依赖问题是用不同宽度的kernel去学习依赖，比如width=2就学习两个词之间的依赖关系，width=3就学习三个词之间的依赖，但长句子的依赖很可能会有很多组合，所以就需要使用到很多不同宽度的kernel，这是不现实的。虽然现在CNN不怎么用来解决S2S的问题，但我觉得它是RNN结构的模型过度到transformer的一个中间桥梁，同时也可以帮助我们理解attention
is all your need这篇文章。感兴趣的可以读一下[Convolutional Sequence to
Sequence Learning 2017](</p>
<h2 id="tensorflow-api实现补充介绍">tensorflow API实现补充介绍</h2>
<h3 id="tf.keras.layers.multiheadattention">tf.keras.layers.MultiHeadAttention</h3>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention">doc</a></p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230309150541287.png" alt="image-20230309150541287">
<figcaption aria-hidden="true">image-20230309150541287</figcaption>
</figure>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20230309150600806.png" alt="image-20230309150600806">
<figcaption aria-hidden="true">image-20230309150600806</figcaption>
</figure>
<p>注意，return的结果包含两个，其中attention_output的shape的第二维是和target
sequence的长度是一致的，并且E是和query的最后一维是一致的。</p>
<p>https://dl.acm.org/doi/10.5555/3305381.3305510)</p>
<h1 id="attention-family">Attention Family</h1>
<p>这个章节整理于<a href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/">blog</a>，这个作者之前写了一篇介绍attention的文章，后面在2023年一月的时候又更新了两篇博客，详细介绍了从2020年以来出现的新的Transformer
models。权当自己学习记录一些我还需要补充的知识。</p>
<blockquote>
<p>The <strong>Transformer</strong> (which will be referred to as
“vanilla Transformer” to distinguish it from other enhanced versions; <a href="https://arxiv.org/abs/1706.03762">Vaswani, et al., 2017</a>) model
has an encoder-decoder architecture, as commonly used in many <a href="https://lilianweng.github.io/posts/2018-06-24-attention/#born-for-translation">NMT</a>
models. Later simplified Transformer was shown to achieve great
performance in language modeling tasks, like in encoder-only <a href="https://lilianweng.github.io/posts/2019-01-31-lm/#bert">BERT</a>
or decoder-only <a href="https://lilianweng.github.io/posts/2019-01-31-lm/#openai-gpt">GPT</a>.</p>
</blockquote>
<h1 id="reference">Reference</h1>
<p>在读transformer论文的时候，有几个概念key，query，value三个概念一下子就抛出来了。在讲transformer的<a href="https://www.youtube.com/watch?v=OyFJWRnt_AY">lecture
video</a>里,我看到有不少评论反应讲者没有将这三个概念讲清楚。我一开始在看cs231n的lecture
ppt时也有点疑惑，老师刚说完general attention layer转到讲self-attention
layer，就直接从h变成了q，确实有点云里雾里。</p>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221206170135727-16703172985855.png" alt="image-20221206170135727">
<figcaption aria-hidden="true">image-20221206170135727</figcaption>
</figure>
<figure>
<img src="/2022/10/27/Attention-and-Transformer-model/image-20221206170101403.png" alt="image-20221206170101403">
<figcaption aria-hidden="true">image-20221206170101403</figcaption>
</figure>
<p>可以从上面的ppt中看出，原来是仅仅有q这个变量的，这是从一开始的h演变来的，而我们可以看到为了“add
more expressivity to the layer”，所以我们在1.输入x输入到FC得到alignment
score之前又加了一个不同的FC 2. 对输入x用attention weights进行weight
sum的过程也加了一个完全不同的FC layer。而我们可以看到这里加的这两个FC
layer是为了增加模型的表现力。这两个FC
layer的输出也就是成了我们所说的key和value。后面的过程就清晰了，首先利用query和key计算attention
weights，然后用attention weights和value进行计算得到context。</p>
<blockquote>
<p>An attention layer does a fuzzy lookup like this, but it's not just
looking for the best key. It combines the <code>values</code> based on
how well the <code>query</code> matches each <code>key</code>.</p>
<p>How does that work? In an attention layer the <code>query</code>,
<code>key</code>, and <code>value</code> are each vectors. Instead of
doing a hash lookup the attention layer combines the <code>query</code>
and <code>key</code> vectors to determine how well they match, the
"attention score". The layer returns the average across all the
<code>values</code>, weighted by the "attention scores</p>
</blockquote>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title>cv2中bitwise_and()</title>
    <url>/2022/10/12/cv2%E4%B8%ADbitwise-and/</url>
    <content><![CDATA[<p>参考文档：</p>
<ol type="1">
<li><a href="https://docs.opencv.org/4.x/d0/d86/tutorial_py_image_arithmetics.html">opencv
tutorial</a></li>
<li><a href="https://docs.opencv.org/4.x/d2/de8/group__core__array.html#ga60b4d04b251ba5eb1392c34425497e14">opencv
官方文档</a></li>
<li><a href="https://stackoverflow.com/questions/44333605/what-does-bitwise-and-operator-exactly-do-in-opencv">stackoverflow
question</a></li>
</ol>
<p>没有多少文档能讲清楚具体是如何计算的。在stackoverflow中的这个问题，第一个高赞答案也只是选取了最普通的一种情况，也就是当image的pixel的值全部是0或者1的时候，and的运算。但是cv2中的bitwise_and()还有一个很重要的参数mask并没有讲清楚，并且当image的像素值有除了0或者1以外的其他值，这时候该如何运算？</p>
<p>官方文档中说mask是决定了哪些位置要进行运算，当mask中某个元素不为0时，我们对相应位置的像素做“与”运算，如果scr1和scr2(bitwise_and的两个参数)的值不是0或者1，会将十进制的值转化为2进制，然后对二进制再进行运算，运算完了之后又会转化为十进制(<a href="https://dsp.stackexchange.com/questions/58276/opencv-how-does-bitwise-not-work">参考</a>)
当mask相应位置的值=0时，任何操作都不会做，不仅如此，结果的图片相应位置的像素值会变成0，也就是黑色（这一点官方文档没说）.</p>
<p>所以经过bitwise_and之后的图片会看到mask中像素值为0的地方全部是黑色。这就相应的把mask中不为0的地方强调突出了。</p>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow-Advanced-Techniques-Specialization专项课程总结</title>
    <url>/2022/08/30/TensorFlow-Advanced-Techniques-Specialization%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="course-1-custom-modelslayers-and-loss-functions">Course 1:
Custom Models,Layers and loss Functions</h1>
<h2 id="多输出-multi-output">多输出 multi output</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Define model layers.</span></span><br><span class="line">input_layer = Input(shape=(<span class="built_in">len</span>(train.columns),))</span><br><span class="line">first_dense = Dense(units=<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(input_layer)</span><br><span class="line">second_dense = Dense(units=<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(first_dense)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Y1 output will be fed directly from the second dense</span></span><br><span class="line">y1_output = Dense(units=<span class="string">&#x27;1&#x27;</span>, name=<span class="string">&#x27;y1_output&#x27;</span>)(second_dense)</span><br><span class="line">third_dense = Dense(units=<span class="string">&#x27;64&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(second_dense)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Y2 output will come via the third dense</span></span><br><span class="line">y2_output = Dense(units=<span class="string">&#x27;1&#x27;</span>, name=<span class="string">&#x27;y2_output&#x27;</span>)(third_dense)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the model with the input layer and a list of output layers</span></span><br><span class="line">model = Model(inputs=input_layer, outputs=[y1_output, y2_output])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify the optimizer, and compile the model with loss functions for both outputs</span></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=optimizer,</span><br><span class="line">              loss=&#123;<span class="string">&#x27;y1_output&#x27;</span>: <span class="string">&#x27;mse&#x27;</span>, <span class="string">&#x27;y2_output&#x27;</span>: <span class="string">&#x27;mse&#x27;</span>&#125;,</span><br><span class="line">              metrics=&#123;<span class="string">&#x27;y1_output&#x27;</span>: tf.keras.metrics.RootMeanSquaredError(),</span><br><span class="line">                       <span class="string">&#x27;y2_output&#x27;</span>: tf.keras.metrics.RootMeanSquaredError()&#125;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>注意在这种多输出的模型架构下，train_y是一个元组set</p>
<h2 id="自定义loss-function">自定义loss function</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">huber_loss</span>(<span class="params">y_true , y_pred</span>):</span> <span class="comment"># y_true在前，y_pred在后</span></span><br><span class="line">    thresold = <span class="number">1</span></span><br><span class="line">    error = y_true - y_pred</span><br><span class="line">    return_type = tf.<span class="built_in">abs</span>(error) &lt;= thresold</span><br><span class="line">    r1 = <span class="number">0.5</span> * tf.square(error)</span><br><span class="line">    r2 = thresold * (tf.<span class="built_in">abs</span>(error) - (<span class="number">0.5</span>*thresold))</span><br><span class="line">    <span class="keyword">return</span> tf.where(return_type , r1 , r2)</span><br><span class="line"></span><br><span class="line">model_huber_loss = tf.keras.models.Model(inputs=<span class="built_in">input</span> , outputs=output_layer)</span><br><span class="line">model_huber_loss.<span class="built_in">compile</span>(optimizer=<span class="string">&quot;sgd&quot;</span> , loss=huber_loss)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>如果loss function有需要传递别的除了y_ture,y_pred参数： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">huber_loss_wrapper</span>(<span class="params">thresold</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">huber_loss</span>(<span class="params">y_true , y_pred</span>):</span></span><br><span class="line">        error = y_true - y_pred</span><br><span class="line">        return_type = tf.<span class="built_in">abs</span>(error) &lt;= thresold</span><br><span class="line">        r1 = <span class="number">0.5</span> * tf.square(error)</span><br><span class="line">        r2 = thresold * (tf.<span class="built_in">abs</span>(error) - (<span class="number">0.5</span>*thresold))</span><br><span class="line">        <span class="keyword">return</span> tf.where(return_type , r1 , r2)</span><br><span class="line">    <span class="keyword">return</span> huber_loss</span><br><span class="line">model_huber_loss_wrapper = tf.keras.models.Model(inputs=<span class="built_in">input</span> , outputs=output_layer)</span><br><span class="line">model_huber_loss_wrapper.<span class="built_in">compile</span>(optimizer=<span class="string">&quot;sgd&quot;</span> , loss=huber_loss_wrapper(thresold=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
也可以将loss写成tensorflow.keras.losses的继承类： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense , Input</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.losses <span class="keyword">import</span> Loss</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Huber</span>(<span class="params">Loss</span>):</span></span><br><span class="line">    thresold = <span class="number">1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self , thresold</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.thresold = thresold</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self , y_true , y_pred</span>):</span></span><br><span class="line">        error = y_true - y_pred</span><br><span class="line">        return_type = tf.<span class="built_in">abs</span>(error) &lt;= self.thresold</span><br><span class="line">        r1 = <span class="number">0.5</span> * tf.square(error)</span><br><span class="line">        r2 = self.thresold * (tf.<span class="built_in">abs</span>(error) - (<span class="number">0.5</span>*self.thresold))</span><br><span class="line">        <span class="keyword">return</span> tf.where(return_type , r1 , r2)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = Input(shape=(<span class="number">1</span>,) , name=<span class="string">&quot;input_layer&quot;</span>)</span><br><span class="line">output_layer = Dense(<span class="number">1</span> , name=<span class="string">&quot;output_layer&quot;</span>)(<span class="built_in">input</span>)</span><br><span class="line">model_huber_loss_class = tf.keras.models.Model(inputs=<span class="built_in">input</span> , outputs=output_layer)</span><br><span class="line"></span><br><span class="line">model_huber_loss_class.<span class="built_in">compile</span>(optimizer=<span class="string">&quot;sgd&quot;</span> , loss=Huber(thresold=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">model_huber_loss_class.fit(xs,ys,epochs=<span class="number">500</span>,verbose=<span class="number">0</span>)</span><br><span class="line">model_huber_loss_class.predict([[<span class="number">10.0</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h2 id="custom-layers-自定义层">custom layers 自定义层</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.nn <span class="keyword">import</span> softmax , relu</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDenseLayerwithActivation</span>(<span class="params">Layer</span>):</span> <span class="comment"># 可以传递units,activation</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self , units = <span class="number">32</span> ,activation = <span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyDenseLayerwithActivation , self).__init__()</span><br><span class="line">        self.units = units</span><br><span class="line">        self.activation = activation</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self , input_shape</span>):</span></span><br><span class="line">        w_init = tf.random_normal_initializer()</span><br><span class="line">        b_init = tf.zeros_initializer()</span><br><span class="line"></span><br><span class="line">        self.w = tf.Variable(initial_value=w_init(shape=(input_shape[-<span class="number">1</span>] , self.units) , dtype=<span class="string">&quot;float32&quot;</span>) , trainable=<span class="literal">True</span> , name=<span class="string">&quot;kernal&quot;</span>)</span><br><span class="line">        self.b = tf.Variable(initial_value=b_init(shape=(self.units , ) , dtype=<span class="string">&quot;float32&quot;</span>) , trainable=<span class="literal">True</span> , name=<span class="string">&quot;bias&quot;</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self , inputs</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.activation( tf.matmul(inputs , self.w) + self.b )</span><br><span class="line"></span><br><span class="line">model_simpledense_activation = Sequential([</span><br><span class="line">    Flatten(input_shape=(<span class="number">28</span>,<span class="number">28</span>)),</span><br><span class="line">    MyDenseLayerwithActivation(units = <span class="number">128</span> , activation=relu),</span><br><span class="line">    MyDenseLayerwithActivation(<span class="number">10</span> , activation = softmax)</span><br><span class="line">])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="lambda-layer">lambda layer</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_lambda = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(input_shape=(<span class="number">28</span>,<span class="number">28</span>)),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">128</span>),</span><br><span class="line">    tf.keras.layers.Lambda(<span class="keyword">lambda</span> x : tf.<span class="built_in">abs</span>(x)),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span> , activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h2 id="自定义model">自定义model</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense , Input , concatenate</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.nn <span class="keyword">import</span> relu </span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.utils.vis_utils <span class="keyword">import</span> plot_model</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyOwnModel</span>(<span class="params">Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,units = <span class="number">30</span> , activation = <span class="string">&quot;relu&quot;</span> , **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden1 = Dense(units , activation=activation , name=<span class="string">&quot;hidden1&quot;</span>)</span><br><span class="line">        self.hidden2 = Dense(units , activation=activation , name=<span class="string">&quot;hidden2&quot;</span>)</span><br><span class="line">        self.main_output = Dense(<span class="number">1</span>)</span><br><span class="line">        self.aux_output = Dense(<span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self , inputs</span>):</span></span><br><span class="line">        input_l , input_r = inputs</span><br><span class="line">        hidden1 = self.hidden1(input_r)</span><br><span class="line">        hidden2 = self.hidden2(hidden1)</span><br><span class="line">        concat = concatenate([input_l , hidden2])</span><br><span class="line">        main_output = self.main_output(concat)</span><br><span class="line">        aux_output  = self.aux_output(hidden2)</span><br><span class="line">        <span class="keyword">return</span> main_output , aux_output</span><br><span class="line"></span><br><span class="line">model = MyOwnModel()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="callbacks">callbacks</h2>
<h3 id="使用tensorboard">使用tensorboard</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> TensorBoard</span><br><span class="line">model = build_model(dense_units=<span class="number">256</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;sgd&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, </span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">  </span><br><span class="line">logdir = os.path.join(<span class="string">&quot;logs&quot;</span>, datetime.datetime.now().strftime(<span class="string">&quot;%Y%m%d-%H%M%S&quot;</span>))</span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir)</span><br><span class="line"></span><br><span class="line">model.fit(train_batches, </span><br><span class="line">          epochs=<span class="number">10</span>, </span><br><span class="line">          validation_data=validation_batches, </span><br><span class="line">          callbacks=[tensorboard_callback])</span><br></pre></td></tr></table></figure>
<p>模型训练完之后使用<code>tensorboard --logdir logs</code>来查看模型训练过程中的loss和acc
。
上面这种方式不太适合在服务器调试model，一般采取checkpoint的方式以一定的时间间隔保存模型参数。
### 使用ModelCheckpoint</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> EarlyStopping, LearningRateScheduler, ModelCheckpoint, CSVLogger, ReduceLROnPlateau</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="comment"># method 1:</span></span><br><span class="line">model = build_model(dense_units=<span class="number">256</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;sgd&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, </span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">  </span><br><span class="line">model.fit(train_batches, </span><br><span class="line">          epochs=<span class="number">5</span>, </span><br><span class="line">          validation_data=validation_batches, </span><br><span class="line">          verbose=<span class="number">2</span>,</span><br><span class="line">          callbacks=[ModelCheckpoint(<span class="string">&#x27;weights.&#123;epoch:02d&#125;-&#123;val_loss:.2f&#125;.h5&#x27;</span>, verbose=<span class="number">1</span>), <span class="comment"># 这样调用会保存多个文件，每个epoch结束时更新文件</span></span><br><span class="line">          ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># method2:</span></span><br><span class="line">model = build_model(dense_units=<span class="number">256</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;sgd&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, </span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">  </span><br><span class="line">model.fit(train_batches, </span><br><span class="line">          epochs=<span class="number">2</span>, </span><br><span class="line">          validation_data=validation_batches, </span><br><span class="line">          verbose=<span class="number">2</span>,</span><br><span class="line">          callbacks=[ModelCheckpoint(<span class="string">&#x27;./saved_model&#x27;</span>, verbose=<span class="number">1</span>) <span class="comment"># 如果传入的是一个文件夹，会在文件夹内创建多个文件</span></span><br><span class="line">          ])</span><br></pre></td></tr></table></figure>
<h3 id="earlystoppingcsvloggerlearningrateschedulerreducelronplateau">EarlyStopping,CSVLogger,LearningRateScheduler,ReduceLROnPlateau</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#EarlyStopping</span></span><br><span class="line">model = build_model(dense_units=<span class="number">256</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;sgd&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, </span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">  </span><br><span class="line">model.fit(train_batches, </span><br><span class="line">          epochs=<span class="number">50</span>, </span><br><span class="line">          validation_data=validation_batches, </span><br><span class="line">          verbose=<span class="number">2</span>,</span><br><span class="line">          callbacks=[EarlyStopping(</span><br><span class="line">              patience=<span class="number">3</span>,</span><br><span class="line">              min_delta=<span class="number">0.05</span>,</span><br><span class="line">              baseline=<span class="number">0.8</span>,</span><br><span class="line">              mode=<span class="string">&#x27;min&#x27;</span>,</span><br><span class="line">              monitor=<span class="string">&#x27;val_loss&#x27;</span>,</span><br><span class="line">              restore_best_weights=<span class="literal">True</span>,</span><br><span class="line">              verbose=<span class="number">1</span>)</span><br><span class="line">          ])</span><br><span class="line"></span><br><span class="line"><span class="comment">#CSVLogger</span></span><br><span class="line">model = build_model(dense_units=<span class="number">256</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;sgd&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, </span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">  </span><br><span class="line">csv_file = <span class="string">&#x27;training.csv&#x27;</span></span><br><span class="line"></span><br><span class="line">model.fit(train_batches, </span><br><span class="line">          epochs=<span class="number">5</span>, </span><br><span class="line">          validation_data=validation_batches, </span><br><span class="line">          callbacks=[CSVLogger(csv_file)</span><br><span class="line">          ])</span><br><span class="line"></span><br><span class="line"><span class="comment">#LearningRateScheduler</span></span><br><span class="line">model = build_model(dense_units=<span class="number">256</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;sgd&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, </span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step_decay</span>(<span class="params">epoch</span>):</span></span><br><span class="line">	initial_lr = <span class="number">0.01</span></span><br><span class="line">	drop = <span class="number">0.5</span></span><br><span class="line">	epochs_drop = <span class="number">1</span></span><br><span class="line">	lr = initial_lr * math.<span class="built_in">pow</span>(drop, math.floor((<span class="number">1</span>+epoch)/epochs_drop))</span><br><span class="line">	<span class="keyword">return</span> lr</span><br><span class="line"></span><br><span class="line">model.fit(train_batches, </span><br><span class="line">          epochs=<span class="number">5</span>, </span><br><span class="line">          validation_data=validation_batches, </span><br><span class="line">          callbacks=[LearningRateScheduler(step_decay, verbose=<span class="number">1</span>),</span><br><span class="line">                    TensorBoard(log_dir=<span class="string">&#x27;./log_dir&#x27;</span>)]) <span class="comment"># 以上介绍的callbacks可以传递多个item</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#ReduceLROnPlateau</span></span><br><span class="line">model = build_model(dense_units=<span class="number">256</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;sgd&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, </span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">  </span><br><span class="line">model.fit(train_batches, </span><br><span class="line">          epochs=<span class="number">50</span>, </span><br><span class="line">          validation_data=validation_batches, </span><br><span class="line">          callbacks=[ReduceLROnPlateau(monitor=<span class="string">&#x27;val_loss&#x27;</span>, </span><br><span class="line">                                       factor=<span class="number">0.2</span>, verbose=<span class="number">1</span>,</span><br><span class="line">                                       patience=<span class="number">1</span>, min_lr=<span class="number">0.001</span>),</span><br><span class="line">                     TensorBoard(log_dir=<span class="string">&#x27;./log_dir&#x27;</span>)])</span><br></pre></td></tr></table></figure>
<h3 id="自定义callbacks">自定义callbacks</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DetectOverfittingCallback</span>(<span class="params">tf.keras.callbacks.Callback</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, threshold=<span class="number">0.7</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DetectOverfittingCallback, self).__init__()</span><br><span class="line">        self.threshold = threshold</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span>(<span class="params">self, epoch, logs=<span class="literal">None</span></span>):</span></span><br><span class="line">        ratio = logs[<span class="string">&quot;val_loss&quot;</span>] / logs[<span class="string">&quot;loss&quot;</span>]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Epoch: &#123;&#125;, Val/Train loss ratio: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(epoch, ratio))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ratio &gt; self.threshold:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Stopping training...&quot;</span>)</span><br><span class="line">            self.model.stop_training = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">model = get_model()</span><br><span class="line">_ = model.fit(x_train, y_train,</span><br><span class="line">              validation_data=(x_test, y_test),</span><br><span class="line">              batch_size=<span class="number">64</span>,</span><br><span class="line">              epochs=<span class="number">3</span>,</span><br><span class="line">              verbose=<span class="number">0</span>,</span><br><span class="line">              callbacks=[DetectOverfittingCallback()])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在tf.keras.callbacks.Callback类中还可以重写的函数还有：</p>
<h4 id="on_traintestpredict_beginself-logsnone"><code>on_(train|test|predict)_begin(self, logs=None)</code></h4>
<p>Called at the beginning of
<code>fit</code>/<code>evaluate</code>/<code>predict</code>.</p>
<h4 id="on_traintestpredict_endself-logsnone"><code>on_(train|test|predict)_end(self, logs=None)</code></h4>
<p>Called at the end of
<code>fit</code>/<code>evaluate</code>/<code>predict</code>.</p>
<h4 id="on_traintestpredict_batch_beginself-batch-logsnone"><code>on_(train|test|predict)_batch_begin(self, batch, logs=None)</code></h4>
<p>Called right before processing a batch during
training/testing/predicting. Within this method, <code>logs</code> is a
dict with <code>batch</code> and <code>size</code> available keys,
representing the current batch number and the size of the batch.</p>
<p>logs是一个字典，keys=[batch,size]，分别表示batch number和batch
size</p>
<h4 id="on_traintestpredict_batch_endself-batch-logsnone"><code>on_(train|test|predict)_batch_end(self, batch, logs=None)</code></h4>
<p>Called at the end of training/testing/predicting a batch. Within this
method, <code>logs</code> is a dict containing the stateful metrics
result.</p>
<p>这里的Logs字典中含有metrics结果</p>
<h3 id="training-specific-methods">Training specific methods</h3>
<p>在训练阶段有一些特别的函数：</p>
<h4 id="on_epoch_beginself-epoch-logsnone"><code>on_epoch_begin(self, epoch, logs=None)</code></h4>
<p>Called at the beginning of an epoch during training.</p>
<h4 id="on_epoch_endself-epoch-logsnone"><code>on_epoch_end(self, epoch, logs=None)</code></h4>
<p>Called at the end of an epoch during training.</p>
<p>可以在自定义callback类中定义display结果的函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Visualization utilities</span></span><br><span class="line">plt.rc(<span class="string">&#x27;font&#x27;</span>, size=<span class="number">20</span>)</span><br><span class="line">plt.rc(<span class="string">&#x27;figure&#x27;</span>, figsize=(<span class="number">15</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_digits</span>(<span class="params">inputs, outputs, ground_truth, epoch, n=<span class="number">10</span></span>):</span></span><br><span class="line">    plt.clf() <span class="comment"># clear the current figure</span></span><br><span class="line"></span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.grid(<span class="literal">None</span>)</span><br><span class="line">    inputs = np.reshape(inputs, [n, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">    inputs = np.swapaxes(inputs, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    inputs = np.reshape(inputs, [<span class="number">28</span>, <span class="number">28</span>*n])</span><br><span class="line">    plt.imshow(inputs)</span><br><span class="line">    plt.xticks([<span class="number">28</span>*x+<span class="number">14</span> <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(n)], outputs)</span><br><span class="line">    <span class="keyword">for</span> i,t <span class="keyword">in</span> <span class="built_in">enumerate</span>(plt.gca().xaxis.get_ticklabels()):</span><br><span class="line">        <span class="keyword">if</span> outputs[i] == ground_truth[i]: </span><br><span class="line">            t.set_color(<span class="string">&#x27;green&#x27;</span>) </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            t.set_color(<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    plt.grid(<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">GIF_PATH = <span class="string">&#x27;./animation.gif&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VisCallback</span>(<span class="params">tf.keras.callbacks.Callback</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, inputs, ground_truth, display_freq=<span class="number">10</span>, n_samples=<span class="number">10</span></span>):</span></span><br><span class="line">        self.inputs = inputs</span><br><span class="line">        self.ground_truth = ground_truth</span><br><span class="line">        self.images = []</span><br><span class="line">        self.display_freq = display_freq</span><br><span class="line">        self.n_samples = n_samples</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span>(<span class="params">self, epoch, logs=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># Randomly sample data</span></span><br><span class="line">        indexes = np.random.choice(<span class="built_in">len</span>(self.inputs), size=self.n_samples)</span><br><span class="line">        X_test, y_test = self.inputs[indexes], self.ground_truth[indexes]</span><br><span class="line">        predictions = np.argmax(self.model.predict(X_test), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Plot the digits</span></span><br><span class="line">        display_digits(X_test, predictions, y_test, epoch, n=self.display_freq)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save the figure</span></span><br><span class="line">        buf = io.BytesIO()</span><br><span class="line">        plt.savefig(buf, <span class="built_in">format</span>=<span class="string">&#x27;png&#x27;</span>)</span><br><span class="line">        buf.seek(<span class="number">0</span>)</span><br><span class="line">        image = Image.<span class="built_in">open</span>(buf)</span><br><span class="line">        self.images.append(np.array(image))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Display the digits every &#x27;display_freq&#x27; number of epochs</span></span><br><span class="line">        <span class="keyword">if</span> epoch % self.display_freq == <span class="number">0</span>:</span><br><span class="line">            plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_train_end</span>(<span class="params">self, logs=<span class="literal">None</span></span>):</span></span><br><span class="line">        imageio.mimsave(GIF_PATH, self.images, fps=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h1 id="course-2-custom-and-distributed-training-with-tensorflow">Course
2 : Custom and Distributed Training with Tensorflow</h1>
<h2 id="导数的计算">导数的计算</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape_2:</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape_1:</span><br><span class="line">        y = x * x * x</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The first gradient calculation should occur at leaset</span></span><br><span class="line">    <span class="comment"># within the outer with block</span></span><br><span class="line">    dy_dx = tape_1.gradient(y, x)</span><br><span class="line">d2y_dx2 = tape_2.gradient(dy_dx, x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dy_dx)</span><br><span class="line"><span class="built_in">print</span>(d2y_dx2)</span><br></pre></td></tr></table></figure>
<h2 id="在fashion-mnist-数据集上写specific的训练过程">在fashion mnist
数据集上写specific的训练过程</h2>
<p>这种训练方式不采用model.fit()的方式去训练模型，而是采用自己写传递导数的方式。增加了灵活度，方便后续进行分布式训练。</p>
<figure>
<img src="/2022/08/30/TensorFlow-Advanced-Techniques-Specialization%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/Users\320150117\AppData\Roaming\Typora\typora-user-images\image-20220830150121062.png" alt="image-20220830150121062">
<figcaption aria-hidden="true">image-20220830150121062</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.losses <span class="keyword">import</span> SparseCategoricalCrossentropy</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.metrics <span class="keyword">import</span> SparseCategoricalAccuracy</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">base_model</span>():</span></span><br><span class="line">    <span class="built_in">input</span>  = Input(shape=(<span class="number">28</span>*<span class="number">28</span> , ) , name = <span class="string">&quot;input_layer&quot;</span>)</span><br><span class="line">    x = Dense(<span class="number">64</span> , activation=<span class="string">&#x27;relu&#x27;</span> , name = <span class="string">&quot;dense1&quot;</span>)(<span class="built_in">input</span>)</span><br><span class="line">    x = Dense(<span class="number">64</span> , activation=<span class="string">&#x27;relu&#x27;</span> , name = <span class="string">&quot;dense2&quot;</span>)(x)</span><br><span class="line">    output = Dense(<span class="number">10</span> , activation=<span class="string">&#x27;softmax&#x27;</span> , name=<span class="string">&quot;output_layer&quot;</span>)(x)</span><br><span class="line">    model = Model(inputs = <span class="built_in">input</span> , outputs = output)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_optimizer</span>(<span class="params">model , x , y_true</span>):</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        logits = model(x)</span><br><span class="line">        loss_val = loss_obj(y_true=y_true , y_pred=logits)</span><br><span class="line">    grad = tape.gradient(loss_val , model.trainable_weights) <span class="comment"># model有6个可以train的weights</span></span><br><span class="line">    optimizer_obj.apply_gradients(<span class="built_in">zip</span>(grad , model.trainable_weights))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> logits , loss_val</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_one_epoch</span>(<span class="params">model , train_data</span>):</span> <span class="comment"># 这里输入的是整个dataset</span></span><br><span class="line">    losses = []</span><br><span class="line">    pbar = tqdm(total=<span class="built_in">len</span>(<span class="built_in">list</span>(<span class="built_in">enumerate</span>(train_data))), position=<span class="number">0</span>, leave=<span class="literal">True</span>, bar_format=<span class="string">&#x27;&#123;l_bar&#125;&#123;bar&#125;| &#123;n_fmt&#125;/&#123;total_fmt&#125; &#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> batch_no , (data , label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data): <span class="comment"># 这个for循环循环的是batch，每次输入模型中训练的是一个batch_size的数据</span></span><br><span class="line">        y_pred , loss = run_optimizer(model , data , label)</span><br><span class="line">        losses.append(loss)</span><br><span class="line">        train_acc_matrix(label , y_pred)</span><br><span class="line">        pbar.set_description(<span class="string">&quot;Training loss for step %s: %.4f&quot;</span> % (<span class="built_in">int</span>(batch_no), <span class="built_in">float</span>(loss)))</span><br><span class="line">        pbar.update()</span><br><span class="line">    <span class="keyword">return</span> losses <span class="comment"># 返回的是一个list，每一个item是一个batch的loss</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perform_validation</span>(<span class="params">model , test_data</span>):</span></span><br><span class="line">    losses = []</span><br><span class="line">    <span class="keyword">for</span> (data , label) <span class="keyword">in</span> test_data:</span><br><span class="line">        y_pred = model(data)</span><br><span class="line">        loss = loss_obj(label , y_pred)</span><br><span class="line">        losses.append(loss)</span><br><span class="line">        val_acc_matrix(label , y_pred)</span><br><span class="line">    <span class="keyword">return</span> losses</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">train , test , epochs = <span class="number">10</span></span>):</span></span><br><span class="line">    model = base_model()</span><br><span class="line"></span><br><span class="line">    history = &#123;&#125;</span><br><span class="line">    history[<span class="string">&#x27;train_loss&#x27;</span>] = []</span><br><span class="line">    history[<span class="string">&#x27;val_loss&#x27;</span>] = []</span><br><span class="line"></span><br><span class="line">    history[<span class="string">&#x27;train_acc&#x27;</span>] = []</span><br><span class="line">    history[<span class="string">&#x27;val_acc&#x27;</span>] = []</span><br><span class="line"></span><br><span class="line">    val_epoch_loss   = []</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Start of epoch %d&#x27;</span> % (epoch,))</span><br><span class="line"></span><br><span class="line">        train_losses = train_one_epoch(model , train_data=train)</span><br><span class="line">        train_acc    = train_acc_matrix.result()</span><br><span class="line">        history[<span class="string">&#x27;train_acc&#x27;</span>].append(train_acc.numpy())</span><br><span class="line">        train_acc_matrix.reset_states()</span><br><span class="line"></span><br><span class="line">        val_losses   = perform_validation(model , test_data=test)</span><br><span class="line">        val_acc      = val_acc_matrix.result()</span><br><span class="line">        history[<span class="string">&#x27;val_acc&#x27;</span>].append(val_acc.numpy())</span><br><span class="line">        val_acc_matrix.reset_states()</span><br><span class="line"></span><br><span class="line">        history[<span class="string">&#x27;train_loss&#x27;</span>].append(np.mean(train_losses)) <span class="comment"># 这里求解的是所有batch的loss总和求平均</span></span><br><span class="line">        history[<span class="string">&#x27;val_loss&#x27;</span>].append(np.mean(val_losses))</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\n Epoch %s: Train loss: %.4f  Validation Loss: %.4f,\</span></span><br><span class="line"><span class="string">         Train Accuracy: %.4f, Validation Accuracy %.4f&#x27;</span> % (epoch, <span class="built_in">float</span>(np.mean(train_losses)), <span class="built_in">float</span>(np.mean(val_losses)),</span><br><span class="line">                                                            <span class="built_in">float</span>(train_acc), <span class="built_in">float</span>(val_acc)))</span><br><span class="line"></span><br><span class="line">    history[<span class="string">&#x27;model&#x27;</span>] = model</span><br><span class="line">    <span class="keyword">return</span> history</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loss_obj       = SparseCategoricalCrossentropy()</span><br><span class="line">optimizer_obj  = Adam()</span><br><span class="line"></span><br><span class="line">train_acc_matrix = SparseCategoricalAccuracy()</span><br><span class="line">val_acc_matrix   = SparseCategoricalAccuracy()</span><br><span class="line"></span><br><span class="line">history = train(train_data , test_data)</span><br></pre></td></tr></table></figure>
<h2 id="分布式训练">分布式训练</h2>
<h3 id="mirrored-strategy">Mirrored strategy</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">strategy = tf.distribute.MirroredStrategy() </span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Number of devices: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(strategy.num_replicas_in_sync))</span><br><span class="line"></span><br><span class="line">BATCH_SIZE_PER_REPLICA = <span class="number">64</span></span><br><span class="line"><span class="comment"># Use for Mirrored Strategy</span></span><br><span class="line">BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up the train and eval data set</span></span><br><span class="line">train_dataset = mnist_train.<span class="built_in">map</span>(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)</span><br><span class="line">eval_dataset = mnist_test.<span class="built_in">map</span>(scale).batch(BATCH_SIZE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use for Mirrored Strategy -- comment out `with strategy.scope():` and deindent for no strategy</span></span><br><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">    model = tf.keras.Sequential([</span><br><span class="line">      tf.keras.layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)),</span><br><span class="line">      tf.keras.layers.MaxPooling2D(),</span><br><span class="line">      tf.keras.layers.Flatten(),</span><br><span class="line">      tf.keras.layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">      tf.keras.layers.Dense(<span class="number">10</span>)</span><br><span class="line">    ])</span><br><span class="line">model.<span class="built_in">compile</span>(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">                optimizer=tf.keras.optimizers.Adam(),</span><br><span class="line">                metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>这种方式会使用该机器的所有gpu,如果想指定使用的GPU可以使用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gpus = tf.config.experimental.list_logical_devices(<span class="string">&quot;GPU&quot;</span>)</span><br><span class="line"><span class="comment"># gpus = tf.config.list_physical_devices(&#x27;GPU&#x27;)</span></span><br><span class="line">strategy = tf.distribute.MirroredStrategy([gpu.name <span class="keyword">for</span> gpu <span class="keyword">in</span> gpus])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Running on multiple GPUs &#x27;</span>, [gpu.name <span class="keyword">for</span> gpu <span class="keyword">in</span> gpus])</span><br></pre></td></tr></table></figure>
<h1 id="course-3-advanced-computer-vision-with-tensorflow">Course 3:
Advanced Computer Vision with Tensorflow</h1>
<h2 id="object-localization">Object Localization</h2>
<p>week1介绍了transfer
learning和利用minist手写体数据集做的一个简单的物体定位+分类的模型。object
localization和object
detection的区别在于后者需要识别出image中所有物体的Box并能实现分类，而前者只是需要检测出最主要的那个object并分类。可参考博客https://towardsdatascience.com/object-localization-in-overfeat-5bb2f7328b62。</p>
<figure>
<img src="https://miro.medium.com/max/1400/1*LOjfqvJ0zDuSSq443Z9SNA.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="object-detection">Object Detection</h2>
<p>如果想要开箱即用的方式，作者介绍了tensorflow的API进行调用：https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md。在该object
detection写好的API下有几个可用的utils可以使用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> object_detection.utils <span class="keyword">import</span> label_map_util</span><br><span class="line"><span class="keyword">from</span> object_detection.utils <span class="keyword">import</span> visualization_utils <span class="keyword">as</span> viz_utils</span><br><span class="line"><span class="keyword">from</span> object_detection.utils <span class="keyword">import</span> ops <span class="keyword">as</span> utils_ops <span class="comment"># 在image segmentation中有使用到mask</span></span><br><span class="line"></span><br><span class="line">PATH_TO_LABELS = <span class="string">&#x27;./models/research/object_detection/data/mscoco_label_map.pbtxt&#x27;</span></span><br><span class="line">category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS) <span class="comment"># category_index是一个字典，key为unique id，values也是一个字典：key是index，value是label_name</span></span><br><span class="line"></span><br><span class="line">viz_utils.visualize_boxes_and_labels_on_image_array(</span><br><span class="line">    image_np_with_detections[<span class="number">0</span>],</span><br><span class="line">    result[<span class="string">&#x27;detection_boxes&#x27;</span>][<span class="number">0</span>],</span><br><span class="line">    (result[<span class="string">&#x27;detection_classes&#x27;</span>][<span class="number">0</span>] + label_id_offset).astype(<span class="built_in">int</span>),</span><br><span class="line">    result[<span class="string">&#x27;detection_scores&#x27;</span>][<span class="number">0</span>],</span><br><span class="line">    category_index,</span><br><span class="line">    use_normalized_coordinates=<span class="literal">True</span></span><br><span class="line">) <span class="comment"># images,boxes,classes,scores</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>另外还介绍了在已有weights上finetune模型的方法。使用的例子是tensorflow官方出的tutorial。https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb</p>
<h3 id="补充介绍object-detection">补充介绍object detection</h3>
<p>这里我又去翻阅了一些review的文章，想了解一下在物体检测领域的其他算法。参考文章
Object Detection With Deep Learning: A Review.</p>
<figure>
<img src="/2022/08/30/TensorFlow-Advanced-Techniques-Specialization%E4%B8%93%E9%A1%B9%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/Users\320150117\AppData\Roaming\Typora\typora-user-images\image-20220913152542940.png" alt="image-20220913152542940">
<figcaption aria-hidden="true">image-20220913152542940</figcaption>
</figure>
<p>该文章将物体检测领域的模型分为两个种类，第一类就是以R-CNN为代表的region
proposal方法，第二类就是遵循一个统一的模式，将其看成是一个regression/classification的问题.</p>
<ol type="1">
<li>region proposal</li>
</ol>
<p>R-CNN,
SPP-net(在R-CNN基础上进行了改进)，Fast-RCNN，Faster-RCNN，R-FCN。</p>
<ol type="1">
<li>regression/classification based</li>
</ol>
<p>MultiBox，AttentionNet，G-CNN，yolo，SSD（single shot Multibox
detector）</p>
<h2 id="image-segmentation">image segmentation</h2>
<p>除了FCN-8和Unet。作者还介绍了Mask-RCNN。</p>
<h2 id="可解释性">可解释性</h2>
<p>Class Activation Map : A class activation map is a matrix that shows
what parts of the image the model was paying attention to when deciding
what class to assign the image.</p>
<p><a href="https://github.com/alexisbcook/ResNetCAM-keras/blob/master/ResNet_CAM.py">代码地址</a></p>
<p><a href="https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/#:~:text=to%20avoid%20overfitting.-,Global%20Average%20Pooling,of%20a%20three-dimensional%20tensor.">代码博客</a></p>
<p><a href="https://towardsdatascience.com/class-activation-mapping-using-transfer-learning-of-resnet50-e8ca7cfd657e">参考博客</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> ast</span><br><span class="line"><span class="keyword">import</span> scipy   </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> cv2     </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.applications.resnet50 <span class="keyword">import</span> ResNet50, preprocess_input</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing <span class="keyword">import</span> image    </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model   </span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretrained_path_to_tensor</span>(<span class="params">img_path</span>):</span></span><br><span class="line">    <span class="comment"># loads RGB image as PIL.Image.Image type</span></span><br><span class="line">    img = image.load_img(img_path, target_size=(<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    <span class="comment"># convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)</span></span><br><span class="line">    x = image.img_to_array(img)</span><br><span class="line">    <span class="comment"># convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor</span></span><br><span class="line">    x = np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># convert RGB -&gt; BGR, subtract mean ImageNet pixel, and return 4D tensor</span></span><br><span class="line">    <span class="keyword">return</span> preprocess_input(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ResNet</span>():</span></span><br><span class="line">    <span class="comment"># define ResNet50 model</span></span><br><span class="line">    model = ResNet50(weights=<span class="string">&#x27;imagenet&#x27;</span>)</span><br><span class="line">    <span class="comment"># model.summary()</span></span><br><span class="line">    <span class="comment"># get AMP layer weights (2048,1000)</span></span><br><span class="line">    all_amp_layer_weights = model.layers[-<span class="number">1</span>].get_weights()[<span class="number">0</span>] <span class="comment"># 这里get_weights()返回的是一个list,list[0]是kernel matrix,list[1]是bias</span></span><br><span class="line">    <span class="comment"># extract wanted output</span></span><br><span class="line">    ResNet_model = Model(inputs=model.<span class="built_in">input</span>, </span><br><span class="line">        outputs=(model.layers[-<span class="number">4</span>].output, model.layers[-<span class="number">1</span>].output)) </span><br><span class="line">    <span class="keyword">return</span> ResNet_model, all_amp_layer_weights</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet_CAM</span>(<span class="params">img_path, model, all_amp_layer_weights</span>):</span></span><br><span class="line">    <span class="comment"># get filtered images from convolutional output + model prediction vector</span></span><br><span class="line">    last_conv_output, pred_vec = model.predict(pretrained_path_to_tensor(img_path))</span><br><span class="line">    <span class="comment"># change dimensions of last convolutional outpu tto 7 x 7 x 2048</span></span><br><span class="line">    last_conv_output = np.squeeze(last_conv_output) </span><br><span class="line">    <span class="comment"># get model&#x27;s prediction (number between 0 and 999, inclusive)</span></span><br><span class="line">    pred = np.argmax(pred_vec)</span><br><span class="line">    <span class="comment"># bilinear upsampling to resize each filtered image to size of original image </span></span><br><span class="line">    mat_for_mult = scipy.ndimage.zoom(last_conv_output, (<span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>), order=<span class="number">1</span>) <span class="comment"># dim: 224 x 224 x 2048</span></span><br><span class="line">    <span class="comment"># get AMP layer weights</span></span><br><span class="line">    amp_layer_weights = all_amp_layer_weights[:, pred] <span class="comment"># dim: (2048,) </span></span><br><span class="line">    <span class="comment"># get class activation map for object class that is predicted to be in the image</span></span><br><span class="line">    final_output = np.dot(mat_for_mult.reshape((<span class="number">224</span>*<span class="number">224</span>, <span class="number">2048</span>)), amp_layer_weights).reshape(<span class="number">224</span>,<span class="number">224</span>) <span class="comment"># dim: 224 x 224</span></span><br><span class="line">    <span class="comment"># return class activation map</span></span><br><span class="line">    <span class="keyword">return</span> final_output, pred</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_ResNet_CAM</span>(<span class="params">img_path, ax, model, all_amp_layer_weights</span>):</span></span><br><span class="line">    <span class="comment"># load image, convert BGR --&gt; RGB, resize image to 224 x 224,</span></span><br><span class="line">    im = cv2.resize(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB), (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    <span class="comment"># plot image</span></span><br><span class="line">    ax.imshow(im, alpha=<span class="number">0.5</span>)</span><br><span class="line">    <span class="comment"># get class activation map</span></span><br><span class="line">    CAM, pred = ResNet_CAM(img_path, model, all_amp_layer_weights)</span><br><span class="line">    <span class="comment"># plot class activation map</span></span><br><span class="line">    ax.imshow(CAM, cmap=<span class="string">&#x27;jet&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">    <span class="comment"># load the dictionary that identifies each ImageNet category to an index in the prediction vector</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;imagenet1000_clsid_to_human.txt&#x27;</span>) <span class="keyword">as</span> imagenet_classes_file:</span><br><span class="line">        imagenet_classes_dict = ast.literal_eval(imagenet_classes_file.read())</span><br><span class="line">    <span class="comment"># obtain the predicted ImageNet category</span></span><br><span class="line">    ax.set_title(imagenet_classes_dict[pred]) </span><br><span class="line"></span><br><span class="line">ResNet_model, all_amp_layer_weights = get_ResNet()</span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">CAM = plot_ResNet_CAM(img_path, ax, ResNet_model, all_amp_layer_weights)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>Saliency Maps：see where the pixels that were most impactful to the
final classification were found.
显示的是哪些pixel会对最终的分类结果造成重大影响。</p>
<p>Grad-CAM Map：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gradModel = Model(inputs=[model.inputs],outputs[model.get_layer(layer_name).output,model.output])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">   <span class="comment"># cast the image tensor to a float-32 data type, pass the</span></span><br><span class="line">   <span class="comment"># forward propagate the image through the gradient model, and grab the loss</span></span><br><span class="line">   <span class="comment"># associated with the specific class index</span></span><br><span class="line">   inputs = tf.cast(img_array, tf.float32)</span><br><span class="line">   (convOutputs, predictions) = gradModel(inputs)</span><br><span class="line">   loss = predictions[:, <span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line"><span class="comment"># use automatic differentiation to compute the gradients</span></span><br><span class="line">grads = tape.gradient(loss, convOutputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute the guided gradients</span></span><br><span class="line">castConvOutputs = tf.cast(convOutputs &gt; <span class="number">0</span>, <span class="string">&quot;float32&quot;</span>)</span><br><span class="line">castGrads = tf.cast(grads &gt; <span class="number">0</span>, <span class="string">&quot;float32&quot;</span>)</span><br><span class="line">guidedGrads = castConvOutputs * castGrads * grads</span><br><span class="line"></span><br><span class="line"><span class="comment"># the convolution and guided gradients have a batch dimension</span></span><br><span class="line"><span class="comment"># (which we don&#x27;t need) so let&#x27;s grab the volume itself and</span></span><br><span class="line"><span class="comment"># discard the batch</span></span><br><span class="line">convOutputs = convOutputs[<span class="number">0</span>]</span><br><span class="line">guidedGrads = guidedGrads[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute the average of the gradient values, and using them</span></span><br><span class="line"><span class="comment"># as weights, compute the ponderation of the filters with</span></span><br><span class="line"><span class="comment"># respect to the weights</span></span><br><span class="line">weights = tf.reduce_mean(guidedGrads, axis=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># grab the spatial dimensions of the input image and resize</span></span><br><span class="line"><span class="comment"># the output class activation map to match the input image</span></span><br><span class="line"><span class="comment"># dimensions</span></span><br><span class="line">(w, h) = (img_array.shape[<span class="number">2</span>], img_array.shape[<span class="number">1</span>])</span><br><span class="line">heatmap = cv2.resize(cam.numpy(), (w, h))</span><br><span class="line"></span><br><span class="line"><span class="comment"># normalize the heatmap such that all values lie in the range</span></span><br><span class="line"><span class="comment"># [0, 1], scale the resulting values to the range [0, 255],</span></span><br><span class="line"><span class="comment"># and then convert to an unsigned 8-bit integer</span></span><br><span class="line">numer = heatmap - np.<span class="built_in">min</span>(heatmap)</span><br><span class="line">denom = (heatmap.<span class="built_in">max</span>() - heatmap.<span class="built_in">min</span>()) + eps</span><br><span class="line">heatmap = numer / denom</span><br></pre></td></tr></table></figure>
<h1 id="course-4-gans">Course 4: GANs</h1>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>卷积和反卷积中的output shape计算</title>
    <url>/2022/08/29/%E5%8D%B7%E7%A7%AF%E5%92%8C%E5%8F%8D%E5%8D%B7%E7%A7%AF%E4%B8%AD%E7%9A%84output-shape%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<h1 id="conv2d">Conv2D</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input_shape = (<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">3</span>)</span><br><span class="line">x = tf.random.normal(input_shape)</span><br><span class="line"></span><br><span class="line">y1 = tf.keras.layers.Conv2D(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&#x27;valid&#x27;</span>)(x) <span class="comment"># output_shape= (input_shape - filter_size + 1) * (input_shape - filter_size + 1)</span></span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">26</span>, <span class="number">26</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y2 = tf.keras.layers.Conv2D(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;same&quot;</span>)(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y3 = tf.keras.layers.Conv2D(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;valid&quot;</span>,strides=<span class="number">2</span>)(x) </span><br><span class="line"><span class="built_in">print</span>(y.shape) <span class="comment"># output_shape= [(input_shape - filter_size)/strides + 1] * [(input_shape - filter_size)/strides + 1]</span></span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">13</span>, <span class="number">13</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y4 = tf.keras.layers.Conv2D(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;same&quot;</span>,strides=<span class="number">2</span>)(x) </span><br><span class="line"><span class="built_in">print</span>(y.shape) <span class="comment"># output_shape= (input_shape - filter_size + 1) * (input_shape - filter_size + 1)</span></span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">14</span>, <span class="number">14</span>, <span class="number">2</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上例中输入是<code>(28,28,3)</code>的<code>shape</code>。
我们在推算<code>output shape</code>时要分两种情况：</p>
<ol type="1">
<li><code>padding="valid"</code>:
<code>output_shape= [(input_shape - filter_size)/strides + 1] * [(input_shape - filter_size)/strides + 1]</code>。如果<code>strides</code>除不尽，则向下取整（取比该数小的那个整数）。所以<code>y3</code>的<code>shape</code>是13</li>
<li><code>padding="same"</code>:
当<code>padding="same"</code>时计算很简单，得到的<code>output</code>的<code>shape</code>一定是<code>input_shape/strides</code>。所以当<code>strides=1</code>时，输入和输出的<code>shape</code>时相等的，比如上面的<code>y2</code>。</li>
</ol>
<p>如果需要更细节的原理可以参考<a href="https://towardsdatascience.com/understand-transposed-convolutions-and-build-your-own-transposed-convolution-layer-from-scratch-4f5d97b2967">博客</a>。这篇文章讲的特别好！重点推荐。</p>
<h1 id="conv2dtranspose">Conv2DTranspose</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y1 = tf.keras.layers.Conv2DTranspose(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;valid&quot;</span>)(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">30</span>, <span class="number">30</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y2 = tf.keras.layers.Conv2DTranspose(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;same&quot;</span>)(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y3 = tf.keras.layers.Conv2DTranspose(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;valid&quot;</span>,strides=<span class="number">2</span>)(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">57</span>, <span class="number">57</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y4 = tf.keras.layers.Conv2DTranspose(<span class="number">2</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape[<span class="number">1</span>:],padding=<span class="string">&quot;same&quot;</span>,strides=<span class="number">2</span>)(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">&gt;&gt;(<span class="number">4</span>, <span class="number">56</span>, <span class="number">56</span>, <span class="number">2</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>同样的总结下这个反卷积我们在推算<code>output shape</code>时也是分两种情况：
1. <code>padding="valid"</code>:
<code>output_length = input_length * stride + max(filter_size - stride, 0)</code>
2. <code>padding="same"</code>:
<code>output_shape=input_shape * strides</code>。当<code>strides=1</code>时，输出<code>shape</code>等于输入<code>shape</code></p>
<p>当<code>padding=valid</code>时的计算有点复杂。再细究一下的话，<code>tensorflow</code>中<code>Conv2DTranspose</code>还接受<code>output_padding</code>这个参数。如果有这个参数的话（默认是None），可以参考<code>tensorflow</code>官方文档给出的<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose">公式</a></p>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>image-segmentation图像分割中的数据读取和处理</title>
    <url>/2022/08/23/image-segmentation%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%92%8C%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<h1 id="用tensorflow.image">用tensorflow.image</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> tensorflow_io <span class="keyword">as</span> tfio</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_image</span>(<span class="params">image_path, mask=<span class="literal">False</span></span>):</span></span><br><span class="line">    image = tf.io.read_file(image_path)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> mask:</span><br><span class="line">        image = tf.image.decode_png(image, channels=<span class="number">1</span>)</span><br><span class="line">        image.set_shape([<span class="literal">None</span>, <span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])</span><br><span class="line">        image = tf.cast(image, tf.int32)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        image = tf.image.decode_png(image, channels=<span class="number">3</span>)</span><br><span class="line">        image.set_shape([<span class="literal">None</span>, <span class="literal">None</span>, <span class="number">3</span>])</span><br><span class="line">        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])</span><br><span class="line">        image = image / <span class="number">255.</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">image_list, mask_list</span>):</span></span><br><span class="line">    image = read_image(image_list)</span><br><span class="line">    mask  = read_image(mask_list, mask=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> image, mask</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_generator</span>(<span class="params">image_list, mask_list, split=<span class="string">&#x27;train&#x27;</span></span>):</span></span><br><span class="line">    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))</span><br><span class="line">    dataset = dataset.shuffle(<span class="number">8</span>*BATCH_SIZE) <span class="keyword">if</span> split == <span class="string">&#x27;train&#x27;</span> <span class="keyword">else</span> dataset </span><br><span class="line">    dataset = dataset.<span class="built_in">map</span>(load_data, num_parallel_calls=tf.data.AUTOTUNE)</span><br><span class="line">    dataset = dataset.batch(BATCH_SIZE, drop_remainder=<span class="literal">True</span>)</span><br><span class="line">    dataset = dataset.prefetch(tf.data.AUTOTUNE)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line">IMAGE_SIZE = <span class="number">128</span></span><br><span class="line">BATCH_SIZE = <span class="number">86</span></span><br><span class="line"></span><br><span class="line">train_dataset = data_generator(images, masks)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Train Dataset:&quot;</span>, train_dataset)</span><br><span class="line"></span><br><span class="line">input_data = os.path.join(root, <span class="string">&#x27;train_images&#x27;</span>)</span><br><span class="line">images = <span class="built_in">sorted</span>(</span><br><span class="line">    [</span><br><span class="line">        os.path.join(input_data, fname)</span><br><span class="line">        <span class="keyword">for</span> fname <span class="keyword">in</span> os.listdir(input_data)</span><br><span class="line">        <span class="keyword">if</span> fname.endswith(exts) <span class="keyword">and</span> <span class="keyword">not</span> fname.startswith(<span class="string">&quot;.&quot;</span>)</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">target_data = os.path.join(root, <span class="string">&#x27;train_masks&#x27;</span>)</span><br><span class="line">masks = <span class="built_in">sorted</span>(</span><br><span class="line">    [</span><br><span class="line">        os.path.join(target_data, fname)</span><br><span class="line">        <span class="keyword">for</span> fname <span class="keyword">in</span> os.listdir(target_data)</span><br><span class="line">        <span class="keyword">if</span> fname.endswith(exts) <span class="keyword">and</span> <span class="keyword">not</span> fname.startswith(<span class="string">&quot;.&quot;</span>)</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of samples:&quot;</span>, <span class="built_in">len</span>(images), <span class="built_in">len</span>(masks))</span><br><span class="line"><span class="keyword">for</span> input_path, target_path <span class="keyword">in</span> <span class="built_in">zip</span>(images[:<span class="number">10</span>], masks[:<span class="number">10</span>]):</span><br><span class="line">    <span class="built_in">print</span>(input_path[-<span class="number">32</span>:], <span class="string">&quot;|&quot;</span>, target_path[-<span class="number">31</span>:], <span class="string">&#x27;|&#x27;</span>, np.unique(cv2.imread(target_path)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以上方式传入data_generator的是image和mask所在图片路径.</p>
<h1 id="用cv2">用cv2</h1>
<p>这种方式适合图片不多的情况下使用，直接读入ndarry里存储。
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">train_img_paths = <span class="built_in">sorted</span>(glob(<span class="string">&#x27;../output/kaggle/working/train/*.jpg&#x27;</span>))[:SAMPLE]</span><br><span class="line">train_mask_paths = <span class="built_in">sorted</span>(glob(<span class="string">&#x27;../output/kaggle/working/train_masks/*.gif&#x27;</span>))[:SAMPLE]</span><br><span class="line"></span><br><span class="line">train_imgs = np.array([cv2.resize(imageio.imread(path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">                        <span class="keyword">for</span> path <span class="keyword">in</span> train_img_paths])</span><br><span class="line"></span><br><span class="line">train_masks = np.array([cv2.resize(imageio.imread(path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">                        <span class="keyword">for</span> path <span class="keyword">in</span> train_mask_paths])</span><br><span class="line"></span><br><span class="line">train_masks = train_masks.astype(np.float32)</span><br><span class="line">train_masks[train_masks&lt;=<span class="number">127</span>] = <span class="number">0.</span></span><br><span class="line">train_masks[train_masks&gt;<span class="number">127</span>] = <span class="number">1.</span></span><br><span class="line">train_masks = np.reshape(train_masks, (*train_masks.shape, <span class="number">1</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h1 id="用generator方式">用generator方式</h1>
<p>推荐用这种方式，占用内存小。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img_paths = <span class="built_in">sorted</span>(glob(<span class="string">&#x27;../output/kaggle/working/train/*.jpg&#x27;</span>))[:<span class="number">500</span>]</span><br><span class="line">mask_paths = <span class="built_in">sorted</span>(glob(<span class="string">&#x27;../output/kaggle/working/train_masks/*.gif&#x27;</span>))[:<span class="number">500</span>]</span><br><span class="line">train_img_files,val_img_files,train_mask_files,val_mask_files = train_test_split(img_paths,mask_paths,test_size=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_mask</span>(<span class="params">masks</span>):</span></span><br><span class="line">    masks[masks&lt;=<span class="number">127</span>] = <span class="number">0.</span></span><br><span class="line">    masks[masks&gt;<span class="number">127</span>] = <span class="number">1.</span></span><br><span class="line">    masks = masks.astype(np.float32)</span><br><span class="line">    masks = np.reshape(masks, (*masks.shape, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> masks</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_img_mask_gen</span>():</span></span><br><span class="line">    <span class="keyword">for</span> img_path,mask_path <span class="keyword">in</span> <span class="built_in">zip</span>(train_img_files,train_mask_files):</span><br><span class="line">        img = cv2.resize(imageio.imread(img_path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">        img = img / <span class="number">127.5</span></span><br><span class="line">        mask = cv2.resize(imageio.imread(mask_path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">        mask = process_mask(mask)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">yield</span> img, mask</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val_img_mask_gen</span>():</span></span><br><span class="line">    <span class="keyword">for</span> img_path,mask_path <span class="keyword">in</span> <span class="built_in">zip</span>(val_img_files,val_mask_files):</span><br><span class="line">        img = cv2.resize(imageio.imread(img_path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">        mask = cv2.resize(imageio.imread(mask_path), (IMG_ROWS, IMG_COLS))</span><br><span class="line">        img = img / <span class="number">127.5</span></span><br><span class="line">        mask = process_mask(mask)</span><br><span class="line">        <span class="keyword">yield</span> img, mask</span><br><span class="line"></span><br><span class="line">train_dataset = tf.data.Dataset.from_generator(train_img_mask_gen,</span><br><span class="line">                                              output_signature=(</span><br><span class="line">                                                    tf.TensorSpec(shape=(IMG_ROWS, IMG_COLS, <span class="number">3</span>), dtype=tf.float32),</span><br><span class="line">                                                    tf.TensorSpec(shape=(IMG_ROWS, IMG_COLS, <span class="number">1</span>), dtype=tf.float32))</span><br><span class="line">                                              )</span><br><span class="line">val_dataset = tf.data.Dataset.from_generator(val_img_mask_gen,</span><br><span class="line">                                             output_signature=(</span><br><span class="line">                                                    tf.TensorSpec(shape=(IMG_ROWS, IMG_COLS, <span class="number">3</span>), dtype=tf.float32),</span><br><span class="line">                                                    tf.TensorSpec(shape=(IMG_ROWS, IMG_COLS, <span class="number">1</span>), dtype=tf.float32))</span><br><span class="line">                                            )</span><br></pre></td></tr></table></figure>
上面这种方式需要创建一个generator函数，该函数不接受参数，如果需要传入参数可以另外新建一个函数，该函数会返回一个不接受任何参数的fun().比如：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img_masks_generator_from_files</span>(<span class="params">img_files, mask_files, sample_weights=<span class="literal">None</span>, width=<span class="number">512</span>, height=<span class="number">512</span>, shuffle=<span class="literal">True</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> sample_weights:</span><br><span class="line">        sample_weights = [[<span class="number">1</span>]] * <span class="built_in">len</span>(img_files) <span class="comment"># Make sample_weights equipped with a broadcastable shape when fed to a tf.Tensor</span></span><br><span class="line">    </span><br><span class="line">    sample_weights = np.array(sample_weights)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(sample_weights.shape) == <span class="number">1</span>:</span><br><span class="line">        sample_weights = sample_weights[..., np.newaxis] <span class="comment"># Make sample_weights equipped with a broadcastable shape when fed to a tf.Tensor</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(<span class="built_in">len</span>(sample_weights.shape) == <span class="number">2</span> <span class="keyword">and</span> sample_weights.shape[-<span class="number">1</span>] == <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        indexes = np.arange(<span class="built_in">len</span>(img_files))</span><br><span class="line">        np.random.shuffle(indexes)</span><br><span class="line">        img_files = [ img_files[i] <span class="keyword">for</span> i <span class="keyword">in</span> indexes ]</span><br><span class="line">        mask_files = [ mask_files[i] <span class="keyword">for</span> i <span class="keyword">in</span> indexes ]</span><br><span class="line">        sample_weights = [ sample_weights[i] <span class="keyword">for</span> i <span class="keyword">in</span> indexes ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f</span>():</span></span><br><span class="line">        <span class="keyword">for</span> img, masks, sample_weight <span class="keyword">in</span> <span class="built_in">zip</span>(img_files, mask_files, sample_weights):</span><br><span class="line">            <span class="keyword">yield</span> img_masks_from_file(img, masks, sample_weight)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> f</span><br><span class="line"></span><br><span class="line">train_img_masks_gen = img_masks_generator_from_files(train_img_files, train_mask_files, train_sample_weights, width=W, height=H)</span><br><span class="line"></span><br><span class="line">train_img_masks_dataset = tf.data.Dataset.from_generator(</span><br><span class="line">    train_img_masks_gen, </span><br><span class="line">    output_signature=(</span><br><span class="line">        tf.TensorSpec(shape=(W, H, <span class="number">1</span>), dtype=tf.float32),</span><br><span class="line">        tf.TensorSpec(shape=(W, H, <span class="built_in">len</span>(train_mask_files[<span class="number">0</span>])), dtype=tf.int32),</span><br><span class="line">        tf.TensorSpec(shape=(<span class="number">1</span>), dtype=tf.float32)</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
上面这种方式还可以换成另一种，在tensorflow.data.Dataset初始化时直接传递args参数:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ds_train = tf.data.Dataset.from_generator(noise_generator,args=[<span class="string">&#x27;train&#x27;</span>, mode],output_types=tf.int32,output_shapes=(<span class="literal">None</span>, <span class="literal">None</span>, n_channels))</span><br></pre></td></tr></table></figure>
<p>官方文档是这样写的：</p>
<blockquote>
<p>(Optional.) A tuple of <a href="https://www.tensorflow.org/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a>
objects that will be evaluated and passed to <code>generator</code> as
NumPy-array arguments.</p>
</blockquote>
<p>关于更多<code>tf.data.Dataset.from_generator</code>的用法可以参考<a href="https://vak.ai/tensorflow/TensorFlow2.0-dataset/">博客</a>。里面有一句话解答了我的疑惑：</p>
<blockquote>
<p>we need to have a python <a href="https://www.programiz.com/python-programming/generator">generator</a>
function which generates <strong>one</strong> training pair needed for
our model.</p>
</blockquote>
<p>这就是说我们在创建<code>generator</code>这个函数的时候，函数返回值应该是一个<code>training pair</code>，也就是<code>X</code>和<code>y</code></p>
<hr>
<p>visualize图片的可视化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span>(<span class="params">**images</span>):</span> <span class="comment"># **images是(key,item)的方式，*images是item list的方式</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;PLot images in one row.&quot;&quot;&quot;</span></span><br><span class="line">    n = <span class="built_in">len</span>(images)</span><br><span class="line">    plt.figure(figsize=(<span class="number">16</span>, <span class="number">5</span>))</span><br><span class="line">    <span class="keyword">for</span> i, (name, image) <span class="keyword">in</span> <span class="built_in">enumerate</span>(images.items()):</span><br><span class="line">        plt.subplot(<span class="number">1</span>, n, i + <span class="number">1</span>)</span><br><span class="line">        plt.xticks([])</span><br><span class="line">        plt.yticks([])</span><br><span class="line">        plt.title(<span class="string">&#x27; &#x27;</span>.join(name.split(<span class="string">&#x27;_&#x27;</span>)).title())</span><br><span class="line">        plt.imshow(image)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">image, mask = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataset.take(<span class="number">1</span>))) <span class="comment"># train_dataset</span></span><br><span class="line"><span class="comment"># image,mask = list(train_dataset.take(1)) </span></span><br><span class="line"><span class="built_in">print</span>(image.shape, mask.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (img, msk) <span class="keyword">in</span> <span class="built_in">zip</span>(image[:<span class="number">5</span>], mask[:<span class="number">5</span>]):</span><br><span class="line">    <span class="built_in">print</span>(mask.numpy().<span class="built_in">min</span>(), mask.numpy().<span class="built_in">max</span>())</span><br><span class="line">    visualize(</span><br><span class="line">        image=img.numpy(),</span><br><span class="line">        gt_mask=msk.numpy(), </span><br><span class="line">    )</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以下是mask标注大于两类（0或者1）的情况下可以观察数据的方式（需要被赋予color）
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Visualization Utilities</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># there are 11 classes in the dataset: one class for each digit (0 to 9) plus the background class</span></span><br><span class="line">n_classes = <span class="number">11</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># assign a random color for each class</span></span><br><span class="line">colors = [<span class="built_in">tuple</span>(np.random.randint(<span class="number">256</span>, size=<span class="number">3</span>) / <span class="number">255.0</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fuse_with_pil</span>(<span class="params">images</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Creates a blank image and pastes input images</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    images (list of numpy arrays) - numpy array representations of the images to paste</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    PIL Image object containing the images</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">  widths = (image.shape[<span class="number">1</span>] <span class="keyword">for</span> image <span class="keyword">in</span> images)</span><br><span class="line">  heights = (image.shape[<span class="number">0</span>] <span class="keyword">for</span> image <span class="keyword">in</span> images)</span><br><span class="line">  total_width = <span class="built_in">sum</span>(widths)</span><br><span class="line">  max_height = <span class="built_in">max</span>(heights)</span><br><span class="line"></span><br><span class="line">  new_im = PIL.Image.new(<span class="string">&#x27;RGB&#x27;</span>, (total_width, max_height))</span><br><span class="line"></span><br><span class="line">  x_offset = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> im <span class="keyword">in</span> images:</span><br><span class="line">    pil_image = PIL.Image.fromarray(np.uint8(im))</span><br><span class="line">    new_im.paste(pil_image, (x_offset,<span class="number">0</span>))</span><br><span class="line">    x_offset += im.shape[<span class="number">1</span>]</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> new_im</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">give_color_to_annotation</span>(<span class="params">annotation</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Converts a 2-D annotation to a numpy array with shape (height, width, 3) where</span></span><br><span class="line"><span class="string">  the third axis represents the color channel. The label values are multiplied by</span></span><br><span class="line"><span class="string">  255 and placed in this axis to give color to the annotation</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    annotation (numpy array) - label map array</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    the annotation array with an additional color channel/axis</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  seg_img = np.zeros( (annotation.shape[<span class="number">0</span>],annotation.shape[<span class="number">1</span>], <span class="number">3</span>) ).astype(<span class="string">&#x27;float&#x27;</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">    segc = (annotation == c)</span><br><span class="line">    seg_img[:,:,<span class="number">0</span>] += segc*( colors[c][<span class="number">0</span>] * <span class="number">255.0</span>)</span><br><span class="line">    seg_img[:,:,<span class="number">1</span>] += segc*( colors[c][<span class="number">1</span>] * <span class="number">255.0</span>)</span><br><span class="line">    seg_img[:,:,<span class="number">2</span>] += segc*( colors[c][<span class="number">2</span>] * <span class="number">255.0</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> seg_img</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_annotation_and_prediction</span>(<span class="params">image, annotation, prediction, iou_list, dice_score_list</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Displays the images with the ground truth and predicted label maps. Also overlays the metrics.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    image (numpy array) -- the input image</span></span><br><span class="line"><span class="string">    annotation (numpy array) -- the ground truth label map</span></span><br><span class="line"><span class="string">    prediction (numpy array) -- the predicted label map</span></span><br><span class="line"><span class="string">    iou_list (list of floats) -- the IOU values for each class</span></span><br><span class="line"><span class="string">    dice_score_list (list of floats) -- the Dice Score for each class</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">  new_ann = np.argmax(annotation, axis=<span class="number">2</span>)</span><br><span class="line">  true_img = give_color_to_annotation(new_ann)</span><br><span class="line">  pred_img = give_color_to_annotation(prediction)</span><br><span class="line"></span><br><span class="line">  image = image + <span class="number">1</span></span><br><span class="line">  image = image * <span class="number">127.5</span></span><br><span class="line">  image = np.reshape(image, (image.shape[<span class="number">0</span>], image.shape[<span class="number">1</span>],))</span><br><span class="line">  image = np.uint8(image)</span><br><span class="line">  images = [image, np.uint8(pred_img), np.uint8(true_img)]</span><br><span class="line"></span><br><span class="line">  metrics_by_id = [(idx, iou, dice_score) <span class="keyword">for</span> idx, (iou, dice_score) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(iou_list, dice_score_list)) <span class="keyword">if</span> iou &gt; <span class="number">0.0</span> <span class="keyword">and</span> idx &lt; <span class="number">10</span>]</span><br><span class="line">  metrics_by_id.sort(key=<span class="keyword">lambda</span> tup: tup[<span class="number">1</span>], reverse=<span class="literal">True</span>)  <span class="comment"># sorts in place</span></span><br><span class="line"></span><br><span class="line">  display_string_list = [<span class="string">&quot;&#123;&#125;: IOU: &#123;&#125; Dice Score: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(idx, iou, dice_score) <span class="keyword">for</span> idx, iou, dice_score <span class="keyword">in</span> metrics_by_id]</span><br><span class="line">  display_string = <span class="string">&quot;\n&quot;</span>.join(display_string_list)</span><br><span class="line"></span><br><span class="line">  plt.figure(figsize=(<span class="number">15</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> idx, im <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">3</span>, idx+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> idx == <span class="number">1</span>:</span><br><span class="line">      plt.xlabel(display_string)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.imshow(im)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_annotation_and_image</span>(<span class="params">image, annotation</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Displays the image and its annotation side by side</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    image (numpy array) -- the input image</span></span><br><span class="line"><span class="string">    annotation (numpy array) -- the label map</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  new_ann = np.argmax(annotation, axis=<span class="number">2</span>)</span><br><span class="line">  seg_img = give_color_to_annotation(new_ann)</span><br><span class="line">  </span><br><span class="line">  image = image + <span class="number">1</span></span><br><span class="line">  image = image * <span class="number">127.5</span></span><br><span class="line">  image = np.reshape(image, (image.shape[<span class="number">0</span>], image.shape[<span class="number">1</span>],))</span><br><span class="line"></span><br><span class="line">  image = np.uint8(image)</span><br><span class="line">  images = [image, seg_img]</span><br><span class="line">  </span><br><span class="line">  images = [image, seg_img]</span><br><span class="line">  fused_img = fuse_with_pil(images)</span><br><span class="line">  plt.imshow(fused_img)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">list_show_annotation</span>(<span class="params">dataset, num_images</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Displays images and its annotations side by side</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    dataset (tf Dataset) -- batch of images and annotations</span></span><br><span class="line"><span class="string">    num_images (int) -- number of images to display</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  ds = dataset.unbatch()</span><br><span class="line"></span><br><span class="line">  plt.figure(figsize=(<span class="number">20</span>, <span class="number">15</span>))</span><br><span class="line">  plt.title(<span class="string">&quot;Images And Annotations&quot;</span>)</span><br><span class="line">  plt.subplots_adjust(bottom=<span class="number">0.1</span>, top=<span class="number">0.9</span>, hspace=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> idx, (image, annotation) <span class="keyword">in</span> <span class="built_in">enumerate</span>(ds.take(num_images)):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">5</span>, idx + <span class="number">1</span>)</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    show_annotation_and_image(image.numpy(), annotation.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># get 10 images from the training set</span></span><br><span class="line">list_show_annotation(training_dataset, <span class="number">10</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h1 id="segmentation中checkpoint的设置">segmentation中checkpoint的设置</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DisplayCallback</span>(<span class="params">tf.keras.callbacks.Callback</span>):</span> <span class="comment"># 每间隔5个spoch显示一次结果</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dataset, epoch_interval=<span class="number">5</span></span>):</span></span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        self.epoch_interval = epoch_interval</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">display</span>(<span class="params">self, display_list, extra_title=<span class="string">&#x27;&#x27;</span></span>):</span></span><br><span class="line">        plt.figure(figsize=(<span class="number">15</span>, <span class="number">15</span>))</span><br><span class="line">        title = [<span class="string">&#x27;Input Image&#x27;</span>, <span class="string">&#x27;True Mask&#x27;</span>, <span class="string">&#x27;Predicted Mask&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(display_list) &gt; <span class="built_in">len</span>(title):</span><br><span class="line">            title.append(extra_title)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(display_list)):</span><br><span class="line">            plt.subplot(<span class="number">1</span>, <span class="built_in">len</span>(display_list), i+<span class="number">1</span>)</span><br><span class="line">            plt.title(title[i])</span><br><span class="line">            plt.imshow(display_list[i])</span><br><span class="line">            plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_mask</span>(<span class="params">self, pred_mask</span>):</span></span><br><span class="line">        pred_mask = (pred_mask &gt; <span class="number">0.5</span>).astype(<span class="string">&quot;int32&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> pred_mask[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show_predictions</span>(<span class="params">self, dataset, num=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="keyword">for</span> image, mask <span class="keyword">in</span> dataset.take(num):</span><br><span class="line">            pred_mask = model.predict(image)</span><br><span class="line">            self.display([image[<span class="number">0</span>], mask[<span class="number">0</span>], self.create_mask(pred_mask)])</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span>(<span class="params">self, epoch, logs=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> epoch <span class="keyword">and</span> epoch % self.epoch_interval == <span class="number">0</span>:</span><br><span class="line">            self.show_predictions(self.dataset)</span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&#x27;\nSample Prediction after epoch &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">30</span></span><br><span class="line">model.fit(</span><br><span class="line">    train_dataset, </span><br><span class="line">    epochs=epochs, </span><br><span class="line">    callbacks=[DisplayCallback(train_dataset)]</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>也需要有常规的checkpoint <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler</span><br><span class="line"></span><br><span class="line">filepath_dice_coeff = <span class="string">&quot;_val_loss.hdf5&quot;</span> <span class="comment"># 保存成一个单独的hdf5文件</span></span><br><span class="line">checkpointer = ModelCheckpoint(filepath_dice_coeff, monitor=<span class="string">&#x27;val_loss&#x27;</span>, verbose=<span class="number">1</span>, save_best_only=<span class="literal">True</span>, mode=<span class="string">&#x27;min&#x27;</span>)<span class="comment"># val_dice_coeff</span></span><br><span class="line">lr_reducer = ReduceLROnPlateau(factor=np.sqrt(<span class="number">0.1</span>), cooldown=<span class="number">0</span>, patience=<span class="number">30</span>, min_lr=<span class="number">0.5e-6</span>)</span><br><span class="line">early_stop = EarlyStopping(monitor=<span class="string">&#x27;val_loss&#x27;</span>, patience=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">hist = seg_classi_model.fit(train_data,</span><br><span class="line">                steps_per_epoch=(train_imgs.shape[<span class="number">0</span>] + batch_size - <span class="number">1</span>) // batch_size,</span><br><span class="line">                epochs=<span class="number">300</span>,</span><br><span class="line">                callbacks=[checkpointer, lr_reducer, early_stop],</span><br><span class="line">                validation_data=val_data,</span><br><span class="line">                validation_steps=(valid_imgs.shape[<span class="number">0</span>] + batch_size - <span class="number">1</span>) // batch_size) </span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>image-segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>image segmentation(图像分割)中的IOU,Dice的计算</title>
    <url>/2022/08/22/image-segmentation-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E4%B8%AD%E7%9A%84IOU-Dice%E7%9A%84%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<p>在图像分割任务中，通常需要将loss设置成dice或者IOU的值，这里总结一下他们的使用方式：</p>
<h1 id="二分类问题mask只有0或者1">二分类问题（mask只有0或者1）</h1>
<p>这种task网络的最后一层通常会加sigmoid激活函数，比如unet实现中最后一层就是一个卷积层：
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv10 = Conv2D(<span class="number">1</span>, (<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">&#x27;sigmoid&#x27;</span>)(conv9)</span><br></pre></td></tr></table></figure> 因此输出的feature
map中每一个pixel的值就是0~1之间的值。</p>
<p>dice loss可以如下计算： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> keras.losses <span class="keyword">import</span> binary_crossentropy</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line">SMOOTH = <span class="number">1.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dice_coef</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    y_true_f = K.flatten(y_true)</span><br><span class="line">    y_pred_f = K.flatten(y_pred)</span><br><span class="line">    intersection = K.<span class="built_in">sum</span>(y_true_f * y_pred_f)</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">2.</span> * intersection + SMOOTH) / (K.<span class="built_in">sum</span>(y_true_f) + K.<span class="built_in">sum</span>(y_pred_f) + SMOOTH)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iou_coef</span>(<span class="params">y_true, y_pred, smooth=<span class="number">1</span></span>):</span></span><br><span class="line">    intersection = K.<span class="built_in">sum</span>(K.<span class="built_in">abs</span>(y_true * y_pred), axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">    union = K.<span class="built_in">sum</span>(y_true,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])+K.<span class="built_in">sum</span>(y_pred,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])-intersection</span><br><span class="line">    iou = K.mean((intersection + smooth) / (union + smooth), axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> iou</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bce_dice_loss</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred) <span class="comment"># 这里也可以用 + (1-dice_coef)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(Adam(learning_rate=<span class="number">1e-4</span>),</span><br><span class="line">              bce_dice_loss,</span><br><span class="line">              metrics=[binary_crossentropy, dice_coef])</span><br></pre></td></tr></table></figure></p>
<h1 id="多分类问题mask有除了0和1以外的其他值">多分类问题（mask有除了0和1以外的其他值）</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">class_wise_metrics</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Computes the class-wise IOU and Dice Score.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    y_true (tensor) - ground truth label maps</span></span><br><span class="line"><span class="string">    y_pred (tensor) - predicted label maps</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  class_wise_iou = []</span><br><span class="line">  class_wise_dice_score = []</span><br><span class="line"></span><br><span class="line">  smoothing_factor = <span class="number">0.00001</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">    intersection = np.<span class="built_in">sum</span>((y_pred == i) * (y_true == i)) <span class="comment"># 计算true positive的pixel个数</span></span><br><span class="line">    y_true_area = np.<span class="built_in">sum</span>((y_true == i)) <span class="comment"># 计算pixcel=i的像素个数</span></span><br><span class="line">    y_pred_area = np.<span class="built_in">sum</span>((y_pred == i))</span><br><span class="line">    combined_area = y_true_area + y_pred_area</span><br><span class="line">    </span><br><span class="line">    iou = (intersection) / (combined_area - intersection + smoothing_factor)</span><br><span class="line">    class_wise_iou.append(iou)</span><br><span class="line">    </span><br><span class="line">    dice_score =  <span class="number">2</span> * ((intersection) / (combined_area + smoothing_factor))</span><br><span class="line">    class_wise_dice_score.append(dice_score)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> class_wise_iou, class_wise_dice_score</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在上述计算过程中需要注意的是，这里的y_pred是要经过np.argmax()的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results = model.predict(test_dataset, steps=test_steps)</span><br><span class="line"><span class="built_in">print</span>(results.shape) <span class="comment"># (192, 64, 84, 11)</span></span><br><span class="line"></span><br><span class="line">results = np.argmax(results, axis=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">integer_slider = <span class="number">105</span> <span class="comment"># 取第105个图片</span></span><br><span class="line">iou, dice_score = class_wise_metrics(np.argmax(y_true_segments[integer_slider], axis=<span class="number">3</span>), results[integer_slider]) </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>python 对Counter内容进行排序</title>
    <url>/2022/08/11/python-%E5%AF%B9Counter%E5%86%85%E5%AE%B9%E8%BF%9B%E8%A1%8C%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="对counter中的内容进行排序">对Counter中的内容进行排序</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">x = Counter(&#123;<span class="string">&#x27;a&#x27;</span>:<span class="number">5</span>, <span class="string">&#x27;b&#x27;</span>:<span class="number">3</span>, <span class="string">&#x27;c&#x27;</span>:<span class="number">7</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line">x.most_common() <span class="comment"># [(&#x27;c&#x27;, 7), (&#x27;a&#x27;, 5), (&#x27;b&#x27;, 3)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="built_in">sorted</span>(x, key=x.get, reverse=<span class="literal">True</span>) <span class="comment"># [&#x27;c&#x27;, &#x27;a&#x27;, &#x27;b&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"><span class="built_in">sorted</span>(x.items(), key=<span class="keyword">lambda</span> pair: pair[<span class="number">1</span>], reverse=<span class="literal">True</span>) <span class="comment"># [(&#x27;c&#x27;, 7), (&#x27;a&#x27;, 5), (&#x27;b&#x27;, 3)]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>一共三种方法，其中第二种方法中传入<code>key=x.get</code>最终返回的是所有的key，而其他两种方法返回的都是排完序之后的list</p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>图像的读取方式</title>
    <url>/2022/06/21/%E5%9B%BE%E5%83%8F%E7%9A%84%E8%AF%BB%E5%8F%96%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<h1 id="pil">PIL</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&quot;1.jpg&quot;</span>)</span><br><span class="line">img.load()</span><br><span class="line"></span><br><span class="line">array = np.asarrary(img)</span><br><span class="line"></span><br><span class="line">img.show() <span class="comment"># view the picture</span></span><br><span class="line"></span><br><span class="line">img.save(<span class="string">&quot;./new_img.jpg&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>PIL.Image包有很多其他的功能，比如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从array生成图片convert it to a Pillow image</span></span><br><span class="line">Image.fromarray(data,<span class="string">&#x27;RGB&#x27;</span>) <span class="comment"># 如果mode=&#x27;L&#x27;,那么只有一个通道</span></span><br></pre></td></tr></table></figure>
<h1 id="cv2">cv2</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">&quot;./1.jpg&quot;</span>) <span class="comment"># 返回的img的channel顺序是BGR</span></span><br><span class="line"></span><br><span class="line">grey_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) <span class="comment"># 转化为灰度图的模式</span></span><br><span class="line">rgb_img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) <span class="comment"># 转化为RGB的模式</span></span><br></pre></td></tr></table></figure>
<p><code>cv2</code>一个有用的method是resize</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv2.resize(raw_img,(width,height))</span><br></pre></td></tr></table></figure>
<h1 id="tensorflow">tensorflow</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">img = tf.keras.preprocessing.image.load_img(<span class="string">&quot;./i.jpg&quot;</span>,target_size=(<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>)) <span class="comment"># Loads an image into PIL format</span></span><br><span class="line">img = tf.keras.preprocessing.image.img_to_array(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从array转化成PIL image 实例</span></span><br><span class="line">img = tf.keras.preprocessing.image.array_to_img(array) <span class="comment"># array是3D numpy array</span></span><br><span class="line"></span><br><span class="line">tf.keras.utils.save_img(path,array)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>python 全角转半角</title>
    <url>/2022/04/15/python-%E5%85%A8%E8%A7%92%E8%BD%AC%E5%8D%8A%E8%A7%92/</url>
    <content><![CDATA[<p>全角与半角转换在处理汉语语料中会经常出现，这里分别说明汉字、数字、字母的unicode编码范围。以及全角与半角的转换方法。最后给出wiki上全角和半角的编码对照表。这里Python需要用Python3版本。</p>
<h3 id="汉字的判断">汉字的判断</h3>
<p>汉字的unicode编码范围 u4e00 到 u9fa5。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_chinese</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断一个unicode是否是汉字&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> uchar &gt;= <span class="string">u&#x27;\u4e00&#x27;</span> <span class="keyword">and</span> uchar&lt;=<span class="string">u&#x27;\u9fa5&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="数字0-9的判断">数字0-9的判断</h3>
<p>数字的unicode编码范围根据全角和半角，有两个不同区域，半角数字 u0030
到 u0039，全角数字 uff10 到 uff19。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_number</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断一个unicode是否是半角数字&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> uchar &gt;= <span class="string">u&#x27;\u0030&#x27;</span> <span class="keyword">and</span> uchar&lt;=<span class="string">u&#x27;\u0039&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_Qnumber</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断一个unicode是否是全角数字&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> uchar &gt;= <span class="string">u&#x27;\uff10&#x27;</span> <span class="keyword">and</span> uchar &lt;= <span class="string">u&#x27;\uff19&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="大小写字母判断">大小写字母判断</h3>
<p>字母的unicode编码根据字母大小写，以及全角和半角共有四个区域。
半角大写字母：u0041 - u005a ，半角小写字母：u0061 - u007a ；
全角大写字母：uff21 - uff3a ， 全角小写字母：uff41 - uff5a 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_alphabet</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断一个unicode是否是半角英文字母&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> (uchar &gt;= <span class="string">u&#x27;\u0041&#x27;</span> <span class="keyword">and</span> uchar &lt;= <span class="string">u&#x27;\u005a&#x27;</span>) <span class="keyword">or</span> (uchar &gt;= <span class="string">u&#x27;\u0061&#x27;</span> <span class="keyword">and</span> uchar &lt;= <span class="string">u&#x27;\u007a&#x27;</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_Qalphabet</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断一个unicode是否是全角英文字母&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> (uchar &gt;= <span class="string">u&#x27;\uff21&#x27;</span> <span class="keyword">and</span> uchar &lt;= <span class="string">u&#x27;\uff3a&#x27;</span>) <span class="keyword">or</span> (uchar &gt;= <span class="string">u&#x27;\uff41&#x27;</span> <span class="keyword">and</span> uchar &lt;= <span class="string">u&#x27;\uff5a&#x27;</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="非汉字和数字字母的判断">非汉字和数字字母的判断</h3>
<p>判断除汉字、数字0-9、字母之外的字符。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_other</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;判断是否非汉字，数字和英文字符&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> (is_chinese(uchar) <span class="keyword">or</span> is_number(uchar) <span class="keyword">or</span> is_alphabet(uchar)):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="全角和半角的转换">全角和半角的转换</h3>
<p>全角半角转换需要用到上面的数字、字母等判断。</p>
<ol type="1">
<li>所有半角转全角，不是半角范围直接返回，空格半角特殊单独处理，其它半角和全角对应公式：半角
= 全角 - 0xfee0</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">B2Q</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;单个字符 半角转全角&quot;&quot;&quot;</span></span><br><span class="line">    inside_code = <span class="built_in">ord</span>(uchar)</span><br><span class="line">    <span class="keyword">if</span> inside_code &lt; <span class="number">0x0020</span> <span class="keyword">or</span> inside_code &gt; <span class="number">0x7e</span>: <span class="comment"># 不是半角字符就返回原来的字符</span></span><br><span class="line">        <span class="keyword">return</span> uchar </span><br><span class="line">    <span class="keyword">if</span> inside_code == <span class="number">0x0020</span>: <span class="comment"># 除了空格其他的全角半角的公式为: 半角 = 全角 - 0xfee0</span></span><br><span class="line">        inside_code = <span class="number">0x3000</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        inside_code += <span class="number">0xfee0</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">chr</span>(inside_code)</span><br></pre></td></tr></table></figure>
<ol type="1">
<li>所有全角转半角，和前面正好相反，公式对应：全角 = 半角 + 0xfee0</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Q2B</span>(<span class="params">uchar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;单个字符 全角转半角&quot;&quot;&quot;</span></span><br><span class="line">    inside_code = <span class="built_in">ord</span>(uchar)</span><br><span class="line">    <span class="keyword">if</span> inside_code == <span class="number">0x3000</span>:</span><br><span class="line">        inside_code = <span class="number">0x0020</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        inside_code -= <span class="number">0xfee0</span></span><br><span class="line">    <span class="keyword">if</span> inside_code &lt; <span class="number">0x0020</span> <span class="keyword">or</span> inside_code &gt; <span class="number">0x7e</span>: <span class="comment">#转完之后不是半角字符返回原来的字符</span></span><br><span class="line">        <span class="keyword">return</span> uchar</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">chr</span>(inside_code)</span><br></pre></td></tr></table></figure>
<ol type="1">
<li>把整个字符串全角转半角，也可以只转部分如数字和字母</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stringQ2B</span>(<span class="params">ustring</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;把字符串全角转半角&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join([Q2B(uchar) <span class="keyword">for</span> uchar <span class="keyword">in</span> ustring])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stringpartQ2B</span>(<span class="params">ustring</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;把字符串中数字和字母全角转半角&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join([Q2B(uchar) <span class="keyword">if</span> is_Qnumber(uchar) <span class="keyword">or</span> is_Qalphabet(uchar) <span class="keyword">else</span> uchar <span class="keyword">for</span> uchar <span class="keyword">in</span> ustring])</span><br></pre></td></tr></table></figure>
<p>测试上面的全角半角转换。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">text = <span class="string">&quot;电影《２０１２》讲述了２０１２年１２月２１日的世界末日，主人公Ｊａｃｋ以及世界各国人民挣扎求生的经历，灾难面前，尽现人间百态。&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;text原文：&quot;</span>, text, sep=<span class="string">&quot;\n&quot;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">text1 = stringQ2B(text)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;全角转半角：&quot;</span>, text1, sep=<span class="string">&quot;\n&quot;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">text2 = stringpartQ2B(text)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数字字母全角转半角：&quot;</span>, text2, sep=<span class="string">&quot;\n&quot;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果如下，只转数字字母与全部转是有区别的：</p>
<figure>
<img src="https:////upload-images.jianshu.io/upload_images/17669901-55fb5ccee96dcf60.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/980/format/webp" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>全角半角转换结果</p>
<h3 id="附全角和半角编码对应表">附全角和半角编码对应表</h3>
<ol type="1">
<li>ASCII内字符的全角和半角，包括数字0-9、大小写字母、标点符号等。</li>
</ol>
<table>
<thead>
<tr class="header">
<th>ASCII</th>
<th>全角字符</th>
<th>Unicode</th>
<th>半角字符</th>
<th>Unicode</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0x20</td>
<td>"　"</td>
<td>U+3000</td>
<td>" "</td>
<td>U+0020</td>
</tr>
<tr class="even">
<td>0x21</td>
<td>！</td>
<td>U+FF01</td>
<td>!</td>
<td>U+0021</td>
</tr>
<tr class="odd">
<td>0x22</td>
<td>＂</td>
<td>U+FF02</td>
<td>"</td>
<td>U+0022</td>
</tr>
<tr class="even">
<td>0x23</td>
<td>＃</td>
<td>U+FF03</td>
<td>#</td>
<td>U+0023</td>
</tr>
<tr class="odd">
<td>0x24</td>
<td>＄</td>
<td>U+FF04</td>
<td>$</td>
<td>U+0024</td>
</tr>
<tr class="even">
<td>0x25</td>
<td>％</td>
<td>U+FF05</td>
<td>%</td>
<td>U+0025</td>
</tr>
<tr class="odd">
<td>0x26</td>
<td>＆</td>
<td>U+FF06</td>
<td>&amp;</td>
<td>U+0026</td>
</tr>
<tr class="even">
<td>0x27</td>
<td>＇</td>
<td>U+FF07</td>
<td>'</td>
<td>U+0027</td>
</tr>
<tr class="odd">
<td>0x28</td>
<td>（</td>
<td>U+FF08</td>
<td>(</td>
<td>U+0028</td>
</tr>
<tr class="even">
<td>0x29</td>
<td>）</td>
<td>U+FF09</td>
<td>)</td>
<td>U+0029</td>
</tr>
<tr class="odd">
<td>0x2A</td>
<td>＊</td>
<td>U+FF0A</td>
<td>*</td>
<td>U+002A</td>
</tr>
<tr class="even">
<td>0x2B</td>
<td>＋</td>
<td>U+FF0B</td>
<td>+</td>
<td>U+002B</td>
</tr>
<tr class="odd">
<td>0x2C</td>
<td>，</td>
<td>U+FF0C</td>
<td>,</td>
<td>U+002C</td>
</tr>
<tr class="even">
<td>0x2D</td>
<td>－</td>
<td>U+FF0D</td>
<td>-</td>
<td>U+002D</td>
</tr>
<tr class="odd">
<td>0x2E</td>
<td>．</td>
<td>U+FF0E</td>
<td>.</td>
<td>U+002E</td>
</tr>
<tr class="even">
<td>0x2F</td>
<td>／</td>
<td>U+FF0F</td>
<td>/</td>
<td>U+002F</td>
</tr>
<tr class="odd">
<td>0x30</td>
<td>０</td>
<td>U+FF10</td>
<td>0</td>
<td>U+0030</td>
</tr>
<tr class="even">
<td>0x31</td>
<td>１</td>
<td>U+FF11</td>
<td>1</td>
<td>U+0031</td>
</tr>
<tr class="odd">
<td>0x32</td>
<td>２</td>
<td>U+FF12</td>
<td>2</td>
<td>U+0032</td>
</tr>
<tr class="even">
<td>0x33</td>
<td>３</td>
<td>U+FF13</td>
<td>3</td>
<td>U+0033</td>
</tr>
<tr class="odd">
<td>0x34</td>
<td>４</td>
<td>U+FF14</td>
<td>4</td>
<td>U+0034</td>
</tr>
<tr class="even">
<td>0x35</td>
<td>５</td>
<td>U+FF15</td>
<td>5</td>
<td>U+0035</td>
</tr>
<tr class="odd">
<td>0x36</td>
<td>６</td>
<td>U+FF16</td>
<td>6</td>
<td>U+0036</td>
</tr>
<tr class="even">
<td>0x37</td>
<td>７</td>
<td>U+FF17</td>
<td>7</td>
<td>U+0037</td>
</tr>
<tr class="odd">
<td>0x38</td>
<td>８</td>
<td>U+FF18</td>
<td>8</td>
<td>U+0038</td>
</tr>
<tr class="even">
<td>0x39</td>
<td>９</td>
<td>U+FF19</td>
<td>9</td>
<td>U+0039</td>
</tr>
<tr class="odd">
<td>0x3A</td>
<td>：</td>
<td>U+FF1A</td>
<td>:</td>
<td>U+003A</td>
</tr>
<tr class="even">
<td>0x3B</td>
<td>；</td>
<td>U+FF1B</td>
<td>;</td>
<td>U+003B</td>
</tr>
<tr class="odd">
<td>0x3C</td>
<td>＜</td>
<td>U+FF1C</td>
<td>&lt;</td>
<td>U+003C</td>
</tr>
<tr class="even">
<td>0x3D</td>
<td>＝</td>
<td>U+FF1D</td>
<td>=</td>
<td>U+003D</td>
</tr>
<tr class="odd">
<td>0x3E</td>
<td>＞</td>
<td>U+FF1E</td>
<td>&gt;</td>
<td>U+003E</td>
</tr>
<tr class="even">
<td>0x3F</td>
<td>？</td>
<td>U+FF1F</td>
<td>?</td>
<td>U+003F</td>
</tr>
<tr class="odd">
<td>0x40</td>
<td>＠</td>
<td>U+FF20</td>
<td>@</td>
<td>U+0040</td>
</tr>
<tr class="even">
<td>0x41</td>
<td>Ａ</td>
<td>U+FF21</td>
<td>A</td>
<td>U+0041</td>
</tr>
<tr class="odd">
<td>0x42</td>
<td>Ｂ</td>
<td>U+FF22</td>
<td>B</td>
<td>U+0042</td>
</tr>
<tr class="even">
<td>0x43</td>
<td>Ｃ</td>
<td>U+FF23</td>
<td>C</td>
<td>U+0043</td>
</tr>
<tr class="odd">
<td>0x44</td>
<td>Ｄ</td>
<td>U+FF24</td>
<td>D</td>
<td>U+0044</td>
</tr>
<tr class="even">
<td>0x45</td>
<td>Ｅ</td>
<td>U+FF25</td>
<td>E</td>
<td>U+0045</td>
</tr>
<tr class="odd">
<td>0x46</td>
<td>Ｆ</td>
<td>U+FF26</td>
<td>F</td>
<td>U+0046</td>
</tr>
<tr class="even">
<td>0x47</td>
<td>Ｇ</td>
<td>U+FF27</td>
<td>G</td>
<td>U+0047</td>
</tr>
<tr class="odd">
<td>0x48</td>
<td>Ｈ</td>
<td>U+FF28</td>
<td>H</td>
<td>U+0048</td>
</tr>
<tr class="even">
<td>0x49</td>
<td>Ｉ</td>
<td>U+FF29</td>
<td>I</td>
<td>U+0049</td>
</tr>
<tr class="odd">
<td>0x4A</td>
<td>Ｊ</td>
<td>U+FF2A</td>
<td>J</td>
<td>U+004A</td>
</tr>
<tr class="even">
<td>0x4B</td>
<td>Ｋ</td>
<td>U+FF2B</td>
<td>K</td>
<td>U+004B</td>
</tr>
<tr class="odd">
<td>0x4C</td>
<td>Ｌ</td>
<td>U+FF2C</td>
<td>L</td>
<td>U+004C</td>
</tr>
<tr class="even">
<td>0x4D</td>
<td>Ｍ</td>
<td>U+FF2D</td>
<td>M</td>
<td>U+004D</td>
</tr>
<tr class="odd">
<td>0x4E</td>
<td>Ｎ</td>
<td>U+FF2E</td>
<td>N</td>
<td>U+004E</td>
</tr>
<tr class="even">
<td>0x4F</td>
<td>Ｏ</td>
<td>U+FF2F</td>
<td>O</td>
<td>U+004F</td>
</tr>
<tr class="odd">
<td>0x50</td>
<td>Ｐ</td>
<td>U+FF30</td>
<td>P</td>
<td>U+0050</td>
</tr>
<tr class="even">
<td>0x51</td>
<td>Ｑ</td>
<td>U+FF31</td>
<td>Q</td>
<td>U+0051</td>
</tr>
<tr class="odd">
<td>0x52</td>
<td>Ｒ</td>
<td>U+FF32</td>
<td>R</td>
<td>U+0052</td>
</tr>
<tr class="even">
<td>0x53</td>
<td>Ｓ</td>
<td>U+FF33</td>
<td>S</td>
<td>U+0053</td>
</tr>
<tr class="odd">
<td>0x54</td>
<td>Ｔ</td>
<td>U+FF34</td>
<td>T</td>
<td>U+0054</td>
</tr>
<tr class="even">
<td>0x55</td>
<td>Ｕ</td>
<td>U+FF35</td>
<td>U</td>
<td>U+0055</td>
</tr>
<tr class="odd">
<td>0x56</td>
<td>Ｖ</td>
<td>U+FF36</td>
<td>V</td>
<td>U+0056</td>
</tr>
<tr class="even">
<td>0x57</td>
<td>Ｗ</td>
<td>U+FF37</td>
<td>W</td>
<td>U+0057</td>
</tr>
<tr class="odd">
<td>0x58</td>
<td>Ｘ</td>
<td>U+FF38</td>
<td>X</td>
<td>U+0058</td>
</tr>
<tr class="even">
<td>0x59</td>
<td>Ｙ</td>
<td>U+FF39</td>
<td>Y</td>
<td>U+0059</td>
</tr>
<tr class="odd">
<td>0x5A</td>
<td>Ｚ</td>
<td>U+FF3A</td>
<td>Z</td>
<td>U+005A</td>
</tr>
<tr class="even">
<td>0x5B</td>
<td>［</td>
<td>U+FF3B</td>
<td>[</td>
<td>U+005B</td>
</tr>
<tr class="odd">
<td>0x5C</td>
<td>＼</td>
<td>U+FF3C</td>
<td>\</td>
<td>U+005C</td>
</tr>
<tr class="even">
<td>0x5D</td>
<td>］</td>
<td>U+FF3D</td>
<td>]</td>
<td>U+005D</td>
</tr>
<tr class="odd">
<td>0x5E</td>
<td>＾</td>
<td>U+FF3E</td>
<td>^</td>
<td>U+005E</td>
</tr>
<tr class="even">
<td>0x5F</td>
<td>＿</td>
<td>U+FF3F</td>
<td>_</td>
<td>U+005F</td>
</tr>
<tr class="odd">
<td>0x60</td>
<td>｀</td>
<td>U+FF40</td>
<td>`</td>
<td>U+0060</td>
</tr>
<tr class="even">
<td>0x61</td>
<td>ａ</td>
<td>U+FF41</td>
<td>a</td>
<td>U+0061</td>
</tr>
<tr class="odd">
<td>0x62</td>
<td>ｂ</td>
<td>U+FF42</td>
<td>b</td>
<td>U+0062</td>
</tr>
<tr class="even">
<td>0x63</td>
<td>ｃ</td>
<td>U+FF43</td>
<td>c</td>
<td>U+0063</td>
</tr>
<tr class="odd">
<td>0x64</td>
<td>ｄ</td>
<td>U+FF44</td>
<td>d</td>
<td>U+0064</td>
</tr>
<tr class="even">
<td>0x65</td>
<td>ｅ</td>
<td>U+FF45</td>
<td>e</td>
<td>U+0065</td>
</tr>
<tr class="odd">
<td>0x66</td>
<td>ｆ</td>
<td>U+FF46</td>
<td>f</td>
<td>U+0066</td>
</tr>
<tr class="even">
<td>0x67</td>
<td>ｇ</td>
<td>U+FF47</td>
<td>g</td>
<td>U+0067</td>
</tr>
<tr class="odd">
<td>0x68</td>
<td>ｈ</td>
<td>U+FF48</td>
<td>h</td>
<td>U+0068</td>
</tr>
<tr class="even">
<td>0x69</td>
<td>ｉ</td>
<td>U+FF49</td>
<td>i</td>
<td>U+0069</td>
</tr>
<tr class="odd">
<td>0x6A</td>
<td>ｊ</td>
<td>U+FF4A</td>
<td>j</td>
<td>U+006A</td>
</tr>
<tr class="even">
<td>0x6B</td>
<td>ｋ</td>
<td>U+FF4B</td>
<td>k</td>
<td>U+006B</td>
</tr>
<tr class="odd">
<td>0x6C</td>
<td>ｌ</td>
<td>U+FF4C</td>
<td>l</td>
<td>U+006C</td>
</tr>
<tr class="even">
<td>0x6D</td>
<td>ｍ</td>
<td>U+FF4D</td>
<td>m</td>
<td>U+006D</td>
</tr>
<tr class="odd">
<td>0x6E</td>
<td>ｎ</td>
<td>U+FF4E</td>
<td>n</td>
<td>U+006E</td>
</tr>
<tr class="even">
<td>0x6F</td>
<td>ｏ</td>
<td>U+FF4F</td>
<td>o</td>
<td>U+006F</td>
</tr>
<tr class="odd">
<td>0x70</td>
<td>ｐ</td>
<td>U+FF50</td>
<td>p</td>
<td>U+0070</td>
</tr>
<tr class="even">
<td>0x71</td>
<td>ｑ</td>
<td>U+FF51</td>
<td>q</td>
<td>U+0071</td>
</tr>
<tr class="odd">
<td>0x72</td>
<td>ｒ</td>
<td>U+FF52</td>
<td>r</td>
<td>U+0072</td>
</tr>
<tr class="even">
<td>0x73</td>
<td>ｓ</td>
<td>U+FF53</td>
<td>s</td>
<td>U+0073</td>
</tr>
<tr class="odd">
<td>0x74</td>
<td>ｔ</td>
<td>U+FF54</td>
<td>t</td>
<td>U+0074</td>
</tr>
<tr class="even">
<td>0x75</td>
<td>ｕ</td>
<td>U+FF55</td>
<td>u</td>
<td>U+0075</td>
</tr>
<tr class="odd">
<td>0x76</td>
<td>ｖ</td>
<td>U+FF56</td>
<td>v</td>
<td>U+0076</td>
</tr>
<tr class="even">
<td>0x77</td>
<td>ｗ</td>
<td>U+FF57</td>
<td>w</td>
<td>U+0077</td>
</tr>
<tr class="odd">
<td>0x78</td>
<td>ｘ</td>
<td>U+FF58</td>
<td>x</td>
<td>U+0078</td>
</tr>
<tr class="even">
<td>0x79</td>
<td>ｙ</td>
<td>U+FF59</td>
<td>y</td>
<td>U+0079</td>
</tr>
<tr class="odd">
<td>0x7A</td>
<td>ｚ</td>
<td>U+FF5A</td>
<td>z</td>
<td>U+007A</td>
</tr>
<tr class="even">
<td>0x7B</td>
<td>｛</td>
<td>U+FF5B</td>
<td>{</td>
<td>U+007B</td>
</tr>
<tr class="odd">
<td>0x7C</td>
<td>｜</td>
<td>U+FF5C</td>
<td>|</td>
<td>U+007C</td>
</tr>
<tr class="even">
<td>0x7D</td>
<td>｝</td>
<td>U+FF5D</td>
<td>}</td>
<td>U+007D</td>
</tr>
<tr class="odd">
<td>0x7E</td>
<td>～</td>
<td>U+FF5E</td>
<td>~</td>
<td>U+007E</td>
</tr>
</tbody>
</table>
<ol type="1">
<li>其它特殊字符的全角和半角</li>
</ol>
<table>
<thead>
<tr class="header">
<th>半角字符</th>
<th>Unicode</th>
<th>全角字符</th>
<th>Unicode</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>⦅</td>
<td>U+2985</td>
<td>｟</td>
<td>U+FF5F</td>
</tr>
<tr class="even">
<td>⦆</td>
<td>U+2986</td>
<td>｠</td>
<td>U+FF60</td>
</tr>
<tr class="odd">
<td>¢</td>
<td>U+00A2</td>
<td>￠</td>
<td>U+FFE0</td>
</tr>
<tr class="even">
<td>£</td>
<td>U+00A3</td>
<td>￡</td>
<td>U+FFE1</td>
</tr>
<tr class="odd">
<td>¬</td>
<td>U+00AC</td>
<td>￢</td>
<td>U+FFE2</td>
</tr>
<tr class="even">
<td>¯</td>
<td>U+00AF</td>
<td>￣</td>
<td>U+FFE3</td>
</tr>
<tr class="odd">
<td>¦</td>
<td>U+00A6</td>
<td>￤</td>
<td>U+FFE4</td>
</tr>
<tr class="even">
<td>¥</td>
<td>U+00A5</td>
<td>￥</td>
<td>U+FFE5</td>
</tr>
<tr class="odd">
<td>₩</td>
<td>U+20A9</td>
<td>￦</td>
<td>U+FFE6</td>
</tr>
<tr class="even">
<td>￨</td>
<td>U+FFE8</td>
<td>│</td>
<td>U+2502</td>
</tr>
<tr class="odd">
<td>￩</td>
<td>U+FFE9</td>
<td>←</td>
<td>U+2190</td>
</tr>
<tr class="even">
<td>￪</td>
<td>U+FFEA</td>
<td>↑</td>
<td>U+2191</td>
</tr>
<tr class="odd">
<td>￫</td>
<td>U+FFEB</td>
<td>→</td>
<td>U+2192</td>
</tr>
<tr class="even">
<td>￬</td>
<td>U+FFEC</td>
<td>↓</td>
<td>U+2193</td>
</tr>
<tr class="odd">
<td>￭</td>
<td>U+FFED</td>
<td>■</td>
<td>U+25A0</td>
</tr>
<tr class="even">
<td>￮</td>
<td>U+FFEE</td>
<td>○</td>
<td>U+25CB</td>
</tr>
</tbody>
</table>
<h3 id="参考">参考</h3>
<p>[1]. <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fzh.wikipedia.org%2Fwiki%2F%E5%85%A8%E5%BD%A2%E5%92%8C%E5%8D%8A%E5%BD%A2">https://zh.wikipedia.org/wiki/%E5%85%A8%E5%BD%A2%E5%92%8C%E5%8D%8A%E5%BD%A2</a>
[2]. <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fwww.voidcn.com%2Farticle%2Fp-njiniuhl-nu.html">http://www.voidcn.com/article/p-njiniuhl-nu.html</a>
[3]. https://www.jianshu.com/p/152e081fec1b</p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>ai and machinelearning for coders book notes</title>
    <url>/2022/04/14/ai-and-machinelearning-for-coders-book-notes/</url>
    <content><![CDATA[<p>该篇为阅读 AI and Machine Learing for Coders
的阅读笔记，相应的课程内容可以移步我另一篇博客《DeepLearning.AI-Tensorfow-Developer-Course课程整理知识点》。该书没有中文版，英文版读起来并不困难，面向的读者是有编程基础并掌握机器学习和深度学习算法理论知识的读者。此篇为本人看这本书时的阅读笔记，并不是全文翻译。主要目的是为了以后当做手册查看一些用法。</p>
<h1 id="chapter-5-nlp-intro">Chapter 5 NLP Intro</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(num_words = <span class="number">100</span>, oov_token=<span class="string">&quot;&lt;OOV&gt;&quot;</span>) <span class="comment"># 取出现频率最高的100个词</span></span><br><span class="line">tokenizer.fit_on_texts(sentences)</span><br><span class="line">word_index = tokenizer.word_index <span class="comment"># &#123;&#x27;&lt;OOV&gt;&#x27;: 1, &#x27;today&#x27;: 2, &#x27;is&#x27;: 3, &#x27;a&#x27;: 4, &#x27;sunny&#x27;: 5, &#x27;day&#x27;: 6, &#x27;rainy&#x27;: 7, &#x27;it&#x27;: 8&#125;</span></span><br><span class="line">sequences = tokenizer.texts_to_sequences(sentences) <span class="comment"># [[2, 3, 4, 1, 6], [1, 8, 1, 7, 1]]</span></span><br><span class="line"></span><br><span class="line">padded = pad_sequences(sequences, padding=<span class="string">&#x27;post&#x27;</span>, maxlen=<span class="number">6</span>,truncating=<span class="string">&#x27;post&#x27;</span>) <span class="comment"># &#x27;post&#x27;表示后补0，maxlen表示最长的sentence只能有6个词语,truncating=&#x27;post&#x27;表示如果一个句子过长，会将前面的words剔除</span></span><br></pre></td></tr></table></figure>
<p>去除stopwords和标点：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line">stopwords = [<span class="string">&quot;a&quot;</span>, ... , <span class="string">&quot;yourselves&quot;</span>]</span><br><span class="line"></span><br><span class="line">table = <span class="built_in">str</span>.maketrans(<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;&#x27;</span>, string.punctuation)</span><br><span class="line"></span><br><span class="line">imdb_sentences = []</span><br><span class="line">train_data = tfds.as_numpy(tfds.load(<span class="string">&#x27;imdb_reviews&#x27;</span>, split=<span class="string">&quot;train&quot;</span>))</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> train_data:</span><br><span class="line">    sentence = <span class="built_in">str</span>(item[<span class="string">&#x27;text&#x27;</span>].decode(<span class="string">&#x27;UTF-8&#x27;</span>).lower())</span><br><span class="line">    soup = BeautifulSoup(sentence)</span><br><span class="line">    sentence = soup.get_text()</span><br><span class="line">    sentence = sentence.replace(<span class="string">&quot;,&quot;</span>, <span class="string">&quot; , &quot;</span>) <span class="comment"># 这四行是为了解决其中有he/she的情况</span></span><br><span class="line">    sentence = sentence.replace(<span class="string">&quot;.&quot;</span>, <span class="string">&quot; . &quot;</span>)</span><br><span class="line">    sentence = sentence.replace(<span class="string">&quot;-&quot;</span>, <span class="string">&quot; - &quot;</span>)</span><br><span class="line">    sentence = sentence.replace(<span class="string">&quot;/&quot;</span>, <span class="string">&quot; / &quot;</span>)</span><br><span class="line">    words = sentence.split()</span><br><span class="line">    filtered_sentence = <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        word = word.translate(table)</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stopwords:</span><br><span class="line">            filtered_sentence = filtered_sentence + word + <span class="string">&quot; &quot;</span></span><br><span class="line">    imdb_sentences.append(filtered_sentence)</span><br><span class="line"></span><br><span class="line">tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=<span class="number">25000</span>)</span><br><span class="line">tokenizer.fit_on_texts(imdb_sentences) <span class="comment"># 这里fit_on_texts（）接受的是一个列表</span></span><br><span class="line">sequences = tokenizer.texts_to_sequences(imdb_sentences)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.word_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取&#123;token:word&#125;</span></span><br><span class="line">reverse_word_index = <span class="built_in">dict</span>(</span><br><span class="line">    [(value, key) <span class="keyword">for</span> (key, value) <span class="keyword">in</span> tokenizer.word_index.items()])</span><br><span class="line"></span><br><span class="line">decoded_review = <span class="string">&#x27; &#x27;</span>.join([reverse_word_index.get(i, <span class="string">&#x27;?&#x27;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> sequences[<span class="number">0</span>]])</span><br></pre></td></tr></table></figure>
<p>如果想用tensorflow内置的corpus编码自己的文本：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(train_data, test_data), info = tfds.load(</span><br><span class="line">    <span class="string">&#x27;imdb_reviews/subwords8k&#x27;</span>, </span><br><span class="line">    split = (tfds.Split.TRAIN, tfds.Split.TEST),</span><br><span class="line">    as_supervised=<span class="literal">True</span>,</span><br><span class="line">    with_info=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">encoder = info.features[<span class="string">&#x27;text&#x27;</span>].encoder</span><br><span class="line"></span><br><span class="line">sample_string = <span class="string">&#x27;Today is a sunny day&#x27;</span></span><br><span class="line"></span><br><span class="line">encoded_string = encoder.encode(sample_string)<span class="comment"># [6427, 4869, 9, 4, 2365, 1361, 606]</span></span><br><span class="line"></span><br><span class="line">original_string = encoder.decode(encoded_string) <span class="comment"># &#x27;Today is a sunny day&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<p>补充说明sklearn中有一个类似的class：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line">vectorizer = CountVectorizer()</span><br><span class="line">vectorizer.fit_transform(imdb_sentences).shape</span><br></pre></td></tr></table></figure>
<p>以上返回的也是一个矩阵，但是每一个array是一个统计每个词的出现次数的向量，比如是0,1,2这些数字，出现几次，那个位置就是几。和<code>keras.preprocessing.text.Tokenizer</code>还是有很大区别的。</p>
<h1 id="chapter-6.-using-word-embeddings">Chapter 6. Using word
Embeddings</h1>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>Fiftyone 使用</title>
    <url>/2022/04/14/Fiftyone-%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>最近在做一个视频处理的task，视频是由帧组成的，如果想抽取视频内的信息，一部分是语音信息，一部分是图像信息。对于图像信息，常规流程是将frames全部抽取出来，然后用ocr的技术抽取图像中的文本。ocr的技术已经相当成熟，而且抽取准确，可以使用百度开源的paddleocr本地化部署，有训练好的模型调用接口就可直接使用。但是有一个很大的问题是，如果视频很长，比如40分钟左右的视频，如果将所有帧全部抽取出来就会很多，有的甚至会达到1万张图片，不仅存储难，而且让ocr模型多处理了很多相同的图片。这时候我就想找找有没有从frames中抽取unique
frame的算法或者工具。其实这里的unique的概念有点难定义，对于运动类的视频，比如一场球赛，其中运动着的人动了，那下一帧和上一帧在同一像素位置的值就不一样了，但是对于ppt类的教育类视频，转到下一页ppt了，那就是算一次很大的变动，上一页的ppt的内容和下一页ppt的内容都需要保存下来，才不会损失视频中的信息。</p>
<p>为此去找了一些文章来看，其中一篇是Methods and Challenges in Shot
Boundary Detection: A Review，里面提到一个概念是Shot Boundry
Detection。它将视频的结构定义为如下的图：</p>
<figure>
<img src="/2022/04/14/Fiftyone-%E4%BD%BF%E7%94%A8/image-20220414163646670.png" alt="image-20220414163646670">
<figcaption aria-hidden="true">image-20220414163646670</figcaption>
</figure>
<p>在搜索的过程中发现了fifityone这个开源project。就先尝试下用这个做一下frames的抽取：</p>
<h1 id="loading-videos">loading videos</h1>
<p>该库有自己定义dataset的方式，和tensorflow一样. load
视频有三种方式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> fiftyone <span class="keyword">as</span> fo</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a dataset from a list of videos</span></span><br><span class="line">dataset = fo.Dataset.from_videos(</span><br><span class="line">    [<span class="string">&quot;/path/to/video1.mp4&quot;</span>, <span class="string">&quot;/path/to/video2.mp4&quot;</span>, ...]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a dataset from a directory of videos</span></span><br><span class="line">dataset = fo.Dataset.from_videos_dir(<span class="string">&quot;/path/to/videos&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a dataset from a glob pattern of videos</span></span><br><span class="line">dataset = fo.Dataset.from_videos_patt(<span class="string">&quot;/path/to/videos/*.mp4&quot;</span>)</span><br><span class="line"></span><br><span class="line">name = <span class="string">&quot;my-dataset&quot;</span></span><br><span class="line">dataset_dir = <span class="string">&quot;/path/to/videos-dir&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the dataset</span></span><br><span class="line">dataset = fo.Dataset.from_dir(</span><br><span class="line">    dataset_dir=dataset_dir,</span><br><span class="line">    dataset_type=fo.types.VideoDirectory,</span><br><span class="line">    name=name,</span><br><span class="line">)</span><br><span class="line">session = fo.launch_app(dataset)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>computer vision(cv)</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepLearning.AI-Tensorfow-Developer-Course课程整理知识点</title>
    <url>/2022/04/05/DeepLearning-AI-Tensorfow-Developer-Course%E8%AF%BE%E7%A8%8B%E6%95%B4%E7%90%86%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
    <content><![CDATA[<p>该课程地址<a href="https://www.coursera.org/professional-certificates/tensorflow-in-practice" class="uri">https://www.coursera.org/professional-certificates/tensorflow-in-practice</a></p>
<p>出品方是
deeplearning.ai,老师是google的，这门专项课程共分为4个单独的课程，每一个课程分为4周的课程，讲者
Laurence Moroney也有一本配套书籍 AI and Machine Learning for Coders
出版了，英文版无中文版，原版书籍可以去Oreilly官方网站找到。所有的代码文件可以在课程教材中找到link。</p>
<p>本博客只记录本人在学习过程中我还不清楚的知识点，不是全部课程的整理。本人是计算机背景，AI方向，建议想学这门课程的小伙伴先去熟练掌握python，其他需要的知识背景还包括：机器学习，深度学习的理论知识（这两点可以先学完Coursera上吴恩达的Machine
Learning，Deep
Learning两个专项课程）。既然说到这里了，还有个自己的小建议，在学习这些课程的时候，最好的方式是一步一步跟着课程来，包括课后练习和课程问答题，会特别！加深自己对于理论知识的掌握。光看视频是不够的！</p>
<p>再有掌握理论知识和会自己熟练写出来中间还有很大的Gap，这也是我会开始学习这门面向coders的课程的原因之一，虽然现在官方API文档写的也很全面，但是这位老师多讲了很多理解这些函数的方式，也给了我们去读Tensorflow源码的兴趣。</p>
<h1 id="课程1-introduction-to-tensorflow-for-artificial-intelligence-machine-learning-and-deep-learning">课程1：
Introduction to TensorFlow for Artificial Intelligence, Machine
Learning, and Deep Learning</h1>
<p>课程一的内容相对简单。对我而言可以学习的是在Horse和人的分类任务中（week4），上传自己从网上load图片让model预测结果的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!wget https://storage.googleapis.com/tensorflow-<span class="number">1</span>-public/course2/week3/horse-<span class="keyword">or</span>-human.<span class="built_in">zip</span></span><br><span class="line">!wget https://storage.googleapis.com/tensorflow-<span class="number">1</span>-public/course2/week3/validation-horse-<span class="keyword">or</span>-human.<span class="built_in">zip</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 运行以下代码之前先将数据集的训练集和验证集下载到本地</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="comment"># Unzip training set</span></span><br><span class="line">local_zip = <span class="string">&#x27;./horse-or-human.zip&#x27;</span></span><br><span class="line">zip_ref = zipfile.ZipFile(local_zip, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">zip_ref.extractall(<span class="string">&#x27;./horse-or-human&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Unzip validation set</span></span><br><span class="line">local_zip = <span class="string">&#x27;./validation-horse-or-human.zip&#x27;</span></span><br><span class="line">zip_ref = zipfile.ZipFile(local_zip, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">zip_ref.extractall(<span class="string">&#x27;./validation-horse-or-human&#x27;</span>)</span><br><span class="line">zip_ref.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># model construction</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line"></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    <span class="comment"># Note the input shape is the desired size of the image 300x300 with 3 bytes color</span></span><br><span class="line">    <span class="comment"># This is the first convolution</span></span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">16</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">300</span>, <span class="number">300</span>, <span class="number">3</span>)),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">    <span class="comment"># The second convolution</span></span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># The third convolution</span></span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># The fourth convolution</span></span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># The fifth convolution</span></span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># Flatten the results to feed into a DNN</span></span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    <span class="comment"># 512 neuron hidden layer</span></span><br><span class="line">    tf.keras.layers.Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    <span class="comment"># Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class (&#x27;horses&#x27;) and 1 for the other (&#x27;humans&#x27;)</span></span><br><span class="line">    tf.keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>) <span class="comment"># 这里是二分类，所以只有一个神经元作为输出，激活函数用sigmoid</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, <span class="comment"># 二分类用binary_crossentropy</span></span><br><span class="line">              optimizer=RMSprop(learning_rate=<span class="number">0.001</span>),</span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line"><span class="comment"># All images will be rescaled by 1./255</span></span><br><span class="line">train_datagen = ImageDataGenerator(rescale=<span class="number">1</span>/<span class="number">255</span>) <span class="comment"># 这里用Tensorflow写好的DataGenerator针对数据集自动产生训练集，注意这里传入的文件夹是./horse-or-human,该文件内有两个文件夹，一个是horse一个是human，generator会用文件夹名字作为分类lable。</span></span><br><span class="line">validation_datagen = ImageDataGenerator(rescale=<span class="number">1</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Flow training images in batches of 128 using train_datagen generator</span></span><br><span class="line">train_generator = train_datagen.flow_from_directory(</span><br><span class="line">        <span class="string">&#x27;./horse-or-human/&#x27;</span>,  <span class="comment"># This is the source directory for training images</span></span><br><span class="line">        target_size=(<span class="number">300</span>, <span class="number">300</span>),  <span class="comment"># All images will be resized to 300x300</span></span><br><span class="line">        batch_size=<span class="number">128</span>,</span><br><span class="line">        <span class="comment"># Since you use binary_crossentropy loss, you need binary labels</span></span><br><span class="line">        class_mode=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Flow validation images in batches of 128 using validation_datagen generator</span></span><br><span class="line">validation_generator = validation_datagen.flow_from_directory(</span><br><span class="line">        <span class="string">&#x27;./validation-horse-or-human/&#x27;</span>,  <span class="comment"># This is the source directory for validation images</span></span><br><span class="line">        target_size=(<span class="number">300</span>, <span class="number">300</span>),  <span class="comment"># All images will be resized to 300x300</span></span><br><span class="line">        batch_size=<span class="number">32</span>,</span><br><span class="line">        <span class="comment"># Since you use binary_crossentropy loss, you need binary labels</span></span><br><span class="line">        class_mode=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit(</span><br><span class="line">      train_generator,</span><br><span class="line">      steps_per_epoch=<span class="number">8</span>,  <span class="comment"># 这里8是1024（train图片的个数）/ train_batch_size(128) = 8</span></span><br><span class="line">      epochs=<span class="number">15</span>,</span><br><span class="line">      verbose=<span class="number">1</span>,</span><br><span class="line">      validation_data = validation_generator,</span><br><span class="line">      validation_steps=<span class="number">8</span>) <span class="comment"># 这里也是val_size（256） / val_batch_size（32）</span></span><br></pre></td></tr></table></figure>
<p>以上模型训练完之后，我们再拿网上下载的图片来看看模型效果如何：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">images = os.listdir(<span class="string">&quot;/tmp/images&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(images)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> images:</span><br><span class="line"> <span class="comment"># predicting images</span></span><br><span class="line"> path = <span class="string">&#x27;/tmp/images/&#x27;</span> + i</span><br><span class="line"> img = image.load_img(path, target_size=(<span class="number">300</span>, <span class="number">300</span>)) <span class="comment"># 这里将图片resize成我们模型需要的大小(300,300)</span></span><br><span class="line"> x = image.img_to_array(img) <span class="comment"># 将img转化成array,x shape (300,300,3)</span></span><br><span class="line"> x /= <span class="number">255</span></span><br><span class="line"> x = np.expand_dims(x, axis=<span class="number">0</span>) <span class="comment"># 这一步是为了扩展为shape (1,300,300,3),因为模型接受的输入是(sample_size,width,height,depth)</span></span><br><span class="line"></span><br><span class="line"> images = np.vstack([x])</span><br><span class="line"> classes = model.predict(images, batch_size=<span class="number">10</span>)</span><br><span class="line"> <span class="built_in">print</span>(classes[<span class="number">0</span>])</span><br><span class="line"> <span class="keyword">if</span> classes[<span class="number">0</span>]&gt;<span class="number">0.5</span>:</span><br><span class="line">   <span class="built_in">print</span>(i + <span class="string">&quot; is a human&quot;</span>)</span><br><span class="line"> <span class="keyword">else</span>:</span><br><span class="line">   <span class="built_in">print</span>(i + <span class="string">&quot; is a horse&quot;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>补充一个在colab中如何将数据下载到本地：<a href="https://blog.csdn.net/jizhidexiaoming/article/details/108302794" class="uri">https://blog.csdn.net/jizhidexiaoming/article/details/108302794</a></p>
</blockquote>
<p>课程1的Week4 的作业是关于笑脸和哭脸的分类，数据集在这个地址 ："<a href="https://storage.googleapis.com/laurencemoroney-blog.appspot.com/happy-or-sad.zip" class="uri">https://storage.googleapis.com/laurencemoroney-blog.appspot.com/happy-or-sad.zip</a>"。github中的作业没有附带这个数据集，需要的读者可以自行用<code>wget</code>在<code>colab</code>中下载，然后将其存储在<code>data</code>文件夹中。</p>
<h2 id="总结">总结</h2>
<p>第一门课总体来说还是很简单的，为了学更多的东西，建议小伙伴深入阅读每一行代码，然后依次去官方API文档里看还有那些参数我们可以传入<code>fit</code>,
<code>complie</code>函数。课程内容是很少的，但如果花时间去钻研还是能学到不少东西。</p>
<p>我在第一门课上总共花的时间是3天时间，加上看视频的时间。</p>
<h1 id="课程2-convolutional-neural-networks-in-tensorflow">课程2:
Convolutional Neural Networks in TensorFlow</h1>
<p>第一周的作业是<code>kaggle</code>上<code>cats</code>和<code>dogs</code>的分类任务
<a href="https://www.kaggle.com/c/dogs-vs-cats" class="uri">https://www.kaggle.com/c/dogs-vs-cats</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"><span class="keyword">from</span> shutil <span class="keyword">import</span> copyfile</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载并解压数据集</span></span><br><span class="line">!wget --no-check-certificate \</span><br><span class="line">    <span class="string">&quot;https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip&quot;</span> \</span><br><span class="line">    -O <span class="string">&quot;/tmp/cats-and-dogs.zip&quot;</span></span><br><span class="line"></span><br><span class="line">local_zip = <span class="string">&#x27;/tmp/cats-and-dogs.zip&#x27;</span></span><br><span class="line">zip_ref   = zipfile.ZipFile(local_zip, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">zip_ref.extractall(<span class="string">&#x27;/tmp&#x27;</span>)</span><br><span class="line">zip_ref.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据DataGenerator需要的格式创建数据集</span></span><br><span class="line"><span class="comment"># Define root directory</span></span><br><span class="line">root_dir = <span class="string">&#x27;/tmp/cats-v-dogs&#x27;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># Empty directory to prevent FileExistsError is the function is run several times</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(root_dir):</span><br><span class="line">  shutil.rmtree(root_dir)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_train_test_dirs</span>(<span class="params">root_path</span>):</span></span><br><span class="line">  <span class="comment"># Use os.makedirs to create your directories with intermediate subdirectories</span></span><br><span class="line">  os.makedirs(os.path.join(root_dir,<span class="string">&quot;training&quot;</span>,<span class="string">&quot;cats&quot;</span>))</span><br><span class="line">  os.makedirs(os.path.join(root_dir,<span class="string">&quot;training&quot;</span>,<span class="string">&quot;dogs&quot;</span>))</span><br><span class="line">  os.makedirs(os.path.join(root_dir,<span class="string">&quot;testing&quot;</span>,<span class="string">&quot;cats&quot;</span>))</span><br><span class="line">  os.makedirs(os.path.join(root_dir,<span class="string">&quot;testing&quot;</span>,<span class="string">&#x27;dogs&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  create_train_test_dirs(root_path=root_dir)</span><br><span class="line"><span class="keyword">except</span> FileExistsError:</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;You should not be seeing this since the upper directory is removed beforehand&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据分割比例分割出训练集和测试集，并将源文件夹中的图片拷贝至相应的文件夹下</span></span><br><span class="line"><span class="comment"># split_data 该函数也需要完成图片的验证：图片length=0，就省略不纳入train或者test</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_data</span>(<span class="params">SOURCE, TRAINING, TESTING, SPLIT_SIZE</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  SOURCE : 例如：PetImages/Cat</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  not_null_files = []</span><br><span class="line">  <span class="keyword">for</span> files <span class="keyword">in</span> os.listdir(SOURCE):</span><br><span class="line">      file_path = os.path.join(SOURCE,files)</span><br><span class="line">      <span class="keyword">if</span> os.path.getsize(file_path) &gt; <span class="number">0</span> :</span><br><span class="line">         not_null_files.append(files)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">         <span class="built_in">print</span>(files + <span class="string">&quot;is zero length, so ignoring&quot;</span>)</span><br><span class="line">  </span><br><span class="line">  count = <span class="built_in">len</span>(not_null_files)</span><br><span class="line">  trains = <span class="built_in">int</span>(count * SPLIT_SIZE)</span><br><span class="line">  tests = count * SPLIT_SIZE</span><br><span class="line"></span><br><span class="line">  shuffle_set = random.sample(not_null_files,count)</span><br><span class="line">  train_set = shuffle_set[<span class="number">0</span>:trains]</span><br><span class="line">  test_set = shuffle_set[trains:]</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 将文件拷贝至指定文件夹</span></span><br><span class="line">  <span class="keyword">for</span> train_file <span class="keyword">in</span> train_set :</span><br><span class="line">      shutil.copyfile(os.path.join(SOURCE,train_file),os.path.join(TRAINING,train_file))</span><br><span class="line">  <span class="keyword">for</span> test_file <span class="keyword">in</span> test_set :</span><br><span class="line">      shutil.copyfile(os.path.join(SOURCE,test_file),os.path.join(TESTING,test_file))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CAT_SOURCE_DIR = <span class="string">&quot;/tmp/PetImages/Cat/&quot;</span></span><br><span class="line">DOG_SOURCE_DIR = <span class="string">&quot;/tmp/PetImages/Dog/&quot;</span></span><br><span class="line"></span><br><span class="line">TRAINING_DIR = <span class="string">&quot;/tmp/cats-v-dogs/training/&quot;</span></span><br><span class="line">TESTING_DIR = <span class="string">&quot;/tmp/cats-v-dogs/testing/&quot;</span></span><br><span class="line"></span><br><span class="line">TRAINING_CATS_DIR = os.path.join(TRAINING_DIR, <span class="string">&quot;cats/&quot;</span>)</span><br><span class="line">TESTING_CATS_DIR = os.path.join(TESTING_DIR, <span class="string">&quot;cats/&quot;</span>)</span><br><span class="line"></span><br><span class="line">TRAINING_DOGS_DIR = os.path.join(TRAINING_DIR, <span class="string">&quot;dogs/&quot;</span>)</span><br><span class="line">TESTING_DOGS_DIR = os.path.join(TESTING_DIR, <span class="string">&quot;dogs/&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Empty directories in case you run this cell multiple times</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(os.listdir(TRAINING_CATS_DIR)) &gt; <span class="number">0</span>:</span><br><span class="line">  <span class="keyword">for</span> file <span class="keyword">in</span> os.scandir(TRAINING_CATS_DIR):</span><br><span class="line">    os.remove(file.path)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(os.listdir(TRAINING_DOGS_DIR)) &gt; <span class="number">0</span>:</span><br><span class="line">  <span class="keyword">for</span> file <span class="keyword">in</span> os.scandir(TRAINING_DOGS_DIR):</span><br><span class="line">    os.remove(file.path)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(os.listdir(TESTING_CATS_DIR)) &gt; <span class="number">0</span>:</span><br><span class="line">  <span class="keyword">for</span> file <span class="keyword">in</span> os.scandir(TESTING_CATS_DIR):</span><br><span class="line">    os.remove(file.path)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(os.listdir(TESTING_DOGS_DIR)) &gt; <span class="number">0</span>:</span><br><span class="line">  <span class="keyword">for</span> file <span class="keyword">in</span> os.scandir(TESTING_DOGS_DIR):</span><br><span class="line">    os.remove(file.path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define proportion of images used for training</span></span><br><span class="line">split_size = <span class="number">.9</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> Messages about zero length images should be printed out</span></span><br><span class="line">split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)</span><br><span class="line">split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型部分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_val_generators</span>(<span class="params">TRAINING_DIR, VALIDATION_DIR</span>):</span></span><br><span class="line">  <span class="comment"># Instantiate the ImageDataGenerator class (don&#x27;t forget to set the rescale argument)</span></span><br><span class="line">  train_datagen = ImageDataGenerator(rescale=<span class="number">1</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Pass in the appropiate arguments to the flow_from_directory method</span></span><br><span class="line">  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,</span><br><span class="line">                                                      batch_size=<span class="number">128</span>,</span><br><span class="line">                                                      class_mode=<span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">                                                      target_size=(<span class="number">150</span>, <span class="number">150</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Instantiate the ImageDataGenerator class (don&#x27;t forget to set the rescale argument)</span></span><br><span class="line">  validation_datagen = ImageDataGenerator(rescale=<span class="number">1</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Pass in the appropiate arguments to the flow_from_directory method</span></span><br><span class="line">  validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,</span><br><span class="line">                                                                batch_size=<span class="number">32</span>,</span><br><span class="line">                                                                class_mode=<span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">                                                                target_size=(<span class="number">150</span>, <span class="number">150</span>))</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">return</span> train_generator, validation_generator</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span>():</span></span><br><span class="line">  <span class="comment"># USE AT LEAST 3 CONVOLUTION LAYERS</span></span><br><span class="line">  model = tf.keras.models.Sequential([ </span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">16</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>)),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>),</span><br><span class="line">  ])</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  model.<span class="built_in">compile</span>(optimizer=tf.keras.optimizers.RMSprop(learning_rate=<span class="number">0.001</span>),</span><br><span class="line">                loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>,</span><br><span class="line">                metrics=[<span class="string">&#x27;accuracy&#x27;</span>]) </span><br><span class="line">  <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_generator, validation_generator = train_val_generators(TRAINING_DIR, TESTING_DIR)</span><br><span class="line">model = create_model()</span><br><span class="line">history = model.fit(train_generator,</span><br><span class="line">                    epochs=<span class="number">15</span>,</span><br><span class="line">                    verbose=<span class="number">1</span>,</span><br><span class="line">                    validation_data=validation_generator)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据history返回的结果画图</span></span><br><span class="line">acc=history.history[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">val_acc=history.history[<span class="string">&#x27;val_accuracy&#x27;</span>]</span><br><span class="line">loss=history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">val_loss=history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">epochs=<span class="built_in">range</span>(<span class="built_in">len</span>(acc)) <span class="comment"># Get number of epochs</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#------------------------------------------------</span></span><br><span class="line"><span class="comment"># Plot training and validation accuracy per epoch</span></span><br><span class="line"><span class="comment">#------------------------------------------------</span></span><br><span class="line">plt.plot(epochs, acc, <span class="string">&#x27;r&#x27;</span>, <span class="string">&quot;Training Accuracy&quot;</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">&#x27;b&#x27;</span>, <span class="string">&quot;Validation Accuracy&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and validation accuracy&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#------------------------------------------------</span></span><br><span class="line"><span class="comment"># Plot training and validation loss per epoch</span></span><br><span class="line"><span class="comment">#------------------------------------------------</span></span><br><span class="line">plt.plot(epochs, loss, <span class="string">&#x27;r&#x27;</span>, <span class="string">&quot;Training Loss&quot;</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">&#x27;b&#x27;</span>, <span class="string">&quot;Validation Loss&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存history至pkl文件，并在colab中将其下载下来</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_history</span>():</span></span><br><span class="line">  <span class="keyword">import</span> pickle</span><br><span class="line">  <span class="keyword">from</span> google.colab <span class="keyword">import</span> files</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;history.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(history.history, f)</span><br><span class="line"></span><br><span class="line">  files.download(<span class="string">&#x27;history.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line">download_history()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="data-augmentation">Data Augmentation</h2>
<p>主要为了解决<code>overfitting</code>的情况(<code>train set</code>上<code>accuracy</code>很高，但是<code>val set</code>上<code>accuracy</code>低。两者相差较大)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_datagen = ImageDataGenerator(</span><br><span class="line">      rotation_range=<span class="number">40</span>,</span><br><span class="line">      width_shift_range=<span class="number">0.2</span>,</span><br><span class="line">      height_shift_range=<span class="number">0.2</span>,</span><br><span class="line">      shear_range=<span class="number">0.2</span>,</span><br><span class="line">      zoom_range=<span class="number">0.2</span>,</span><br><span class="line">      horizontal_flip=<span class="literal">True</span>,</span><br><span class="line">      fill_mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>rotation_range</code> is a value in degrees (0–180) within
which to randomly rotate pictures. 旋转角度</li>
<li><code>width_shift</code> and <code>height_shift</code> are ranges
(as a fraction of total width or height) within which to randomly
translate pictures vertically or horizontally. 主体平移</li>
<li><code>shear_range</code> is for randomly applying shearing
transformations. 剪切</li>
<li><code>zoom_range</code> is for randomly zooming inside pictures.
放大缩小</li>
<li><code>horizontal_flip</code> is for randomly flipping half of the
images horizontally. This is relevant when there are no assumptions of
horizontal assymmetry (e.g. real-world pictures). 是否镜像</li>
<li><code>fill_mode</code> is the strategy used for filling in newly
created pixels, which can appear after a rotation or a width/height
shift. 用什么值去填充旋转或者平移之后缺失的pixel的值</li>
</ul>
<p>完整代码是，仅仅是在初始化<code>ImageDataGenerator</code>时有一点改变：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create new model</span></span><br><span class="line">model_for_aug = create_model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># This code has changed. Now instead of the ImageGenerator just rescaling</span></span><br><span class="line"><span class="comment"># the image, we also rotate and do other operations</span></span><br><span class="line">train_datagen = ImageDataGenerator(</span><br><span class="line">      rescale=<span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">      rotation_range=<span class="number">40</span>,</span><br><span class="line">      width_shift_range=<span class="number">0.2</span>,</span><br><span class="line">      height_shift_range=<span class="number">0.2</span>,</span><br><span class="line">      shear_range=<span class="number">0.2</span>,</span><br><span class="line">      zoom_range=<span class="number">0.2</span>,</span><br><span class="line">      horizontal_flip=<span class="literal">True</span>,</span><br><span class="line">      fill_mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line"></span><br><span class="line">test_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Flow training images in batches of 20 using train_datagen generator</span></span><br><span class="line">train_generator = train_datagen.flow_from_directory(</span><br><span class="line">        train_dir,  <span class="comment"># This is the source directory for training images</span></span><br><span class="line">        target_size=(<span class="number">150</span>, <span class="number">150</span>),  <span class="comment"># All images will be resized to 150x150</span></span><br><span class="line">        batch_size=<span class="number">20</span>,</span><br><span class="line">        <span class="comment"># Since we use binary_crossentropy loss, we need binary labels</span></span><br><span class="line">        class_mode=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Flow validation images in batches of 20 using test_datagen generator</span></span><br><span class="line">validation_generator = test_datagen.flow_from_directory(</span><br><span class="line">        validation_dir,</span><br><span class="line">        target_size=(<span class="number">150</span>, <span class="number">150</span>),</span><br><span class="line">        batch_size=<span class="number">20</span>,</span><br><span class="line">        class_mode=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the new model</span></span><br><span class="line">history_with_aug = model_for_aug.fit(</span><br><span class="line">      train_generator,</span><br><span class="line">      steps_per_epoch=<span class="number">100</span>,  <span class="comment"># 2000 images = batch_size * steps</span></span><br><span class="line">      epochs=EPOCHS,</span><br><span class="line">      validation_data=validation_generator,</span><br><span class="line">      validation_steps=<span class="number">50</span>,  <span class="comment"># 1000 images = batch_size * steps</span></span><br><span class="line">      verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>这里贴一下作者写的画history的函数，可复用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_loss_acc</span>(<span class="params">history</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;Plots the training and validation loss and accuracy from a history object&#x27;&#x27;&#x27;</span></span><br><span class="line">  acc = history.history[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">  val_acc = history.history[<span class="string">&#x27;val_accuracy&#x27;</span>]</span><br><span class="line">  loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">  val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">  epochs = <span class="built_in">range</span>(<span class="built_in">len</span>(acc))</span><br><span class="line"></span><br><span class="line">  plt.plot(epochs, acc, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;Training accuracy&#x27;</span>)</span><br><span class="line">  plt.plot(epochs, val_acc, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;Validation accuracy&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&#x27;Training and validation accuracy&#x27;</span>)</span><br><span class="line"></span><br><span class="line">  plt.figure()</span><br><span class="line"></span><br><span class="line">  plt.plot(epochs, loss, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">  plt.plot(epochs, val_loss, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&#x27;Training and validation loss&#x27;</span>)</span><br><span class="line">  plt.legend()</span><br><span class="line"></span><br><span class="line">  plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="transfer-learning-迁移学习">transfer learning 迁移学习</h2>
<p>基本思想:
将别人训练好的模型的前几层的参数拿过来组装到你的模型前面，然后用你的训练集重新训练后面的<code>dense layers</code>的参数.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先下载Inception的pre-trained models，.h5文件</span></span><br><span class="line">!wget --no-check-certificate \</span><br><span class="line">    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \</span><br><span class="line">    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5</span><br><span class="line">   </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.applications.inception_v3 <span class="keyword">import</span> InceptionV3</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Model</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the weights file you downloaded into a variable</span></span><br><span class="line">local_weights_file = <span class="string">&#x27;/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the base model.</span></span><br><span class="line"><span class="comment"># Set the input shape and remove the dense layers.</span></span><br><span class="line">pre_trained_model = InceptionV3(input_shape = (<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>), </span><br><span class="line">                                include_top = <span class="literal">False</span>, <span class="comment"># 将最后一层dense去掉</span></span><br><span class="line">                                weights = <span class="literal">None</span>) <span class="comment"># 不需要默认的参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pre-trained weights you downloaded.</span></span><br><span class="line">pre_trained_model.load_weights(local_weights_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Freeze the weights of the layers. 将Inception网络中的参数都lock住，不让我们的训练集在上面fit</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> pre_trained_model.layers:</span><br><span class="line">  layer.trainable = <span class="literal">False</span></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment"># Choose `mixed_7` as the last layer of your base model</span></span><br><span class="line">last_layer = pre_trained_model.get_layer(<span class="string">&#x27;mixed7&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;last layer output shape: &#x27;</span>, last_layer.output_shape)</span><br><span class="line">last_output = last_layer.output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Flatten the output layer to 1 dimension 将Inception中mixed7层作为我们模型的输入层</span></span><br><span class="line">x = layers.Flatten()(last_output)</span><br><span class="line"><span class="comment"># Add a fully connected layer with 1,024 hidden units and ReLU activation</span></span><br><span class="line">x = layers.Dense(<span class="number">1024</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(x)</span><br><span class="line"><span class="comment"># Add a dropout rate of 0.2</span></span><br><span class="line">x = layers.Dropout(<span class="number">0.2</span>)(x)                  </span><br><span class="line"><span class="comment"># Add a final sigmoid layer for classification</span></span><br><span class="line">x = layers.Dense  (<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)(x)           </span><br><span class="line"></span><br><span class="line"><span class="comment"># Append the dense network to the base model</span></span><br><span class="line">model = Model(pre_trained_model.<span class="built_in">input</span>, x) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the model summary. See your dense network connected at the end.</span></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the training parameters</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer = RMSprop(learning_rate=<span class="number">0.0001</span>), </span><br><span class="line">              loss = <span class="string">&#x27;binary_crossentropy&#x27;</span>, </span><br><span class="line">              metrics = [<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure>
<h1 id="课程3nlp">课程3：NLP</h1>
<h2 id="tokenizer中的num_words参数">Tokenizer中的num_words参数</h2>
<p>https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do/53512348#53512348</p>
<p><code>tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)</code>使用word_index
=
tokenizer.word_index得到的word_index的长度是和vocab_size没有关系的，vocab_size指定的是只有出现频率排列前面vocab_size的词语才会被编码，如果没有，则会被跳过忽略。</p>
<p>如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Define input sentences</span></span><br><span class="line">sentences = [</span><br><span class="line">    <span class="string">&#x27;i love my dog&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;I, love my cat&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;You love my dog!&#x27;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the Tokenizer class</span></span><br><span class="line">tokenizer = Tokenizer(num_words = <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate indices for each word in the corpus</span></span><br><span class="line">tokenizer.fit_on_texts(sentences)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the indices and print it</span></span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line"><span class="built_in">print</span>(word_index)</span><br><span class="line">&gt;&gt; &#123;<span class="string">&#x27;love&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;my&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;i&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;dog&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;cat&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;you&#x27;</span>: <span class="number">6</span>&#125;</span><br><span class="line"></span><br><span class="line">tokenizer.texts_to_sequences(sentences)</span><br><span class="line">&gt;&gt; [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">2</span>]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>用<code>LSTM</code>，<code>GRU</code>对<code>IMDB</code>数据集实现分类：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow_datasets <span class="keyword">as</span> tfds</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据</span></span><br><span class="line">imdb, info = tfds.load(<span class="string">&#x27;imdb_reviews&#x27;</span>, with_info=<span class="literal">True</span>, as_supervised=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># Get the train and test sets</span></span><br><span class="line">train_data, test_data = imdb[<span class="string">&#x27;train&#x27;</span>], imdb[<span class="string">&#x27;test&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize sentences and labels lists</span></span><br><span class="line">training_sentences = []</span><br><span class="line">training_labels = []</span><br><span class="line"></span><br><span class="line">testing_sentences = []</span><br><span class="line">testing_labels = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Loop over all training examples and save the sentences and labels</span></span><br><span class="line"><span class="keyword">for</span> s,l <span class="keyword">in</span> train_data:</span><br><span class="line">  training_sentences.append(s.numpy().decode(<span class="string">&#x27;utf8&#x27;</span>))</span><br><span class="line">  training_labels.append(l.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Loop over all test examples and save the sentences and labels</span></span><br><span class="line"><span class="keyword">for</span> s,l <span class="keyword">in</span> test_data:</span><br><span class="line">  testing_sentences.append(s.numpy().decode(<span class="string">&#x27;utf8&#x27;</span>))</span><br><span class="line">  testing_labels.append(l.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert labels lists to numpy array</span></span><br><span class="line">training_labels_final = np.array(training_labels)</span><br><span class="line">testing_labels_final = np.array(testing_labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tokenize数据</span></span><br><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">vocab_size = <span class="number">10000</span></span><br><span class="line">max_length = <span class="number">120</span></span><br><span class="line">trunc_type=<span class="string">&#x27;post&#x27;</span></span><br><span class="line">oov_tok = <span class="string">&quot;&lt;OOV&gt;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the Tokenizer class</span></span><br><span class="line">tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate the word index dictionary for the training sentences</span></span><br><span class="line">tokenizer.fit_on_texts(training_sentences)</span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate and pad the training sequences</span></span><br><span class="line">sequences = tokenizer.texts_to_sequences(training_sentences)</span><br><span class="line">padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate and pad the test sequences</span></span><br><span class="line">testing_sequences = tokenizer.texts_to_sequences(testing_sentences)</span><br><span class="line">testing_padded = pad_sequences(testing_sequences,maxlen=max_length)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立模型</span></span><br><span class="line"><span class="comment"># Hyperparameters</span></span><br><span class="line">embedding_dim = <span class="number">64</span></span><br><span class="line">lstm1_dim = <span class="number">64</span></span><br><span class="line">lstm2_dim = <span class="number">32</span></span><br><span class="line">dense_dim = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the model</span></span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">    tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),</span><br><span class="line">    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm1_dim, return_sequences=<span class="literal">True</span>)),</span><br><span class="line">    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm2_dim)),</span><br><span class="line">    tf.keras.layers.Dense(dense_dim, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the model summary</span></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<h2 id="generate-next-word-产生文本">generate next word 产生文本</h2>
<p>这门课主要注意的是老师处理序列的方式，也就是产生训练<code>features</code>和<code>labels</code>的方式。其他的和前述课程一样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Initialize the Tokenizer class</span></span><br><span class="line">tokenizer = Tokenizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate the word index dictionary</span></span><br><span class="line">tokenizer.fit_on_texts(corpus)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the total words. You add 1 for the index `0` which is just the padding token.</span></span><br><span class="line">total_words = <span class="built_in">len</span>(tokenizer.word_index) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;word index dictionary: <span class="subst">&#123;tokenizer.word_index&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;total words: <span class="subst">&#123;total_words&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the sequences list</span></span><br><span class="line">input_sequences = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Loop over every line</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> corpus:</span><br><span class="line"></span><br><span class="line">	<span class="comment"># Tokenize the current line 这里将每一个句子变成token</span></span><br><span class="line">	token_list = tokenizer.texts_to_sequences([line])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">	<span class="comment"># Loop over the line several times to generate the subphrases</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(token_list)): <span class="comment"># 该循环是为了产生n_gram的训练集</span></span><br><span class="line">		</span><br><span class="line">		<span class="comment"># Generate the subphrase</span></span><br><span class="line">		n_gram_sequence = token_list[:i+<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Append the subphrase to the sequences list</span></span><br><span class="line">		input_sequences.append(n_gram_sequence)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the length of the longest line</span></span><br><span class="line">max_sequence_len = <span class="built_in">max</span>([<span class="built_in">len</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> input_sequences])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pad all sequences</span></span><br><span class="line">input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding=<span class="string">&#x27;pre&#x27;</span>)) <span class="comment"># 将所有句子都处理成一样长，这里用&quot;pre&quot;的padding模式是为了后面取lables（最后一个单词）的方便。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create inputs and label by splitting the last token in the subphrases</span></span><br><span class="line">xs, labels = input_sequences[:,:-<span class="number">1</span>],input_sequences[:,-<span class="number">1</span>] <span class="comment"># 最后一个词语作为labels，也就是我们要预测的东西，依据的输入就是它前面的单词</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert the label into one-hot arrays</span></span><br><span class="line">ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyperparameters</span></span><br><span class="line">embedding_dim = <span class="number">100</span></span><br><span class="line">lstm_units = <span class="number">150</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the model</span></span><br><span class="line">model = Sequential([</span><br><span class="line">          Embedding(total_words, embedding_dim, input_length=max_sequence_len-<span class="number">1</span>), <span class="comment"># 这里一定要记得-1,是因为我们取了sequence中最后一个单词作为labels</span></span><br><span class="line">          Bidirectional(LSTM(lstm_units)),</span><br><span class="line">          Dense(total_words, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use categorical crossentropy because this is a multi-class problem</span></span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, </span><br><span class="line">    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), </span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the model summary</span></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line">history = model.fit(xs, ys, epochs=epochs)</span><br></pre></td></tr></table></figure>
<p>基于字符集合的<code>generate text</code>可以参考 <a href="https://www.tensorflow.org/text/tutorials/text_generation" class="uri">https://www.tensorflow.org/text/tutorials/text_generation</a></p>
<h1 id="课程4-序列">课程4 序列</h1>
<p>选取<code>train,val,test</code>集合的标准，要选取包含整个序列的特征。<code>fixed-partitioning or Roll-foward partitioning</code>。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensorflow笔记</title>
    <url>/2022/04/03/Tensorflow%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="tf.keras.losses">tf.keras.losses</h1>
<h2 id="categorical_crossentropy-vs-sparse_categorical_crossentropy"><code>categorical_crossentropy</code>
VS <code>sparse_categorical_crossentropy</code></h2>
<p><code>sparse_categorical_crossentropy</code>的用法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fmnist = tf.keras.datasets.fashion_mnist</span><br><span class="line"></span><br><span class="line">(training_images, training_labels), (test_images, test_labels) = fmnist.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the classification model</span></span><br><span class="line">model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), </span><br><span class="line">                                    tf.keras.layers.Dense(<span class="number">128</span>, activation=tf.nn.relu), </span><br><span class="line">                                    tf.keras.layers.Dense(<span class="number">10</span>, activation=tf.nn.softmax)])</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer = tf.optimizers.Adam(),</span><br><span class="line">              loss = <span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>,</span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">model.fit(training_images, training_labels, epochs=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">classifications = model.predict(test_images)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(classifications[<span class="number">0</span>]) <span class="comment"># [6.7404380e-06 9.1553174e-09 7.0345862e-08 1.8929207e-07 2.0586524e-06 4.4078561e-03 4.8766628e-06 2.5287211e-02 5.4279792e-07 9.7029042e-01]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>]) <span class="comment"># 9</span></span><br></pre></td></tr></table></figure>
<p>也就是<code>labels</code>是数字模式（1-9）分类类型，不是binary的模式，<code>classifications</code>是经过<code>softmax</code>激活层之后的结果，是一个<code>list</code>，每一个<code>item</code>代表的是该类的概率。这种情况我们用<code>sparse_categorical_crossentropy</code>，如果loss传入的是<code>categorical_crossentropy</code>是会报错的。</p>
<p><code>categorical_crossentropy</code>的用法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_true = [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">y_pred = [[<span class="number">0.05</span>, <span class="number">0.95</span>, <span class="number">0</span>], [<span class="number">0.1</span>, <span class="number">0.8</span>, <span class="number">0.1</span>]]</span><br><span class="line"><span class="comment"># Using &#x27;auto&#x27;/&#x27;sum_over_batch_size&#x27; reduction type.</span></span><br><span class="line">cce = tf.keras.losses.CategoricalCrossentropy()</span><br><span class="line">cce(y_true, y_pred).numpy()</span><br></pre></td></tr></table></figure>
<p>可以看出，CategoricalCrossentropy（）接受的是one-hot编码的labels，预测结果可以是softmax</p>
<blockquote>
<p>关于两者间的区别，更多的细节可以参考Tensorflow
官方文档和该博客[https://gombru.github.io/2018/05/23/cross_entropy_loss/]</p>
</blockquote>
<h1 id="callback回调函数"><code>callback()</code>回调函数</h1>
<p>参考 https://www.tensorflow.org/guide/keras/custom_callback</p>
<p>用途：可以在训练、评估或推断过程中自定义 Keras
模型行为，最常见的就是当accuracy达到某个阈值就停止训练。</p>
<p>所有回调函数都将 <code>keras.callbacks.Callback</code>
类作为子类，并重写在训练、测试和预测的各个阶段调用的一组方法。回调函数对于在训练期间了解模型的内部状态和统计信息十分有用。</p>
<p>您可以将回调函数的列表（作为关键字参数
<code>callbacks</code>）传递给以下模型方法：</p>
<ul>
<li><code>keras.Model.fit()</code></li>
<li><code>keras.Model.evaluate()</code></li>
<li><code>keras.Model.predict()</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myCallback</span>(<span class="params">tf.keras.callbacks.Callback</span>):</span> <span class="comment"># 这里是一定要继承Callback类的</span></span><br><span class="line">        <span class="comment"># Define the correct function signature for on_epoch_end</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span>(<span class="params">self, epoch, logs</span>):</span> <span class="comment"># 这里是在每一个epoch停止才会检查accuracy，如果在某个epoch中间出现了大于阈值的情况是不会停止的</span></span><br><span class="line">            <span class="keyword">if</span> logs.get(<span class="string">&#x27;accuracy&#x27;</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> logs.get(<span class="string">&#x27;accuracy&#x27;</span>) &gt; <span class="number">0.99</span>: <span class="comment"># @KEEP</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;\nReached 99% accuracy so cancelling training!&quot;</span>) </span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Stop training once the above condition is met</span></span><br><span class="line">                self.model.stop_training = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()</span><br><span class="line">x_train = x_train / <span class="number">255.0</span></span><br><span class="line">callbacks = myCallback()</span><br><span class="line">model = tf.keras.models.Sequential([ </span><br><span class="line">        tf.keras.layers.Flatten(input_shape=(<span class="number">28</span>, <span class="number">28</span>)),</span><br><span class="line">        tf.keras.layers.Dense(units=<span class="number">512</span>,activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">        tf.keras.layers.Dense(units=<span class="number">10</span>,activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">    ]) </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, </span><br><span class="line">              loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, </span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>]) </span><br><span class="line"></span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">10</span>, callbacks=[callbacks])</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="con2d-and-maxpooling2d">Con2D and MaxPooling2D</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = tf.keras.datasets.fashion_mnist</span><br><span class="line">(training_images, training_labels), (test_images, test_labels) = data.load_data()</span><br><span class="line">training_images = training_images.reshape(<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>) <span class="comment"># 这里是一定要reshape的，Con2D作为网络第一层，接受的input是一个4维的array，(sample_size,width,height,depth)</span></span><br><span class="line">training_images = training_images / <span class="number">255.0</span></span><br><span class="line">test_images = test_images.reshape(<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line">test_images = test_images / <span class="number">255.0</span></span><br><span class="line"><span class="comment"># Define the model</span></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">                                                         </span><br><span class="line">  <span class="comment"># Add convolutions and max pooling</span></span><br><span class="line">  tf.keras.layers.Conv2D(<span class="number">64</span>, kernel_size=(<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)), <span class="comment"># 这里input_shape=(width,height,depth) grey picture depth是1，RGB picture是3</span></span><br><span class="line">  tf.keras.layers.MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">  tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">  tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Add the same layers as before</span></span><br><span class="line">  tf.keras.layers.Flatten(),</span><br><span class="line">  tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">  tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the model summary</span></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use same settings</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;\nMODEL TRAINING:&#x27;</span>)</span><br><span class="line">model.fit(training_images, training_labels, epochs=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate on the test set</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;\nMODEL EVALUATION:&#x27;</span>)</span><br><span class="line">test_loss = model.evaluate(test_images, test_labels)</span><br></pre></td></tr></table></figure>
<ul>
<li>There are other approaches to pooling, such as <em>min</em> pooling,
which takes the smallest pixel value from the pool, and <em>average</em>
pooling, which takes the overall average value</li>
<li>When using this layer as the first layer in a model, provide the
keyword argument <code>input_shape</code> (tuple of integers or
<code>None</code>, does not include the sample axis), e.g.
<code>input_shape=(128, 128, 3)</code> for 128x128 RGB pictures in
<code>data_format="channels_last"</code>. You can use <code>None</code>
when a dimension has variable size</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>sklearn总结</title>
    <url>/2022/03/29/sklearn%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="model">model</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier <span class="comment"># 随机森林</span></span><br><span class="line"><span class="comment"># RandomForestClassifier随机森林：n_estimators 森林中树的数量(default=100), max_features 寻找最优划分时考虑的最大数目的特征,</span></span><br><span class="line">rf_clf_model = RandomForestClassifier(n_estimators=<span class="number">150</span>, max_depth=<span class="number">7</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>
<h1 id="metric">metric</h1>
<h2 id="accuracyprecisionrcall-f1_score">accuracy,precision,rcall,
f1_score</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, f1_score,precision_score,recall_score</span><br></pre></td></tr></table></figure>
<h1 id="model_selection">model_selection</h1>
<h2 id="交叉验证-cross_val_score">交叉验证 cross_val_score</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from sklearn.model_selection import cross_val_score</span><br></pre></td></tr></table></figure>
<p>cross_val_score交叉验证
首个参数为estimator(也就是要执行fit的model),cv
折数，默认是5折验证，返回的cv长度的score</p>
<p>参考 https://blog.csdn.net/qq_36523839/article/details/80707678</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets	<span class="comment">#自带数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split,cross_val_score	<span class="comment">#划分数据 交叉验证</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier  <span class="comment">#一个简单的模型，只有K一个参数，类似K-means</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">iris = datasets.load_iris()		<span class="comment">#加载sklearn自带的数据集</span></span><br><span class="line">X = iris.data 			<span class="comment">#这是数据</span></span><br><span class="line">y = iris.target 		<span class="comment">#这是每个数据所对应的标签</span></span><br><span class="line">train_X,test_X,train_y,test_y = train_test_split(X,y,test_size=<span class="number">1</span>/<span class="number">3</span>,random_state=<span class="number">3</span>)	<span class="comment">#这里划分数据以1/3的来划分 训练集训练结果 测试集测试结果</span></span><br><span class="line">k_range = <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">31</span>)</span><br><span class="line">cv_scores = []		<span class="comment">#用来放每个模型的结果值</span></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> k_range:</span><br><span class="line">    knn = KNeighborsClassifier(n)   <span class="comment">#knn模型，这里一个超参数可以做预测，当多个超参数时需要使用另一种方法GridSearchCV</span></span><br><span class="line">    scores = cross_val_score(knn,train_X,train_y,cv=<span class="number">10</span>,scoring=<span class="string">&#x27;accuracy&#x27;</span>)  <span class="comment">#cv：选择每次测试折数  accuracy：评价指标是准确度,可以省略使用默认值，具体使用参考下面。</span></span><br><span class="line">    cv_scores.append(scores.mean())</span><br><span class="line">plt.plot(k_range,cv_scores)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;K&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)		<span class="comment">#通过图像选择最好的参数</span></span><br><span class="line">plt.show()</span><br><span class="line">best_knn = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)	<span class="comment"># 选择最优的K=3传入模型</span></span><br><span class="line">best_knn.fit(train_X,train_y)			<span class="comment">#训练模型</span></span><br><span class="line"><span class="built_in">print</span>(best_knn.score(test_X,test_y))	<span class="comment">#看看评分</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>sklearn</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title>numpy基础总结</title>
    <url>/2022/03/27/numpy%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="numpy.linalg.norm">numpy.linalg.norm</h1>
<p><code>linalg.norm(x,ord=None,axis=None,keepdims=False)</code></p>
<p>其中x可以传入array，也可以是matrix。</p>
<p>ord参数针对x是array还是matrix是不一样的计算：</p>
<table>
<thead>
<tr class="header">
<th>ord</th>
<th>norm for matrices</th>
<th>norm for vectors</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>None</td>
<td>Frobenius norm</td>
<td>2-norm</td>
</tr>
<tr class="even">
<td>‘fro’</td>
<td>Frobenius norm</td>
<td>–</td>
</tr>
<tr class="odd">
<td>‘nuc’</td>
<td>nuclear norm</td>
<td>–</td>
</tr>
<tr class="even">
<td>inf</td>
<td>max(sum(abs(x), axis=1))</td>
<td>max(abs(x))</td>
</tr>
<tr class="odd">
<td>-inf</td>
<td>min(sum(abs(x), axis=1))</td>
<td>min(abs(x))</td>
</tr>
<tr class="even">
<td>0</td>
<td>–</td>
<td>sum(x != 0)</td>
</tr>
<tr class="odd">
<td>1</td>
<td>max(sum(abs(x), axis=0))</td>
<td>as below</td>
</tr>
<tr class="even">
<td>-1</td>
<td>min(sum(abs(x), axis=0))</td>
<td>as below</td>
</tr>
<tr class="odd">
<td>2</td>
<td>2-norm (largest sing. value)</td>
<td>L2范数</td>
</tr>
<tr class="even">
<td>-2</td>
<td>smallest singular value</td>
<td>as below</td>
</tr>
<tr class="odd">
<td>other</td>
<td>–</td>
<td>sum(abs(x)<strong>ord)</strong>(1./ord)</td>
</tr>
</tbody>
</table>
<p>如果传入axis这个参数，axis=0表示每一列为向量，以每一列的向量为基础计算。然后就转化为了向量运算。</p>
<p>其中keepdims如果是True，那么规范化的轴将作为尺寸1留在结果中，使用此选项，结果将针对原始
x 正确广播。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.arange(<span class="number">9</span>) - <span class="number">4</span></span><br><span class="line">b = a.reshape((<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">a</span><br><span class="line">Out[<span class="number">7</span>]: array([-<span class="number">4</span>, -<span class="number">3</span>, -<span class="number">2</span>, -<span class="number">1</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>])</span><br><span class="line">b</span><br><span class="line">Out[<span class="number">8</span>]: </span><br><span class="line">array([[-<span class="number">4</span>, -<span class="number">3</span>, -<span class="number">2</span>],</span><br><span class="line">       [-<span class="number">1</span>,  <span class="number">0</span>,  <span class="number">1</span>],</span><br><span class="line">       [ <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line">LA.norm(b,axis=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">9</span>]: array([<span class="number">5.38516481</span>, <span class="number">1.41421356</span>, <span class="number">5.38516481</span>])</span><br><span class="line">LA.norm(b,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">Out[<span class="number">10</span>]: </span><br><span class="line">array([[<span class="number">5.38516481</span>],</span><br><span class="line">       [<span class="number">1.41421356</span>],</span><br><span class="line">       [<span class="number">5.38516481</span>]])</span><br></pre></td></tr></table></figure>
<p>从以上结果看出：如果keepdims等于True，那么输出的矩阵将会是3乘以1的矩阵，输入矩阵是3乘以3。如果用Input/
norm(input)是不会报错的。</p>
]]></content>
      <categories>
        <category>numpy</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql常用命令</title>
    <url>/2022/03/17/mysql%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h1 id="select">select</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT * FROM table_name; # 选取所有内容</span><br><span class="line"></span><br><span class="line">SELECT DISTINCT column_name,column_name</span><br><span class="line">FROM table_name; # 一个列可能包含不同的值，该句可以列出不同的值</span><br><span class="line"></span><br><span class="line">SELECT column_name,column_name</span><br><span class="line">FROM table_name</span><br><span class="line">WHERE column_name operator value; # OPERATOR部分可以是: =,&gt;,&lt;,between,in,like</span><br><span class="line"># WHERE后可以跟逻辑符号 and,or,not</span><br></pre></td></tr></table></figure>
<p>mysql语句对大小写不敏感，select == SELECT</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from emp where not sal &gt; 1500;</span><br><span class="line"></span><br><span class="line">Select * from emp where comm is null;</span><br><span class="line"></span><br><span class="line">Select * from emp where sal in (5000,3000,1500);</span><br><span class="line"></span><br><span class="line">SELECT * FROM Websites WHERE alexa &gt; 15 AND (country=&#x27;CN&#x27; OR country=&#x27;USA&#x27;); # AND和OR可以结合使用</span><br></pre></td></tr></table></figure>
<p>Like模糊查询</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Select * from emp where ename like &#x27;M%&#x27;;</span><br></pre></td></tr></table></figure>
<p>查询 EMP 表中 Ename 列中有 M 的值，M 为要查询内容中的模糊信息。</p>
<ul>
<li><strong>%</strong> 表示多个字值，**_** 下划线表示一个字符；</li>
<li><strong>M%</strong> :
为能配符，正则表达式，表示的意思为模糊查询信息为 M 开头的。</li>
<li><strong>%M%</strong> : 表示查询包含M的所有内容。</li>
<li><strong>%M_</strong> : 表示查询以M在倒数第二位的所有内容。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT column_name,column_name</span><br><span class="line">FROM table_name</span><br><span class="line">ORDER BY column_name,column_name ASC|DESC; # 默认是升序</span><br><span class="line"></span><br><span class="line">INSERT INTO table_name (column1,column2,column3,...)</span><br><span class="line">VALUES (value1,value2,value3,...); # 这种方式可以向table_name表格中插入一行，可以只向某一列插入，其他自动</span><br><span class="line"></span><br><span class="line">UPDATE table_name</span><br><span class="line">SET column1=value1,column2=value2,...</span><br><span class="line">WHERE some_column=some_value; # 用于更新某条记录 </span><br><span class="line"># 比如：</span><br><span class="line">UPDATE Websites </span><br><span class="line">SET alexa=&#x27;5000&#x27;, country=&#x27;USA&#x27; </span><br><span class="line">WHERE name=&#x27;菜鸟教程&#x27;;</span><br><span class="line"></span><br><span class="line">DELETE FROM table_name</span><br><span class="line">WHERE some_column=some_value; # 删除某一行记录</span><br></pre></td></tr></table></figure>
<h1 id="高级用法">高级用法</h1>
<h2 id="join">join</h2>
<figure>
<img src="/2022/03/17/mysql%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/sql-join.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># QL INNER JOIN（简单的 JOIN）。 SQL INNER JOIN 从多个表中返回满足 JOIN 条件的所有行。</span><br><span class="line">SELECT Websites.id, Websites.name, access_log.count, access_log.date</span><br><span class="line">FROM Websites</span><br><span class="line">INNER JOIN access_log</span><br><span class="line">ON Websites.id=access_log.site_id;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>INNER JOIN</strong>：如果表中有至少一个匹配，则返回行</li>
<li><strong>LEFT
JOIN</strong>：即使右表中没有匹配，也从左表返回所有的行</li>
<li><strong>RIGHT
JOIN</strong>：即使左表中没有匹配，也从右表返回所有的行</li>
<li><strong>FULL JOIN</strong>：只要其中一个表中存在匹配，则返回行</li>
</ul>
<h2 id="union">union</h2>
<p>UNION 操作符用于合并两个或多个 SELECT 语句的结果集。</p>
<p>请注意，UNION 内部的每个 SELECT
语句必须拥有相同数量的列。列也必须拥有相似的数据类型。同时，每个 SELECT
语句中的列的顺序必须相同。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT column_name(s) FROM table1</span><br><span class="line">UNION</span><br><span class="line">SELECT column_name(s) FROM table2;</span><br></pre></td></tr></table></figure>
<p><strong>注释：</strong>默认地，UNION
操作符选取不同的值。如果允许重复的值，请使用 UNION ALL。</p>
<h3 id="sql-union-all-语法">SQL UNION ALL 语法</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT column_name(s) FROM table1</span><br><span class="line">UNION ALL</span><br><span class="line">SELECT *column_name(s) FROM table2;</span><br></pre></td></tr></table></figure>
<p><strong>注释：</strong>UNION 结果集中的列名总是等于 UNION 中第一个
SELECT 语句中的列名。</p>
<p>前者只会列出选出的值中不同的一些数据条，后者是所有的都返回。</p>
<h2 id="count">Count</h2>
<p>COUNT() 函数返回匹配指定条件的行数。</p>
<hr>
<h3 id="sql-countcolumn_name-语法">SQL COUNT(column_name) 语法</h3>
<p>COUNT(column_name) 函数返回指定列的值的数目（NULL 不计入）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT COUNT(column_name) FROM table_name;</span><br></pre></td></tr></table></figure>
<h3 id="sql-count-语法">SQL COUNT(*) 语法</h3>
<p>COUNT(*) 函数返回表中的记录数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT COUNT(*) FROM table_name;</span><br></pre></td></tr></table></figure>
<h3 id="sql-countdistinct-column_name-语法">SQL COUNT(DISTINCT
column_name) 语法</h3>
<p>COUNT(DISTINCT column_name) 函数返回指定列的不同值的数目：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT COUNT(DISTINCT column_name) FROM table_name;</span><br></pre></td></tr></table></figure>
<p><strong>注释：</strong>COUNT(DISTINCT) 适用于 ORACLE 和 Microsoft SQL
Server，但是无法用于 Microsoft Access。</p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>pandas遇到的问题汇总</title>
    <url>/2022/03/17/pandas%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<h1 id="join">join()</h1>
<p>Examples</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df = pd.DataFrame(&#123;&#x27;key&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;, &#x27;K3&#x27;, &#x27;K4&#x27;, &#x27;K5&#x27;],</span><br><span class="line">...                    &#x27;A&#x27;: [&#x27;A0&#x27;, &#x27;A1&#x27;, &#x27;A2&#x27;, &#x27;A3&#x27;, &#x27;A4&#x27;, &#x27;A5&#x27;]&#125;)</span><br><span class="line">&gt;&gt;&gt; df</span><br><span class="line">  key   A</span><br><span class="line">0  K0  A0</span><br><span class="line">1  K1  A1</span><br><span class="line">2  K2  A2</span><br><span class="line">3  K3  A3</span><br><span class="line">4  K4  A4</span><br><span class="line">5  K5  A5</span><br><span class="line">&gt;&gt;&gt; other = pd.DataFrame(&#123;&#x27;key&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;],</span><br><span class="line">...                       &#x27;B&#x27;: [&#x27;B0&#x27;, &#x27;B1&#x27;, &#x27;B2&#x27;]&#125;)</span><br><span class="line">&gt;&gt;&gt; other</span><br><span class="line">  key   B</span><br><span class="line">0  K0  B0</span><br><span class="line">1  K1  B1</span><br><span class="line">2  K2  B2</span><br></pre></td></tr></table></figure>
<p>Join DataFrames using their indexes.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df.join(other, lsuffix=&#x27;_caller&#x27;, rsuffix=&#x27;_other&#x27;)</span><br><span class="line">  key_caller   A key_other    B</span><br><span class="line">0         K0  A0        K0   B0</span><br><span class="line">1         K1  A1        K1   B1</span><br><span class="line">2         K2  A2        K2   B2</span><br><span class="line">3         K3  A3       NaN  NaN</span><br><span class="line">4         K4  A4       NaN  NaN</span><br><span class="line">5         K5  A5       NaN  NaN</span><br></pre></td></tr></table></figure>
<p>如果不想以现有index为基础去Join，比如上面的例子想用key这个index去Join，可以：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt; df.join(other.set_index(&#x27;key&#x27;), on=&#x27;key&#x27;)</span><br><span class="line">  key   A    B</span><br><span class="line">0  K0  A0   B0</span><br><span class="line">1  K1  A1   B1</span><br><span class="line">2  K2  A2   B2</span><br><span class="line">3  K3  A3  NaN</span><br><span class="line">4  K4  A4  NaN</span><br><span class="line">5  K5  A5  NaN</span><br></pre></td></tr></table></figure>
<h1 id="merge">merge()</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df1 = pd.DataFrame(&#123;&#x27;lkey&#x27;: [&#x27;foo&#x27;, &#x27;bar&#x27;, &#x27;baz&#x27;, &#x27;foo&#x27;],</span><br><span class="line">                    &#x27;value&#x27;: [1, 2, 3, 5]&#125;)</span><br><span class="line">df2 = pd.DataFrame(&#123;&#x27;rkey&#x27;: [&#x27;foo&#x27;, &#x27;bar&#x27;, &#x27;baz&#x27;, &#x27;foo&#x27;],</span><br><span class="line">                    &#x27;value&#x27;: [5, 6, 7, 8]&#125;)</span><br><span class="line">df1</span><br><span class="line">    lkey value</span><br><span class="line">0   foo      1</span><br><span class="line">1   bar      2</span><br><span class="line">2   baz      3</span><br><span class="line">3   foo      5</span><br><span class="line">df2</span><br><span class="line">    rkey value</span><br><span class="line">0   foo      5</span><br><span class="line">1   bar      6</span><br><span class="line">2   baz      7</span><br><span class="line">3   foo      8</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="选择多列">选择多列</h1>
<p>有时候需要选择DataFrame中的多个列组成一个新的DataFrame，这时候要用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df[[&#x27;column1&#x27;,&#x27;column2&#x27;]] # 注意这里是双层中括号！</span><br></pre></td></tr></table></figure>
<h1 id="drop-满足条件的行">drop 满足条件的行</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_clear = df.drop(df[df[<span class="string">&#x27;x&#x27;</span>]&lt;<span class="number">0.01</span>].index)</span><br><span class="line"><span class="comment"># 也可以使用多个条件</span></span><br><span class="line">df_clear = df.drop(df[(df[<span class="string">&#x27;x&#x27;</span>]&lt;<span class="number">0.01</span>) | (df[<span class="string">&#x27;x&#x27;</span>]&gt;<span class="number">10</span>)].index) <span class="comment">#删除x小于0.01或大于10的行</span></span><br></pre></td></tr></table></figure>
<h1 id="dropna-丢掉某一列中有空的行">dropna 丢掉某一列中有空的行</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.dropna(subset=[<span class="string">&#x27;column_name&#x27;</span>])</span><br></pre></td></tr></table></figure>
<h1 id="applyapplymapmap">apply，applymap，map</h1>
<h2 id="apply">apply</h2>
<p>官方解释是 apply a function along an axis of the dataframe.</p>
<p>apply内函数的是Series，也就是在Series上做函数操作。index可以是dataframe的index，也可以是dataframe的columns(axis=1)。axis=0时，apply
the function to each
column，也就是每一列为一个整体去实施函数。axis=1时，每一行作为一个整体去实施函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.apply(np.<span class="built_in">sum</span>, axis=<span class="number">0</span>)</span><br><span class="line">df.apply(np.<span class="built_in">sum</span>, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="applymap">applymap</h2>
<p>官方解释是 apply a function to a dataframe
elementwise.和apply不一样的是对dataframe中每一个元素分别作函数。</p>
<p>applymap允许实施一个函数，这个函数接受和返回的都是一个标量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.applymap(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(<span class="built_in">str</span>(x)))</span><br></pre></td></tr></table></figure>
<h2 id="map">map</h2>
<p>map调用的对象只能是Series，而上面两个方法的调用对象是Dataframe。</p>
]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>linux命令</title>
    <url>/2022/03/15/linux%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h1 id="tar-压缩以及解压">tar 压缩以及解压</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar -czvf test.tar.gz a.txt   //压缩 a.c文件为test.tar.gz</span><br><span class="line">tar -xzvf test.tar.gz // 解压</span><br><span class="line">tar -tzvf test.tar.gz //列出压缩文件的内容</span><br></pre></td></tr></table></figure>
<h1 id="linux中环境变量">linux中环境变量</h1>
<p>参考https://www.cjavapy.com/article/2250/</p>
<h2 id="介绍">1、介绍</h2>
<p>Linux中环境变量包括系统级和用户级</p>
<p>1）系统级</p>
<p><strong>/etc/environment</strong>：系统在登录时读取的第一个文件，用于为所有进程设置环境变量。系统使用此文件时并不是执行此文件中的命令，而是根据<code>KEY=VALUE</code>模式的代码，对<code>KEY</code>赋值以<code>VALUE</code>，因此文件中如果要定义<code>PATH</code>环境变量，只需加入类似如<code>PATH=$PATH:/xxx/bin</code>的代码即可。</p>
<p><strong>/etc/profile</strong>：是系统登录时执行的第二个文件，可以用于设定针对全系统所有用户的环境变量。该文件一般是调用<code>/etc/bash.bashrc</code>文件。</p>
<p><strong>/etc/bash.bashrc</strong>：系统级的<code>bashrc</code>文件，为每一个运行bash
shell的用户执行此文件。此文件会在用户每次打开shell时执行一次。</p>
<p><strong>注意</strong>：<code>/etc/environment</code>是设置整个系统的环境，而<code>/etc/profile</code>是设置所有用户的环境，前者与登录用户无关，后者与登录用户有关。
这两个文件修改后一般都要重启系统才能生效。</p>
<p>2）用户级</p>
<p><strong>~/.profile:</strong>
是对应当前登录用户的<code>profile</code>文件，用于定制当前用户的个人工作环境。</p>
<p>每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次。默认情况下，会设置一些环境变量，执行用户的<code>.bashrc</code>文件。</p>
<p><strong>~/.bashrc</strong>:
是对应当前登录用户的bash初始化文件，当用户每次打开shell时，系统都会执行此文件一次。通常设置环境变量修改这个文件。</p>
<p>上述配置文件执行先后顺序如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/etc/enviroment `–&gt; `/etc/profile` –&gt; `~/.profile` –&gt; `/etc/bash.bashrc `–&gt; `~/.bashrc</span><br></pre></td></tr></table></figure>
<h2 id="环境变量的作用">2、环境变量的作用</h2>
<p>环境变量相当于给系统或用户应用程序设置的一些参数，具体起什么作用这当然和具体的环境变量相关。比如<code>PATH</code>，是告诉系统，当要求系统运行一个程序而没有告诉它程序所在的完整路径时，系统除了在当前目录下面寻找此程序外，还应到哪些目录下去寻找；再如tc或vc++中，<code>set include=path1;path2</code>;
是告诉编译程序到哪里去找.h类型的文件；当然不仅仅是指定什么路径，还有其它的作用的，如<code>set dircmd=/4</code>
设置一个环境变量的作用是在使用dir命令时会把<code>/4</code>作为缺省的参数添加到你的dir命令之后，就像你的每个命令都加了/4参数，它实际上是给命令解释程序<code>command</code>设置的一个环境变量，并且是给<code>dir</code>这个内部命令。</p>
<h2 id="配置环境变量的方法">3、配置环境变量的方法</h2>
<p>1）临时环境变量</p>
<p>linux下设定环境变量时，如果只是临时用一下，可以直接在shell下用<code>set</code>或<code>export</code>命令设定环境变量。但是只能在当前shell环境下可以用，切换或关闭重新进入就会失效。具体配置方法，如下，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#终端输入：</span><br><span class="line">export MYSQLPATH=/home/mysql  #MYSQLPATH设置为该路径</span><br><span class="line">#终端查看一个特定环境变量包含的内容，比如，MYSQLPATH，PATH</span><br><span class="line">echo $PATH</span><br><span class="line">echo $MYSQLPATH</span><br></pre></td></tr></table></figure>
<p>2）永久环境变量</p>
<p>设置的环境变量，需要经常使用的，而不是临时使用，把上面的设置环境变量命令写到上面提到的相应配置文件中即可，则可以每次开机或打开shell时自动设置，</p>
<p><strong>例如，</strong></p>
<p>只需要当前用户生效的环境变量：</p>
<p>终端中输入：<code>sudo vi ~/.bashrc</code>，编辑这个文件，在其末尾添加：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export MYSQLPATH=/home/mysql:$MYSQLPATH</span><br><span class="line"># path采用:来分隔,冒号左右不需要空格.</span><br><span class="line"># :$MYSQLPATH在后面新添加的优先搜索，$MYSQLPATH:在前面说明新添加的最后搜索，不加代表新路径设置为MYSQLPATH路径。</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：在终端执行，<code>source ~/.bashrc</code>
，使其立即生效，或者重启电脑即可。</p>
<p>设置所有用户生效的环境变更：</p>
<p>终端中输入：<code>sudo vi /etc/profile</code>，编辑这个文件，在其末尾添加：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export MYSQLPATH=/home/mysql:$MYSQLPATH</span><br><span class="line"># path采用:来分隔,冒号左右不需要空格.</span><br><span class="line"># :$MYSQLPATH在后面新添加的优先搜索，$MYSQLPATH:在前面说明新添加的最后搜索，不加代表新路径设置为MYSQLPATH路径。</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：在终端执行，<code>source /etc/profile</code>
，使其立即生效，或者重启电脑即可。</p>
<h1 id="查看linux版本信息">查看linux版本信息</h1>
<p><a href="https://blog.csdn.net/lu_embedded/article/details/44350445" class="uri">https://blog.csdn.net/lu_embedded/article/details/44350445</a></p>
<h1 id="查看进程">查看进程</h1>
<p><a href="https://blog.csdn.net/lechengyuyuan/article/details/16337233" class="uri">https://blog.csdn.net/lechengyuyuan/article/details/16337233</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ps -ef # 查看所有本机进程</span><br><span class="line">ps -ef |grep python # 查看python进程</span><br><span class="line"></span><br><span class="line">kill -9 pid # 杀死某个进程</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/li528405176/article/details/83379164" class="uri">https://blog.csdn.net/li528405176/article/details/83379164</a></p>
<h1 id="将程序留在后台运行">将程序留在后台运行</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nohup python -u test.py &gt; out.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>
<p>参考 <a href="https://www.jianshu.com/p/4041c4e6e1b0" class="uri">https://www.jianshu.com/p/4041c4e6e1b0</a></p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>git常用命令总结</title>
    <url>/2022/01/13/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>github官网给出的教程挺通俗易懂的，移步[https://docs.github.com/cn/get-started/using-git]</p>
<p>本地配置ssh和github，参考官方文档[https://docs.github.com/cn/authentication/connecting-to-github-with-ssh/about-ssh]</p>
<h1 id="远程仓库使用">远程仓库使用</h1>
<p>在本地设置推送到远程仓库的用户名:[https://docs.github.com/cn/get-started/getting-started-with-git/setting-your-username-in-git]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git config --global user.name &quot;your name&quot;</span><br></pre></td></tr></table></figure>
<p>远程URL是 Git 一种指示“您的代码存储位置”的绝佳方式，您只能推送到两类
URL 地址：</p>
<ul>
<li>HTTPS URL，如 <code>https://github.com/user/repo.git</code></li>
<li>SSH URL，如 <code>git@github.com:user/repo.git</code></li>
</ul>
<p>Git 将远程 URL 与名称相关联，您的默认远程通常名为
<code>origin</code></p>
<p>创建远程仓库，并将其命名为master</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote add master &lt;REMOTE_URL&gt; </span><br></pre></td></tr></table></figure>
<p>如果后面想更改url，使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote set-url origin &lt;new_url&gt;</span><br></pre></td></tr></table></figure>
<p>查看远程仓库设置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote -v</span><br></pre></td></tr></table></figure>
<p>重命名远程仓库</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote rename origin destination</span><br></pre></td></tr></table></figure>
<p>删除远程仓库</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote rm destination</span><br></pre></td></tr></table></figure>
<p>推送提交至远程仓库，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git push &lt;REMOTENAME&gt; &lt;LOCALBRANCHNAME&gt;:&lt;REMOTEBRANCHNAME&gt; </span><br></pre></td></tr></table></figure>
<p>拉取某远程仓库距离上一次抓取之后的工作：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git fetch &lt;remote_name&gt;</span><br><span class="line">必须注意 git fetch 命令只会将数据下载到你的本地仓库——它并不会自动合并或修改你当前的工作。 当准备好时你必须手动将其合并入你的工作。</span><br></pre></td></tr></table></figure>
<p>显示某远程仓库的信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote show &lt;remote_name&gt;</span><br></pre></td></tr></table></figure>
<h1 id="忽略文件">忽略文件</h1>
<p><a href="https://docs.github.com/cn/get-started/getting-started-with-git/ignoring-files">见文档</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">touch .gitignore #该命令在项目根目录会创建一个.gitignore文件,然后往该文件中填东西</span><br></pre></td></tr></table></figure>
<p>如果有些我们不需要跟踪的文件已经提交到了暂存区，那么使用下面的命令来删除暂存区的该文件，再将该文件写入.gitignore文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git rm --cached FILENAME 该命令直接将暂存区的那个版本删除了</span><br><span class="line">或者 git restore --staged &lt;file&gt; 该命令会用暂存区的代码覆盖掉工作区的代码</span><br></pre></td></tr></table></figure>
<h1 id="在自己的project中添加别人的project">在自己的project中添加别人的project</h1>
<p><a href="https://devconnected.com/how-to-add-and-update-git-submodules/" class="uri">https://devconnected.com/how-to-add-and-update-git-submodules/</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git submodule add &lt;remote_url&gt; &lt;destination_folder&gt;</span><br><span class="line"></span><br><span class="line">git commit -m &quot;Added the submodule to the project.&quot;</span><br><span class="line"></span><br><span class="line">git push</span><br><span class="line"></span><br><span class="line">git submodule update --init --recursive # 如果想要拉取别人仓库里的submodule到本地</span><br></pre></td></tr></table></figure>
<h1 id="分支">分支</h1>
<p>创建分支：当执行git init时，默认创建名字是master的分支</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git branch &lt;branch_name&gt;</span><br></pre></td></tr></table></figure>
<p>切换到某分支:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git checkout &lt;branch_name&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>分支切换会改变你工作目录中的文件。在切换分支时，一定要注意你工作目录里的文件会被改变。
如果是切换到一个较旧的分支，你的工作目录会恢复到该分支最后一次提交时的样子。
如果 Git 不能干净利落地完成这个任务，它将禁止切换分支。</p>
</blockquote>
<p>上面两条命令可以使用一条命令搞定：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git checkout -b &lt;branch_name&gt;</span><br></pre></td></tr></table></figure>
<p>在一条分支上，比如hotfix修改一些文件提交后，需要回到master分支并将hotfix上的分支的修改合并到master</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git merge hotfix</span><br></pre></td></tr></table></figure>
<p>这是一个Fast-forward。合并完之后master会和hotfix指向同一个位置，这时可以删除hotfix这个分支：<code>git branch -d hotfix</code></p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>python OS 文件/目录常用方法总结</title>
    <url>/2021/12/30/python-OS-%E6%96%87%E4%BB%B6-%E7%9B%AE%E5%BD%95%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.makedirs(path[, mode]) </span><br><span class="line">递归文件夹创建函数。像mkdir(), 但创建的所有intermediate-level文件夹需要包含子文件夹。</span><br><span class="line"></span><br><span class="line">os.walk(top[, topdown=<span class="literal">True</span>[, onerror=<span class="literal">None</span>[, followlinks=<span class="literal">False</span>]]])</span><br><span class="line">输出在文件夹中的文件名通过在树中游走，向上或者向下。返回的是一个三元组(root,dirs,files),是一个生成器类型，需要用<span class="keyword">for</span>遍历读取</span><br><span class="line"></span><br><span class="line">os.chdir(path)</span><br><span class="line">改变当前工作目录</span><br><span class="line">	</span><br><span class="line">os.listdir(path)</span><br><span class="line">返回path指定的文件夹包含的文件或文件夹的名字的列表。</span><br></pre></td></tr></table></figure>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(<span class="string">&quot;.&quot;</span>, topdown=<span class="literal">False</span>):</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> files:</span><br><span class="line">        <span class="built_in">print</span>(os.path.join(root, name))</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> dirs:</span><br><span class="line">        <span class="built_in">print</span>(os.path.join(root, name))</span><br><span class="line">        </span><br><span class="line">path = <span class="string">&quot;/var/www/html/&quot;</span></span><br><span class="line">dirs = os.listdir( path )</span><br><span class="line"><span class="comment"># 输出所有文件和文件夹</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> dirs:</span><br><span class="line">   <span class="built_in">print</span> (file)</span><br></pre></td></tr></table></figure>
<p><code>os.walk</code>和<code>os.listdir</code>两个函数的区别在于前者会遍历到子文件夹中的子文件，而后者只是返回你传入的path中的文件夹名字和文件名字。</p>
<p><code>os</code>库中有一个<code>path</code>的模块，专门用于处理文件<code>path</code>相关的属性信息。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">os.path.abspath(path)</span><br><span class="line">返回绝对路径</span><br><span class="line"></span><br><span class="line">os.path.basename(path)</span><br><span class="line">返回文件名</span><br><span class="line"></span><br><span class="line">os.path.exists(path)</span><br><span class="line">如果路径 path 存在，返回 True；如果路径 path 不存在，返回 False。</span><br><span class="line"></span><br><span class="line">os.path.join(path1[, path2[, ...]])</span><br><span class="line">把目录和文件名合成一个路径</span><br><span class="line"></span><br><span class="line">os.path.split(path)</span><br><span class="line">把路径分割成 dirname 和 basename，返回一个元组</span><br><span class="line"></span><br><span class="line">os.path.splitext(path)</span><br><span class="line">分割路径，返回路径名和文件扩展名的元组</span><br><span class="line"></span><br><span class="line">os.path.walk(path, visit, arg)</span><br><span class="line">遍历path，进入每个目录都调用visit函数，visit函数必须有3个参数(arg, dirname, names)，dirname表示当前目录的目录名，names代表当前目录下的所有文件名，args则为walk的第三个参数</span><br></pre></td></tr></table></figure>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>( os.path.basename(<span class="string">&#x27;/root/runoob.txt&#x27;</span>) )   <span class="comment"># 返回文件名</span></span><br><span class="line"><span class="built_in">print</span>( os.path.dirname(<span class="string">&#x27;/root/runoob.txt&#x27;</span>) )    <span class="comment"># 返回目录路径</span></span><br><span class="line"><span class="built_in">print</span>( os.path.split(<span class="string">&#x27;/root/runoob.txt&#x27;</span>) )      <span class="comment"># 分割文件名与路径</span></span><br><span class="line"><span class="built_in">print</span>( os.path.join(<span class="string">&#x27;root&#x27;</span>,<span class="string">&#x27;test&#x27;</span>,<span class="string">&#x27;runoob.txt&#x27;</span>) )  <span class="comment"># 将目录和文件名合成一个路径</span></span><br><span class="line"><span class="built_in">print</span>( os.path.splitext(<span class="string">&#x27;/root/runoob.txt&#x27;</span>) )</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">runoob.txt</span><br><span class="line">/root</span><br><span class="line">(&#x27;/root&#x27;, &#x27;runoob.txt&#x27;)</span><br><span class="line">root/test/runoob.txt</span><br><span class="line">(&#x27;/root/run/test&#x27;, &#x27;.txt&#x27;)</span><br></pre></td></tr></table></figure>
<p>上面的split和splitext，前者分割出了文件名和路径，而后者可以分割出路径名和扩展名，如果想要获得文件的扩展名，可以用splitext，传入文件的path就可以了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">os.mknod(filename) 创建</span><br></pre></td></tr></table></figure>
<h1 id="复制文件和删除文件移动文件">复制文件和删除文件,移动文件</h1>
<p>如果是删除一个目录，可以使用以下两种方式:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="comment"># method 1</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(root_dir):</span><br><span class="line">  shutil.rmtree(root_dir) <span class="comment"># 这里不可以使用os.removedirs(),removedirs只可以删除非空的文件夹，rmdir也是</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># method 2</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(os.listdir(root_dir)) &gt; <span class="number">0</span>:</span><br><span class="line">  <span class="keyword">for</span> file <span class="keyword">in</span> os.scandir(root_dir):</span><br><span class="line">    os.remove(file.path)</span><br></pre></td></tr></table></figure>
<p>如果想复制一个文件到另外一个文件夹，参考 <a href="https://zhuanlan.zhihu.com/p/35725217" class="uri">https://zhuanlan.zhihu.com/p/35725217</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">copyfile(source_file, destination_file)</span><br></pre></td></tr></table></figure>
<p>记住这里第二个参数一定要是可写入的文件名字，而不是目录。</p>
<h2 id="移动文件">移动文件</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"> </span><br><span class="line">file_source = <span class="string">&#x27;Path/Of/Directory&#x27;</span></span><br><span class="line">file_destination = <span class="string">&#x27;Path/Of/Directory&#x27;</span></span><br><span class="line"> </span><br><span class="line">get_files = os.listdir(file_source)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> g <span class="keyword">in</span> get_files:</span><br><span class="line">    shutil.move(file_source + g, file_destination)</span><br></pre></td></tr></table></figure>
<h1 id="读写csv文件">读写csv文件</h1>
<p>第一个方式是用pandas，具体不介绍。</p>
<p>这里总结一下csv库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取csv文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filename,<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">     csv_reader = csv.reader(csvfile)</span><br><span class="line">     <span class="keyword">for</span> row <span class="keyword">in</span> csv_reader:</span><br><span class="line">        <span class="built_in">print</span>(row[<span class="number">1</span>]) <span class="comment"># 用列表的index取值</span></span><br><span class="line"><span class="comment"># 写csv文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filename,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    csv_writer = csv.writer(csvfile)</span><br><span class="line">    spamwriter.writerow([<span class="string">&#x27;Spam&#x27;</span>, <span class="string">&#x27;Lovely Spam&#x27;</span>, <span class="string">&#x27;Wonderful Spam&#x27;</span>]) <span class="comment"># writerow接受一个list，所有值都会写在一行里</span></span><br><span class="line">    spamwriter.writerows([[],[],[]]) <span class="comment"># writerows写入多行，每一行是一个列表，传进去的是列表的列表</span></span><br></pre></td></tr></table></figure>
<p>除了写入list，还可以写字典类型的数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># my data rows as dictionary objects </span></span><br><span class="line">mydict =[&#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;COE&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;9.0&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Nikhil&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;2&#x27;</span>&#125;, </span><br><span class="line">         &#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;COE&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;9.1&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Sanchit&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;2&#x27;</span>&#125;, </span><br><span class="line">         &#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;IT&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;9.3&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Aditya&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;2&#x27;</span>&#125;, </span><br><span class="line">         &#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;SE&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;9.5&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Sagar&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;1&#x27;</span>&#125;, </span><br><span class="line">         &#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;MCE&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;7.8&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Prateek&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;3&#x27;</span>&#125;, </span><br><span class="line">         &#123;<span class="string">&#x27;branch&#x27;</span>: <span class="string">&#x27;EP&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>: <span class="string">&#x27;9.1&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Sahil&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="string">&#x27;2&#x27;</span>&#125;] </span><br><span class="line">    </span><br><span class="line"><span class="comment"># field names </span></span><br><span class="line">fields = [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;branch&#x27;</span>, <span class="string">&#x27;year&#x27;</span>, <span class="string">&#x27;cgpa&#x27;</span>] </span><br><span class="line">    </span><br><span class="line"><span class="comment"># name of csv file </span></span><br><span class="line">filename = <span class="string">&quot;university_records.csv&quot;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># writing to csv file </span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> csvfile: </span><br><span class="line">    <span class="comment"># creating a csv dict writer object </span></span><br><span class="line">    writer = csv.DictWriter(csvfile, fieldnames = fields) </span><br><span class="line">        </span><br><span class="line">    <span class="comment"># writing headers (field names) </span></span><br><span class="line">    writer.writeheader() </span><br><span class="line">        </span><br><span class="line">    <span class="comment"># writing data rows </span></span><br><span class="line">    writer.writerows(mydict)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python3.7: error while loading shared libraries: libpython3.7m.so.1.0</title>
    <url>/2021/12/29/Python3-7-error-while-loading-shared-libraries-libpython3-7m-so-1-0/</url>
    <content><![CDATA[<p>在ubuntu上安装了其他版本的python之后遇到如下报错：</p>
<p>参考[https://stackoverflow.com/questions/58649177/python3-7-error-while-loading-shared-libraries-libpython3-7m-so-1-0]</p>
<p>对于我的这种情况，先查看<code>libpython3.7m.so.1.0</code>这个文件在哪里？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">locate libpython3.7m.so.1.0</span><br></pre></td></tr></table></figure>
<p>上面的命令如果返回无，要先更新下系统内文件系统的index字典</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">updatedb</span><br></pre></td></tr></table></figure>
<p>找出libpython3.7m.so.1.0在哪一个lib文件夹内，然后将该lib路径加入搜索路径，可以通过：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=/lib:/usr/lib:/usr/local/lib</span><br></pre></td></tr></table></figure>
<p>以上方式只对该session起作用，如果reboot了系统就会失效，想要永久的方式:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ldconfig /usr/local/lib </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>pandas中对于时间TimeStamp的处理</title>
    <url>/2021/12/14/pandas%E4%B8%AD%E5%AF%B9%E4%BA%8E%E6%97%B6%E9%97%B4TimeStamp%E7%9A%84%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<h1 id="pandas.to_datetime"><code>pandas.to_datetime()</code></h1>
<p><strong>pandas.to_datetime(</strong><em>arg</em><strong>,</strong>
<em>errors='raise'</em><strong>,</strong>
<em>dayfirst=False</em><strong>,</strong>
<em>yearfirst=False</em><strong>,</strong>
<em>utc=None</em><strong>,</strong>
<em>format=None</em><strong>,</strong>
<em>exact=True</em><strong>,</strong>
<em>unit=None</em><strong>,</strong>
<em>infer_datetime_format=False</em><strong>,</strong>
<em>origin='unix'</em><strong>,</strong>
<em>cache=True</em><strong>)</strong>[<a href="https://github.com/pandas-dev/pandas/blob/v1.3.5/pandas/core/tools/datetimes.py#L676-L919">source]</a><a href="https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html?highlight=to_datetime#pandas.to_datetime">¶</a></p>
<p>Parameters</p>
<ul>
<li><p><strong>arg</strong> int, float, str, datetime, list, tuple, 1-d
array, Series, DataFrame/dict-like</p>
<p>The object to convert to a datetime</p></li>
<li><p><strong>format</strong> str, default None</p></li>
</ul>
<p>The strftime to parse time, eg “<code>%d/%m/%Y</code>”, note that
“<code>%f</code>” will parse all the way up to nanoseconds.</p>
<hr>
<p>该函数可以接受一个series，可以接受一个dateFrame。如果不确定它是否可以以默认的格式去解析你的时间，format参数可以不传递。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;year&#x27;</span>: [<span class="number">2015</span>, <span class="number">2016</span>],</span><br><span class="line"><span class="meta">... </span>                   <span class="string">&#x27;month&#x27;</span>: [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line"><span class="meta">... </span>                   <span class="string">&#x27;day&#x27;</span>: [<span class="number">4</span>, <span class="number">5</span>]&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.to_datetime(df)</span><br><span class="line"><span class="number">0</span>   <span class="number">2015</span>-02-04</span><br><span class="line"><span class="number">1</span>   <span class="number">2016</span>-03-05</span><br><span class="line">dtype: datetime64[ns]</span><br></pre></td></tr></table></figure>
<p>上面的这种产生<code>datetime</code>的方式，在创建dateframe的时候，可以指定缩写或者缩写的复数形式，其他形式不接受：<code>[‘year’, ‘month’, ‘day’, ‘minute’, ‘second’, ‘ms’, ‘us’, ‘ns’])</code></p>
<h1 id="pandas.to_timedelta"><code>pandas.to_timedelta()</code></h1>
<p>timedelta是两个时间之间的差值，该函数可以帮助我们求两个<code>timestamp</code>之间的差是多少（单位可以是<code>days,hours,minutes,seconds</code>）</p>
<p>Parameters</p>
<ul>
<li><p><strong>arg</strong> str, timedelta, list-like or Series</p></li>
<li><p><strong>unit</strong> str, optional</p>
<p>Denotes the unit of the arg for numeric arg. Defaults to
<code>"ns"</code>.</p>
<p>Possible values:</p>
<ul>
<li>‘W’</li>
<li>‘D’ / ‘days’ / ‘day’</li>
<li>‘hours’ / ‘hour’ / ‘hr’ / ‘h’</li>
<li>‘m’ / ‘minute’ / ‘min’ / ‘minutes’ / ‘T’</li>
<li>‘S’ / ‘seconds’ / ‘sec’ / ‘second’</li>
<li>‘ms’ / ‘milliseconds’ / ‘millisecond’ / ‘milli’ / ‘millis’ /
‘L’</li>
<li>‘us’ / ‘microseconds’ / ‘microsecond’ / ‘micro’ / ‘micros’ /
‘U’</li>
<li>‘ns’ / ‘nanoseconds’ / ‘nano’ / ‘nanos’ / ‘nanosecond’ / ‘N’</li>
</ul></li>
</ul>
<hr>
<p>这里如果传入的是str，是不允许再传入unit参数了，不然会报错。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; pd.to_timedelta(<span class="string">&#x27;15days 2hours&#x27;</span>)</span><br><span class="line">Timedelta(<span class="string">&#x27;15 days 02:00:00&#x27;</span>)</span><br><span class="line"></span><br><span class="line">&gt;&gt; pd.to_timedelta(<span class="string">&#x27;1 days 06:05:01.00003&#x27;</span>)</span><br><span class="line">Timedelta(<span class="string">&#x27;1 days 06:05:01.000030&#x27;</span>)</span><br><span class="line"></span><br><span class="line">&gt;&gt; pd.to_timedelta(<span class="number">4</span>,unit=<span class="string">&#x27;days&#x27;</span>)</span><br><span class="line">Timedelta(<span class="string">&#x27;4 days 00:00:00&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="series.dt"><code>Series.dt()</code></h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; seconds_series = pd.Series(pd.date_range(<span class="string">&quot;2000-01-01&quot;</span>, periods=<span class="number">3</span>, freq=<span class="string">&quot;s&quot;</span>))</span><br><span class="line">seconds_series</span><br><span class="line"><span class="number">0</span>   <span class="number">2000</span>-01-01 <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line"><span class="number">1</span>   <span class="number">2000</span>-01-01 <span class="number">00</span>:<span class="number">00</span>:01</span><br><span class="line"><span class="number">2</span>   <span class="number">2000</span>-01-01 <span class="number">00</span>:<span class="number">00</span>:02</span><br><span class="line">dtype: datetime64[ns]</span><br><span class="line">&gt;&gt; seconds_series.dt.second</span><br><span class="line"><span class="number">0</span>    <span class="number">0</span></span><br><span class="line"><span class="number">1</span>    <span class="number">1</span></span><br><span class="line"><span class="number">2</span>    <span class="number">2</span></span><br><span class="line">dtype: int64</span><br></pre></td></tr></table></figure>
<p>dt是Series的一个方法，当调用dt时，Series中必须是timestamp的格式。</p>
<p>当调用完dt后可以获取时间的具体年份等信息：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">seconds_series.dt.date <span class="comment"># 2000-01-01</span></span><br><span class="line">seconds_series.dt.hour <span class="comment"># 00</span></span><br><span class="line">seconds_series.dt.quarter <span class="comment"># 返回第几季度</span></span><br><span class="line">seconds_series.dt.time <span class="comment"># 00:00:00</span></span><br><span class="line">seconds_series.dt.year <span class="comment"># 2000</span></span><br><span class="line">seconds_series.dt.month <span class="comment"># 01</span></span><br><span class="line">seconds_series.dt.day <span class="comment"># 01</span></span><br><span class="line">seconds_series.dt.weekday <span class="comment"># 返回一个0-6的数，0表示周一，6表示周日</span></span><br><span class="line">seconds_series.dt.dayname() <span class="comment"># 会返回星期的名字：Monday</span></span><br></pre></td></tr></table></figure>
<p>有的时候我们想获取某一天是全年中的第几周，这时候weekday就不管用了，此时采用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dseries.dt.isocalendar()[<span class="string">&#x27;week&#x27;</span>] </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>pandas中count(),value_counts(),unique)区别</title>
    <url>/2021/12/13/pandas%E4%B8%ADcount-value-counts-unique-%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<h1 id="count">count()</h1>
<p>不管是Series还是DateFrame，两个类都有这个方法。</p>
<p>对于DateFrame，返回的是每一列中非空的元素的个数，可以指定是以行去统计还是以列去统计。</p>
<p>对于Series，返回的是Series中非空元素的个数。</p>
<p>对于groupby，返回的是每一组中元素的个数，这里的个数不包含空字段。</p>
<h1 id="value_counts">value_counts()</h1>
<p>对于DateFrame，返回的是unique行的个数，这里的unique行指的是只要其中一个column的值不一样就算不一样的行。</p>
<p>对于Series，返回的是每一个unique元素的个数，也就是每一个unique的元素，它会统计它在Series中出现了多少次。</p>
<h1 id="unique">unique()</h1>
<p>对于Series，返回的是所有unique的元素们，如果你想统计所有不一样的元素有多少个，可以统计list中元素的个数，如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">len(df[&#x27;a&#x27;].unique().tolist())  </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>Part-3-Model Combination</title>
    <url>/2021/12/06/Part-3-Model-Combination/</url>
    <content><![CDATA[<h1 id="方差variance-偏差bias">方差Variance &amp; 偏差bias</h1>
<p>方差衡量的是train的过程中的loss和dev set
loss之间的差距，bias衡量的是train set loss和真实human
loss之间的差距。</p>
<p>减小bias：
用更复杂的模型，用更适合的优化算法RMSprop、Momentum，超参数搜索，Boosting，Stacking</p>
<p>减小variance: 用更简单的模型，正则化L1/L2,drop
out，用更多的数据，Bagging，Stacking</p>
<figure>
<img src="/2021/12/06/Part-3-Model-Combination/image-20211206151138567.png" alt="image-20211206151138567">
<figcaption aria-hidden="true">image-20211206151138567</figcaption>
</figure>
<h2 id="bagging---bootstrap-aggragrating">Bagging - Bootstrap
AGGragrating</h2>
<ul>
<li>同时训练n个模型，最终取平均作为结果</li>
<li>每一个模型在m个样本中训练，m个样本是对训练数据bootstrap采样来的（有放回的采样）</li>
</ul>
]]></content>
      <categories>
        <category>李沐-实用机器学习(学习笔记)</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>在linux上配置GPU深度学习环境</title>
    <url>/2021/12/03/%E5%9C%A8linux%E4%B8%8A%E9%85%8D%E7%BD%AEGPU%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<h1 id="配置云服务器">配置云服务器</h1>
<p>系统 ： ubuntu 18.04</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 更新所有环境</span><br><span class="line">sudo apt update </span><br><span class="line"></span><br><span class="line"># 安装开发所需的基本包</span><br><span class="line">sudo apt install build-essential</span><br><span class="line"></span><br><span class="line"># 安装cuda.这里会连同显卡驱动一起安装</span><br><span class="line">wget https://developer.download.nvidia.com/compute/cuda/11.5.1/local_installers/cuda_11.5.1_495.29.05_linux.run</span><br><span class="line">sudo sh cuda_11.5.1_495.29.05_linux.run</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 去anaconda官方复制miniconda的下载链接</span><br><span class="line">wget https://repo.anaconda.com/miniconda/Miniconda3-py38_4.10.3-Linux-x86_64.sh</span><br><span class="line"></span><br><span class="line"># 进入conda环境</span><br><span class="line">bash</span><br><span class="line"></span><br><span class="line">之后就继续用pip安装所需要的包</span><br><span class="line">apt-get install python3.8</span><br></pre></td></tr></table></figure>
<p>这里需要知道，如果我们在云端配置的环境，需要将云端的jupyter
notebook的运行端口映射到本地来，可以这样做：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh -L8888:localhost:8888 ubuntu@100.20.65.33</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果使用aws配置远端服务器，在本地连接时使用:</p>
<p>$ chomd 400 Downloads/d2l.pem</p>
<p>$ ssh -i Downloads/d2l.pem ubuntu@54.245.23.40</p>
<p>以上两条命令中将密钥path和ip地址都替换成自己的。</p>
</blockquote>
<h1 id="docker中的tensorflow-gpu配置">docker中的tensorflow-gpu配置</h1>
<p>首先在宿主机（本机）安装好<code>NVIDIA GPU</code>的驱动程序，然后对于每一个容器都要各自安装对应版本的<code>cuda</code>和<code>cudnn</code>。</p>
<h2 id="下载tensorflow-docker镜像">下载TensorFlow Docker镜像</h2>
<p>官方 TensorFlow Docker 映像位于 <a href="https://hub.docker.com/r/tensorflow/tensorflow/">tensorflow/tensorflow</a>
Docker Hub 代码库中。映像版本按照以下格式进行<a href="https://hub.docker.com/r/tensorflow/tensorflow/tags/">标记</a>：</p>
<table>
<colgroup>
<col style="width: 15%">
<col style="width: 84%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">标记</th>
<th style="text-align: left;">说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>latest</code></td>
<td style="text-align: left;">TensorFlow CPU
二进制映像的最新版本。（默认版本）</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>nightly</code></td>
<td style="text-align: left;">TensorFlow 映像的每夜版。（不稳定）</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em><code>version</code></em></td>
<td style="text-align: left;">指定 TensorFlow
二进制映像的版本，例如：2.1.0</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>devel</code></td>
<td style="text-align: left;">TensorFlow <code>master</code>
开发环境的每夜版。包含 TensorFlow 源代码。</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>custom-op</code></td>
<td style="text-align: left;">用于开发 TF
自定义操作的特殊实验性映像。详见<a href="https://github.com/tensorflow/custom-op">此处</a>。</td>
</tr>
</tbody>
</table>
<p>每个基本标记都有会添加或更改功能的变体：</p>
<table>
<colgroup>
<col style="width: 17%">
<col style="width: 82%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">标记变体</th>
<th style="text-align: left;">说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>tag-gpu</code></td>
<td style="text-align: left;">支持 GPU 的指定标记版本。（<a href="https://www.tensorflow.org/install/docker?hl=zh_cn#gpu_support">详见下文</a>）</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>tag-jupyter</code></td>
<td style="text-align: left;">针对 Jupyter 的指定标记版本（包含
TensorFlow 教程笔记本）</td>
</tr>
</tbody>
</table>
<p>您可以一次使用多个变体。例如，以下命令会将 TensorFlow
版本映像下载到计算机上：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker pull tensorflow/tensorflow                     # latest stable release</span><br><span class="line">docker pull tensorflow/tensorflow:devel-gpu           # nightly dev release w/ GPU support</span><br><span class="line">docker pull tensorflow/tensorflow:latest-gpu-jupyter  # latest release w/ GPU support and Jupyter</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意如果要用gpu版本的tensorflow，需要pull带有gpu tag的镜像.
<code>docker pull tensorflow/tensorflow:latest-gpu</code>。如果不带gpu标签，会默认下载CPU版本的tensorflow。</p>
</blockquote>
<h3 id="验证tensorflow-gpu">验证tensorflow gpu</h3>
<p>查看是否有GPU：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">gpu_device_name = tf.test.gpu_device_name()</span><br><span class="line"><span class="built_in">print</span>(gpu_device_name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># GPU是否可用,返回True或者False</span></span><br><span class="line">tf.test.is_gpu_available()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.client <span class="keyword">import</span> device_lib</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出所有的本地机器设备</span></span><br><span class="line">local_device_protos = device_lib.list_local_devices()</span><br><span class="line"><span class="comment"># 打印</span></span><br><span class="line"><span class="comment">#     print(local_device_protos)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只打印GPU设备</span></span><br><span class="line">[<span class="built_in">print</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> local_device_protos <span class="keyword">if</span> x.device_type == <span class="string">&#x27;GPU&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h2 id="docker中安装多种cuda版本并切换">docker中安装多种cuda版本并切换</h2>
<p>去<code>cuda</code>官网下载所需版本，以<code>.run</code>结尾。楼主系统为<code>linux</code>。</p>
<p>进入到放置 <code>cuda_9.0.176_384.81_linux.run</code> 的目录：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo chmod +x cuda_9.0.176_384.81_linux.run # 为 cuda_9.0.176_384.81_linux.run 添加可执行权限</span><br><span class="line">./cuda_9.0.176_384.81_linux.run # 安装 cuda_9.0.176_384.81_linux.run</span><br></pre></td></tr></table></figure>
<p>在安装过程中截取其中比较重要的几个选择：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Do you accept the previously read EULA?</span><br><span class="line">accept/decline/quit: accept</span><br><span class="line"></span><br><span class="line">Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 384.81?</span><br><span class="line">(y)es/(n)o/(q)uit: n # 如果在这之前已经安装好更高版本的显卡驱动就不需要再重复安装，如果需要重复安装就选择 yes,此外还需要关闭图形界面。</span><br><span class="line"></span><br><span class="line">Install the CUDA 9.0 Toolkit?</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line"></span><br><span class="line">Enter Toolkit Location</span><br><span class="line"> [ default is /usr/local/cuda-9.0 ]: # 一般选择默认即可，也可以选择安装在其他目录，在需要用的时候指向该目录或者使用软连接 link 到 /usr/local/cuda。</span><br><span class="line"></span><br><span class="line">/usr/local/cuda-9.0 is not writable.</span><br><span class="line">Do you wish to run the installation with &#x27;sudo&#x27;?</span><br><span class="line">(y)es/(n)o: y</span><br><span class="line"></span><br><span class="line">Please enter your password: </span><br><span class="line">Do you want to install a symbolic link at /usr/local/cuda? # 是否将安装目录通过软连接的方式 link 到 /usr/local/cuda，yes or no 都可以，取决于你是否使用 /usr/local/cuda 为默认的 cuda 目录。</span><br><span class="line">(y)es/(n)o/(q)uit: n</span><br><span class="line"></span><br><span class="line">Install the CUDA 9.0 Samples?</span><br><span class="line">(y)es/(n)o/(q)uit: n</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>选择的汇总： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Driver:   Not Selected</span><br><span class="line">Toolkit:  Installed in /usr/local/cuda-9.0</span><br><span class="line">Samples:  Not Selected</span><br><span class="line"></span><br><span class="line">Please make sure that</span><br><span class="line"> -   PATH includes /usr/local/cuda-9.0/bin</span><br><span class="line"> -   LD_LIBRARY_PATH includes /usr/local/cuda-9.0/lib64, or, add /usr/local/cuda-9.0/lib64 to /etc/ld.so.conf and run ldconfig as root</span><br><span class="line"></span><br><span class="line">To uninstall the CUDA Toolkit, run the uninstall script in /usr/local/cuda-9.0/bin</span><br><span class="line"></span><br><span class="line">Please see CUDA_Installation_Guide_Linux.pdf in /usr/local/cuda-9.0/doc/pdf for detailed information on setting up CUDA.</span><br><span class="line"></span><br><span class="line">***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 384.00 is required for CUDA 9.0 functionality to work.</span><br><span class="line">To install the driver using this installer, run the following command, replacing &lt;CudaInstaller&gt; with the name of this run file:</span><br><span class="line">    sudo &lt;CudaInstaller&gt;.run -silent -driver</span><br></pre></td></tr></table></figure> 安装完成后可以在 <code>/usr/local</code>
目录下看到：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cuda-11.1 # 之前安装的cuda-11.1 </span><br><span class="line">cuda-9.0 # 刚刚安装的cuda-9.0 </span><br><span class="line">cuda # cuda-10.0 的软连接</span><br></pre></td></tr></table></figure>
<p>多版本切换：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#在切换cuda版本时</span><br><span class="line">rm -rf /usr/local/cuda#删除之前创建的软链接</span><br><span class="line">sudo ln -s /usr/local/cuda-8.0/ /usr/local/cuda/</span><br><span class="line">nvcc --version #查看当前 cuda 版本</span><br><span class="line"></span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2016 NVIDIA Corporation</span><br><span class="line">Built on Mon_Jan_23_12:24:11_CST_2017</span><br><span class="line">Cuda compilation tools, release 8.0, V8.0.62</span><br><span class="line"></span><br><span class="line">#cuda8.0 切换到 cuda9.0 </span><br><span class="line">rm -rf /usr/local/cuda</span><br><span class="line">sudo ln -s /usr/local/cuda-9.0/ /usr/local/cuda/</span><br><span class="line">nvcc --version</span><br></pre></td></tr></table></figure>
<p>上面的前提是<code>linux</code>系统的环境变量中<code>(~./bashrc文件)</code>，<code>cuda</code>的路径是<code>/usr/local/cuda</code></p>
<h2 id="tensorflow_decision_forests使用">tensorflow_decision_forests使用</h2>
<p>我一开始pull了tensorflow-gpu版本的docker环境，想用一下tensorflow的tensorflow_decision_forests库，该库是随机森林的集合库，内有很多算法可以用。官方使用document在[https://www.tensorflow.org/decision_forests/tutorials/beginner_colab?hl=zh_cn]。</p>
<p>我一开始没注意，后来发现我在docker环境中直接用pip安装该库时，帮我又安装了cpu版本的tensorflow，这样就和我的gpu版本冲突了，然后去网上搜了一下，发现该库现在还有很多使用限制：1.
仅仅支持linux，不支持windows和mac 2. 仅支持cpu
，还没有gpu版本，见[https://github.com/tensorflow/decision-forests/issues/38].作者的意思是用gpu训练会更复杂，更详细的我就没看了。</p>
<p>然后我的做法是在该tensorflow-gpu的docker环境中使用<strong>virtualenv</strong>创建一个tensorfow的cpu虚拟环境，然后再用pip安装TF-DF这个包。避免污染docker主环境中的tensorflow-gpu。</p>
<blockquote>
<p>之所以不用miniconda，是因为conda会默认替代掉我容器自带的python以及安装好的tensorflow-gpu。我只是想用一下TFDF这个库，不想太折腾。</p>
</blockquote>
<p>此处贴virtualenv的命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install virtualenv</span><br><span class="line"></span><br><span class="line">virtualenv venv # 在项目目录中执行，会创建一个名称为venv的虚拟环境，venv文件夹下包含python和pip</span><br><span class="line"></span><br><span class="line">source venv/bin/activate # 激活环境</span><br><span class="line"></span><br><span class="line">激活完了之后就pip安装tensorflow就行了</span><br></pre></td></tr></table></figure>
<h1 id="本地机器vscode配置使用远程服务器中运行的容器">本地机器vscode配置使用远程服务器中运行的容器</h1>
<p>参考 [https://zhuanlan.zhihu.com/p/80099904]</p>
<p>其中有几个地方需要注意一下的是：</p>
<ol type="1">
<li><p>如果遇到在本地机器vscode中使用插件remote
ssh连接不上容器的问题时，需要<code>vim /etc/ssh/sshd_config    ,将PermitRootLogin的值改为yes（去掉前面的#号）</code></p></li>
<li><p>远程容器里面还需要有python插件，才能在本地机器vscode中debug代码</p></li>
<li><p>上面博主服务器端口映射的是容器的22端口，我没有尝试过用其他端口映射。22端口是ssh登陆的默认端口。</p></li>
</ol>
<h1 id="cudnn深度学习库的安装和验证">cudnn深度学习库的安装和验证</h1>
<p>参考[https://blog.csdn.net/caicaiatnbu/article/details/87626491]</p>
<p>注意点：</p>
<ol type="1">
<li>下载对应的linux版本的cudnn</li>
</ol>
<h1 id="conda安装cuda和cudnn">conda安装cuda和cudnn</h1>
<p>如果使用的是conda的环境，可以单独使用conda install
cuda来在虚拟环境中安装不同于本机版本的cuda和cudnn，而不需要使用我上面提到的那种方式(每次换cuda版本需要更换/usr/local/cuda的软链接的方式)。</p>
<p>conda创建了虚拟环境后，激活进入虚拟环境</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda install cudatoolkit=10.2</span><br></pre></td></tr></table></figure>
<p>以上命令安装了10.2版本的cuda，然后可以使用<code>cuda search cudnn</code>找一下合适版本的cudnn，然后还是用<code>cuda install cudnn=版本号</code>来安装</p>
<blockquote>
<p>注意：这里用conda安装的cuda和cudnn，是无法像在本机使用nvcc
-V来检查版本的，所以即使你在虚拟环境激活的情况下使用 nvcc
-V的命令会返回无此命令或者是返回的还是本机cuda的版本号！</p>
</blockquote>
<p>那可能有人会问，如果想在conda虚拟环境下测试cuda和cudnn是否安装成功怎么办？</p>
<p>目前的办法是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># In[]:</span><br><span class="line">import torch</span><br><span class="line"># cpu</span><br><span class="line">print(torch.__version__)</span><br><span class="line"># gpu</span><br><span class="line">print(torch.cuda.is_available())</span><br><span class="line"></span><br><span class="line"># In[]:</span><br><span class="line">import tensorflow as tf</span><br><span class="line"># cpu</span><br><span class="line">print(tf.__version__)</span><br><span class="line"># v1 to test gpu</span><br><span class="line">print(tf.test.is_gpu_available())</span><br><span class="line"># v2 to test gpu</span><br><span class="line">print(tf.config.list_physical_devices(&#x27;GPU&#x27;))</span><br></pre></td></tr></table></figure>
<p>参考[https://blog.csdn.net/qq_37774098/article/details/109895048]</p>
<h1 id="docker拉取的pytorch-gpu版找不到cuda和cudnn的位置为何">docker拉取的pytorch-gpu版找不到cuda和cudnn的位置，为何？</h1>
<p>参考 https://blog.csdn.net/ljp1919/article/details/106209358</p>
<h1 id="tensorflow和pytorch验证gpu是否可用">tensorflow和pytorch验证GPU是否可用</h1>
<h2 id="tensorflow">tensorflow</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"> </span><br><span class="line">gpu_device_name = tf.test.gpu_device_name()</span><br><span class="line"><span class="built_in">print</span>(gpu_device_name)</span><br><span class="line"><span class="comment"># 查看是否可用</span></span><br><span class="line">tf.test.is_gpu_available()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.client <span class="keyword">import</span> device_lib</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 列出所有的本地机器设备</span></span><br><span class="line">local_device_protos = device_lib.list_local_devices()</span><br><span class="line"><span class="comment"># 打印</span></span><br><span class="line"><span class="comment"># print(local_device_protos)</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 只打印GPU设备</span></span><br><span class="line">[<span class="built_in">print</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> local_device_protos <span class="keyword">if</span> x.device_type == <span class="string">&#x27;GPU&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h2 id="pytorch">pytorch</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">flag = torch.cuda.is_available()</span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;CUDA可使用&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;CUDA不可用&quot;</span>)</span><br><span class="line"></span><br><span class="line">ngpu= <span class="number">1</span></span><br><span class="line"><span class="comment"># Decide which device we want to run on</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> (torch.cuda.is_available() <span class="keyword">and</span> ngpu &gt; <span class="number">0</span>) <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;驱动为：&quot;</span>,device)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;GPU型号： &quot;</span>,torch.cuda.get_device_name(<span class="number">0</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> 	torch</span><br><span class="line"><span class="keyword">import</span>  time</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br><span class="line"><span class="comment"># print(&#x27;hello, world.&#x27;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">10000</span>, <span class="number">1000</span>)</span><br><span class="line">b = torch.randn(<span class="number">1000</span>, <span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line">c = torch.matmul(a, b)</span><br><span class="line">t1 = time.time()</span><br><span class="line"><span class="built_in">print</span>(a.device, t1 - t0, c.norm(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">a = a.to(device)</span><br><span class="line">b = b.to(device)</span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line">c = torch.matmul(a, b)</span><br><span class="line">t2 = time.time()</span><br><span class="line"><span class="built_in">print</span>(a.device, t2 - t0, c.norm(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line">c = torch.matmul(a, b)</span><br><span class="line">t2 = time.time()</span><br><span class="line"><span class="built_in">print</span>(a.device, t2 - t0, c.norm(<span class="number">2</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="附录docker使用">附录：docker使用</h1>
<p>创建容器：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --gpus all -d -v /:/host -p 3333:22 tensorflow/tensorflow:2.4.0-gpu tail -f /var/log/dpkg.log # 创建容器并启动</span><br><span class="line">docker create --name postgres nginx:latest # 创建容器并不启动</span><br><span class="line"></span><br><span class="line">docker start # 启动一个或多个已经被停止的容器</span><br><span class="line"></span><br><span class="line">docker stop # 停止一个运行中的容器</span><br><span class="line"></span><br><span class="line">docker restart # 重启容器</span><br></pre></td></tr></table></figure>
<p>进入容器：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec -it [container_id] /bin/bash</span><br><span class="line"></span><br><span class="line">docker inpect # 获取容器/镜像的元数据</span><br></pre></td></tr></table></figure>
<p>停止容器运行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker kill -s KILL [container_id]</span><br><span class="line">或者 docker stop [container_id]</span><br></pre></td></tr></table></figure>
<p>删除一个或多个容器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker rm -f db01 db02 # 强制删除，容器可以是在运行着的状态</span><br><span class="line">或者 docker rm db01</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>docker save 与 docker export
区别参考:[https://jingsam.github.io/2017/08/26/docker-save-and-docker-export.html]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker save -o images.tar postgres:9.6 # 打包postgres镜像</span><br><span class="line">docker load -9 images.tar # 载入镜像</span><br></pre></td></tr></table></figure>
<p>docker
save的应用场景是，如果你的应用是使用docker-compose.yml编排的多个镜像组合，但你要部署的客户服务器并不能连外网。这时，你可以使用docker
save将用到的镜像打个包，然后拷贝到客户服务器上使用docker load载入。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker export -o postgres-export.tar postgres # 将容器postgres打包成一个tar</span><br><span class="line">docker import postgres-export.tar postgres:latest # 这是将tar包import成一个镜像，镜像和tag名字自定义</span><br></pre></td></tr></table></figure>
<p>docker
export的应用场景主要用来制作基础镜像，比如你从一个ubuntu镜像启动一个容器，然后安装一些软件和进行一些设置后，使用docker
export保存为一个基础镜像。然后，把这个镜像分发给其他人使用，比如作为基础的开发环境。</p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>命令行运行python脚本时传入参数的三种方式</title>
    <url>/2021/11/26/%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%BF%90%E8%A1%8Cpython%E8%84%9A%E6%9C%AC%E6%97%B6%E4%BC%A0%E5%85%A5%E5%8F%82%E6%95%B0%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<h1 id="sys.argv">sys.argv</h1>
<p>sys模块是很常用的模块，
它封装了与python解释器相关的数据，例如sys.modules里面有已经加载了的所有模块信息，sys.path里面是PYTHONPATH的内容，而sys.argv则封装了传入的参数数据。
使用sys.argv接收上面第一个命令中包含的参数方式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">print(sys.argv[0],sys.argv[1],sys.argv[2])</span><br></pre></td></tr></table></figure>
<p>其中<code>sys.argv[0]</code>是该脚本的名称，<code>sys.argv[1]</code>才是第一个参数，<code>sys.argv</code>是一个列表</p>
<p>用这种方式，命令行调用方式为：</p>
<p><code>python script.py parameter1 parameter2</code></p>
<h1 id="argparse">argparse</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;Process some integers.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;integers&#x27;</span>, metavar=<span class="string">&#x27;N&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, nargs=<span class="string">&#x27;+&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;an integer for the accumulator&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--sum&#x27;</span>, dest=<span class="string">&#x27;accumulate&#x27;</span>, action=<span class="string">&#x27;store_const&#x27;</span>,</span><br><span class="line">                    const=<span class="built_in">sum</span>, default=<span class="built_in">max</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;sum the integers (default: find the max)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"><span class="built_in">print</span>(args.accumulate(args.integers))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>使用PaddleVideo实现在attention_lstm模型上训练youtube-8m数据</title>
    <url>/2021/11/25/%E4%BD%BF%E7%94%A8PaddleVideo%E5%AE%9E%E7%8E%B0%E5%9C%A8attention-lstm%E6%A8%A1%E5%9E%8B%E4%B8%8A%E8%AE%AD%E7%BB%83youtube-8m%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<h1 id="安装">安装</h1>
<p>参照
https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/install.md</p>
<p>注意这里创建虚拟环境的时候要使用python=3.7，因为3.7之后的版本，paddleVideo只支持Linux系统。</p>
<p>其他要求： <code>cuda &gt;= 10.1</code>,
<code>cuDNN &gt;= 7.6.4</code></p>
<p>安装完<code>paddlepaddle</code>之后，再安装<code>paddleVideo</code></p>
<h1 id="快速开始">快速开始</h1>
<p>这里官方教程写的不是很详细，安装完之后让直接用命令行方式启动程序，因为<code>paddleVideo</code>这个库里面有三个inference模型，分别是</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Inference models that Paddle provides are listed as follows:</span><br><span class="line"></span><br><span class="line">&#123;&#x27;TSN&#x27;, &#x27;ppTSM&#x27;, &#x27;TSM&#x27;&#125;</span><br></pre></td></tr></table></figure>
<p>这里安装好环境之后，直接命令行是会报错的，因为还没有下载模型参数以及label，会报<code>'If you want to use your own model, Please input model_file as model path!'</code>的错误。</p>
<p>这时候进入python环境，跑以下程序:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> ppvideo <span class="keyword">import</span> PaddleVideo</span><br><span class="line">clas = PaddleVideo(model_name=<span class="string">&#x27;ppTSM&#x27;</span>,use_gpu=<span class="literal">False</span>,use_tensorrt=<span class="literal">False</span>)</span><br><span class="line">video_file=<span class="string">&#x27;data/example.avi&#x27;</span></span><br><span class="line">result=clas.predict(video_file)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<p>当你enter第二句话的时候，程序就会开始下载<code>InferenceModel</code>，大概有90.6兆，下载完之后再执行下面几行的程序，其中<code>video_file</code>这里要注意一下，程序里是相对路径。</p>
<p>下载的模型会放在
<code>C:\Users\XXXX\.paddlevideo_inference\inference_model\ppTSM</code></p>
<p>下载的label的txt在<code>'D:\\XXXXX\\anaconda_envs\\paddleVideo\\lib\\site-packages\\ppvideo\\tools\\../data/k400/Kinetics-400_label_list.txt'</code>也就是虚拟环境那个ppvideo里</p>
<h1 id="训练attention-lstm模型">训练attention-lstm模型</h1>
<p>前两节安装完之后，快速开始是为了测试安装正确与否。paddleVideo只有attention-lstm模型，并没有提供在youtube-8m上训练后的参数和label。所以这部分我们使用paddle框架来自己训练。</p>
<h2 id="下载youtube-8m数据集并转换为paddlepaddle处理的格式">下载youtube-8m数据集并转换为paddlepaddle处理的格式</h2>
<p>整个数据集包含3844个训练数据文件和3844个验证数据文件（TFRecord格式）</p>
<p>在linux系统下用curl下载，在windows下可以利用git的bash命令行的方式下载：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl data.yt8m.org/download.py | partition=2/frame/train mirror=asia python</span><br><span class="line"></span><br><span class="line">curl data.yt8m.org/download.py | partition=2/frame/validate mirror=asia python</span><br><span class="line"></span><br><span class="line">curl data.yt8m.org/download.py | partition=2/frame/test mirror=asia python</span><br></pre></td></tr></table></figure>
<p>数据下载完成之后，因为paddlepaddle需要使用pickle的数据格式，所以需要用<code>https://github.com/PaddlePaddle/PaddleVideo/</code>该官方仓库里data/yt8m下的脚本<code>tf2pkl.py</code>脚本进行转换。<code>tf2pkl.py</code>文件运行时需要两个参数，分别是数据源tf文件存放路径和转化后的pkl文件存放路径。</p>
<blockquote>
<p>由于TFRecord文件的读取需要用到Tensorflow，用户要先安装Tensorflow，或者在安装有Tensorflow的环境中转化完数据，再拷贝到data/dataset/youtube8m/pkl目录下。为了避免和PaddlePaddle环境冲突，建议先在其他地方转化完成再将数据拷贝过来。</p>
</blockquote>
<p>上面在转换数据格式的时候还要注意，<code>tf2pkl.py</code>文件用的tensorflow是1.X的版本，用2.0之后版本的需要重新创建虚拟环境，如果不想麻烦，可以直接在这个paddleVideo的虚拟环境里面安装<code>tensorflow-gpu==1.14.0</code></p>
<p>这里如果安装的是gpu版本，需要有对应的cuda版本支持，1.14.0需要10.0的cuda，我的服务器是11.2的cuda，所以这里我就直接安装的是cpu版本的tensorflow。其他的版本支持请查阅：https://www.tensorflow.org/install/source#gpu</p>
<p><img src="/2021/11/25/%E4%BD%BF%E7%94%A8PaddleVideo%E5%AE%9E%E7%8E%B0%E5%9C%A8attention-lstm%E6%A8%A1%E5%9E%8B%E4%B8%8A%E8%AE%AD%E7%BB%83youtube-8m%E6%95%B0%E6%8D%AE/image-20211126133025867.png"></p>
<ul>
<li>在linux中命令：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python tf2pkl.py ./tf/train ./pkl/train</span><br><span class="line">python tf2pkl.py ./tf/val ./pkl/val</span><br></pre></td></tr></table></figure>
<ul>
<li>在windows中我的方案：</li>
</ul>
<p>我看到<code>tf2pkl.py</code>脚本里是用sys命令行的方式调用的，然后他整个脚本都是在linux的模式下的模式。其中有几个地方改动一下就可以适用于windows：</p>
<ol type="1">
<li>删掉以下几行：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># assert (len(sys.argv) == 3)</span><br><span class="line"># source_dir = sys.argv[1]</span><br><span class="line"># target_dir = sys.argv[2]</span><br></pre></td></tr></table></figure>
<p>禁用命令行调用的模式</p>
<ol start="2" type="1">
<li>将record_dir
变成你想转化的文件的文件夹的路径，比如<code>\data\dataset\youtube8m\tf</code></li>
<li>将main函数中将outputdir路径改成你要存储pickle文件的文件夹的路径。</li>
</ol>
<p>完成上述步骤之后，直接在IDE中运行<code>tf2pkl.py</code>就可以开始转化文件了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=1</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>Video-AutoTagging视频标签自动提取 文献</title>
    <url>/2021/11/25/Video-AutoTagging%E8%A7%86%E9%A2%91%E6%A0%87%E7%AD%BE%E8%87%AA%E5%8A%A8%E6%8F%90%E5%8F%96-%E6%96%87%E7%8C%AE/</url>
    <content><![CDATA[<h1 id="综述-review">综述 review</h1>
<h2 id="a-survey-on-machine-learning-techniques-for-auto-labeling-of-video-audio-and-text-data-2021">A
Survey on Machine Learning Techniques for Auto Labeling of Video, Audio,
and Text Data (2021)</h2>
<p><a href="https://arxiv.org/abs/2109.03784" class="uri">https://arxiv.org/abs/2109.03784</a></p>
<p>section2介绍对于video类型数据的优化标签提取的技术，section5介绍数据标签化的tools</p>
<h2 id="a-comprehensive-study-of-deep-video-action-recognition-2020">A
Comprehensive Study of Deep Video Action Recognition (2020)</h2>
<p>这是李沐团队2020年的新作，主要介绍视频中动作recognition的工作。对于视频分类没有过多介绍。但是文章提到了一篇文章<a href="https://arxiv.org/pdf/1609.06782.pdf">Deep learning for Video
classification and Captioning</a>,</p>
<figure>
<img src="/2021/11/25/Video-AutoTagging%E8%A7%86%E9%A2%91%E6%A0%87%E7%AD%BE%E8%87%AA%E5%8A%A8%E6%8F%90%E5%8F%96-%E6%96%87%E7%8C%AE/image-20211129140929765.png" alt="image-20211129140929765">
<figcaption aria-hidden="true">image-20211129140929765</figcaption>
</figure>
<p>提到的这篇文章更符合我的task要求，有需要的小伙伴可以去读一下。好了回到李沐团队的这篇综述，我觉得质量很高，结构逻辑比一般综述要清晰很多，不装逼，而且实操性很强，给出了在同一个数据集上的各个算法的比较结果，代码也<a href="1Model%20zoo%20in%20both%20PyTorch%20and%20MXNet:%20https://cv.gluon.%20ai/model_zoo/action_recognition.html">公开了</a>。</p>
<p>第二章首先介绍在视频动作识别领域比较常见的数据集： <img src="/2021/11/25/Video-AutoTagging%E8%A7%86%E9%A2%91%E6%A0%87%E7%AD%BE%E8%87%AA%E5%8A%A8%E6%8F%90%E5%8F%96-%E6%96%87%E7%8C%AE/image-20211129141439430.png" alt="image-20211129141439430"></p>
<p>介绍完数据集之后开始介绍模型部分（Section
3），其中3.5章很有意思，介绍了一些近10年在视频动作识别task上比较popular的技术。第4章介绍做的实验和evaluation。</p>
<h2 id="deep-learning-for-video-classification-and-captioning">Deep
learning for Video classification and Captioning</h2>
<p>整体来说，这篇综述水平一般，仅仅做了总结，大家从里面找到自己有用的东西就好，比如综述之类的，参考意义不大。我的重点在数据集的介绍上，所以这里仅记录这部分。</p>
<p>在数据集上，作者介绍了：</p>
<figure>
<img src="/2021/11/25/Video-AutoTagging%E8%A7%86%E9%A2%91%E6%A0%87%E7%AD%BE%E8%87%AA%E5%8A%A8%E6%8F%90%E5%8F%96-%E6%96%87%E7%8C%AE/image-20211129154801583.png" alt="image-20211129154801583">
<figcaption aria-hidden="true">image-20211129154801583</figcaption>
</figure>
<p>其中，不是纯粹动作识别分类task的数据集有：</p>
<p><strong>Kodak Consumer Videos dataset</strong>
，<strong>MCG-WEBVdataset</strong>，<strong>Columbia Consumer Videos
(CCV) dataset</strong></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>Part 2:Model Validation模型验证</title>
    <url>/2021/11/22/Part-2-Model-Validation%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/</url>
    <content><![CDATA[<p>前提：已有训练好的模型</p>
<h1 id="evaluation-metrics">Evaluation Metrics</h1>
<h2 id="model-metrics">model metrics</h2>
<h3 id="accuracy-precision-recall">Accuracy, Precision, Recall</h3>
<p>分类问题常用的metrics:</p>
<p>Accuracy ：分类正确的概率，正确分类的个数/样本总数<img src="/2021/11/22/Part-2-Model-Validation%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/image-20211122111445504.png" alt="image-20211122111445504"></p>
<p>Precision：对于某一个具体的类，正确分类个数的占比（<strong>主要是为了处理类别分布不均衡的情况，重点关注你需要的类</strong>）</p>
<p><img src="/2021/11/22/Part-2-Model-Validation%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/image-20211122110908650.png"></p>
<p>也可以说是:</p>
<p><img src="/2021/11/22/Part-2-Model-Validation%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/image-20211122111419758.png"></p>
<p>Recall ：分子和Precision一样，但是分母不一样</p>
<p><img src="/2021/11/22/Part-2-Model-Validation%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/image-20211122111353554.png"></p>
<p>这三个概念再补充说一下，特别容易混淆：</p>
<p><img src="/2021/11/22/Part-2-Model-Validation%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/image-20211122111608127.png"></p>
<p>以上来自维基百科的定义。</p>
<p><img src="/2021/11/22/Part-2-Model-Validation%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/800px-Precisionrecall.svg.png"></p>
<p>这里的False Positive的意思是:
你的模型预测成了1，但是真实情况是该样本的分类是0，所以是你的模型预测错误了。计算Precision的时候，我们在我们的模型全部预测成1的情况下，计算我们有多少个样本真的预测正确了。</p>
<p>而Recall的意思是：在真实所有属于1分类的样本中，我们计算我们模型预测正确的样本个数占比。</p>
<p><code>F1 Score</code>就是平衡P和R的Metric：<code>2pr/(p+r)</code></p>
<h3 id="aucroc">AUC&amp;ROC</h3>
<p>经常用于二分类问题，ROC曲线下的面积值就是AUC，ROC曲线：</p>
<p><img src="/2021/11/22/Part-2-Model-Validation%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/image-20211122132511911.png"></p>
<p><img src="/2021/11/22/Part-2-Model-Validation%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/1024px-Roc_curve.svg.png"></p>
<p>当AUC=1时，说明model能完美区分两个类别，当AUC=0.5时，model最差，基本上等于随机分的。当AUC=0时，并不代表模型很差，而是把两个类别搞反了，0类分类成了1类，这时候只需要调整一下model的符号即可。在实际操作过程中，我们通常是去优化AUC的值，让它能从0.5优化成1。<strong>该metric通常用于广告行业。</strong></p>
<h3 id="business-metrics">Business Metrics</h3>
<p>take Ads for example：</p>
<ul>
<li>Latency : 广告要出来的和其他内容一样快</li>
<li>ASN : 平均每一页展示的广告</li>
<li>CTR：user click through rate真实的用户点击率</li>
<li>ACP : average privce advertiser pays per click</li>
</ul>
<p><img src="/2021/11/22/Part-2-Model-Validation%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/image-20211122133426363.png"></p>
<h1 id="underfitting-overfitting">Underfitting &amp; overfitting</h1>
<p><em>training error（训练误差）</em> vs <em>generalization
error（泛化误差）</em></p>
<p>Overfit : train的error低，而泛化误差高。高bias。</p>
<p>Underfit: train的error高，泛化误差也很高。高Variance。</p>
<p>最好的情况是：泛化误差低，训练误差也要比较低，<strong>最好的那个模型可能也会有一点overfit</strong>。</p>
<p><img src="/2021/11/22/Part-2-Model-Validation%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/image-20211122145236131.png"></p>
<h2 id="model-complexity-模型复杂度">model complexity 模型复杂度</h2>
<ul>
<li>不同种类的模型，很难比较他们的复杂度</li>
<li>同一种模型（比如都属于神经网络模型），通常比较：
<ul>
<li>num of parameters</li>
<li>参数可以选择的值的范围</li>
</ul></li>
</ul>
<p><img src="/2021/11/22/Part-2-Model-Validation%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/image-20211122150220071.png"></p>
<h2 id="data-capacity-数据复杂度">Data capacity 数据复杂度</h2>
<ul>
<li>样本个数</li>
<li>每一个样本的elements</li>
<li>time/space structure</li>
<li>diversity</li>
</ul>
<p>泛化误差和什么有关？ 模型</p>
<ul>
<li>加入正则项可以使得复杂模型的泛化误差降低</li>
<li>随机下降训练方法可以使得模型泛化误差降低</li>
</ul>
<h1 id="模型验证-model-validation">模型验证 model Validation</h1>
<p>test dataset
：<strong>只能用一次</strong>，为了得到模型的无偏估计（泛化误差）。所以就算没有设置测试集也没有关系，测试集的最终目的是对最终所选定的神经网络模型做出无偏估计，如果不需要这个具体的值，也可以不设置测试集。所以如果只有验证集，没有测试集，需要做的就是在训练集上尝试不同的模型框架，在验证集上评估这些模型，然后迭代并选出适用的模型。在实际的使用过程中，大家会用“训练-测试集”来指代训练验证集。也就是test数据集当做validation来用，同时提供挑选合适的模型以及计算无偏估计的两功能。</p>
<p>Validation dataset :
<strong>可以用多次</strong>，通常作为训练集的一部分。通常用于选出合适的模型以及超参数。</p>
<h2 id="k折交叉验证">k折交叉验证</h2>
<p>将数据集分为K份，每次拿一份来做验证集，最终取平均error。</p>
<p>K通常取5或者10,当数据集比较小时，可以选用较大的K，循环多次。当数据足够多，model训练比较贵，那么选用直接切的方式选验证集。</p>
<h2 id="common-mistakes">Common mistakes</h2>
<ul>
<li><p>验证集中有来自训练集的样本，导致模型结果特别好（比如数据集中的样本有重复的）</p></li>
<li><p>验证集最好是和最终模型部署的生产环境的数据差不多</p></li>
</ul>
]]></content>
      <categories>
        <category>李沐-实用机器学习(学习笔记)</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo图片插入问题：在主页和文章详情页同时显示图片</title>
    <url>/2021/11/17/hexo%E5%9B%BE%E7%89%87%E6%8F%92%E5%85%A5%E9%97%AE%E9%A2%98%EF%BC%9A%E5%9C%A8%E4%B8%BB%E9%A1%B5%E5%92%8C%E6%96%87%E7%AB%A0%E8%AF%A6%E6%83%85%E9%A1%B5%E5%90%8C%E6%97%B6%E6%98%BE%E7%A4%BA%E5%9B%BE%E7%89%87/</url>
    <content><![CDATA[<h1 id="设置静态文件根目录">设置静态文件根目录</h1>
<p><code>_config.yml</code>中有个<code>url</code>和<code>root</code>参数
如果你部署的地址是<code>http://yoursite.com/child</code>，需要设置下面两个参数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">url: &#x27;http://yoursite.com/child&#x27; // 部署的域名</span><br><span class="line">root: &#x27;/child/&#x27; // 部署的根目录</span><br><span class="line">12</span><br></pre></td></tr></table></figure>
<h1 id="设置资源文件夹">设置资源文件夹</h1>
<p>资源（Asset）代表 source
文件夹中除了文章以外的所有文件，例如图片、CSS、JS
文件等。比方说，如果你的Hexo项目中只有少量图片，那最简单的方法就是将它们放在
<code>source/images</code> 文件夹中。然后通过类似于
<code>![](/images/image.jpg)</code> 的方法访问它们。</p>
<p>&lt; !--more--&gt;</p>
<p>文章资源文件夹
对于那些想要更有规律地提供图片和其他资源以及想要将他们的资源分布在各个文章上的人来说，Hexo也提供了更组织化的方式来管理资源。这个稍微有些复杂但是管理资源非常方便的功能可以通过<strong>将
<code>config.yml</code> 文件中的 <code>post_asset_folder</code> 选项设为
true 来打开</strong>。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">_config.yml</span><br><span class="line">post_asset_folder: true</span><br><span class="line">12</span><br></pre></td></tr></table></figure>
<p>当资源文件管理功能打开后，Hexo将会在你<strong>每一次通过
<code>hexo new [layout] &lt;title&gt;</code>
命令创建新文章时自动创建一个文件夹</strong>。这个资源文件夹将会有与这个
<code>markdown</code>
文件一样的名字。将所有与你的文章有关的资源放在这个关联文件夹中之后，你可以通过相对路径来引用它们，这样你就得到了一个更简单而且方便得多的工作流。</p>
<p>相对路径引用的标签插件 通过常规的 markdown
语法和相对路径来引用图片和其它资源可能会导致它们在存档页或者主页上显示不正确。在Hexo
2时代，社区创建了很多插件来解决这个问题。但是，随着Hexo 3
的发布，许多新的标签插件被加入到了核心代码中。这使得你可以更简单地在文章中引用你的资源。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% asset_path slug %&#125;</span><br><span class="line">&#123;% asset_img slug [title] %&#125;</span><br><span class="line">&#123;% asset_link slug [title] %&#125;</span><br><span class="line">123</span><br></pre></td></tr></table></figure>
<p>比如说：当你打开文章资源文件夹功能后，你把一个
<code>example.jpg</code>
图片放在了你的资源文件夹中，如果通过使用相对路径的常规 markdown 语法
<code>![](/example.jpg)</code> ，它将 不会
出现在首页上。（但是它会在文章中按你期待的方式工作）</p>
<p>正确的引用图片方式是使用下列的标签插件而不是 markdown ：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% asset_img example.jpg avatar %&#125;</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<p><strong>通过这种方式，图片将会同时出现在文章和主页以及归档页中。通过<code>&#123;% asset_img 图片名称 图片说明 %&#125;</code></strong></p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Sigmoid和Softmax使用区别（Tensorflow）</title>
    <url>/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/</url>
    <content><![CDATA[<p>一直都对这两个函数的一些概念理不清楚，今天就整理一下，结合吴恩达老师和李沐给出的Tensorflow的coding过程，一并整理厘清概念和这两者的区别。</p>
<h1 id="sigmoid">Sigmoid</h1>
<p>Sigmoid通常用于逻辑回归Logistic
Regression的二分类中，output出一个概率值（比如：预测一张图片是一只猫的概率）。它的公式为：
<span class="math display">\[
\frac{1}{1+e^{-z}}
\]</span> 图示为:</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/Sigmoid_function_01.png"></p>
<p>特点：y的值是介于[0,1]之间的，而x是负无穷到正无穷的。x=0时，y=0.5。</p>
<p>LR的网络结构为:</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117112204836.png"></p>
<p>在将Sigmoid用于LR任务中时，模型的输入是一个特征向量X，这时的loss
function使用：</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117104751440.png"></p>
<p>这里<strong>不会</strong>使用MSE，原因是：</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117104923278.png"></p>
<p>loss
function是在单个训练样本中定义的，它衡量的是算法在单个训练样本中表现如何，为了衡量算法在全部训练样本上的表现，需要定义代价函数（cost
function），在LR中代价函数即为对m个样本的损失函数求和再平均：</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117105155751.png"></p>
<p>虽然有这么多计算步骤，但是在tensorflow中，我们并不需要自己计算sigmoid后的值a，然后将a和y放到J函数中计算中整体的cost，只需要一个函数就可以帮助我们实现。</p>
<p><code>tf.nn.sigmoid_cross_entropy_with_logits(logits=z,labels=y)</code></p>
<blockquote>
<p>注意：上述的z是before the final sigmoid
activation的值，也就是还没有传入Sigmoid前，只是经过线性计算后的值。</p>
</blockquote>
<h1 id="softmax">Softmax</h1>
<p>softmax函数是对sigmoid的推广，用于处理<strong>多分类</strong>的问题。公式：</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117111137512.png"></p>
<p><strong>它的输入是一个向量，输出也是一个向量</strong>。不同于Sigmoid的输入（实数），输出（介于0,1之间的概率值）。Sigmoid的输出是一个向量，其中
向量中的每一个元素的范围都在(0,1)之间，它能将一个含任意<em>实数</em>的K维向量压缩到另一个K维向量中。</p>
<p>它在线性模型中的应用方式为：对接最后一层，输出一个向量。</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117111549950.png"></p>
<p>它的loss function是:</p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117113151953.png"></p>
<p>其中q为输出向量y的维度。</p>
<p>那么它的cost function J应该是将整个训练集的损失总和:
通常叫做<strong>cross-entropy loss交叉熵损失函数</strong></p>
<p><img src="/2021/11/17/Sigmoid%E5%92%8CSoftmax%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB%EF%BC%88Tensorflow%EF%BC%89/image-20211117113353952.png"></p>
<p>在tensorflow中该过程只需要一个函数来实现:</p>
<p><code>tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))</code></p>
<blockquote>
<ul>
<li>其中logits也是传入softmax激活函数之前的结果，也就是经过线性计算之后的Z</li>
<li>logits和labels也必须是相同的shape （num of examples,
num_classes）</li>
</ul>
</blockquote>
<p>用tensorflow的完整实现即为:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span>(<span class="params">Z3, Y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)</span></span><br><span class="line"><span class="string">    Y -- &quot;true&quot; labels vector placeholder, same shape as Z3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - Tensor of the cost function</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>    </span><br><span class="line">    <span class="comment"># to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)</span></span><br><span class="line">    logits = tf.transpose(Z3)</span><br><span class="line">    labels = tf.transpose(Y)  </span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment"># Do the training loop</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line"></span><br><span class="line">        epoch_cost = <span class="number">0.</span>                       <span class="comment"># Defines a cost related to an epoch</span></span><br><span class="line">        num_minibatches = <span class="built_in">int</span>(m / minibatch_size) <span class="comment"># number of minibatches of size minibatch_size in the train set</span></span><br><span class="line">        seed = seed + <span class="number">1</span></span><br><span class="line">        minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Select a minibatch</span></span><br><span class="line">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line"></span><br><span class="line">            <span class="comment"># IMPORTANT: The line that runs the graph on a minibatch.</span></span><br><span class="line">            <span class="comment"># Run the session to execute the &quot;optimizer&quot; and the &quot;cost&quot;, the feedict should contain a minibatch for (X,Y).</span></span><br><span class="line">            _ , minibatch_cost = sess.run([optimizer, cost], feed_dict=&#123;X: minibatch_X, Y: minibatch_Y&#125;)</span><br><span class="line"></span><br><span class="line">            epoch_cost += minibatch_cost / num_minibatches</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Print the cost every epoch</span></span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="literal">True</span> <span class="keyword">and</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span> (<span class="string">&quot;Cost after epoch %i: %f&quot;</span> % (epoch, epoch_cost))</span><br><span class="line">                <span class="keyword">if</span> print_cost == <span class="literal">True</span> <span class="keyword">and</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                    costs.append(epoch_cost)    </span><br></pre></td></tr></table></figure>
<p>以上的实现是将里面的计算步骤都展示出来的实现，也就是我们得到Z之后再进一步的得到loss。在李沐的教程中，用tensorflow实现softmax回归使用了更高级的API来实现。</p>
<h2 id="tensorflow-keras模块对softmax更简洁的实现">tensorflow
keras模块对softmax更简洁的实现</h2>
<p>softmax回归的输出层是一个全连接层。因此，为了实现我们的模型，我们只需在<code>Sequential</code>中添加一个带有10个输出的全连接层。同样，在这里，<code>Sequential</code>并不是必要的，但我们可能会形成这种习惯。因为在实现深度模型时，<code>Sequential</code>将无处不在。我们仍然以均值0和标准差0.01随机初始化权重。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = tf.keras.models.Sequential()</span><br><span class="line">net.add(tf.keras.layers.Flatten(input_shape=(<span class="number">28</span>, <span class="number">28</span>)))</span><br><span class="line">weight_initializer = tf.keras.initializers.RandomNormal(mean=<span class="number">0.0</span>, stddev=<span class="number">0.01</span>)</span><br><span class="line">net.add(tf.keras.layers.Dense(<span class="number">10</span>, kernel_initializer=weight_initializer))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>以上实现的输入是28*28大小的灰度图片，分类类别数为10。</p>
</blockquote>
<p>在这里，我们使用学习率为0.1的小批量随机梯度下降作为优化算法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)</span><br><span class="line">trainer = tf.keras.optimizers.SGD(learning_rate=.1)</span><br><span class="line">num_epochs = 10</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习-学习笔记-part1:预备知识</title>
    <url>/2021/11/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-part1-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<p>这本书的地址：https://zh.d2l.ai/index.html
中文版，也有英文版，主页有链接过去。</p>
<blockquote>
<p>之前学习过吴恩达的深度学习课程，觉得很优秀。这本书从实践的角度，一步一步实现深度学习的算法，基础实用。有兴趣的还可以去B站follow李沐老师的课程，讲的很好，非常有实践性意义。开此博客是记录我在阅读和跟随这本书的实践过程中的一些心得，我会的知识我不会再记录，这里只会记录对我来说是新知识的知识点，其他详情可以去github上看完整的md文档<a href="https://github.com/d2l-ai/d2l-en" class="uri">https://github.com/d2l-ai/d2l-en</a>。</p>
</blockquote>
<h1 id="数据操作">数据操作</h1>
<p>在MXNet中，<code>NDArray</code>是一个类，也是存储和变换数据的主要工具。</p>
<p>我们也可以将多个<code>NDArray</code>连结（concatenate）。下面分别在行上（维度0，即形状中的最左边元素）和列上（维度1，即形状中左起第二个元素）连结两个矩阵。可以看到，输出的第一个<code>NDArray</code>在维度0的长度（66）为两个输入矩阵在维度0的长度之和（3+33+3），而输出的第二个<code>NDArray</code>在维度1的长度（88）为两个输入矩阵在维度1的长度之和（4+44+4）。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">In [15]:</span><br><span class="line">nd.concat(X, Y, dim=0), nd.concat(X, Y, dim=1)</span><br><span class="line">Out[15]:</span><br><span class="line">(</span><br><span class="line"> [[ 0.  1.  2.  3.]</span><br><span class="line">  [ 4.  5.  6.  7.]</span><br><span class="line">  [ 8.  9. 10. 11.]</span><br><span class="line">  [ 2.  1.  4.  3.]</span><br><span class="line">  [ 1.  2.  3.  4.]</span><br><span class="line">  [ 4.  3.  2.  1.]]</span><br><span class="line"> &lt;NDArray 6x4 @cpu(0)&gt;,</span><br><span class="line"> [[ 0.  1.  2.  3.  2.  1.  4.  3.]</span><br><span class="line">  [ 4.  5.  6.  7.  1.  2.  3.  4.]</span><br><span class="line">  [ 8.  9. 10. 11.  4.  3.  2.  1.]]</span><br><span class="line"> &lt;NDArray 3x8 @cpu(0)&gt;)</span><br></pre></td></tr></table></figure>
<h2 id="处理缺失值">处理缺失值</h2>
<p><em>get_dummies()</em> <em>fillna()</em></p>
<p>[<strong>对于<code>inputs</code>中的类别值或离散值，我们将“NaN”视为一个类别。</strong>]
由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”，
<code>pandas</code>可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。
巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。
缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">os.makedirs(os.path.join(&#x27;..&#x27;, &#x27;data&#x27;), exist_ok=True)</span><br><span class="line">data_file = os.path.join(&#x27;..&#x27;, &#x27;data&#x27;, &#x27;house_tiny.csv&#x27;)</span><br><span class="line">with open(data_file, &#x27;w&#x27;) as f:</span><br><span class="line">    f.write(&#x27;NumRooms,Alley,Price\n&#x27;)  # 列名</span><br><span class="line">    f.write(&#x27;NA,Pave,127500\n&#x27;)  # 每行表示一个数据样本</span><br><span class="line">    f.write(&#x27;2,NA,106000\n&#x27;)</span><br><span class="line">    f.write(&#x27;4,NA,178100\n&#x27;)</span><br><span class="line">    f.write(&#x27;NA,NA,140000\n&#x27;)</span><br><span class="line">    </span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line">print(data)</span><br><span class="line"></span><br><span class="line">inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]</span><br><span class="line">inputs = inputs.fillna(inputs.mean())</span><br><span class="line"></span><br><span class="line">inputs = pd.get_dummies(inputs, dummy_na=True)</span><br><span class="line">print(inputs)</span><br></pre></td></tr></table></figure>
<p>思考题：丢失掉缺失值最多的列</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">data.drop(data.isnull().sum().idxmax(),axis=1)</span><br></pre></td></tr></table></figure>
<p><code>idmax</code>返回sum()最多的id，这里<code>axis=1</code>不能少，drop默认丢弃行。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>李沐-斯坦福21秋季：实用机器学习（学习笔记）Part I: Basic ML Modeling</title>
    <url>/2021/11/16/%E6%9D%8E%E6%B2%90-%E6%96%AF%E5%9D%A6%E7%A6%8F21%E7%A7%8B%E5%AD%A3%EF%BC%9A%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%89Part-I-Basic-ML-Modeling/</url>
    <content><![CDATA[<h1 id="数据部分-data-part">数据部分 Data Part</h1>
<h2 id="数据获取-data-acquisition">数据获取 Data Acquisition</h2>
<p>比较popular的数据集：</p>
<p><img src="/2021/11/16/%E6%9D%8E%E6%B2%90-%E6%96%AF%E5%9D%A6%E7%A6%8F21%E7%A7%8B%E5%AD%A3%EF%BC%9A%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%89Part-I-Basic-ML-Modeling/image-20211112163641407.png"></p>
<p>更多的数据集list见 : <a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research" class="uri">https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research</a></p>
<p>哪里找到数据集:</p>
<p><img src="/2021/11/16/%E6%9D%8E%E6%B2%90-%E6%96%AF%E5%9D%A6%E7%A6%8F21%E7%A7%8B%E5%AD%A3%EF%BC%9A%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%89Part-I-Basic-ML-Modeling/image-20211112163931292.png"></p>
<p>数据的生成：（在数据不够的情况下的技术）</p>
<ul>
<li>GAN 无监督生成数据</li>
<li>数据增强 Data Augmentations</li>
</ul>
<p>例子：Back Translation 语义相同，句子不一样的句子</p>
<h2 id="探索性数据分析-exploratory-data-analysis">探索性数据分析
Exploratory data analysis</h2>
<p><code>data = pd.read_csv('house_sales.zip')</code>pandas可以支持读取压缩文件，text文本文件建议存储成zip，处理起来更迅速。</p>
<p><code>seaborn</code>库能画比matplotlib更多的图形。</p>
<p>自留作业：将数据下载下来看一遍，过一遍代码（To do）</p>
<h2 id="数据清理-data-cleaning">数据清理 Data Cleaning</h2>
<p>data very noisy ---&gt; data cleaning</p>
<p>数据清洗工具Trifacta Wrangler<a href="https://www.trifacta.com/" class="uri">https://www.trifacta.com/</a></p>
<h2 id="数据变换-data-transformation">数据变换 Data Transformation</h2>
<h3 id="对数值化column的normalization的方式">对数值化column的Normalization的方式:</h3>
<p><img src="/2021/11/16/%E6%9D%8E%E6%B2%90-%E6%96%AF%E5%9D%A6%E7%A6%8F21%E7%A7%8B%E5%AD%A3%EF%BC%9A%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%89Part-I-Basic-ML-Modeling/image-20211115153221386.png"></p>
<p>Z-score: 最常见，使得均值为0，方差为1。</p>
<h3 id="图片-picture">图片 Picture</h3>
<ol type="1">
<li>reduce image size减小图片尺寸，ML good at low-resolution images</li>
<li>对于jpeg格式的文件，如果存储为中等质量的图，会导致精度下降1%。最好在下采样时采用高质量存储</li>
<li>Image whitening</li>
</ol>
<h3 id="视频-video">视频 Video</h3>
<ol type="1">
<li>often use short video clips(&lt;10s)</li>
</ol>
<h3 id="文本text">文本text</h3>
<ol type="1">
<li>stemming and lemmatization 词根化和语法化: a word ---&gt; a common
base form</li>
<li>Tokenization 词源化：text ---&gt; a list of tokens</li>
</ol>
<p>词源可以是一个词，亦可以是一个character，或者是一个subwords</p>
<h3 id="summary">Summary</h3>
<p>Transform data into formats preferred by ML algorithms</p>
<ul>
<li><p>Tabular: normalize real value features</p></li>
<li><p>Images: cropping, downsampling, whitening</p></li>
<li><p>Videos: clipping, sampling frames</p></li>
<li><p>Text: stemming, lemmatization, tokenization</p></li>
</ul>
<p>Need to balance storage, quality, and loading speed</p>
<h2 id="特征工程-feature-engineering">特征工程 Feature Engineering</h2>
<h3 id="tabular-data-features">tabular data features</h3>
<ul>
<li>Int/float: directly use or or bin to unique int values</li>
<li>种类数据：one-hot encoding</li>
</ul>
<p>对于很多类的情况，将常见的类别保留，其他的类都归类为"Unkown"</p>
<ul>
<li>Date-time：转化为list</li>
</ul>
<p><code>[year, month, day, day_of_year, week_of_year, day_of_week]</code></p>
<ul>
<li>Feature combination: 用于组合比较相关的特征</li>
</ul>
<h3 id="文本text-features">文本text features</h3>
<ul>
<li>用token features表示text
<ol type="1">
<li>Bag-of-words(BoW) model</li>
</ol>
词典 --&gt; one-hot 没有语义信息了</li>
</ul>
<p>​ 2. Word Embeddings(e.g. Word2Vec)</p>
<ul>
<li>pre-trained language models(e.g. BERT,GPT-3)</li>
</ul>
<h3 id="图片视频-features">图片、视频 features</h3>
<ul>
<li>pre-trained model</li>
</ul>
<ol type="1">
<li>ResNet: trained with ImageNet (image classification)</li>
<li>I3D: trained with Kinetics (action classification)</li>
</ol>
<h3 id="总结">总结</h3>
<p>对于文本，图片和视频，一般采取DL的方式抽取特征，但对于Tabular的数据，暂时还没有特别好的深度学习模型来抽取，一般靠手动。</p>
<h2 id="data-part部分总结">Data part部分总结</h2>
<ul>
<li>label的质量 vs data的量之间要trade-off</li>
<li>大数据的管理：存储，处理，版本控制，安全</li>
<li>数据质量</li>
</ul>
<ol type="1">
<li>多样性</li>
<li>unbiased</li>
<li>fairness</li>
</ol>
<h1 id="ml-model-part">ML model part</h1>
<p>监督学习的种类：</p>
<p><img src="/2021/11/16/%E6%9D%8E%E6%B2%90-%E6%96%AF%E5%9D%A6%E7%A6%8F21%E7%A7%8B%E5%AD%A3%EF%BC%9A%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%89Part-I-Basic-ML-Modeling/image-20211116094045676.png"></p>
<h2 id="决策树-decision-tree">决策树 Decision Tree</h2>
<p>pros:</p>
<ol type="1">
<li>可以解释</li>
<li>可以处理数值型数据，也可以处理分类型数据（classification&amp;regression）</li>
</ol>
<p>cons:</p>
<ol type="1">
<li>non-robust(可以用ensemble集成 learning)</li>
<li>Complex tree</li>
<li>难以部署和并发执行</li>
</ol>
<p>特点：</p>
<ul>
<li>树模型是在工业界用的比较多的 Simple，fine-tune，often gives
satisfied results</li>
<li>可以通过ensemble trees去reduce bias&amp;variance
<ul>
<li>RF: trees trained in parallel with randomness</li>
<li>GBDT: train in sequential on residuals</li>
</ul></li>
</ul>
<h3 id="提升tree-model的方法">提升Tree Model的方法：</h3>
<p>Random Forest：</p>
<ol type="1">
<li>每一棵树都是单独训练的</li>
<li>对于分类任务，采取majority
voting；对于回归任务，对所有树的结果取平均</li>
<li>随机性来自于哪里？</li>
</ol>
<ul>
<li>随机采样训练样本来训练树：Bagging with
replacement，一个样本在一次训练中可以出现多次</li>
<li>随机采样feature中的subset来训练</li>
</ul>
<p>Gradient Boosting Decision Tree:</p>
<ol type="1">
<li>和RF不一样的是，GBDT是顺序地train多个树</li>
<li>在第<code>t</code>个时间步，前<code>t-1</code>个训练好的树的sum<code>Ft()</code></li>
</ol>
<p>训练一个新的树<code>f(t)</code>on
<code>&#123;(xi,yi-Ft(xi))&#125; i from 1...t-1</code></p>
<h2 id="线性模型-linear-model">线性模型 linear model</h2>
<h3 id="线性模型做回归">线性模型做回归</h3>
<ul>
<li>回归问题的output是一个实数值，所以objective通常为MSE（均方误差）</li>
</ul>
<h3 id="线性模型做分类多分类">线性模型做分类（多分类）</h3>
<ul>
<li>output用向量表示，y用one-hot</li>
<li>minimize loss MSE（注意：这里是向量的MSE）</li>
<li>predict label为向量的元素中最大值的那个类别</li>
</ul>
<p>以上方式有一个cons:将不重要的分类的值的loss也算进去了，MSE计算的是向量的每一个元素的差，所以可以用Softmax
Regression来解决：</p>
<p>将上述输出的向量<code>o</code>输入到Softmax操作子中，输出仍然是一个向量，但是向量中的元素的和是为1的，而且每一个元素的范围都在(0,1)之间。这里虽然做了非线性的变化，但是还是一个线性模型。</p>
<p><img src="/2021/11/16/%E6%9D%8E%E6%B2%90-%E6%96%AF%E5%9D%A6%E7%A6%8F21%E7%A7%8B%E5%AD%A3%EF%BC%9A%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%89Part-I-Basic-ML-Modeling/image-20211116111024159.png"></p>
<p>在求loss的阶段，比较两个概率型的向量用cross-entropy loss来做:</p>
<p><img src="/2021/11/16/%E6%9D%8E%E6%B2%90-%E6%96%AF%E5%9D%A6%E7%A6%8F21%E7%A7%8B%E5%AD%A3%EF%BC%9A%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%89Part-I-Basic-ML-Modeling/image-20211116111220963.png"></p>
<p>以上方式将模型转移到更加关注正确类的性能上来，而不是放在别的类上。</p>
<ul>
<li>线性模型通常使用MSE作为loss function</li>
<li>Softmax Regression用于多分类的问题
<ul>
<li>将输出的向量用softmax处理为概率型向量，并使用cross-entropy当做loss
function</li>
<li>cross-entropy用于求两个向量间的loss</li>
</ul></li>
</ul>
<h2 id="neural-network神经网络">Neural Network神经网络</h2>
<p>课程在这里介绍了三种模型：MLP（感知机），CNN和RNN。总体来说介绍的比较简单，如果想深入了解模型细节，建议移步吴恩达老师的深度学习课程，吴恩达的课程中这三种模型都花了一课时的时间来讲，特别是CNN部分讲的特别的好，结合吴恩达老师的作业学起来很受益。</p>
<h3 id="cnn">CNN</h3>
<p>在图片中寻找object的两个宗旨:</p>
<ol type="1">
<li>translation invariance
模型对于图片上不同位置的object的输出是类似的</li>
</ol>
<p>在卷积中的体现就是：每一次卷积核Kernel对某个位置计算完之后，会平移至下一个区域继续用该权重值进行计算。</p>
<ol type="1">
<li>locality 图片中相邻像素点的pixel相似</li>
</ol>
<p>在卷积中的体现就是只会计算图片中局部位置的加权和，如3*3大小的地方。</p>
<h4 id="pooling-layer-汇聚层meanmax-pooling">Pooling Layer
汇聚层：（mean/max pooling）</h4>
<p>why pooling？卷积计算的output值对于位置相当敏感。</p>
<h4 id="cnn的作用">cnn的作用</h4>
<p>用于抽取空间信息，task只要满足以上说的两种特征，就可以使用CNN来做。</p>
<ul>
<li>卷积层之后要有Activation</li>
<li>using Pooling layer 去reduce location sensitivity</li>
</ul>
<p>现在常用的CNN有: AlexNet, VGG, Inceptions, ResNet, MobileNet</p>
<h2 id="model-selection">Model Selection</h2>
<p>Tabular数据：Trees，线性模型，MLP（多个全连接层堆起来，中间加入non-linear
activations）</p>
<p>Text数据：RNNs（全连接层中接入过去的信息）</p>
<p>Images/audio/video: CNNs</p>
<p>对于Text和Images/audio/video，前者是需要处理时序性特征（1维），后者是空间性特征（2维），对于这两个任务，<code>Transformers</code>都可以解决。</p>
<p>​</p>
<p>​</p>
]]></content>
      <categories>
        <category>李沐-实用机器学习(学习笔记)</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>LCP 40. 心算挑战</title>
    <url>/2021/11/10/LCP-40-%E5%BF%83%E7%AE%97%E6%8C%91%E6%88%98/</url>
    <content><![CDATA[<h1 id="题目">题目：</h1>
<p>「力扣挑战赛」心算项目的挑战比赛中，要求选手从 N 张卡牌中选出 cnt
张卡牌，若这 cnt 张卡牌数字总和为偶数，则选手成绩「有效」且得分为 cnt
张卡牌数字总和。 给定数组 cards 和 cnt，其中 cards[i] 表示第 i
张卡牌上的数字。
请帮参赛选手计算最大的有效得分。若不存在获取有效得分的卡牌方案，则返回
0。</p>
<p>示例 1：</p>
<p>输入：cards = [1,2,8,9], cnt = 3</p>
<p>输出：18</p>
<p>解释：选择数字为 1、8、9 的这三张卡牌，此时可获得最大的有效得分
1+8+9=18。</p>
<p>示例 2：</p>
<p>输入：cards = [3,3,1], cnt = 1</p>
<p>输出：0</p>
<p>解释：不存在获取有效得分的卡牌方案。</p>
<p>提示：</p>
<p>1 &lt;= cnt &lt;= cards.length &lt;= 10^5 1 &lt;= cards[i] &lt;=
1000</p>
<p>来源：力扣（LeetCode）
链接：https://leetcode-cn.com/problems/uOAnQW</p>
<h1 id="思路">思路</h1>
<p>排序先取最大的cnt个数，如果它们的和是偶数直接输出，不然就找一个已取的最小的奇数换成剩下未取的最大的偶数，或者找一个已取的最小的偶数换成剩下未取的最大奇数</p>
<h1 id="解答">解答</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxmiumScore</span>(<span class="params">self, cards: <span class="type">List</span>[<span class="built_in">int</span>], cnt: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span></span><br><span class="line">        cards.sort()</span><br><span class="line">        length = <span class="built_in">len</span>(cards)</span><br><span class="line">        start = length-cnt</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">sum</span>(cards[start:]) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">sum</span>(cards[start:])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sums = <span class="built_in">sum</span>(cards[start:])</span><br><span class="line">            left_min_ou,left_min_ji,left_max_ou,left_max_ji = self.find(cards[:start].copy())</span><br><span class="line">            right_min_ou,right_min_ji,right_max_ou,right_max_ji = self.find(cards[start:].copy())</span><br><span class="line">            tmp1,tmp2 = -<span class="number">1</span>,-<span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> right_min_ji != -<span class="number">1</span> <span class="keyword">and</span> left_max_ou!= -<span class="number">1</span>:</span><br><span class="line">                tmp1 = sums - right_min_ji + left_max_ou</span><br><span class="line">            <span class="keyword">if</span> right_min_ou != -<span class="number">1</span> <span class="keyword">and</span> left_max_ji!= -<span class="number">1</span> :</span><br><span class="line">                tmp2 = sums - right_min_ou + left_max_ji</span><br><span class="line">            <span class="keyword">if</span> tmp1!= -<span class="number">1</span> <span class="keyword">or</span> tmp2!= -<span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">max</span>(tmp1,tmp2)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span> </span><br><span class="line">            </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find</span>(<span class="params">self, li</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(li) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span></span><br><span class="line">        li.sort()</span><br><span class="line">        min_ou, min_ji, max_ou, max_ji = -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> li:</span><br><span class="line">            <span class="keyword">if</span> num % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> min_ou == -<span class="number">1</span>:</span><br><span class="line">                min_ou = num</span><br><span class="line">            <span class="keyword">elif</span> num % <span class="number">2</span> != <span class="number">0</span> <span class="keyword">and</span> min_ji == -<span class="number">1</span>:</span><br><span class="line">                min_ji = num</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> <span class="built_in">reversed</span>(li):</span><br><span class="line">            <span class="keyword">if</span> num % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> max_ou == -<span class="number">1</span>:</span><br><span class="line">                max_ou = num</span><br><span class="line">            <span class="keyword">elif</span> num % <span class="number">2</span> != <span class="number">0</span> <span class="keyword">and</span> max_ji == -<span class="number">1</span>:</span><br><span class="line">                max_ji = num</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> min_ou, min_ji, max_ou, max_ji</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>Bot Composer使用</title>
    <url>/2021/11/05/Bot-Composer%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>Bot Framework Composer是Microsoft Framework中的一部分,属于Azure Bot
Service。开发者在开发Chat Bot时有几种开发的选择：Bot Framework
Composer，Bot Framework SDK，Power Vitual
Agents。本文主要介绍第一个Composer，它是一个建立Chat Bot的开源IDE，以Bot
Framework
SDK为基础，图形界面的开发方式对开发者比较友好，而且可以配合使用各种插件。</p>
<p><a href="https://docs.microsoft.com/en-us/composer/">官方文档</a>
英文，无中文。文档读起来很简单</p>
<h1 id="安装">安装</h1>
<p>环境需求：<code>Node.js</code>和<code>C#</code></p>
<p>直接去官网<a href="https://aka.ms/bf-composer-download-win" class="uri">https://aka.ms/bf-composer-download-win</a>下载exe可执行文件后安装。</p>
<h1 id="quick-start">Quick Start</h1>
<p>官方文档给出了建立一个简单天气询问的chatbot，可以跟着做一遍，很详细的教程，做完基本会对Composer有一个基本的印象。</p>
<h1 id="task">task</h1>
<p>我们是想做一个简单的chat bot能够实现调用weixin ai
chatbot的闲聊功能，也就是：</p>
<p>用户在ChatBot中发出一个Question，ChatBot将该Question发回weixin
api，然后weixin会返回一个Answer，然后ChatBot将该Answer返回给用户，这样就完成了一轮对话。之后ChatBot循环这个过程。</p>
<h2 id="weixin-chatbot-api的使用">weixin chatbot api的使用</h2>
<p>到微信的智能对话平台创建一个机器人，会得到机器人的access
token。详细教程参见<a href="https://developers.weixin.qq.com/doc/aispeech/platform/get-start.html" class="uri">https://developers.weixin.qq.com/doc/aispeech/platform/get-start.html</a></p>
<h2 id="搭建bot过程中的问题">搭建Bot过程中的问题</h2>
<h3 id="cannot-post-activity.-unauthorized.">1. Cannot post activity.
Unauthorized.</h3>
<p>这个问题遇到过多次，每次都是突然就好了。这里整理几个在网上搜集的可能的问题：先说一下我自己测试的环境是local环境，也就是不需要设置App
ID和App password，因为我暂时还没有部署到线上。</p>
<h4 id="stackoverflow-问题地址">stackoverflow <a href="https://stackoverflow.com/questions/58463852/cannot-post-activity-unauthorized-when-testing-from-bot-emulator">问题地址</a></h4>
<ol type="1">
<li><p>在emulator中测试bot的时候，输入 <strong>Microsoft App ID</strong>
and <strong>Microsoft App Password</strong></p>
<p>这个方式只对你在Azure上注册了bot才有用，在本地测试bot的时候是不会拥有这两个变量的。</p></li>
<li><p>如果你是用composer设计的bot，可以去到该bot的文件夹下，找到settings文件夹下的appsettings.json（botname.json），设置以上两个字段为空:</p></li>
<li><figure>
<img src="/2021/11/05/Bot-Composer%E4%BD%BF%E7%94%A8/kQznR.png" alt="kQznR">
<figcaption aria-hidden="true">kQznR</figcaption>
</figure></li>
</ol>
<p>以上两种方式对我都没用。</p>
<h4 id="将composer以管理员方式运行">将Composer以管理员方式运行</h4>
<p>找到Composer的可执行文件，让Composer在管理员权限下运行。</p>
<p>It works!</p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql查看当前使用的配置文件my.cnf的方法</title>
    <url>/2021/11/05/mysql%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8D%E4%BD%BF%E7%94%A8%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6my-cnf%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h1 id="mysql-查看当前使用的配置文件my.cnf的方法">mysql
查看当前使用的配置文件my.cnf的方法</h1>
<p>my.cnf是mysql启动时加载的配置文件，一般会放在mysql的安装目录中，用户也可以放在其他目录加载。</p>
<p>安装mysql后，系统中会有多个my.cnf文件，有些是用于测试的。</p>
<p>使用locate my.cnf命令可以列出所有的my.cnf文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">命令</span><br><span class="line">locate my.cnf</span><br><span class="line"></span><br><span class="line">输出</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/include/default_my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/federated/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/ndb/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/ndb_big/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/ndb_binlog/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/ndb_rpl/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/ndb_team/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/rpl/extension/bhs/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/rpl/my.cnf</span><br><span class="line">/usr/local/Cellar/mysql/5.6.24/mysql-test/suite/rpl_ndb/my.cnf</span><br></pre></td></tr></table></figure>
<p>当我们需要修改配置文件时，需要找到mysql启动时是加载了哪个my.cnf文件。</p>
<h3 id="查看是否使用了指定目录的my.cnf">查看是否使用了指定目录的my.cnf</h3>
<p>启动mysql后，我们查看mysql的进程，看看是否有设置使用指定目录的my.cnf文件，如果有则表示mysql启动时是加载了这个配置文件。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">命令</span><br><span class="line">ps aux|grep mysql|grep &#x27;my.cnf&#x27;</span><br><span class="line"></span><br><span class="line">输出</span><br><span class="line">fdipzone         25174   0.0  0.0  3087244    600   ??  S     4:12下午   0:01.14 /usr/local/Cellar/mysql/5.6.24/bin/mysqld --defaults-file=/usr/local/Cellar/mysql/5.6.24/my.cnf --basedir=/usr/local/Cellar/mysql/5.6.24 --datadir=/usr/local/var/mysql --plugin-dir=/usr/local/Cellar/mysql/5.6.24/lib/plugin --bind-address=127.0.0.1 --log-error=/usr/local/var/mysql/TerrydeMacBook-Air.local.err --pid-file=/usr/local/var/mysql/TerrydeMacBook-Air.local.pid</span><br><span class="line">fdipzone         25064   0.0  0.0  2452824      4   ??  S     4:12下午   0:00.03 /bin/sh /usr/local/opt/mysql/bin/mysqld_safe --defaults-file=/usr/local/Cellar/mysql/5.6.24/my.cnf --bind-address=127.0.0.1 --datadir=/usr/local/var/mysql</span><br></pre></td></tr></table></figure>
<p>可以看到/usr/local/Cellar/mysql/5.6.24/my.cnf就是mysql启动加载的配置文件。</p>
<p>如果上面的命令没有输出，表示没有设置使用指定目录的my.cnf。</p>
<h3 id="查看mysql默认读取my.cnf的目录">查看mysql默认读取my.cnf的目录</h3>
<p>如果没有设置使用指定目录的my.cnf，mysql启动时会读取安装目录根目录及默认目录下的my.cnf文件。</p>
<p>查看mysql启动时读取配置文件的默认目录。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">命令</span><br><span class="line">mysql --help|grep &#x27;my.cnf&#x27;</span><br><span class="line"></span><br><span class="line">输出</span><br><span class="line">                      order of preference, my.cnf, $MYSQL_TCP_PORT,</span><br><span class="line">/etc/my.cnf /etc/mysql/my.cnf /usr/local/etc/my.cnf ~/.my.cnf</span><br></pre></td></tr></table></figure>
<p>/etc/my.cnf, /etc/mysql/my.cnf, /usr/local/etc/my.cnf, ~/.my.cnf
这些就是mysql默认会搜寻my.cnf的目录，顺序排前的优先。</p>
<h3 id="启动时没有使用配置文件">启动时没有使用配置文件</h3>
<p>如果没有设置使用指定目录my.cnf文件及默认读取目录没有my.cnf文件，表示mysql启动时并没有加载配置文件，而是使用默认配置。</p>
<p>需要修改配置，可以在mysql默认读取的目录中，创建一个my.cnf文件(例如:/etc/my.cnf)，把需要修改的配置内容写入，重启mysql后即可生效。</p>
<h1 id="mysql使用group-by查询报错select-list-is-not-in-group-by-clause-and-contains-nonaggregated-column...解决方案">mysql使用group
by查询报错SELECT list is not in GROUP BY clause and contains
nonaggregated column...解决方案</h1>
<p><strong>MySQL5.7.5后only_full_group_by成为sql_mode的默认选项之一，这可能导致一些sql语句失效。</strong>
比如在使用<strong>group by</strong>进行分组查询报错</p>
<h3 id="查看自己的sql_mode配置">查看自己的sql_mode配置</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">在sql命令行中输入select @@sql_mode;这时我们能够看到自己的sql_mode配置,其中如果有ONLY_FULL_GROUP_BY,那它就是group by查询报错的罪魁祸首了</span><br></pre></td></tr></table></figure>
<h3 id="解决办法">解决办法</h3>
<p>命令行打开mysql.cnf,默认路径为/etc/mysql/conf.d/mysql.cnf,如果找不到可以使用whereis进行查询</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sql_mode = STRICT_TRANS_TABLES, NO_ZERO_IN_DATE, NO_ZERO_DATE, ERROR_FOR_DIVISION_BY_ZERO,      NO_AUTO_CREATE_USER, NO_ENGINE_SUBSTITUTION</span><br></pre></td></tr></table></figure>
<p>保存退出重启mysql</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo service mysqld restart #对于linux是mysql</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>其他技术</tag>
      </tags>
  </entry>
  <entry>
    <title>Anaconda使用</title>
    <url>/2021/11/05/Anaconda%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>Anaconda 安装包可以到 <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/" class="uri">https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/</a>
下载。</p>
<p>TUNA 还提供了 Anaconda
仓库与第三方源（conda-forge、msys2、pytorch等，查看完整列表）的镜像，各系统都可以通过修改用户目录下的
.condarc 文件。Windows 用户无法直接创建名为 .condarc 的文件，可先执行
conda config --set show_channel_urls yes 生成该文件之后再修改。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: true</span><br><span class="line">channel_alias: https://mirrors.tuna.tsinghua.edu.cn/anaconda</span><br><span class="line">default_channels:</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line">  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br></pre></td></tr></table></figure>
<p>一般表示 conda
应用程序的配置文件，在用户的家目录（windows：C:\users\username\，linux：/home/username/）。但对于.condarc配置文件，是一种可选的（optional）运行期配置文件，其默认情况下是不存在的，但当用户第一次运行
conda config命令时，将会在用户的家目录创建该文件。</p>
<h3 id="换国内源">换国内源</h3>
<ul>
<li>查看现在的源地址：<code>conda config --show-sources</code></li>
<li>设置搜索时显示通道地址
<code>conda config --set show_channel_urls yes</code></li>
<li>添加镜像源
<code>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</code></li>
</ul>
<h1 id="虚拟环境">虚拟环境</h1>
<p>常用命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda list 查看安装了那些包</span><br><span class="line"></span><br><span class="line">conda env list或者conda info -e 查看当前存在哪些虚拟环境</span><br><span class="line"></span><br><span class="line">conda update conda 更新当前conda</span><br><span class="line"></span><br><span class="line">conda create -n env_name python=3.6 创建虚拟环境</span><br><span class="line"></span><br><span class="line">activate env_name 激活某虚拟环境</span><br><span class="line"></span><br><span class="line">python --version 检查当前python版本</span><br><span class="line"></span><br><span class="line">conda install -n env_name [package] 安装某个包至某一个虚拟环境下</span><br><span class="line"></span><br><span class="line">deactivate 关闭当前虚拟环境</span><br><span class="line"></span><br><span class="line">conda remove -n env_name --all 删除某虚拟环境</span><br><span class="line"></span><br><span class="line">conda remove package_name 删除某个包</span><br><span class="line">pip unistall package_name</span><br><span class="line"></span><br><span class="line">conda env export &gt; requirements.yaml 将当前环境下安装的包保存为yaml文件</span><br><span class="line">conda env update -f=/path/requirements.yaml</span><br><span class="line"></span><br><span class="line">(如果不要conda，用pip的时候导出的是txt文件)</span><br><span class="line">pip freeze &gt; requirements.txt</span><br><span class="line">pip install -r /path/requirements.txt</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>其他技术</tag>
      </tags>
  </entry>
</search>
