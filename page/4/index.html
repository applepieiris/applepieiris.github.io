<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto+Sans+SC:300,300italic,400,400italic,700,700italic%7CArial:300,300italic,400,400italic,700,700italic%7Csans-serif:300,300italic,400,400italic,700,700italic%7CPingFang+SC:300,300italic,400,400italic,700,700italic%7CMicrosoft+YaHei:300,300italic,400,400italic,700,700italic%7CSource+Han+Serif:300,300italic,400,400italic,700,700italic%7Cserif:300,300italic,400,400italic,700,700italic%7CFira+Code:300,300italic,400,400italic,700,700italic%7CConsolas:300,300italic,400,400italic,700,700italic%7Cmonospace:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.8.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="Be smart! Keep Learning">
<meta property="og:type" content="website">
<meta property="og:title" content="YAN&#39;s Blog">
<meta property="og:url" content="http://example.com/page/4/index.html">
<meta property="og:site_name" content="YAN&#39;s Blog">
<meta property="og:description" content="Be smart! Keep Learning">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="YAN">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/4/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/4/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>YAN's Blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">YAN's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="YAN"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">YAN</p>
  <div class="site-description" itemprop="description">Be smart! Keep Learning</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">38</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/applepieiris" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;applepieiris" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/applepieiris" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/" class="post-title-link" itemprop="url">LLM评测/Evaluation</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-16 14:33:33" itemprop="dateCreated datePublished" datetime="2023-08-16T14:33:33+08:00">2023-08-16</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2023-08-29 13:24:10" itemprop="dateModified" datetime="2023-08-29T13:24:10+08:00">2023-08-29</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>4.7k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>4 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>最近在关注模型performance评估的问题，打算在这个主题上做一个整理，也是受到很多博客和文章的启发写这篇文章，所以就将所有推荐阅读的文章放在前面，感兴趣的小伙伴可以拓展阅读。</p>
<ol type="1">
<li>老刘说NLP 公众号中8.10发的一篇文章《如何让自己的大模型榜单评分更高》
这篇文章有点借鉴了hugging face的<a target="_blank" rel="noopener" href="https://huggingface.co/blog/zh/evaluating-mmlu-leaderboard">Open
LLM 排行榜近况</a></li>
<li>https://mp.weixin.qq.com/s/WiF3yRU5MZS7ulLTP8FIpw</li>
</ol>
<p>首先说一下这个<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">LLM榜单</a>,
有四个benchmark，其中上面的博客就是重点讲了为什么同样一个模型比如LLaMA在MMLU上评测的结果会不如llama文章中提的效果，trick就在作者使用MMLU这个benchmark的方式有很大不同，这里来看看MMLU这个benchmark。</p>
<h1 id="mmlu-benchmark">MMLU benchmark</h1>
<p>首先看一下这个数据集到底是什么数据集，长什么样子，先给出文章中的定义：</p>
<blockquote>
<p><strong>MMLU</strong> (<strong>Massive Multitask Language
Understanding</strong>) is a new benchmark designed to measure knowledge
acquired during pretraining by evaluating models exclusively in
zero-shot and few-shot settings.</p>
</blockquote>
<p>这个评测集合里包含了57个学科，也就是57个task。原始的数据集长这样，里面的每个问题包含四个可能选项，且每个问题只有一个正确答案。：</p>
<figure>
<img src="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/image-20230816145305589.png" alt="image-20230816145305589">
<figcaption aria-hidden="true">image-20230816145305589</figcaption>
</figure>
<p>可以看到基本上就是question，answer的组织。注意这里看到原始数据的时候我还有点没看明白，作者的readme中也没写，还是对beginner有点不友好，第一列表示question，第二到第四列表示四个选项，最后一列是答案。所以可以看到原作者在<a target="_blank" rel="noopener" href="https://github.com/hendrycks/test/blob/master/evaluate.py">evaluation</a>的代码中这样处理的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">choices = [<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>, <span class="string">&quot;C&quot;</span>, <span class="string">&quot;D&quot;</span>] <span class="comment"># 首先定义选项</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval</span>(<span class="params">args, subject, engine, dev_df, test_df</span>):</span></span><br><span class="line">    cors = []</span><br><span class="line">    all_probs = []</span><br><span class="line">    answers = choices[:test_df.shape[<span class="number">1</span>]-<span class="number">2</span>] <span class="comment"># 对于每一个csv文件读取进来后取answers</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">label = test_df.iloc[i, test_df.shape[<span class="number">1</span>]-<span class="number">1</span>] <span class="comment"># label这里其实是取得最后一列，也就是答案</span></span><br></pre></td></tr></table></figure>
<p>但这个评测数据集在用来评测LLM的过程中衍生出了很多版本，基本是prompt的变化：</p>
<figure>
<img src="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/640.png" alt="MMLU的不同实现">
<figcaption aria-hidden="true">MMLU的不同实现</figcaption>
</figure>
<p>同样的问答对，比如上面的选择题，Harness没有指令，并且衍生的两个版本也就是helm和harness版本还加了Question这个前缀，harness在选线之前还加了Choices。就这么一点差距，就导致同一个llm的出来的分数不一样：</p>
<figure>
<img src="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/LLM-01-bis-01.png" alt="LLM在不同MMLU实现上的评分">
<figcaption aria-hidden="true">LLM在不同MMLU实现上的评分</figcaption>
</figure>
<blockquote>
<p>关于如何使用这个benchmark，参考<a target="_blank" rel="noopener" href="https://github.com/hendrycks/test/blob/master/evaluate.py">MMLU原始实现</a>，作者写的是用chatgpt来产生答案，prompt为：<code>prompt = "The following are multiple choice questions (with answers) about &#123;&#125;.\n\n".format(format_subject(subject))</code></p>
</blockquote>
<p>这三种实现方式不仅prompt的形式不同，也就是上面提到的。并且它在计算F1score的时候的机制也不同。</p>
<ol type="1">
<li>原始实现</li>
</ol>
<p>在原始实现中的评估的代码是这样写的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> ans <span class="keyword">in</span> answers:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        lprobs.append(c[<span class="string">&quot;choices&quot;</span>][<span class="number">0</span>][<span class="string">&quot;logprobs&quot;</span>][<span class="string">&quot;top_logprobs&quot;</span>][-<span class="number">1</span>][<span class="string">&quot; &#123;&#125;&quot;</span>.<span class="built_in">format</span>(ans)]) <span class="comment"># c是chatgpt的回答</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Warning: &#123;&#125; not found. Artificially adding log prob of -100.&quot;</span>.<span class="built_in">format</span>(ans))</span><br><span class="line">    lprobs.append(-<span class="number">100</span>)</span><br><span class="line">    pred = &#123;<span class="number">0</span>: <span class="string">&quot;A&quot;</span>, <span class="number">1</span>: <span class="string">&quot;B&quot;</span>, <span class="number">2</span>: <span class="string">&quot;C&quot;</span>, <span class="number">3</span>: <span class="string">&quot;D&quot;</span>&#125;[np.argmax(lprobs)]</span><br><span class="line">    probs = softmax(np.array(lprobs))</span><br><span class="line"></span><br><span class="line">    cor = pred == label</span><br><span class="line">    cors.append(cor)</span><br><span class="line">    all_probs.append(probs)</span><br></pre></td></tr></table></figure>
<p>该方法在评估的时候，仅仅比较了模型对四个选项字母的预测概率，哪个选项的概率高就选哪个，即便是在极端情况下四个选项的概率值都很低的情况下也会选择某个选项，但其实模型有时候会回答很多不相关的东西（都是很高的概率的token），所以这种方式有点”放水“，整体评估出来的分数会偏高。</p>
<ol start="2" type="1">
<li><a target="_blank" rel="noopener" href="https://github.com/stanford-crfm/helm">HELM实现</a></li>
</ol>
<p>HELM实现是根据模型预测的下一个输出词元的概率来选择输出文本，并将生成的文本与正确答案的文本进行对比。这种方式有效避免了如果模型的答案中出现概率高的token不是选项中的任意一个，那么就会判为错误答案。</p>
<p>看了helm的代码仓库，着实有点丰富。内容很多我都没有找到在哪个文件里做的evaluation的计算，只知道了读取csv的地方。有好心的小伙伴可以私信我告诉我在哪里。</p>
<ol start="3" type="1">
<li>harness实现</li>
</ol>
<p>这是hugging
face的llm榜单所用的实现。它不再是只是统计选项，而是连同选项字母以及后面的答案一起被考虑进来，计算的是整个序列的概率（获取每个词元的概率
(与上面其他实现一样)
并求它们的联合概率），那么很容易一些长文本的联合概率会比短文本的联合概率大，所以作者说可以在联合概率的基础上在做一个归一化，也就是用对数联合概率/
token数。</p>
<figure>
<img src="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/image-20230817100952429.png" alt="MMLU三种实现对于模型输出的总结">
<figcaption aria-hidden="true">MMLU三种实现对于模型输出的总结</figcaption>
</figure>
<p>例如实现如下，基于GPT2计算句子联合概率的一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">data = [</span><br><span class="line">    <span class="string">&quot;A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN)&quot;</span>,</span><br><span class="line">    <span class="string">&quot;The term MLP is used ambiguously, sometimes loosely to any feedforward ANN, sometimes strictly to refer to networks composed of multiple layers of perceptrons (with threshold activation); see § Terminology&quot;</span>,</span><br><span class="line">    <span class="string">&#x27;Multilayer perceptrons are sometimes colloquially referred to as &quot;vanilla&quot; neural networks, especially when they have a single hidden layer.[1]&#x27;</span>,</span><br><span class="line">    <span class="string">&quot;An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function.&quot;</span>,</span><br><span class="line">]</span><br><span class="line">model = transformers.GPT2LMHeadModel.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line">tok = transformers.GPT2Tokenizer.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line">tgs = []</span><br><span class="line"><span class="keyword">for</span> dat <span class="keyword">in</span> data:</span><br><span class="line">    random.seed(dat)</span><br><span class="line">    <span class="comment"># print(model(tok.encode(dat, return_tensors=&quot;pt&quot;))[0][0])</span></span><br><span class="line">    toks = tok.encode(dat, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">    ind = random.randrange(<span class="built_in">len</span>(toks[<span class="number">0</span>]) - <span class="number">1</span>)</span><br><span class="line">    logits = F.log_softmax(model(toks)[<span class="number">0</span>], dim=-<span class="number">1</span>)[:, :-<span class="number">1</span>]  <span class="comment"># [batch, seq, vocab]</span></span><br><span class="line">    res = torch.gather(logits, <span class="number">2</span>, toks[:, <span class="number">1</span>:].unsqueeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    tgs.append(<span class="built_in">float</span>(res[ind:].<span class="built_in">sum</span>()))</span><br></pre></td></tr></table></figure>
<p>在“老刘说NLP”的博客中也提到了一点，就是上面的方式都是开源模型，所以很容易就能得到每一个token的预测概率，所以返回结果可以拆的这么细致来分析。如果是闭源模型只返回response的话，这时候就需要用正则的方式来抽取回答内容里的选项，比如<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.04813">CEVAL</a>的测试方案：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_cot_answer</span>(<span class="params">self, line, gen_ans</span>):</span></span><br><span class="line">    m = re.findall(<span class="string">r&#x27;所以答案是(.+?)。&#x27;</span>, gen_ans, re.M)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(m) &gt; <span class="number">0</span> <span class="keyword">and</span> m[-<span class="number">1</span>] <span class="keyword">in</span> self.choices:</span><br><span class="line">        <span class="keyword">return</span> m[-<span class="number">1</span>], <span class="literal">True</span></span><br><span class="line">    answer_patterns = [</span><br><span class="line">        <span class="string">r&#x27;([ABCD])是正确的&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;选项([ABCD])正确&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;答案为([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;答案是([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;答案([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;选择([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;答案：([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;选择答案([ABCD])&#x27;</span></span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># RE extraction</span></span><br><span class="line">    <span class="keyword">for</span> answer_pattern <span class="keyword">in</span> answer_patterns:</span><br><span class="line">        m = re.search(answer_pattern, gen_ans, re.M)</span><br><span class="line">        <span class="keyword">if</span> m:</span><br><span class="line">            answer = m.group(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> answer, <span class="literal">False</span></span><br><span class="line">        <span class="comment"># only containing one choice-character</span></span><br><span class="line">        m = re.findall(<span class="string">r&#x27;[ABCD]&#x27;</span>, gen_ans, re.M)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(m) == <span class="number">1</span>:</span><br><span class="line">            answer = m[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">return</span> answer, <span class="literal">False</span></span><br><span class="line">        answer_word_counter = <span class="number">0</span></span><br><span class="line">        <span class="comment"># only containing one choice-context</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> self.choices:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(line[<span class="string">f&#x27;<span class="subst">&#123;c&#125;</span>&#x27;</span>]) <span class="keyword">in</span> gen_ans:</span><br><span class="line">                answer = c</span><br><span class="line">                answer_word_counter += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> answer_word_counter == <span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">return</span> answer, <span class="literal">False</span></span><br><span class="line">                <span class="keyword">return</span> <span class="string">&#x27;-&#x27;</span>, <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>对CLEVA评测平台感兴趣的可以看原文paper或者参考<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/WiF3yRU5MZS7ulLTP8FIpw">文章</a>。原文说CLEVA是专门为评估中文语言模型而设计的平台。</p>
</blockquote>
<h1 id="section"></h1>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/30/%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AAinformation-retrival/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/30/%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AAinformation-retrival/" class="post-title-link" itemprop="url">训练一个information retrival</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-30 10:54:28" itemprop="dateCreated datePublished" datetime="2023-06-30T10:54:28+08:00">2023-06-30</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2023-08-29 13:24:02" itemprop="dateModified" datetime="2023-08-29T13:24:02+08:00">2023-08-29</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>3.2k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>3 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>近期中文大语言模型出现了好多产品，独领风骚的chatglm-6B确实表现很不错，所以大家就纷纷下场想做一个自己领域的大语言模型，最近看到一篇<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2305.15062">Lawyer LLaMA Technical
Report</a>，作者基于llama模型做了一个法律的语言模型。我觉得这篇文章值得想在垂直领域做语言模型的研究者参考，特别是它所采取的路径：</p>
<figure>
<img src="/2023/06/30/%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AAinformation-retrival/image-20230630105909352.png" alt="training process of lawyer LLaMA">
<figcaption aria-hidden="true">training process of lawyer
LLaMA</figcaption>
</figure>
<p>也有<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/XYJMDND4Od41WECqlxt3Fw">博客</a>粗略介绍了这篇文章。</p>
<p>我重点关注的是这个工作里的信息抽取：</p>
<figure>
<img src="/2023/06/30/%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AAinformation-retrival/image-20230630110134378.png" alt="image-20230630110134378">
<figcaption aria-hidden="true">image-20230630110134378</figcaption>
</figure>
<p>思路大概就是和斯坦福的dsp一个思路，用户问一个问题，第一种方式就是直接把这个问题抛给llm，让它去回答，第二种就是在把问题抛给llm之前先过一道retrival模型，这个模型会抽取一些和回答这个问题相关的一些context，将这些context加入prompt中，然后再抛给llm。这有一个专门的名词，在Demonstrate-Search-Predict:
Composing retrieval and language models for knowledge-intensive NLP
中有所介绍：
<em>retrieve-then-read</em>。有兴趣的可以再看一下斯坦福在做这部分工作时出的<a target="_blank" rel="noopener" href="https://github.com/stanfordnlp/dsp/blob/main/intro.ipynb">notebook</a>介绍，看完你就知道为啥要retrieve一些context加入到prompt中去。</p>
<p>可惜的是laywer
llama这篇文章并没有详细的介绍它的retrieve咋做的，文章中也是一笔带过，数据规模包括组织形式一概没说。我主要是参考斯坦福的dsp这框架中retrieve的实现，它是基于ColBERTv2做的抽取模块，但其实colbert预训练的模型是在微软的msmarco数据集上训练的，也就是没有中文，那必然对中文的适配度应该就不太行，所以我就想找找有咩有好心的博主写过自己用colbert在自己组织的语料库上训练的过程，发现没有，所以这就是我写这篇文章的动机啦，希望能给后面想用自己组建的数据集训练一个colbert做信息抽取的童鞋一点参考。</p>
<p>以下的内容参考<a target="_blank" rel="noopener" href="https://github.com/stanford-futuredata/ColBERT">colbert
github</a></p>
<h1 id="数据组织">数据组织</h1>
<h1 id="模型使用">模型使用</h1>
<h2 id="用官方训练好的checkpoints">用官方训练好的checkpoints</h2>
<p>参考https://github.com/stanford-futuredata/ColBERT/blob/main/docs/intro.ipynb</p>
<p>里面的输就LoTTE的url已经失效了，包括checkpoints的下载地址。可以到hugging
face的仓库找</p>
<ul>
<li>colbertv2.0 checkpoints :
https://huggingface.co/colbert-ir/colbertv2.0/tree/main</li>
<li>lotte 数据集地址：
https://huggingface.co/datasets/colbertv2/lotte/tree/main</li>
</ul>
<p>注意上面这个lotte的数据集的组织方式和colbert官方github仓库里的介绍不一样，这一点上colbert2这个代码仓库的维护做的蛮糟糕的，比羊驼可差远了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.insert(<span class="number">0</span>, <span class="string">&#x27;../&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面这些包是colbert写好的类，可以导入预训练的checkpoints和你自己的querys，documents</span></span><br><span class="line"><span class="keyword">from</span> colbert.infra <span class="keyword">import</span> Run, RunConfig, ColBERTConfig</span><br><span class="line"><span class="keyword">from</span> colbert.data <span class="keyword">import</span> Queries, Collection</span><br><span class="line"><span class="keyword">from</span> colbert <span class="keyword">import</span> Indexer, Searcher</span><br><span class="line"></span><br><span class="line">dataroot = <span class="string">&#x27;downloads/lotte&#x27;</span> <span class="comment"># 假设你已经把lotte放到downloads文件夹下</span></span><br><span class="line">dataset = <span class="string">&#x27;lifestyle&#x27;</span></span><br><span class="line">datasplit = <span class="string">&#x27;dev&#x27;</span></span><br><span class="line"></span><br><span class="line">queries = os.path.join(dataroot, dataset, datasplit, <span class="string">&#x27;questions.search.tsv&#x27;</span>) <span class="comment"># questions.search.txv没找到</span></span><br><span class="line">collection = os.path.join(dataroot, dataset, datasplit, <span class="string">&#x27;collection.tsv&#x27;</span>) <span class="comment"># collections.tsv也没找到</span></span><br><span class="line"></span><br><span class="line">queries = Queries(path=queries)</span><br><span class="line">collection = Collection(path=collection)</span><br><span class="line"></span><br><span class="line"><span class="string">f&#x27;Loaded <span class="subst">&#123;<span class="built_in">len</span>(queries)&#125;</span> queries and <span class="subst">&#123;<span class="built_in">len</span>(collection):,&#125;</span> passages&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># indexing 主要用于对所有的document进行index</span></span><br><span class="line">nbits = <span class="number">2</span>   <span class="comment"># encode each dimension with 2 bits</span></span><br><span class="line">doc_maxlen = <span class="number">300</span>   <span class="comment"># truncate passages at 300 tokens</span></span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&#x27;downloads/colbertv2.0&#x27;</span></span><br><span class="line">index_name = <span class="string">f&#x27;<span class="subst">&#123;dataset&#125;</span>.<span class="subst">&#123;datasplit&#125;</span>.<span class="subst">&#123;nbits&#125;</span>bits&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Run().context(RunConfig(nranks=<span class="number">4</span>, experiment=<span class="string">&#x27;notebook&#x27;</span>)):  <span class="comment"># nranks specifies the number of GPUs to use.</span></span><br><span class="line">    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits)</span><br><span class="line"></span><br><span class="line">    indexer = Indexer(checkpoint=checkpoint, config=config)</span><br><span class="line">    indexer.index(name=index_name, collection=collection, overwrite=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># search </span></span><br><span class="line"><span class="keyword">with</span> Run().context(RunConfig(experiment=<span class="string">&#x27;notebook&#x27;</span>)):</span><br><span class="line">    searcher = Searcher(index=index_name)</span><br><span class="line">query = queries[<span class="number">37</span>]   <span class="comment"># or supply your own query</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;#&gt; <span class="subst">&#123;query&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find the top-3 passages for this query</span></span><br><span class="line">results = searcher.search(query, k=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the top-k retrieved passages</span></span><br><span class="line"><span class="keyword">for</span> passage_id, passage_rank, passage_score <span class="keyword">in</span> <span class="built_in">zip</span>(*results):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\t [<span class="subst">&#123;passage_rank&#125;</span>] \t\t <span class="subst">&#123;passage_score:<span class="number">.1</span>f&#125;</span> \t\t <span class="subst">&#123;searcher.collection[passage_id]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>ok
偶然看到官方仓库昨天更新了intro的notebook，我看到里面下载数据集的仓库变化了，然后导入包的时候也写得更清晰了，加了条件：https://github.com/stanford-futuredata/ColBERT/blob/main/docs/intro2new.ipynb</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/20/%E6%B5%85%E6%9E%90stanford-alpaca%E7%BE%8A%E9%A9%BC%E4%BB%A3%E7%A0%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/20/%E6%B5%85%E6%9E%90stanford-alpaca%E7%BE%8A%E9%A9%BC%E4%BB%A3%E7%A0%81/" class="post-title-link" itemprop="url">浅析stanford alpaca羊驼代码</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-20 15:30:20" itemprop="dateCreated datePublished" datetime="2023-06-20T15:30:20+08:00">2023-06-20</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2023-08-29 13:24:02" itemprop="dateModified" datetime="2023-08-29T13:24:02+08:00">2023-08-29</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>7.2k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>7 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>斯坦福的羊驼alpaca开启了大家微调大语言模型的先河，现在很多国内的工作都是基于斯坦福的羊驼模型的范式来微调chatglm-6B,alpaca的<a target="_blank" rel="noopener" href="https://github.com/tatsu-lab/stanford_alpaca">github</a>.</p>
<p>之前我一个很厉害的师兄说过，要想写代码写的厉害或者有所提高，最重要的就是要多读别人优秀的代码，多思考别人为什么这么写，如果让自己写的话是不是可以做到如此高效。这就是我想写这篇博客的原因，我是最近几个月才接触的huggingface的transformer库，发现很多API虽然设计的很简单，但里面功能丰富，不可能一下子就掌握住，所以我的办法是多看别人是如何写的，我的主要参考repo就是alpaca和chatglm-6B官方repo给出的那些finetune
LLM的代码。</p>
<p>回到羊驼alpca这份代码，不得不说斯坦福的这份代码写的真的很优秀，值得一句一句去debug。</p>
<h1 id="数据准备部分">数据准备部分</h1>
<p>代码在<code>generate_instruction.py</code>内。</p>
<p>这部分代码主要功能是实现由seed_tasks.jsonl作为模板，让GPT3.5来根据两个seed生成一些instruction和input，output。思想基于self-instruct的理念，在我另外一篇博客instrcution
tuning中有所详细介绍。</p>
<p>这里的启动函数是<code>def generate_instruction_following_data()</code>,
首先会将seed_tasks读取进来，然后从中随机选取num_prompt_instructions个数据，默认是三个：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)</span><br></pre></td></tr></table></figure>
<p>注意这里羊驼采用了向chatgpt传输batch请求的方式，也就是一次性向chatgpt传输多个prompt，程序里默认是5个prompt一起传输给gpt，然后每一个prompt长什么样子呢？</p>
<p>举一个简单的例子, 下面这是一个seed_task</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;id&quot;: &quot;seed_task_1&quot;, &quot;name&quot;: &quot;antonym_relation&quot;, &quot;instruction&quot;: &quot;What is the relation between the given pairs?&quot;, &quot;instances&quot;: [&#123;&quot;input&quot;: &quot;Night : Day :: Right : Left&quot;, &quot;output&quot;: &quot;The relation between the given pairs is that they are opposites.&quot;&#125;], &quot;is_classification&quot;: false&#125;</span><br></pre></td></tr></table></figure>
<p>作者将三个seed_task拼接在一起，然后前面加上事先定义好的prompt：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_prompt</span>(<span class="params">prompt_instructions</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Encode multiple prompt instructions into a single string.&quot;&quot;&quot;</span></span><br><span class="line">    prompt = <span class="built_in">open</span>(<span class="string">&quot;./prompt.txt&quot;</span>).read() + <span class="string">&quot;\n&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, task_dict <span class="keyword">in</span> <span class="built_in">enumerate</span>(prompt_instructions):</span><br><span class="line">        (instruction, <span class="built_in">input</span>, output) = task_dict[<span class="string">&quot;instruction&quot;</span>], task_dict[<span class="string">&quot;input&quot;</span>], task_dict[<span class="string">&quot;output&quot;</span>]</span><br><span class="line">        instruction = re.sub(<span class="string">r&quot;\s+&quot;</span>, <span class="string">&quot; &quot;</span>, instruction).strip().rstrip(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">        <span class="built_in">input</span> = <span class="string">&quot;&lt;noinput&gt;&quot;</span> <span class="keyword">if</span> <span class="built_in">input</span>.lower() == <span class="string">&quot;&quot;</span> <span class="keyword">else</span> <span class="built_in">input</span></span><br><span class="line">        prompt += <span class="string">f&quot;###\n&quot;</span></span><br><span class="line">        prompt += <span class="string">f&quot;<span class="subst">&#123;idx + <span class="number">1</span>&#125;</span>. Instruction: <span class="subst">&#123;instruction&#125;</span>\n&quot;</span></span><br><span class="line">        prompt += <span class="string">f&quot;<span class="subst">&#123;idx + <span class="number">1</span>&#125;</span>. Input:\n<span class="subst">&#123;<span class="built_in">input</span>&#125;</span>\n&quot;</span></span><br><span class="line">        prompt += <span class="string">f&quot;<span class="subst">&#123;idx + <span class="number">1</span>&#125;</span>. Output:\n<span class="subst">&#123;output&#125;</span>\n&quot;</span></span><br><span class="line">    prompt += <span class="string">f&quot;###\n&quot;</span></span><br><span class="line">    prompt += <span class="string">f&quot;<span class="subst">&#123;idx + <span class="number">2</span>&#125;</span>. Instruction:&quot;</span></span><br><span class="line">    <span class="keyword">return</span> prompt</span><br></pre></td></tr></table></figure>
<p>事先定义好的prompt长这样：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">You are asked to come up with a set of 20 diverse task instructions. These task instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions.</span><br><span class="line"></span><br><span class="line">Here are the requirements:</span><br><span class="line">1. Try not to repeat the verb for each instruction to maximize diversity.</span><br><span class="line">2. The language used for the instruction also should be diverse. For example, you should combine questions with imperative instrucitons.</span><br><span class="line">3. The type of instructions should be diverse. The list should include diverse types of tasks like open-ended generation, classification, editing, etc.</span><br><span class="line">4. A GPT language model should be able to complete the instruction. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a reminder because it cannot perform any action.</span><br><span class="line">5. The instructions should be in English.</span><br><span class="line">6. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a question is permitted.</span><br><span class="line">7. You should generate an appropriate input to the instruction. The input field should contain a specific example provided for the instruction. It should involve realistic data and should not contain simple placeholders. The input should provide substantial content to make the instruction challenging but should ideally not exceed 100 words.</span><br><span class="line">8. Not all instructions require input. For example, when a instruction asks about some general information, &quot;what is the highest peak in the world&quot;, it is not necssary to provide a specific context. In this case, we simply put &quot;&lt;noinput&gt;&quot; in the input field.</span><br><span class="line">9. The output should be an appropriate response to the instruction and the input. Make sure the output is less than 100 words.</span><br><span class="line"></span><br><span class="line">List of 20 tasks:</span><br></pre></td></tr></table></figure>
<p>理解起来就是作者在list of 20 tasks后面跟上了三个seed
tasks，也就是给gpt打个样，让它知道按这个模式去生成。这里有个地方值得参考的：</p>
<blockquote>
<p>用模板的时候给每一个example配上分隔符，这里作者采用了###,
不仅如此，作者还采用了序号的方式，这些都是为了方便后面对gpt返回的text进行处理</p>
</blockquote>
<p>这里还有一个值得学习的地方在utils.py内，我们在使用openai的api获取回复时，有时候会遇到prompt过长的问题，羊驼catch了这个报错，将prompt的长度变为原来的80%，然后再向gpt发送请求，正是由于这份耐心，这个代码的耦合性就没那么高，所以易用性非常强，非常值得野生程序员学习。</p>
<hr>
<p>gpt返回的文本羊驼模型还做了similarity的计算，将相似度超过一定阈值的instrcution剔除掉。这部分代码可以直接复用。</p>
<h1 id="train中的数据处理">train中的数据处理</h1>
<p>羊驼模型采用了一个函数生成transformers库所需要的参数：
<code>make_supervised_data_module</code>，该函数返回一个dict，其中字典的key就是我们初始化Trainer类所需要的train_dataset,eval_dataset和data_collator。这个函数里首先是构建数据集的类<code>SupervisedDataset</code>，继承自Dataset.
注意pytorch里规定如果你想要创建一个自建的Dataset，这个继承自Dataset的子类必须重写<code>__len__</code>和<code>__getitem__</code>两个方法，羊驼这里写的是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SupervisedDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Dataset for supervised fine-tuning.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data_path: <span class="built_in">str</span>, tokenizer: transformers.PreTrainedTokenizer</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SupervisedDataset, self).__init__()</span><br><span class="line">        logging.warning(<span class="string">&quot;Loading data...&quot;</span>)</span><br><span class="line">        list_data_dict = utils.jload(data_path)</span><br><span class="line"></span><br><span class="line">        logging.warning(<span class="string">&quot;Formatting inputs...&quot;</span>)</span><br><span class="line">        prompt_input, prompt_no_input = PROMPT_DICT[<span class="string">&quot;prompt_input&quot;</span>], PROMPT_DICT[<span class="string">&quot;prompt_no_input&quot;</span>]</span><br><span class="line">        sources = [</span><br><span class="line">            prompt_input.format_map(example) <span class="keyword">if</span> example.get(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;&quot;</span>) != <span class="string">&quot;&quot;</span> <span class="keyword">else</span> prompt_no_input.format_map(example)</span><br><span class="line">            <span class="keyword">for</span> example <span class="keyword">in</span> list_data_dict</span><br><span class="line">        ]</span><br><span class="line">        targets = [<span class="string">f&quot;<span class="subst">&#123;example[<span class="string">&#x27;output&#x27;</span>]&#125;</span><span class="subst">&#123;tokenizer.eos_token&#125;</span>&quot;</span> <span class="keyword">for</span> example <span class="keyword">in</span> list_data_dict]</span><br><span class="line"></span><br><span class="line">        logging.warning(<span class="string">&quot;Tokenizing inputs... This may take some time...&quot;</span>)</span><br><span class="line">        data_dict = preprocess(sources, targets, tokenizer)</span><br><span class="line"></span><br><span class="line">        self.input_ids = data_dict[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">        self.labels = data_dict[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.input_ids)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, i</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(input_ids=self.input_ids[i], labels=self.labels[i]) <span class="comment"># 这里key必须是input_ids和labels,这是由于是llama模型的规定。</span></span><br></pre></td></tr></table></figure>
<p>羊驼模型用的DataCollator是自定义的，首先解释下datacollator是什么东西，transformers的官方文档的解释是：</p>
<blockquote>
<p>Data collators are objects that will form a batch by using a list of
dataset elements as input. These elements are of the same type as the
elements of <code>train_dataset</code> or <code>eval_dataset</code>.</p>
</blockquote>
<p>也就是你把所有的文本用tokenizer转化成input_ids和labels之后，要把他们组织成batch的形式，不仅如此，collator还能做一些数据处理的工作。它的输入就是我们之前的数据集，注意我们数据集的组织形式每一个数据sample它是一个字典，字典有两个key。所以羊驼这里首先将其拆分,
一句话解决，非常善于利用[ for
句式]，让我写的话应该是写成非常冗余的两个for循环。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input_ids, labels = <span class="built_in">tuple</span>([instance[key] <span class="keyword">for</span> instance <span class="keyword">in</span> instances] <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">&quot;input_ids&quot;</span>, <span class="string">&quot;labels&quot;</span>))</span><br></pre></td></tr></table></figure>
<p>后面就是很简单的train了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainer.train()</span><br><span class="line">trainer.save_state()</span><br><span class="line">trainer.save_model(output_dir=training_args.output_dir)</span><br></pre></td></tr></table></figure>
<p>我当时看到这里的时候有点奇怪，因为我之前的习惯还是tensorflow那一套，基本上你把数据处理完之后还要prefetech，batch等，但是这里感觉transformer全部做了集成，可以仔细看羊驼模型的训练启动命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=4 --master_port=&lt;your_random_port&gt; train.py \</span><br><span class="line">    --model_name_or_path <span class="string">&quot;facebook/opt-6.7b&quot;</span> \</span><br><span class="line">    --data_path ./alpaca_data.json \</span><br><span class="line">    --bf16 True \</span><br><span class="line">    --output_dir &lt;your_output_dir&gt; \</span><br><span class="line">    --num_train_epochs 3 \</span><br><span class="line">    --per_device_train_batch_size 4 \</span><br><span class="line">    --per_device_eval_batch_size 4 \</span><br><span class="line">    --gradient_accumulation_steps 8 \</span><br><span class="line">    --evaluation_strategy <span class="string">&quot;no&quot;</span> \</span><br><span class="line">    --save_strategy <span class="string">&quot;steps&quot;</span> \</span><br><span class="line">    --save_steps 2000 \</span><br><span class="line">    --save_total_limit 1 \</span><br><span class="line">    --learning_rate 2e-5 \</span><br><span class="line">    --weight_decay 0. \</span><br><span class="line">    --warmup_ratio 0.03 \</span><br><span class="line">    --lr_scheduler_type <span class="string">&quot;cosine&quot;</span> \</span><br><span class="line">    --logging_steps 1 \</span><br><span class="line">    --fsdp <span class="string">&quot;full_shard auto_wrap&quot;</span> \</span><br><span class="line">    --fsdp_transformer_layer_cls_to_wrap <span class="string">&#x27;OPTDecoderLayer&#x27;</span> \</span><br><span class="line">    --tf32 True</span><br></pre></td></tr></table></figure>
<p>其中的per_device_train_batch_size就指定了一个device上弄几个数据，也就是batch_size是多少，另外loss计算这些都是pretrained模型定好的，所以不用管，包括optimizer，所以我们在训练的时候只需要指定训练过程中用到的参数，比如保存步数，学习率，训练多少个epoch等。这是finetune大语言模型和之前做深度学习模型不一样的地方，技术往往更新的太快，都快看不懂大家写的代码了，怎么咔咔一两句就开始训练了，所以函数集成太厉害也不是只有好处。</p>
<h1 id="推荐阅读">推荐阅读</h1>
<ul>
<li>https://mp.weixin.qq.com/s/ehEM04xmeJyqB4z7rmLKBQ
讲解self-instruct方法</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YAN</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Symbols count total: </span>
    <span title="Symbols count total">189k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">2:51</span>
  </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
