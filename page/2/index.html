<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto+Sans+SC:300,300italic,400,400italic,700,700italic%7CArial:300,300italic,400,400italic,700,700italic%7Csans-serif:300,300italic,400,400italic,700,700italic%7CPingFang+SC:300,300italic,400,400italic,700,700italic%7CMicrosoft+YaHei:300,300italic,400,400italic,700,700italic%7CSource+Han+Serif:300,300italic,400,400italic,700,700italic%7Cserif:300,300italic,400,400italic,700,700italic%7CFira+Code:300,300italic,400,400italic,700,700italic%7CConsolas:300,300italic,400,400italic,700,700italic%7Cmonospace:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.8.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="Be smart! Keep Learning">
<meta property="og:type" content="website">
<meta property="og:title" content="YAN&#39;s Blog">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="YAN&#39;s Blog">
<meta property="og:description" content="Be smart! Keep Learning">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="YAN">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>YAN's Blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">YAN's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="YAN"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">YAN</p>
  <div class="site-description" itemprop="description">Be smart! Keep Learning</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">38</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/applepieiris" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;applepieiris" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/applepieiris" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/01/02/Transformer%E6%8B%86%E8%A7%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/01/02/Transformer%E6%8B%86%E8%A7%A3/" class="post-title-link" itemprop="url">Transformer拆解</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-01-02 13:32:33" itemprop="dateCreated datePublished" datetime="2024-01-02T13:32:33+08:00">2024-01-02</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2024-08-30 09:38:23" itemprop="dateModified" datetime="2024-08-30T09:38:23+08:00">2024-08-30</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>6.9k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>6 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>这篇博客主要记录Transformer架构的代码实现。以下是参考资料 - <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1706.03762">Attention is All you need</a> -
<a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2018-06-24-attention/">Attention?
Attention!</a> - <a target="_blank" rel="noopener" href="https://github.com/lilianweng/transformer-tensorflow">lilian
wen的tensorflow版本的实现</a> - <a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">illustrated-transformer</a>
强烈建议看illustrated
transformer这篇博客，是跟paper介绍的transformer架构完全对齐的 - <a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/transformer.py">pytorch
transformer实现</a></p>
<p>​ 这是pytorch的官方实现</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/harvardnlp/annotated-transformer/tree/master">annotated
transformer</a></li>
</ul>
<p>​ 斯坦福出的transformer架构的实现tutorial</p>
<p>我自己想实现的一遍的原因在于：</p>
<ol type="1">
<li>transformer的文章读了很多遍，但是很多细节还是没有去深究。</li>
<li>斯坦福的实现完全遵照的是paper的架构，但是我觉得还是实现的过于复杂了，我想遵循lilian的tensorflow实现把原生的tranformer架构实现一下</li>
<li>我对pytorch的掌握没有tensorflow好，感觉现在pytorch基本上成为深度学习网络的主流，特别是大模型出来之后，hugginggface的transformer库也是支持pytorch更好一点，更大的社区。（此时有点后悔当时系统学习的是tensorflow而不是pytorch）</li>
</ol>
<p>transformer的整体架构:
encoder-decoder两大模块，encoder模块内有重复的6个子模块，decoder模块内也有重复的6个子模块。</p>
<figure>
<img src="/2024/01/02/Transformer%E6%8B%86%E8%A7%A3/transformer.png" alt="Transformer model">
<figcaption aria-hidden="true">Transformer model</figcaption>
</figure>
<p>我们采用自上而下的方式来看这两个模块</p>
<h1 id="transformer整体架构">Transformer整体架构</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    define the whole architecture of Transformer in:</span></span><br><span class="line"><span class="string">        Vaswani et al. Attention is All You Need. NIPS 2017.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_heads=<span class="number">8</span>, d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, num_enc_layers=<span class="number">6</span>, num_dec_layers=<span class="number">6</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 drop_rate=<span class="number">0.1</span>, warmup_steps=<span class="number">400</span>, pos_encoding_type=<span class="string">&#x27;sinusoid&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ls_epsilon=<span class="number">0.1</span>, use_label_smoothing=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 model_name=<span class="string">&#x27;transformer&#x27;</span>, tf_sess_config=<span class="literal">None</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.h = num_heads</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.d_ff = d_ff</span><br><span class="line"></span><br><span class="line">        self.num_enc_layers = num_enc_layers</span><br><span class="line">        self.num_dec_layers = num_dec_layers</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Dropout regularization: added in every sublayer before layer_norm(...) and</span></span><br><span class="line">        <span class="comment"># applied to embedding + positional encoding.</span></span><br><span class="line">        self.drop_rate = drop_rate</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Label smoothing epsilon</span></span><br><span class="line">        self.ls_epsilon = ls_epsilon</span><br><span class="line">        self.use_label_smoothing = use_label_smoothing</span><br><span class="line">        self.pos_encoding_type = pos_encoding_type</span><br><span class="line"></span><br><span class="line">        <span class="comment"># For computing the learning rate</span></span><br><span class="line">        self.warmup_steps = warmup_steps</span><br><span class="line"></span><br><span class="line">        self.config = <span class="built_in">dict</span>(</span><br><span class="line">            num_heads=self.h,</span><br><span class="line">            d_model=self.d_model,</span><br><span class="line">            d_ff=self.d_ff,</span><br><span class="line">            num_enc_layers=self.num_enc_layers,</span><br><span class="line">            num_dec_layers=self.num_dec_layers,</span><br><span class="line">            drop_rate=self.drop_rate,</span><br><span class="line">            warmup_steps=self.warmup_steps,</span><br><span class="line">            ls_epsilon=self.ls_epsilon,</span><br><span class="line">            use_label_smoothing=self.use_label_smoothing,</span><br><span class="line">            pos_encoding_type=self.pos_encoding_type,</span><br><span class="line">            model_name=self.model_name,</span><br><span class="line">            tf_sess_config=self.tf_sess_config,</span><br><span class="line">        )</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span> <span class="comment"># 这里进行拼接，将encoder和decoder两大模块拼接在一起</span></span><br><span class="line">            enc_out = self.encoder(src, src_mask)</span><br><span class="line">            dec_out = self.decoder(enc_out, src_mask, tgt, tgt_mask)</span><br><span class="line">            <span class="keyword">return</span> dec_out</span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<h1 id="tranformer-encoder">Tranformer Encoder</h1>
<figure>
<img src="/2024/01/02/Transformer%E6%8B%86%E8%A7%A3/image-20240102135835011.png" alt="image-20240102135835011">
<figcaption aria-hidden="true">image-20240102135835011</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span>(<span class="params">module, N</span>):</span></span><br><span class="line">    <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder_layer, num_enc_layers</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        self.num_enc_layers = num_enc_layers</span><br><span class="line">        self.encoder_layers = clones(encoder_layer,num_enc_layers) <span class="comment"># 将encoder_layer复制6次</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src, src_mask</span>):</span></span><br><span class="line">        out = src</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.encoder_layers:</span><br><span class="line">            out = layer(out, src_mask)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>这里实现了一个<code>clones</code>帮助函数，我想过在这里用for循环，lilian在这里就是用的for循环：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">out = inp  <span class="comment"># now, (batch, seq_len, embed_size)</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_enc_layers):</span><br><span class="line">        out = self.encoder_layer(out, input_mask, <span class="string">f&#x27;enc_<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>注意这里的每一个encoder_layer的参数都是独立的，也就是有6份encoder_layer的参数需要训练，tensorflow为什么可行？是因为它这里使用了variable_scope的概念，上面的tensorflow实现每一次out和input_mask进来都是和不同的数值进行的运算。如果在pytorch中想实现这种方式，要先把encoder_layer复制六遍，每一次输入进来都拿不同的layer做运算。</p>
<h2 id="encoder-layer">encoder layer</h2>
<p>接下来我们实现encoder layer中的细节部分，它包含两个sub-layer： 1）
self-attention + Add&amp;layer_Norm 2) position-wise feed forward +
Add&amp;layer_Norm</p>
<figure>
<img src="/2024/01/02/Transformer%E6%8B%86%E8%A7%A3/image-20240102135835011.png" alt="image-20240102135835011">
<figcaption aria-hidden="true">image-20240102135835011</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model: the number of expected features in the input (required).</span></span><br><span class="line"><span class="string">        nhead: the number of heads in the multiheadattention models (required).</span></span><br><span class="line"><span class="string">        dim_feedforward: the dimension of the feedforward network model (default=2048).</span></span><br><span class="line"><span class="string">    About:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># One multi-head attention + one feed-forward</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, n_head, dim_feedforward, dropout = <span class="number">0.1</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.self_attn = MultiheadAttention(d_model, n_head)</span><br><span class="line">        self.norm_1 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="comment"># Implementation of Feedforward model(Two linear transformation together with one dropout)</span></span><br><span class="line">        self.linear1 = nn.Linear(d_model, dim_feedforward, )</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.linear2 = nn.Linear(dim_feedforward, d_model)</span><br><span class="line">        self.norm_2 = nn.LayerNorm(d_model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__ff_block</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># feed forward layer contains two linear</span></span><br><span class="line">        out = F.relu(self.linear1(x))</span><br><span class="line">        out = self.dropout(out)</span><br><span class="line">        out = self.linear2(out)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src, src_mask</span>):</span></span><br><span class="line">        out = src</span><br><span class="line">        out = self.norm_1(out + self.self_attn(out, src_mask))<span class="comment"># 这里在pytorch的官方实现中在self_attention后还加了一个dropout</span></span><br><span class="line">        out = self.norm_2(out + self.__ff_block(out))</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h3 id="self-attention">self attention</h3>
<p>我一开始查阅的资料是<a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">illustrated-transformer</a>,
这个博客内没有具体的实现。后来我参考的是lilian
wen的tensorflow实现。在lilian的实现里对于multihead
attention是这样写的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multihead_attention</span>(<span class="params">self, query, memory=<span class="literal">None</span>, mask=<span class="literal">None</span>, scope=<span class="string">&#x27;attn&#x27;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            query (tf.tensor): of shape (batch, q_size, d_model)</span></span><br><span class="line"><span class="string">            memory (tf.tensor): of shape (batch, m_size, d_model)</span></span><br><span class="line"><span class="string">            mask (tf.tensor): shape (batch, q_size, k_size)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:h</span></span><br><span class="line"><span class="string">            a tensor of shape (bs, q_size, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> memory <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            memory = query</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="comment"># Linear project to d_model dimension: [batch, q_size/k_size, d_model]</span></span><br><span class="line">            Q = tf.layers.dense(query, self.d_model, activation=tf.nn.relu)</span><br><span class="line">            K = tf.layers.dense(memory, self.d_model, activation=tf.nn.relu)</span><br><span class="line">            V = tf.layers.dense(memory, self.d_model, activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Split the matrix to multiple heads and then concatenate to have a larger</span></span><br><span class="line">            <span class="comment"># batch size: [h*batch, q_size/k_size, d_model/num_heads]</span></span><br><span class="line">            Q_split = tf.concat(tf.split(Q, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            K_split = tf.concat(tf.split(K, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            V_split = tf.concat(tf.split(V, self.h, axis=<span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">            mask_split = tf.tile(mask, [self.h, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Apply scaled dot product attention</span></span><br><span class="line">            out = self.scaled_dot_product_attention(Q_split, K_split, V_split, mask=mask_split)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Merge the multi-head back to the original shape</span></span><br><span class="line">            out = tf.concat(tf.split(out, self.h, axis=<span class="number">0</span>), axis=<span class="number">2</span>)  <span class="comment"># [bs, q_size, d_model]</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># The final linear layer and dropout.</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dense(out, self.d_model)</span></span><br><span class="line">            <span class="comment"># out = tf.layers.dropout(out, rate=self.drop_rate, training=self._is_training)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>以上的实现其实和博客内的内容有点相左，博客写的是：</p>
<blockquote>
<p>As we’ll see next, with multi-headed attention we have not only one,
but multiple sets of Query/Key/Value weight matrices (the Transformer
uses eight attention heads, so we end up with eight sets for each
encoder/decoder). Each of these sets is randomly initialized. Then,
after training, each set is used to project the input embeddings (or
vectors from lower encoders/decoders) into a different representation
subspace.</p>
</blockquote>
<p>结合作者给出的图片：</p>
<figure>
<img src="/2024/01/02/Transformer%E6%8B%86%E8%A7%A3/image-20240104131802468.png" alt="image-20240104131802468">
<figcaption aria-hidden="true">image-20240104131802468</figcaption>
</figure>
<p>我一开始的理解是每一个head都有一份单独的W
sets（WQ,WK,WV）。每一个head经过了scaled attention的计算</p>
<figure>
<img src="/2024/01/02/Transformer%E6%8B%86%E8%A7%A3/image-20240104132257007.png" alt="image-20240104132257007">
<figcaption aria-hidden="true">image-20240104132257007</figcaption>
</figure>
<p>得到的Z的shape都是<code>(batch, seq_len, embeded_size)</code>，所以才会有WO这个线性变化（blog里说的）：</p>
<figure>
<img src="/2024/01/02/Transformer%E6%8B%86%E8%A7%A3/image-20240104132406484.png" alt="image-20240104132406484">
<figcaption aria-hidden="true">image-20240104132406484</figcaption>
</figure>
<p>但我看完代码之后发现并不是我想的那样。我觉得这篇博客写的有点问题。后来又找到了一篇<a target="_blank" rel="noopener" href="https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853">博客</a>，能够解答我的疑问。它最重要的话是：</p>
<blockquote>
<p>However, the important thing to understand is that this is a logical
split only. The Query, Key, and Value are not physically split into
separate matrices, one for each Attention head. A single data matrix is
used for the Query, Key, and Value, respectively, with logically
separate sections of the matrix for each Attention head. Similarly,
there are not separate Linear layers, one for each Attention head. All
the Attention heads share the same Linear layer but simply operate on
their ‘own’ logical section of the data matrix.</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/" class="post-title-link" itemprop="url">LLaMA系列模型浅析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-12-12 14:56:24" itemprop="dateCreated datePublished" datetime="2023-12-12T14:56:24+08:00">2023-12-12</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2025-05-08 13:11:23" itemprop="dateModified" datetime="2025-05-08T13:11:23+08:00">2025-05-08</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>10 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>这篇博客主要记录博主在探索meta开源的llama大模型，包括它的数据，training代码以及评测方法。之所以想记录下来，主要是因为llama的论文写的极其的细致，完美践行了开源这个词(感谢meta!)，第二个原因是它的文档以及社区都很活跃，使用人群广泛，我们可以借助很多中文社区的复现情况去一探大公司在实现一个大模型时候的考量，以及思考它为什么会这样做。</p>
<p>之前写过一篇关于斯坦福的alpaca的代码的解析，后来看过很多关于微调大模型(supervised
finetuning)的代码仓库，大家的实现思路基本上都可以追溯到alpaca的这份代码。</p>
<p>首先我会将所有我参考的资料罗列在前面，方便大家查找： - <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama/tree/main">llama代码仓库</a>
这个仓库是介绍如何下载llama模型 - <a href="lll">llama "食谱"</a>
一开始我想在上一个llama仓库中找到相关的train代码，找了半天发现根本没有。后来才发现meta官方将所有finetune(pretrain
from scrach)的代码放在这个仓库，适合developer - <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09288">Llama 2: Open Foundation and
Fine-Tuned Chat Models</a> llama2的research paper。强烈建议食用</p>
<p>中文社区的LLama的工作 - <a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2">Chinese LLaMA
Alpaca2</a></p>
<p>这个仓库同样有配套的文章<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.08177.pdf">Efficient and Effective Text
Encoding for Chinese LLaMA and Alpaca</a></p>
<p>这个仓库的工作主要是两个：</p>
<ol type="1">
<li>扩充了llama原来的token，也就是中文的那部分</li>
<li>用新的中文数据在llama上进行了continue
pretraining，并且发布了在instruction数据上的微调模型</li>
</ol>
<p>研究思路很简单，在别人模型上继续预训练，并参照alpaca对预训练的模型进行instruction
finetune让其具备follow instructions的能力。我们首先从这个Chinese
LLaMA代码仓库看起。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/06/Sequence%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%AD%E5%A4%84%E7%90%86%E4%B8%8D%E5%AE%9A%E9%95%BF%E6%95%B0%E6%8D%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/06/Sequence%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%AD%E5%A4%84%E7%90%86%E4%B8%8D%E5%AE%9A%E9%95%BF%E6%95%B0%E6%8D%AE/" class="post-title-link" itemprop="url">Sequence分类问题中处理不定长数据</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-12-06 10:55:07" itemprop="dateCreated datePublished" datetime="2023-12-06T10:55:07+08:00">2023-12-06</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2024-08-30 09:38:18" itemprop="dateModified" datetime="2024-08-30T09:38:18+08:00">2024-08-30</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>6.2k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>6 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>之前写过一篇关于stanford
alpaca的代码的分析，最近在kaggle上看到一个检测某段长文本是否是AI生成的任务<a target="_blank" rel="noopener" href="https://www.kaggle.com/c/llm-detect-ai-generated-text">LLM -
Detect AI Generated
Text</a>,自己也在尝试做这个任务的时候，发现斯坦福的这份代码真是常看常新。对于数据的准备部分，有很多选择，比如在创建Dataset的时候就把所有的字符串数据tokenize好，在get_item()的函数返回时就返回input_ids，也可以是像斯坦福的<a target="_blank" rel="noopener" href="https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py">这份代码</a>一样，先把数据读取进来然后再用<code>DataCollator</code>处理（padding）。</p>
<p>我之前没有发现斯坦福这份代码这么写的真正原因，直到我自己来处理这种不定长的序列输入时才发现这样写的绝妙，因为我们都知道矩阵是每一行都需要是同样的size，所以斯坦福的写法在数据处理前期一直在用list，而不是batch。</p>
<p>先来说说transformer的对于序列分类的官方教程的写法<a target="_blank" rel="noopener" href="https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/sequence_classification.ipynb">传送门</a></p>
<p>transformer的这个教程直接使用的是自己的数据集，已经规整为datasets了，首先它对数据集使用map函数做了截断的处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;distilbert-base-uncased&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_function</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">&quot;text&quot;</span>], truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">tokenized_imdb = imdb.<span class="built_in">map</span>(preprocess_function, batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>然后重点来了，注意在上面的<code>preprocess_function</code>中并没有对序列进行padding，只是对过长的序列做了截断。接着作者使用了<code>datacollatorwithpadding</code>，给出的理由是：</p>
<blockquote>
<p>It's more efficient to <em>dynamically pad</em> the sentences to the
longest length in a batch during collation, instead of padding the whole
dataset to the maximum length.</p>
</blockquote>
<p>我们可以用官方的文档中看到对于datacollator的<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main_classes/data_collator">定义</a>：</p>
<blockquote>
<p>Data collators are objects that will form a batch by using a list of
dataset elements as input. These elements are of the same type as the
elements of <code>train_dataset</code> or <code>eval_dataset</code></p>
</blockquote>
<p>也就是data
collators的输入是一个list，list里的每一个元素跟train_dataset中的元素是一样的。在data
collator中你可以做一些processing，如padding，random
masking。我们接下来可以看到斯坦福的羊驼代码就是将padding的步骤放到了data
collator内。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorWithPadding</span><br><span class="line"></span><br><span class="line">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span><br></pre></td></tr></table></figure>
<p>在这里的教程里，作者使用了<code>DataCollatorWithPadding</code>，它会动态地pad
inputs。我看到文档里还有<code>class transformers.DataCollatorForTokenClassification</code>这个类，maybe可以处理不定长输入，留作后续探索。</p>
<p>transformer的这个教程还是过于简单了，在实际的case中情况会复杂一点。接下来我们看斯坦福的羊驼咋处理不定长sequence的。</p>
<p>首先它先把Dataset定义好：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    sources: <span class="type">Sequence</span>[<span class="built_in">str</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">    targets: <span class="type">Sequence</span>[<span class="built_in">str</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">    tokenizer: transformers.PreTrainedTokenizer,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="type">Dict</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Preprocess the data by tokenizing.&quot;&quot;&quot;</span></span><br><span class="line">    examples = [s + t <span class="keyword">for</span> s, t <span class="keyword">in</span> <span class="built_in">zip</span>(sources, targets)]</span><br><span class="line">    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) <span class="keyword">for</span> strings <span class="keyword">in</span> (examples, sources)]</span><br><span class="line">    input_ids = examples_tokenized[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">    labels = copy.deepcopy(input_ids)</span><br><span class="line">    <span class="keyword">for</span> label, source_len <span class="keyword">in</span> <span class="built_in">zip</span>(labels, sources_tokenized[<span class="string">&quot;input_ids_lens&quot;</span>]):</span><br><span class="line">        label[:source_len] = IGNORE_INDEX</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(input_ids=input_ids, labels=labels)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SupervisedDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Dataset for supervised fine-tuning.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data_path: <span class="built_in">str</span>, tokenizer: transformers.PreTrainedTokenizer</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SupervisedDataset, self).__init__()</span><br><span class="line">        logging.warning(<span class="string">&quot;Loading data...&quot;</span>)</span><br><span class="line">        list_data_dict = utils.jload(data_path)</span><br><span class="line"></span><br><span class="line">        logging.warning(<span class="string">&quot;Formatting inputs...&quot;</span>)</span><br><span class="line">        prompt_input, prompt_no_input = PROMPT_DICT[<span class="string">&quot;prompt_input&quot;</span>], PROMPT_DICT[<span class="string">&quot;prompt_no_input&quot;</span>]</span><br><span class="line">        sources = [</span><br><span class="line">            prompt_input.format_map(example) <span class="keyword">if</span> example.get(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;&quot;</span>) != <span class="string">&quot;&quot;</span> <span class="keyword">else</span> prompt_no_input.format_map(example)</span><br><span class="line">            <span class="keyword">for</span> example <span class="keyword">in</span> list_data_dict</span><br><span class="line">        ]</span><br><span class="line">        targets = [<span class="string">f&quot;<span class="subst">&#123;example[<span class="string">&#x27;output&#x27;</span>]&#125;</span><span class="subst">&#123;tokenizer.eos_token&#125;</span>&quot;</span> <span class="keyword">for</span> example <span class="keyword">in</span> list_data_dict] <span class="comment"># 将output之后加上[EOS]</span></span><br><span class="line"></span><br><span class="line">        logging.warning(<span class="string">&quot;Tokenizing inputs... This may take some time...&quot;</span>)</span><br><span class="line">        data_dict = preprocess(sources, targets, tokenizer)</span><br><span class="line"></span><br><span class="line">        self.input_ids = data_dict[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">        self.labels = data_dict[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.input_ids)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, i</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(input_ids=self.input_ids[i], labels=self.labels[i])</span><br></pre></td></tr></table></figure>
<p>玄机在：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_tokenize_fn</span>(<span class="params">strings: <span class="type">Sequence</span>[<span class="built_in">str</span>], tokenizer: transformers.PreTrainedTokenizer</span>) -&gt; <span class="type">Dict</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Tokenize a list of strings.&quot;&quot;&quot;</span></span><br><span class="line">    tokenized_list = [</span><br><span class="line">        tokenizer(</span><br><span class="line">            text,</span><br><span class="line">            return_tensors=<span class="string">&quot;pt&quot;</span>,</span><br><span class="line">            padding=<span class="string">&quot;longest&quot;</span>,</span><br><span class="line">            max_length=tokenizer.model_max_length,</span><br><span class="line">            truncation=<span class="literal">True</span>,</span><br><span class="line">        ) <span class="comment"># 这里做了循环，也就是对strings这个list里的每一个sequence单独做的tokenize，然后把这些不等长的input_ids一同放到一个list里。之所以用list，是因为list里可以存储不等长的list。一直到这一步都没有做padding</span></span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> strings</span><br><span class="line">    ]</span><br><span class="line">    input_ids = labels = [tokenized.input_ids[<span class="number">0</span>] <span class="keyword">for</span> tokenized <span class="keyword">in</span> tokenized_list]</span><br><span class="line">    input_ids_lens = labels_lens = [</span><br><span class="line">        tokenized.input_ids.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>().item() <span class="keyword">for</span> tokenized <span class="keyword">in</span> tokenized_list</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        labels=labels,</span><br><span class="line">        input_ids_lens=input_ids_lens,</span><br><span class="line">        labels_lens=labels_lens,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>当我们调用<code>SupervisedDataset</code>实例化数据后我们来看看数据长什么样子</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">train_dataset中有两个key，一个Input_ids,一个labels</span><br><span class="line">input_ids中的值长这样：</span><br><span class="line">tensor([    2, 45943,    16,    41, 15741,    14,  7448,    10,  3685,     4,</span><br><span class="line">        21062,    10,  1263,    14, 16574, 25830,     5,  2069,     4, 50118,</span><br><span class="line">        50118, 48134, 41241,    35, 50118, 31033,   130,  4965,    13,  4959,</span><br><span class="line">         2245,     4, 50118, 50118, 48134, 19121,    35,   134,     4, 43800,</span><br><span class="line">           10,  9320,  5626,     8,   146,   686,     7,   680,  2710,     9,</span><br><span class="line">        12849,     8,  8942,     4,  1437, 50118,   176,     4, 30450,  4595,</span><br><span class="line">            7,   489,   110,   809,  2171,     8,   670,     4,  1437, 50118,</span><br><span class="line">          246,     4,  2315,   615,  3581,     8,  3014,    10,  4292,  3581,</span><br><span class="line">         3078,     4,     2])</span><br><span class="line">labels中的值长这样：</span><br><span class="line">tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,</span><br><span class="line">         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,</span><br><span class="line">         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,</span><br><span class="line">         -100,  -100,  -100,  -100,  -100,  -100,  -100,   134,     4, 43800,</span><br><span class="line">           10,  9320,  5626,     8,   146,   686,     7,   680,  2710,     9,</span><br><span class="line">        12849,     8,  8942,     4,  1437, 50118,   176,     4, 30450,  4595,</span><br><span class="line">            7,   489,   110,   809,  2171,     8,   670,     4,  1437, 50118,</span><br><span class="line">          246,     4,  2315,   615,  3581,     8,  3014,    10,  4292,  3581,</span><br><span class="line">         3078,     4,     2])</span><br></pre></td></tr></table></figure>
<p>而且input_ids中每一个值的长度都是不同的，这是因为没有做padding的结果，<strong>仅仅</strong>是将所有的过长的sequence截断了。</p>
<p>羊驼的代码将所有的padding细节都放到了collator里：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataCollatorForSupervisedDataset</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Collate examples for supervised fine-tuning.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    tokenizer: transformers.PreTrainedTokenizer</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, instances: <span class="type">Sequence</span>[<span class="type">Dict</span>]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span></span><br><span class="line">        input_ids, labels = <span class="built_in">tuple</span>([instance[key] <span class="keyword">for</span> instance <span class="keyword">in</span> instances] <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">&quot;input_ids&quot;</span>, <span class="string">&quot;labels&quot;</span>))</span><br><span class="line">        input_ids = torch.nn.utils.rnn.pad_sequence(</span><br><span class="line">            input_ids, batch_first=<span class="literal">True</span>, padding_value=self.tokenizer.pad_token_id</span><br><span class="line">        )</span><br><span class="line">        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=<span class="literal">True</span>, padding_value=IGNORE_INDEX)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            labels=labels,</span><br><span class="line">            attention_mask=input_ids.ne(self.tokenizer.pad_token_id), <span class="comment"># see https://pytorch.org/docs/stable/generated/torch.ne.html#torch.ne</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>这里作者写了一个自己的类，继承自object。这里没有继承transformer的DefaultDataCollator，暂时不清楚用意，但我觉得应该也可以。这个类实现了一个<code>__call__</code>方法，接受的是一个Sequence（可迭代对象），对象中是字典（input_ids,
labels），我们上面在创建数据集的时候getitem每次返回一个dict，这个dict里有input_id和label。现在的collator接受的是这个字典的list，也就是有很多个数据（batch_size），我们对这个batch里的数据统一进行padding，这样就实现了在batch内部去pad，避免将所有的字符串都pad成最长的字符长度。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YAN</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Symbols count total: </span>
    <span title="Symbols count total">189k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">2:51</span>
  </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
