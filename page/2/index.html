<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=consolas:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.8.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="Be smart! Keep Learning">
<meta property="og:type" content="website">
<meta property="og:title" content="YAN&#39;s Blog">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="YAN&#39;s Blog">
<meta property="og:description" content="Be smart! Keep Learning">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="YAN">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>YAN's Blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">YAN's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="YAN"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">YAN</p>
  <div class="site-description" itemprop="description">Be smart! Keep Learning</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">48</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/applepieiris" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/" class="post-title-link" itemprop="url">LLaMA系列模型浅析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-12-12 14:56:24" itemprop="dateCreated datePublished" datetime="2023-12-12T14:56:24+08:00">2023-12-12</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2023-12-29 14:12:38" itemprop="dateModified" datetime="2023-12-29T14:12:38+08:00">2023-12-29</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>10 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>这篇博客主要记录博主在探索meta开源的llama大模型，包括它的数据，training代码以及评测方法。之所以想记录下来，主要是因为llama的论文写的极其的细致，完美践行了开源这个词(感谢meta!)，第二个原因是它的文档以及社区都很活跃，使用人群广泛，我们可以借助很多中文社区的复现情况去一探大公司在实现一个大模型时候的考量，以及思考它为什么会这样做。</p>
<p>之前写过一篇关于斯坦福的alpaca的代码的解析，后来看过很多关于微调大模型(supervised
finetuning)的代码仓库，大家的实现思路基本上都可以追溯到alpaca的这份代码。</p>
<p>首先我会将所有我参考的资料罗列在前面，方便大家查找： - <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama/tree/main">llama代码仓库</a>
这个仓库是介绍如何下载llama模型 - <a href="lll">llama "食谱"</a>
一开始我想在上一个llama仓库中找到相关的train代码，找了半天发现根本没有。后来才发现meta官方将所有finetune(pretrain
from scrach)的代码放在这个仓库，适合developer - <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09288">Llama 2: Open Foundation and
Fine-Tuned Chat Models</a> llama2的research paper。强烈建议食用</p>
<p>中文社区的LLama的工作 - <a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2">Chinese LLaMA
Alpaca2</a></p>
<p>这个仓库同样有配套的文章<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.08177.pdf">Efficient and Effective Text
Encoding for Chinese LLaMA and Alpaca</a></p>
<p>这个仓库的工作主要是两个：</p>
<ol type="1">
<li>扩充了llama原来的token，也就是中文的那部分</li>
<li>用新的中文数据在llama上进行了continue
pretraining，并且发布了在instruction数据上的微调模型</li>
</ol>
<p>研究思路很简单，在别人模型上继续预训练，并参照alpaca对预训练的模型进行instruction
finetune让其具备follow instructions的能力。我们首先从这个Chinese
LLaMA代码仓库看起。</p>
<h1 id="chinese-llama">Chinese LLaMA</h1>
<p>作者自述做这份工作的原因是原生llama模型的词汇表中仅包含1000+个中文字符，所以首要任务是要扩充llama的词表。他们首先训练了一个中文的tokenizer，然后将其与llama的tokenizer进行融合，融合后的tokenizer拥有49953个token,
那么输入的词汇表数就从32000扩充到了49953。作者的实验还发现用新的融合后的tokenizer去tokenize序列要比旧的tokenizer编码后的序列要短。那很自然的就减少了很多计算量。</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231212161342629.png" alt="image-20231212161342629">
<figcaption aria-hidden="true">image-20231212161342629</figcaption>
</figure>
<p>在准备好tokenizer之后就到了训练环节，作者在这里没有采用全参数微调而是采用了Lora这种高效微调的方式。其实我看到这里是有疑问的，当然作者也在issue中做了回答：</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231212162349967.png" alt="image-20231212162349967">
<figcaption aria-hidden="true">image-20231212162349967</figcaption>
</figure>
<blockquote>
<p>我个人认为continue
pretraining是需要全参数微调的，而且还是在扩充了词表的情况下。</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/blob/main/scripts/training/run_clm_pt_with_peft.py">预训练脚本</a>,这个脚本是作者在<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py">transformers库的run_clm.py</a>上修改的，至于中文预训练数据部分，作者采用了20G的纯文本数据，并将他们分成了每个block
512个token。我们来看看代码是怎么写的，源代码在<a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/blob/main/scripts/training/run_clm_pt_with_peft.py">run_clm_pt_with_peft</a>,可以先将<a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/tree/main">Chinese-LLaMA-Alpaca-2</a>拉到本地，在文件姐scripts里可以看到training文件夹里有两个训练代码，一个是pretrain的，一个是sft的。我们先看前面这个pretrain的，它的训练任务很好理解，就是用decoder这种模型架构训练一个输入序列的下一个单词。</p>
<p>作者在这个仓库里没有放训练数据，我们先在该仓库里创建一个<code>./data</code>,里面放一些txt格式的数据用于测试，比如一些小说啥的，训练脚本在处理数据时会自动对他们进行读取并chunk成512长度的序列。作者在paper里提到的他们team训练的tokenizer也一并在scripts的tokenizer文件夹内，要跑通train这个代码需要在run_pt.sh内将这些参数都制定好。</p>
<p>先来看load数据以及处理部分的重点代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">files = [file.name <span class="keyword">for</span> file <span class="keyword">in</span> path.glob(<span class="string">&quot;*.txt&quot;</span>)]</span><br><span class="line"><span class="keyword">for</span> idx, file <span class="keyword">in</span> <span class="built_in">enumerate</span>(files):<span class="comment"># files为./data文件夹内所有的txt</span></span><br><span class="line">    data_file = os.path.join(path, file)</span><br><span class="line">    filename = <span class="string">&#x27;&#x27;</span>.join(file.split(<span class="string">&quot;.&quot;</span>)[:-<span class="number">1</span>])</span><br><span class="line">    cache_path = os.path.join(data_args.data_cache_dir, filename+<span class="string">f&quot;_<span class="subst">&#123;block_size&#125;</span>&quot;</span>)</span><br><span class="line">    os.makedirs(cache_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        processed_dataset = datasets.load_from_disk(cache_path, keep_in_memory=<span class="literal">False</span>) <span class="comment"># 首先使用datasets导入</span></span><br><span class="line">        logger.info(<span class="string">f&#x27;training datasets-<span class="subst">&#123;filename&#125;</span> has been loaded from disk&#x27;</span>)</span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            cache_dir = os.path.join(data_args.data_cache_dir, filename+<span class="string">f&quot;_text_<span class="subst">&#123;block_size&#125;</span>&quot;</span>)</span><br><span class="line">            os.makedirs(cache_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">            raw_dataset = load_dataset(<span class="string">&quot;text&quot;</span>, data_files=data_file, cache_dir=cache_dir, keep_in_memory=<span class="literal">False</span>)</span><br><span class="line">            logger.info(<span class="string">f&quot;<span class="subst">&#123;file&#125;</span> has been loaded&quot;</span>)</span><br><span class="line">            tokenized_dataset = raw_dataset.<span class="built_in">map</span>(</span><br><span class="line">                tokenize_function,</span><br><span class="line">                batched=<span class="literal">True</span>,</span><br><span class="line">                num_proc=data_args.preprocessing_num_workers,</span><br><span class="line">                remove_columns=<span class="string">&quot;text&quot;</span>,</span><br><span class="line">                load_from_cache_file=<span class="literal">True</span>,</span><br><span class="line">                keep_in_memory=<span class="literal">False</span>,</span><br><span class="line">                cache_file_names = &#123;k: os.path.join(cache_dir, <span class="string">&#x27;tokenized.arrow&#x27;</span>) <span class="keyword">for</span> k <span class="keyword">in</span> raw_dataset&#125;,</span><br><span class="line">                desc=<span class="string">&quot;Running tokenizer on dataset&quot;</span>,</span><br><span class="line">            )</span><br><span class="line">            grouped_datasets = tokenized_dataset.<span class="built_in">map</span>(</span><br><span class="line">                group_texts,</span><br><span class="line">                batched=<span class="literal">True</span>,</span><br><span class="line">                num_proc=data_args.preprocessing_num_workers,</span><br><span class="line">                load_from_cache_file=<span class="literal">True</span>,</span><br><span class="line">                keep_in_memory=<span class="literal">False</span>,</span><br><span class="line">                cache_file_names = &#123;k: os.path.join(cache_dir, <span class="string">&#x27;grouped.arrow&#x27;</span>) <span class="keyword">for</span> k <span class="keyword">in</span> tokenized_dataset&#125;,</span><br><span class="line">                desc=<span class="string">f&quot;Grouping texts in chunks of <span class="subst">&#123;block_size&#125;</span>&quot;</span>,</span><br><span class="line">            ) <span class="comment"># </span></span><br><span class="line">            processed_dataset = grouped_datasets</span><br><span class="line">            processed_dataset.save_to_disk(cache_path)</span><br><span class="line">            <span class="keyword">if</span> idx == <span class="number">0</span>: <span class="comment"># </span></span><br><span class="line">                lm_datasets = processed_dataset[<span class="string">&#x27;train&#x27;</span>]</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 如果有多于2个txt,那么将这些数据叠加起来</span></span><br><span class="line">                <span class="keyword">assert</span> lm_datasets.features.<span class="built_in">type</span> == processed_dataset[<span class="string">&quot;train&quot;</span>].features.<span class="built_in">type</span></span><br><span class="line">                lm_datasets = concatenate_datasets([lm_datasets, processed_dataset[<span class="string">&quot;train&quot;</span>]])</span><br></pre></td></tr></table></figure>
<p>内有两个帮助函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_function</span>(<span class="params">examples</span>):</span></span><br><span class="line">        <span class="keyword">with</span> CaptureLogger(tok_logger) <span class="keyword">as</span> cl:</span><br><span class="line">            output = tokenizer(examples[<span class="string">&quot;text&quot;</span>]) <span class="comment"># 仅仅做了tokenize这一个动作,而且会在每一个序列的结尾都加上EOS,由于设置了tokenizer.add_eos_token = True</span></span><br><span class="line">        <span class="comment"># clm input could be much much longer than block_size</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;Token indices sequence length is longer than the&quot;</span> <span class="keyword">in</span> cl.out:</span><br><span class="line">            tok_logger.warning(</span><br><span class="line">                <span class="string">&quot;^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits&quot;</span></span><br><span class="line">                <span class="string">&quot; before being passed to the model.&quot;</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_texts</span>(<span class="params">examples</span>):</span> <span class="comment"># 在这个函数里程序将tokenize之后的input_ids和attention_mask进行chunk，保证每个chunk大小都是block_size的</span></span><br><span class="line">    <span class="comment"># Concatenate all texts.</span></span><br><span class="line">    concatenated_examples = &#123;k: <span class="built_in">list</span>(chain(*examples[k])) <span class="keyword">for</span> k <span class="keyword">in</span> examples.keys()&#125;</span><br><span class="line">    total_length = <span class="built_in">len</span>(concatenated_examples[<span class="built_in">list</span>(examples.keys())[<span class="number">0</span>]])</span><br><span class="line">    <span class="comment"># We drop the small remainder, we could add padding if the model supported it instead of this drop, you can</span></span><br><span class="line">    <span class="comment"># customize this part to your needs.</span></span><br><span class="line">    <span class="keyword">if</span> total_length &gt;= block_size:</span><br><span class="line">        total_length = (total_length // block_size) * block_size</span><br><span class="line">        <span class="comment"># Split by chunks of max_len.</span></span><br><span class="line">        result = &#123;</span><br><span class="line">            k: [t[i : i + block_size] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, total_length, block_size)]</span><br><span class="line">            <span class="keyword">for</span> k, t <span class="keyword">in</span> concatenated_examples.items()</span><br><span class="line">        &#125;</span><br><span class="line">        result[<span class="string">&quot;labels&quot;</span>] = result[<span class="string">&quot;input_ids&quot;</span>].copy() <span class="comment"># 这里labels设置成和input_ids一模一样</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<p>可以看到基本采用transformer的库来实现的数据的导入以及process，总体来说使用datasets还是比较方便的。</p>
<p>再来看如何做的lora train：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, TaskType, get_peft_model, PeftModel, get_peft_model_state_dict</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> training_args.full_finetuning: <span class="comment"># 默认模型是全参数微调</span></span><br><span class="line">        <span class="keyword">if</span> training_args.peft_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            logger.info(<span class="string">&quot;Peft from pre-trained model&quot;</span>)</span><br><span class="line">            model = PeftModel.from_pretrained(model, training_args.peft_path, device_map=device_map)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            logger.info(<span class="string">&quot;Init new peft model&quot;</span>)</span><br><span class="line">            target_modules = training_args.trainable.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">            modules_to_save = training_args.modules_to_save</span><br><span class="line">            <span class="keyword">if</span> modules_to_save <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                modules_to_save = modules_to_save.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">            lora_rank = training_args.lora_rank</span><br><span class="line">            lora_dropout = training_args.lora_dropout</span><br><span class="line">            lora_alpha = training_args.lora_alpha</span><br><span class="line">            logger.info(<span class="string">f&quot;target_modules: <span class="subst">&#123;target_modules&#125;</span>&quot;</span>)</span><br><span class="line">            logger.info(<span class="string">f&quot;lora_rank: <span class="subst">&#123;lora_rank&#125;</span>&quot;</span>)</span><br><span class="line">            peft_config = LoraConfig(</span><br><span class="line">                task_type=TaskType.CAUSAL_LM,</span><br><span class="line">                target_modules=target_modules,</span><br><span class="line">                inference_mode=<span class="literal">False</span>,</span><br><span class="line">                r=lora_rank, lora_alpha=lora_alpha,</span><br><span class="line">                lora_dropout=lora_dropout,</span><br><span class="line">                modules_to_save=modules_to_save) <span class="comment"># LoraConfig是PEFT这个包内的</span></span><br><span class="line">            model = get_peft_model(model, peft_config)</span><br><span class="line">        model.print_trainable_parameters()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>该仓库的instruction
finetune的代码和alpaca的思路一样，很多写法都一模一样。不过因为作者在做pretrain的时候用的是lora的形式，所以在sft的时候也需要在这个基础模型上进行微调。作者在<code>run_clm_sft_with_peft.py</code>中是类似于pt脚本中的写法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> training_args.peft_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    logger.info(<span class="string">&quot;Peft from pre-trained model&quot;</span>)</span><br><span class="line">    model = PeftModel.from_pretrained(model, training_args.peft_path, device_map=device_map)</span><br></pre></td></tr></table></figure>
<p>这里的peft_path是需要在train的时候传入参数的，也就是我们在pretrain时候通过call_back函数保存的lora参数,
模型组装好之后训练。博主认为这时候是所有参数一起调整了，包含lora部分以及llama2基础模型部分。</p>
<blockquote>
<p>一点题外话：在阅读Chinese
LLaMA这份代码的时候发现了其中一个作者崔一鸣的博客，内有一个关于大模型的纵览介绍挺适合初学者熟悉大模型的相关技术，也适合面试的盆友回顾以及对自己还没掌握透的知识进行查漏补缺的。<a target="_blank" rel="noopener" href="https://ymcui.com/talk/20230826_baai_llm_tutorial.pdf"><strong>[Methods
and Practices for Large Pre-trained Language
Models](https://ymcui.com/talk/20230826_baai_llm_tutorial.pdf)</strong></a></p>
<p>建议配合<a target="_blank" rel="noopener" href="https://karpathy.ai/stateofgpt.pdf">stateofgpt</a>食用</p>
</blockquote>
<h1 id="llama">LLaMA</h1>
<h2 id="拓展补充介绍">拓展补充介绍</h2>
<p>LLaMA的1和2版本在模型架构上大多数相似，其中三个关键技术使羊驼模型区别于其他模型，这里摘一下llama2
research paper中的描述：</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231213162604171.png" alt="image-20231213162604171">
<figcaption aria-hidden="true">image-20231213162604171</figcaption>
</figure>
<h3 id="rmsnorm">RMSNorm</h3>
<p>在介绍RMSNorm之前补充一下Batch Normalization以及Layer
Normalization</p>
<p>参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://medium.com/@florian_algo/batchnorm-and-layernorm-2637f46a998b">BatchNorm
and LayerNorm</a></li>
<li><a target="_blank" rel="noopener" href="https://tungmphung.com/deep-learning-normalization-methods/">Deep
Learning normalization methods</a></li>
<li><a target="_blank" rel="noopener" href="https://ai.stackexchange.com/questions/27309/what-are-the-consequences-of-layer-norm-vs-batch-norm">[What
are the consequences of layer norm vs batch
norm?](https://ai.stackexchange.com/questions/27309/what-are-the-consequences-of-layer-norm-vs-batch-norm)</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/70065235/understanding-torch-nn-layernorm-in-nlp">[Understanding
torch.nn.LayerNorm in
nlp](https://stackoverflow.com/questions/70065235/understanding-torch-nn-layernorm-in-nlp)</a></li>
</ul>
<p><img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/0_K45DoPRbhC5-dqq1-17025178394161.webp"></p>
<p>上面图片中，每一行属于一个batch的数据，不用管这个batch内的数据是2维的还是1维的。</p>
<h4 id="batch-normalization">Batch Normalization</h4>
<blockquote>
<p>for each dimension of the input, all data points in the batch are
gathered and normalized with the same mean and standard deviation</p>
<p>BN的所有计算都在一个batch以内，也就是我们用到的数据只是这个batch内的数据，不会涉及到其他batch的数据</p>
</blockquote>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214093939577.png" alt="image-20231214093939577">
<figcaption aria-hidden="true">image-20231214093939577</figcaption>
</figure>
<p>上面的伪代码中的<code>x</code>可以是一个向量，如果是向量的情况下涉及到的x的相加都是向量的运算。值得注意的是在卷积层里，<code>dimension</code>指的是channel维度的</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214094523587.png" alt="image-20231214094523587">
<figcaption aria-hidden="true">image-20231214094523587</figcaption>
</figure>
<p>也就是不同channel计算出的μ和σ是不同的。</p>
<blockquote>
<p>the input data is normalized separately for each channel in a
convolutional layer.</p>
</blockquote>
<p>而在全连接层，<code>dimension</code>就是指feature维度。</p>
<h4 id="layer-normalization">Layer Normalization</h4>
<blockquote>
<p>with LayerNorm, we normalize each data point separately. Moreover,
each data point’s mean and variance are shared over all hidden units
(i.e. neurons) of the layer</p>
</blockquote>
<p>跟batch没关系，在layer层面去计算均值和方差。比如在全连接层，输入是125个神经元的话，就对这些神经元进行归一化。也就是数据中的每一个data
points都是独立进行归一化的，和其他data
points无关。那么对于卷积层来说的话就有两种计算方式：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41978699/article/details/122778085">参考</a></p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214100917596.png" alt="image-20231214100917596">
<figcaption aria-hidden="true">image-20231214100917596</figcaption>
</figure>
<p>看pytorch的<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">doc</a>多采取前一种全部一股脑求平均和方差的方式。</p>
<p>RMSNorm</p>
<p>RMSNorm的research paper写着一部分写的特别清楚，推荐查看原文<a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf">Root
Mean Square Layer Normalization</a></p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214103540564.png" alt="image-20231214103540564">
<figcaption aria-hidden="true">image-20231214103540564</figcaption>
</figure>
<p>RMSNorm去除了LN的求平均数的过程，并且将LN中的除以方差变成了除以<code>root mean square</code>。来看llama中的代码实现：<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama/blob/main/llama/model.py#L63">llama/llama/model.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_norm</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply the RMSNorm normalization to the input tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x (torch.Tensor): The input tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            torch.Tensor: The normalized tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x * torch.rsqrt(x.<span class="built_in">pow</span>(<span class="number">2</span>).mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) + self.eps) <span class="comment"># eps防止除以0</span></span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="swiglu">SwiGLU</h3>
<p>阅读知乎这篇博客<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/650237644">大模型基础｜激活函数｜从ReLU
到SwiGLU</a></p>
<h3 id="rotary-embedding-rope">Rotary Embedding, RoPE</h3>
<h4 id="attention-is-all-you-need中的position-embedding">Attention is
All you need中的position embedding</h4>
<p>首先回顾下在Attention is all you
need原文paper中对于位置编码的公式：</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231215135557606.png" alt="image-20231215135557606">
<figcaption aria-hidden="true">image-20231215135557606</figcaption>
</figure>
<p>我一开始理解这两个公式的时候很困难，后来查了一些资料，发现很多人也在这里由一些困惑，包括tensorflow官方的实现方式<a target="_blank" rel="noopener" href="https://www.tensorflow.org/tutorials/text/transformer?hl=zh-cn#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%88positional_encoding%EF%BC%89">位置编码</a>，tensorflow的官方给出的代码是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_angles</span>(<span class="params">pos, i, d_model</span>):</span></span><br><span class="line">  angle_rates = <span class="number">1</span> / np.power(<span class="number">10000</span>, (<span class="number">2</span> * (i//<span class="number">2</span>)) / np.float32(d_model))</span><br><span class="line">  <span class="keyword">return</span> pos * angle_rates</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span>(<span class="params">position, d_model</span>):</span></span><br><span class="line">  angle_rads = get_angles(np.arange(position)[:, np.newaxis],</span><br><span class="line">                          np.arange(d_model)[np.newaxis, :],</span><br><span class="line">                          d_model)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 将 sin 应用于数组中的偶数索引（indices）；2i</span></span><br><span class="line">  angle_rads[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(angle_rads[:, <span class="number">0</span>::<span class="number">2</span>]) <span class="comment"># 不懂双冒号切片的可参考： https://stackoverflow.com/questions/3453085/what-is-double-colon-in-python-when-subscripting-sequences</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 将 cos 应用于数组中的奇数索引；2i+1</span></span><br><span class="line">  angle_rads[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(angle_rads[:, <span class="number">1</span>::<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">  pos_encoding = angle_rads[np.newaxis, ...]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> tf.cast(pos_encoding, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<p>在<code>get_angles</code>方法里10000的指数系数中tensorflow的实现多加了一个<code>i//2</code>。这里我非常困惑，后来发现stackflow上也有同样的发问：</p>
<ul>
<li>[<a target="_blank" rel="noopener" href="https://datascience.stackexchange.com/questions/79995/explanation-about-i-2-in-positional-encoding-in-tensorflow-tutorial-about-trans/126053#126053">Explanation
about i//2 in positional encoding in tensorflow tutorial about
transformers</a></li>
<li>[<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/69036265/why-does-the-i-need-to-be-divided-by-2-in-caculating-positional-encoding">Why
does the 'i' need to be divided by 2 in caculating positional
encoding?</a></li>
</ul>
<p>推荐阅读一下<a target="_blank" rel="noopener" href="https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/">A
Gentle Introduction to Positional Encoding in Transformer Models, Part
1</a>。该作者的实现方式更符合人类的理解方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPositionEncoding</span>(<span class="params">seq_len, d, n=<span class="number">10000</span></span>):</span></span><br><span class="line">    P = np.zeros((seq_len, d))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(seq_len):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="built_in">int</span>(d/<span class="number">2</span>)): <span class="comment"># 这里只循环d//2次</span></span><br><span class="line">            denominator = np.power(n, <span class="number">2</span>*i/d)</span><br><span class="line">            P[k, <span class="number">2</span>*i] = np.sin(k/denominator)</span><br><span class="line">            P[k, <span class="number">2</span>*i+<span class="number">1</span>] = np.cos(k/denominator)</span><br><span class="line">    <span class="keyword">return</span> P</span><br><span class="line"> </span><br><span class="line">P = getPositionEncoding(seq_len=<span class="number">4</span>, d=<span class="number">4</span>, n=<span class="number">100</span>)</span><br><span class="line"><span class="built_in">print</span>(P)</span><br></pre></td></tr></table></figure>
<p>那么该怎么理解paper中的公式以及tensorflow//2的这个实现呢。就拿某一个sequence中的token来举例子，如果我们想要编码的向量长度是20,也就是d=20。那么tensorflow的做法是首先创建一个长度为20的向量，然后依次求其中的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">该token的position encoding所有应该求值得index</span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>]</span><br><span class="line">angle_rates = <span class="number">1</span> / np.power(<span class="number">10000</span>, (<span class="number">2</span> * (i//<span class="number">2</span>)) / np.float32(d_model)) 这句话</span><br><span class="line"><span class="number">10000</span>的指数(<span class="number">2</span> * (i//<span class="number">2</span>))是</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">18</span>, <span class="number">18</span>]</span><br></pre></td></tr></table></figure>
<p>所以对照paper中的公式表达的意思就是：</p>
<p>在向量的偶数index位置，比如0，2...等，公式里的2i就等于它的index</p>
<p>在向量的奇数index位置，比如1,3...等，公式里的10000的指数也就是2i的位置应该取这个奇数的前一个偶数值。</p>
<p>那么我们来看看tensorflow的这份代码就对上了：</p>
<p>10000的指数部分出现的值为：
<code>[0, 0, 2, 2, 4, 4, 6, 6, 8, 8, 10, 10, 12, 12, 14, 14, 16, 16, 18, 18]</code></p>
<p>所以paper里的这个公式要将2i当作一个整体来看。</p>
<h4 id="roperotary-position-embedding"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.09864v5.pdf">RoPE(rotary Position
Embedding)</a></h4>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231215152735803.png" alt="image-20231215152735803">
<figcaption aria-hidden="true">image-20231215152735803</figcaption>
</figure>
<p>RoPE进一步改进了绝对位置编码，是一种在transformer
attention中的Q和K上添加相对位置信息的方法</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231215152803294.png" alt="image-20231215152803294">
<figcaption aria-hidden="true">image-20231215152803294</figcaption>
</figure>
<p>首先作者将隐藏层的向量每两个维度编成一组，看成2维的向量；然后对于特定位置m的x1,x2，将他们旋转mθ角度，用新的x1,x2值替换老的值加入到query和key中。</p>
<h3 id="grouped-query-attention-gqa">Grouped-Query Attention (GQA)</h3>
<p>GQA是llama2相较于llama1新采用的技术，它是一种提升推理速度的方法，主要针对多头注意力机制进行改进，与KV
Cache搭配使用</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/06/Sequence%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%AD%E5%A4%84%E7%90%86%E4%B8%8D%E5%AE%9A%E9%95%BF%E6%95%B0%E6%8D%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/06/Sequence%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%AD%E5%A4%84%E7%90%86%E4%B8%8D%E5%AE%9A%E9%95%BF%E6%95%B0%E6%8D%AE/" class="post-title-link" itemprop="url">Sequence分类问题中处理不定长数据</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-12-06 10:55:07" itemprop="dateCreated datePublished" datetime="2023-12-06T10:55:07+08:00">2023-12-06</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2024-08-30 09:38:18" itemprop="dateModified" datetime="2024-08-30T09:38:18+08:00">2024-08-30</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>6.2k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>6 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>之前写过一篇关于stanford
alpaca的代码的分析，最近在kaggle上看到一个检测某段长文本是否是AI生成的任务<a target="_blank" rel="noopener" href="https://www.kaggle.com/c/llm-detect-ai-generated-text">LLM -
Detect AI Generated
Text</a>,自己也在尝试做这个任务的时候，发现斯坦福的这份代码真是常看常新。对于数据的准备部分，有很多选择，比如在创建Dataset的时候就把所有的字符串数据tokenize好，在get_item()的函数返回时就返回input_ids，也可以是像斯坦福的<a target="_blank" rel="noopener" href="https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py">这份代码</a>一样，先把数据读取进来然后再用<code>DataCollator</code>处理（padding）。</p>
<p>我之前没有发现斯坦福这份代码这么写的真正原因，直到我自己来处理这种不定长的序列输入时才发现这样写的绝妙，因为我们都知道矩阵是每一行都需要是同样的size，所以斯坦福的写法在数据处理前期一直在用list，而不是batch。</p>
<p>先来说说transformer的对于序列分类的官方教程的写法<a target="_blank" rel="noopener" href="https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/sequence_classification.ipynb">传送门</a></p>
<p>transformer的这个教程直接使用的是自己的数据集，已经规整为datasets了，首先它对数据集使用map函数做了截断的处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;distilbert-base-uncased&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_function</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">&quot;text&quot;</span>], truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">tokenized_imdb = imdb.<span class="built_in">map</span>(preprocess_function, batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>然后重点来了，注意在上面的<code>preprocess_function</code>中并没有对序列进行padding，只是对过长的序列做了截断。接着作者使用了<code>datacollatorwithpadding</code>，给出的理由是：</p>
<blockquote>
<p>It's more efficient to <em>dynamically pad</em> the sentences to the
longest length in a batch during collation, instead of padding the whole
dataset to the maximum length.</p>
</blockquote>
<p>我们可以用官方的文档中看到对于datacollator的<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main_classes/data_collator">定义</a>：</p>
<blockquote>
<p>Data collators are objects that will form a batch by using a list of
dataset elements as input. These elements are of the same type as the
elements of <code>train_dataset</code> or <code>eval_dataset</code></p>
</blockquote>
<p>也就是data
collators的输入是一个list，list里的每一个元素跟train_dataset中的元素是一样的。在data
collator中你可以做一些processing，如padding，random
masking。我们接下来可以看到斯坦福的羊驼代码就是将padding的步骤放到了data
collator内。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorWithPadding</span><br><span class="line"></span><br><span class="line">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span><br></pre></td></tr></table></figure>
<p>在这里的教程里，作者使用了<code>DataCollatorWithPadding</code>，它会动态地pad
inputs。我看到文档里还有<code>class transformers.DataCollatorForTokenClassification</code>这个类，maybe可以处理不定长输入，留作后续探索。</p>
<p>transformer的这个教程还是过于简单了，在实际的case中情况会复杂一点。接下来我们看斯坦福的羊驼咋处理不定长sequence的。</p>
<p>首先它先把Dataset定义好：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    sources: <span class="type">Sequence</span>[<span class="built_in">str</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">    targets: <span class="type">Sequence</span>[<span class="built_in">str</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">    tokenizer: transformers.PreTrainedTokenizer,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="type">Dict</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Preprocess the data by tokenizing.&quot;&quot;&quot;</span></span><br><span class="line">    examples = [s + t <span class="keyword">for</span> s, t <span class="keyword">in</span> <span class="built_in">zip</span>(sources, targets)]</span><br><span class="line">    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) <span class="keyword">for</span> strings <span class="keyword">in</span> (examples, sources)]</span><br><span class="line">    input_ids = examples_tokenized[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">    labels = copy.deepcopy(input_ids)</span><br><span class="line">    <span class="keyword">for</span> label, source_len <span class="keyword">in</span> <span class="built_in">zip</span>(labels, sources_tokenized[<span class="string">&quot;input_ids_lens&quot;</span>]):</span><br><span class="line">        label[:source_len] = IGNORE_INDEX</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(input_ids=input_ids, labels=labels)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SupervisedDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Dataset for supervised fine-tuning.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data_path: <span class="built_in">str</span>, tokenizer: transformers.PreTrainedTokenizer</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SupervisedDataset, self).__init__()</span><br><span class="line">        logging.warning(<span class="string">&quot;Loading data...&quot;</span>)</span><br><span class="line">        list_data_dict = utils.jload(data_path)</span><br><span class="line"></span><br><span class="line">        logging.warning(<span class="string">&quot;Formatting inputs...&quot;</span>)</span><br><span class="line">        prompt_input, prompt_no_input = PROMPT_DICT[<span class="string">&quot;prompt_input&quot;</span>], PROMPT_DICT[<span class="string">&quot;prompt_no_input&quot;</span>]</span><br><span class="line">        sources = [</span><br><span class="line">            prompt_input.format_map(example) <span class="keyword">if</span> example.get(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;&quot;</span>) != <span class="string">&quot;&quot;</span> <span class="keyword">else</span> prompt_no_input.format_map(example)</span><br><span class="line">            <span class="keyword">for</span> example <span class="keyword">in</span> list_data_dict</span><br><span class="line">        ]</span><br><span class="line">        targets = [<span class="string">f&quot;<span class="subst">&#123;example[<span class="string">&#x27;output&#x27;</span>]&#125;</span><span class="subst">&#123;tokenizer.eos_token&#125;</span>&quot;</span> <span class="keyword">for</span> example <span class="keyword">in</span> list_data_dict] <span class="comment"># 将output之后加上[EOS]</span></span><br><span class="line"></span><br><span class="line">        logging.warning(<span class="string">&quot;Tokenizing inputs... This may take some time...&quot;</span>)</span><br><span class="line">        data_dict = preprocess(sources, targets, tokenizer)</span><br><span class="line"></span><br><span class="line">        self.input_ids = data_dict[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">        self.labels = data_dict[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.input_ids)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, i</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(input_ids=self.input_ids[i], labels=self.labels[i])</span><br></pre></td></tr></table></figure>
<p>玄机在：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_tokenize_fn</span>(<span class="params">strings: <span class="type">Sequence</span>[<span class="built_in">str</span>], tokenizer: transformers.PreTrainedTokenizer</span>) -&gt; <span class="type">Dict</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Tokenize a list of strings.&quot;&quot;&quot;</span></span><br><span class="line">    tokenized_list = [</span><br><span class="line">        tokenizer(</span><br><span class="line">            text,</span><br><span class="line">            return_tensors=<span class="string">&quot;pt&quot;</span>,</span><br><span class="line">            padding=<span class="string">&quot;longest&quot;</span>,</span><br><span class="line">            max_length=tokenizer.model_max_length,</span><br><span class="line">            truncation=<span class="literal">True</span>,</span><br><span class="line">        ) <span class="comment"># 这里做了循环，也就是对strings这个list里的每一个sequence单独做的tokenize，然后把这些不等长的input_ids一同放到一个list里。之所以用list，是因为list里可以存储不等长的list。一直到这一步都没有做padding</span></span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> strings</span><br><span class="line">    ]</span><br><span class="line">    input_ids = labels = [tokenized.input_ids[<span class="number">0</span>] <span class="keyword">for</span> tokenized <span class="keyword">in</span> tokenized_list]</span><br><span class="line">    input_ids_lens = labels_lens = [</span><br><span class="line">        tokenized.input_ids.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>().item() <span class="keyword">for</span> tokenized <span class="keyword">in</span> tokenized_list</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        labels=labels,</span><br><span class="line">        input_ids_lens=input_ids_lens,</span><br><span class="line">        labels_lens=labels_lens,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>当我们调用<code>SupervisedDataset</code>实例化数据后我们来看看数据长什么样子</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">train_dataset中有两个key，一个Input_ids,一个labels</span><br><span class="line">input_ids中的值长这样：</span><br><span class="line">tensor([    2, 45943,    16,    41, 15741,    14,  7448,    10,  3685,     4,</span><br><span class="line">        21062,    10,  1263,    14, 16574, 25830,     5,  2069,     4, 50118,</span><br><span class="line">        50118, 48134, 41241,    35, 50118, 31033,   130,  4965,    13,  4959,</span><br><span class="line">         2245,     4, 50118, 50118, 48134, 19121,    35,   134,     4, 43800,</span><br><span class="line">           10,  9320,  5626,     8,   146,   686,     7,   680,  2710,     9,</span><br><span class="line">        12849,     8,  8942,     4,  1437, 50118,   176,     4, 30450,  4595,</span><br><span class="line">            7,   489,   110,   809,  2171,     8,   670,     4,  1437, 50118,</span><br><span class="line">          246,     4,  2315,   615,  3581,     8,  3014,    10,  4292,  3581,</span><br><span class="line">         3078,     4,     2])</span><br><span class="line">labels中的值长这样：</span><br><span class="line">tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,</span><br><span class="line">         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,</span><br><span class="line">         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,</span><br><span class="line">         -100,  -100,  -100,  -100,  -100,  -100,  -100,   134,     4, 43800,</span><br><span class="line">           10,  9320,  5626,     8,   146,   686,     7,   680,  2710,     9,</span><br><span class="line">        12849,     8,  8942,     4,  1437, 50118,   176,     4, 30450,  4595,</span><br><span class="line">            7,   489,   110,   809,  2171,     8,   670,     4,  1437, 50118,</span><br><span class="line">          246,     4,  2315,   615,  3581,     8,  3014,    10,  4292,  3581,</span><br><span class="line">         3078,     4,     2])</span><br></pre></td></tr></table></figure>
<p>而且input_ids中每一个值的长度都是不同的，这是因为没有做padding的结果，<strong>仅仅</strong>是将所有的过长的sequence截断了。</p>
<p>羊驼的代码将所有的padding细节都放到了collator里：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataCollatorForSupervisedDataset</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Collate examples for supervised fine-tuning.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    tokenizer: transformers.PreTrainedTokenizer</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, instances: <span class="type">Sequence</span>[<span class="type">Dict</span>]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span></span><br><span class="line">        input_ids, labels = <span class="built_in">tuple</span>([instance[key] <span class="keyword">for</span> instance <span class="keyword">in</span> instances] <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">&quot;input_ids&quot;</span>, <span class="string">&quot;labels&quot;</span>))</span><br><span class="line">        input_ids = torch.nn.utils.rnn.pad_sequence(</span><br><span class="line">            input_ids, batch_first=<span class="literal">True</span>, padding_value=self.tokenizer.pad_token_id</span><br><span class="line">        )</span><br><span class="line">        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=<span class="literal">True</span>, padding_value=IGNORE_INDEX)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            labels=labels,</span><br><span class="line">            attention_mask=input_ids.ne(self.tokenizer.pad_token_id), <span class="comment"># see https://pytorch.org/docs/stable/generated/torch.ne.html#torch.ne</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>这里作者写了一个自己的类，继承自object。这里没有继承transformer的DefaultDataCollator，暂时不清楚用意，但我觉得应该也可以。这个类实现了一个<code>__call__</code>方法，接受的是一个Sequence（可迭代对象），对象中是字典（input_ids,
labels），我们上面在创建数据集的时候getitem每次返回一个dict，这个dict里有input_id和label。现在的collator接受的是这个字典的list，也就是有很多个数据（batch_size），我们对这个batch里的数据统一进行padding，这样就实现了在batch内部去pad，避免将所有的字符串都pad成最长的字符长度。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/" class="post-title-link" itemprop="url">构建和评估RAG应用</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-12-05 09:56:05" itemprop="dateCreated datePublished" datetime="2023-12-05T09:56:05+08:00">2023-12-05</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2024-08-30 09:38:27" itemprop="dateModified" datetime="2024-08-30T09:38:27+08:00">2024-08-30</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>8.3k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>8 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>最近吴恩达出了一个小课程，传送门: <a target="_blank" rel="noopener" href="https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/">Building
and Evaluating Advanced RAG Applications</a>。<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1494y1E7H9?p=3&amp;vd_source=f998f640fc8575504e3e97753bf817f4">B站</a>也有人搬运了，有中英文字幕。最近也正好在做RAG相关的项目，看到这个课程里有一些新的东西，权当在这篇博客里总结记录。</p>
<p>另外还推荐阅读一篇综述<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.05876">Trends in Integration of
Knowledge and Large Language Models: A Survey and Taxonomy of Methods,
Benchmarks, and Applications</a>, 该综述的第三章详细介绍了retrieval
augmentation的方法。我这篇博客会首先理顺一些理论，然后再介绍吴恩达课程里的知识（个人认为吴大佬出的关于LLM的一系列shot
course可食用性不够高，比如上面说的这个RAG相关的课怎么看都觉得是在推广LlamaIndex这个框架，对于原理一句话带过，很多细节不清楚）。</p>
<p>3.2节提到的两个工作值得注意：</p>
<ol type="1">
<li>Query2doc</li>
</ol>
<blockquote>
<p>Query2doc prompts the LLMs to generate a pseudo-document by employing
a few-shot prompting paradigm. Subsequently, the original query is
expanded by incorporating the pseudo-document. The retriever module uses
this new query to retrieve a list of relevant documents.</p>
</blockquote>
<ol start="2" type="1">
<li>Rewrite-Retrieve-Read</li>
</ol>
<blockquote>
<p>Different with Query2doc,they adopt a trainable language model to
perform the rewriting step</p>
</blockquote>
<p>在抽取的context的使用上，我们一般的认知是加入到prompt里，告诉LLM根据这个context回答某个query，这篇综述在3.2节还概括介绍了另外两种使用knowledge的方式：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20231205145030752.png" alt="image-20231205145030752">
<figcaption aria-hidden="true">image-20231205145030752</figcaption>
</figure>
<p>我个人认为第二种方式实操性差一点，第三种和第一种应该是大家会普遍采取的方式，第二种需要更多精细的prompt设计。</p>
<hr>
<p>以下为课程相关的 ，传送门: <a target="_blank" rel="noopener" href="https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/">Building
and Evaluating Advanced RAG Applications</a>. 课程笔记参考<a target="_blank" rel="noopener" href="https://medium.com/@LakshmiNarayana_U/frameworks-in-focus-building-and-evaluating-advanced-rag-with-trulens-and-llamaindex-insights-19db95ffcf6e">Frameworks
in Focus: ‘Building and Evaluating Advanced RAG’ with TruLens and
LlamaIndex Insights</a></p>
<h1 id="构建-construction">构建 Construction</h1>
<p>简单的RAG构建的资料太多太多了，最简易的RAG构建可以参考<a target="_blank" rel="noopener" href="https://huggingface.co/learn/cookbook/rag_zephyr_langchain">Simple
RAG for GitHub issues using Hugging Face Zephyr and LangChain</a>.</p>
<p>RAG中两个最核心的模块： Retrieval 和 Generation
(Read)，内部都有很多可以enhance的地方。这里列举一些可以查阅的资料，内整理了一些对于RAG的enhancement的点：</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.19473">Retrieval-Augmented
Generation for AI-Generated Content: A Survey</a> Chapter 3</li>
<li><a target="_blank" rel="noopener" href="https://medium.com/aimonks/retrieval-augmented-generation-rag-enhancement-for-llm-based-prediction-relp-59645a67dcdb">Retrieval
Augmented Generation (RAG) Enhancement for LLM-based Prediction —
RELP</a></li>
<li><a target="_blank" rel="noopener" href="https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6">Advanced
RAG Techniques: an Illustrated Overview</a> 这个博客整理的挺全面的</li>
<li><a target="_blank" rel="noopener" href="https://github.com/huggingface/cookbook/blob/main/notebooks/en/advanced_rag.ipynb">Advanced
RAG on Hugging Face documentation using LangChain</a></li>
</ul>
<figure>
<img src="https://camo.githubusercontent.com/738a616ea3fc69c8c0a0f26deae64b0f88e6e1d430db5c0454f1127b362b2e98/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f68756767696e67666163652f636f6f6b626f6f6b2d696d616765732f7265736f6c76652f6d61696e2f5241475f776f726b666c6f772e706e67" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>上面这张图来自于langchain的cookbook，蓝色部分是作者认为<code>all possibilities for system enhancement</code>。我这里只对一些我关注的技术做整理和探索。</p>
<h2 id="chunking">Chunking</h2>
<p>有一堆文档，如何将这些文档切分成“完美的”chunk。</p>
<p>我比较关注的是对PDF格式的文件的处理，比较有参考价值的资料：<a target="_blank" rel="noopener" href="https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb">5
Levels Of Text
Splitting</a>，内介绍的level1和level2的切分方式都是现在比较常见的。</p>
<p>对于PDF中的图片，也有<a target="_blank" rel="noopener" href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Multi_modal_RAG.ipynb">博客</a>进行了探索</p>
<p><img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/下载-17152333305102.png"></p>
<p>对于PDF中table的处理，一个可行的方式是用Unstuctured这个library抽取出HTML格式的table，然后用LLM将其summary一下，那么对于retriveal的时候，是将summary的vector和query的vector去进行比对的，如果match上了，就会把原生的HTML的表格表示输入给LLM去生成最终的答案。做法详见<a target="_blank" rel="noopener" href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb">Semi-structured
RAG</a>。<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/下载%20(1)-17152442960173.png"></p>
<p>对于PDF中图片的处理，也是对image先用LLM总结描述一下。其实<a target="_blank" rel="noopener" href="https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb">5
Levels Of Text
Splitting</a>里面介绍的方法都是可以的，但我觉得实操会有一定的难度。因为PDF中的images是会被单独放到一个文件夹里的，前后夹的文本其实是丢失了，这样不可避免的就会丢失一定的语义信息。表格其实还好，但是很多时候贴了一张图片之后，后面的文字基本上是相关联的。这时候需要把图片的信息和后面的文字结合起来就需要知道每一个图片所在pdf的位置，我目前看到的资料还没有很好的解决这个问题。</p>
<h1 id="评估-evaluation">评估 Evaluation</h1>
<p>该课程建议从三个维度来评测一个RAG Application的好坏：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20231205105234142.png" alt="image-20231205105234142">
<figcaption aria-hidden="true">image-20231205105234142</figcaption>
</figure>
<ul>
<li>问题和回答的相关性</li>
<li>根据问题抽取出来的context和问题的相关性</li>
<li>回答和context的相关性</li>
</ul>
<p>该课程主要目的是宣传自己的框架Trulens(目前该框架在github有1.8k
star，热度不咋高)，如果想了解Evaluation的全景知识建议看一下<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.10997">Retrieval-Augmented Generation
for Large Language Models: A Survey</a></p>
<p>上面的review中很重要的两个总结：</p>
<ol type="1">
<li>上面所说的三个quality
score如何计算？可以看到仍然是我们熟悉的一些metrics</li>
</ol>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240228142327120.png" alt="image-20240228142327120">
<figcaption aria-hidden="true">image-20240228142327120</figcaption>
</figure>
<ol start="2" type="1">
<li>现有的可用评估框架有哪些？</li>
</ol>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240508133011839.png" alt="image-20240508133011839">
<figcaption aria-hidden="true">image-20240508133011839</figcaption>
</figure>
<p>我们上面提到的课程里使用的就是该表格中列出的TruLens.
上面这张表格总结的还不是特别全面，而且没有datasets的整理，24年新出的文章<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.17043">CRUD-RAG: A Comprehensive Chinese
Benchmark for Retrieval-Augmented Generation of Large Language
Models</a> 中对这部分做了整理：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240228152011016.png" alt="image-20240228152011016">
<figcaption aria-hidden="true">image-20240228152011016</figcaption>
</figure>
<blockquote>
<p>这里做一下update，在作者写这篇文章时，综述<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.10997">Retrieval-Augmented Generation
for Large Language Models: A
Survey</a>还未对评估的数据集做整理，但最近一期3月的论文更新中已经有了这部分的内容。主要增添了对于每一个评测任务的数据集的整理。</p>
</blockquote>
<p>其中[7]就是RGB，它数据的生成是利用一系列收集到的news
report，然后利用LLM来基于这些report生成relevant
events,questions和answers。[38]是ARES，利用flan-t5来生成的一系列合成query和answer。其中比较重要的一列是是否有金标准，也就是上图中的倒数第二列。
13,12以及38分别是TruLens-Eval，RAGAS和ARES，这三个是不需要金标准的，不过代价是需要用到Chatgpt来做自动评估呀，这些可都是白花花的银子。使用Trulens-Eval都是需要配置openai的API的。</p>
<h2 id="langchain-benchmark">LangChain Benchmark</h2>
<p>对于想要快速去搭建一个评估RAG的框架的人来说，最好是有现成的可以直接用的评估体系，省去自己搜集数据以及编写各种计算metrics的麻烦。langchain提供了这么一个benchmark包，<a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langchain-benchmarks/notebooks/retrieval/comparing_techniques.html">介绍传送门</a>,截止到24年3月，该库已经包含了三个开源数据集，两个是上面介绍的python文档和pdf的QA问答数据集，还有一个是正在开发中的基于PPT的问答数据集：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240314142718109.png" alt="image-20240314142718109">
<figcaption aria-hidden="true">image-20240314142718109</figcaption>
</figure>
<p>这份langchain官方教程里用了好多新的tool，其中一个就是Smith,
在notebook中clone的所有数据集都可以在这个平台上看到，有点像console。LangChain
Docs Q&amp;A的数据长这样：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;question&quot;</span>: <span class="string">&quot;How can I parallelize calls in LangChain?&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;answer&quot;</span>: <span class="string">&quot;To make parallel calls from a LangChain object, use the &#x27;batch()&#x27; (or asynchronous &#x27;abatch()&#x27;) method. You can also use a `RunnableParallel` object.&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>langchain-benchmark总体而言还处于初期，对于retrival的task也只有三个数据集做支撑，定制化的程度不是特别高。具体可以参考<a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langchain-benchmarks/">langchain-benchmark官方教程</a>。</p>
<p>今天在看huggingface官网文档的时候又看到官方出了新的evaluation的<a target="_blank" rel="noopener" href="https://github.com/huggingface/cookbook/blob/main/notebooks/en/rag_evaluation.ipynb">guidebook</a>，这份代码里写的相当详细，不再是一个普通的RAG评估流程，还介绍了评估数据集的生成方式，最重要的是还做了数据集的filtering，这份教程对于企业内部生成自己的评估数据集是有很大的参考价值的。</p>
<h2 id="crud">CRUD</h2>
<p>现在我们花点篇幅来详细说一下CRUD这个中文评估benchmark。作者的出发点在于评估一个RAG的应用，要区别于评估一个LLM模型，下面这句话是作者从四个维度来评估RAG的出发点：</p>
<blockquote>
<p>Lewis et al. [25] argue that the core of RAG systems is their
interactive way of combining LLMs with external knowledge sources</p>
</blockquote>
<p>RAG和LLM的交互方式，也就是RAG帮助LLM做了哪些东西让LLM能更好的回答问题，作者觉得是这四个方面：Create，Read，Update和Delete.</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240314135617343.png" alt="image-20240314135617343">
<figcaption aria-hidden="true">image-20240314135617343</figcaption>
</figure>
<p>Read很常见，RAG会从知识库中搜集更多的信息来供LLM回答问题，Update主要是为了解决LLM无法回答具有时效性的问题，或者当时训练模型时没有加入的信息，Delete这点其实在我看来有点牵强。Read和Update这两点确实是评估一个RAG很关键的方面。</p>
<p>做RAG的评估，最重要的两点就是：</p>
<ol type="1">
<li>数据集的准备，作者打算从上面四个维度去衡量一个RAG的好坏，那就得准备相应的数据集，这部分的工作是我们平时自己做测评的重点</li>
<li>测评metrics的选择，除了我们熟知的BLEU，ROUGE，还有bert判分。其中还有作者基于QuestEval创造的RAGQuestEval评分。这个metrics还挺有意思的。这里放在这里详细介绍下：</li>
</ol>
<p>首先基于ground truth
sentence生成一系列的问题，生成问题的prompt设计是这样的：</p>
<blockquote>
<p>你是一位新闻编辑，现在，你被提供了一篇新闻，请先从新闻中抽取出你认为重要的所有关键信息（通常为一个关键词，包含文章中的所有实体和名词性短语），然后，根据关键信息设计几个问题，考验大家能否正确回答问题。用json的形式返回答案。以下是个例子。</p>
<p>新闻：2014年，全国新增并网光伏发电容量1060万千瓦，约占全球新增容量的四分之一。其中，全国新增光伏电站855万千瓦，分布式205万千瓦。据统计，2014年中国光伏发电量达到了250亿千瓦时，同比增⻓超过
200%。</p>
<p>{json_response}</p>
<p>现在你需要为这篇新闻设计问题，尽量涵盖大多数关键信息，请尽量让答案可以用两三个词回答，答案不能太长，key_info包含文章中的所有实体和名词性短语，question与key_info一一对应，数量一致，输出用json的格式：</p>
<p>{news}</p>
</blockquote>
<p>注意这里先让LLM抽取文章中的所有实体和名词性短语作为关键信息，question是根据这些关键信息生成的。问题生成完之后分别用reference
sentence和ground truth
sentence作为context，去让LLM回答上面生成的问题。如果遇到无法回答的问题就让LLM答“无法回答”.
最后一步针对回答的结果计算precision 和 recall。</p>
<p>该文章作者在数据的处理方面，选择去爬取网上最新的news，然后用这8000个新闻建立了三个task的数据集：open-domain
multi-document
summarization(考察RAG的delete能力)，text-continuation(考察RAG的Generation能力)，question-answering(read能力)和hallucination
modification(考察RAG的Update能力)。</p>
<p>其实仔细看上面review中的总结，CRUD这篇文章里提到的应该考察RAG的“哪些能力”还是不够全面的，而且我个人认为CRUD里面仅仅是以end-to-end的方式计算generated
anwser和gound
truth之间的差距也是不太可取的，它没有涉及到RAG里面很重要的一个环节：retrieval。更全面的方式应该是计算三种quality
scores（具体参考review的介绍）：</p>
<ul>
<li>context relevance： query 和 context 的关系</li>
<li>faithfulness(groundness)：answer 和 context 的关系</li>
</ul>
<p><strong>This measures the factual consistency of the generated answer
againest the given context</strong></p>
<p>主要用于检测LLM的幻觉。这里<a target="_blank" rel="noopener" href="https://aws.amazon.com/cn/blogs/china/automated-rag-project-assessment-testing-using-trulens/">博客</a>
对trulens的计算方式做了详细介绍，注意它里面的prompt的设计。Ragas框架对于faithfulness的计算查看<a target="_blank" rel="noopener" href="https://docs.ragas.io/en/latest/concepts/metrics/faithfulness.html">Faithfulness</a>，也是用chatgpt来把answer中的statement拆开然后分别去与召回的context做对照，可以查看ragas框架计算faithfulness的<a target="_blank" rel="noopener" href="https://github.com/explodinggradients/ragas/blob/c5eac536000fcbc3d9fb9a741dfe10163cdc3cce/src/ragas/metrics/_faithfulness.py#L108">代码</a>.</p>
<p><em>My spicy comment:
trulens和ragas两者还挺类似的，就是ragas除了计算faithfulness，还多了好几个metrics，如context
precision, context recall, context entity recall。其实就是把context
relevance这个metric拆分地更细了。不仅如此，ragas把answer
relevance也拆的更细了，它包含了answer correctness, answer
relevance和answer similarity.
相比较而言，ragas在笔者写这篇文章的时候，star数是要比trulens多的，前者4.8k，后者1.8k。而且issues明显要多于trulens，直觉上看应该是ragas用的人比较多。</em></p>
<p>在整理这部分metrics的时候，也搜了一下大家都在用什么样的框架来评估自己的RAG，看到reddit上也有人有这样的疑问<a target="_blank" rel="noopener" href="https://www.reddit.com/r/LangChain/comments/1bijg75/why_is_everyone_using_ragas_for_rag_evaluation/">Why
is everyone using RAGAS for RAG evaluation? For me it looks very
unreliable</a>,
我觉得其中一个回答比较贴合当下对于RAG评估的一个现状：</p>
<blockquote>
<p>There is no proper techincal report, paper, or any experiment that
ragas metric is useful and effective to evaluate LLM performance. That's
why I do not choose ragas at my <a target="_blank" rel="noopener" href="https://github.com/Marker-Inc-Korea/AutoRAG">AutoRAG</a> tool. I
use metrics like G-eval or sem score that has proper experiment and
result that shows such metrics are effective. I think evaluating LLM
generation performance is not easy problem and do not have silver
bullet. All we can do is doing lots of experiment and mixing various
metrics for reliable result. In this term, ragas can be a opiton... (If
i am missing ragas experiment or benchmark result, let me know)</p>
<p>https://www.reddit.com/r/LangChain/comments/1bijg75/comment/kvoj1q8/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button</p>
</blockquote>
<ul>
<li>answer relevance： answer 和 query 的关系</li>
</ul>
<p>至于计算出上面这三个方面的数值，有多种方式。有用LLM的，比如Trulens就是用的chatgpt，也可以用claude，参考见<a target="_blank" rel="noopener" href="https://aws.amazon.com/cn/blogs/china/automated-rag-project-assessment-testing-using-trulens/">基于大语言模型知识问答应用落地实践
– 使用 TruLens 做自动化 RAG
项目评估测试</a>。也有直接计算相似度的，比如我们熟悉的bert
score，rouge-L。review在这里也进行了整理：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240508132450374.png" alt="image-20240508132450374">
<figcaption aria-hidden="true">image-20240508132450374</figcaption>
</figure>
<h2 id="customization模式">Customization模式</h2>
<p>其实在具体的业务场景下，如果已经搭建了一套RAG系统，如何来评估这个RAG系统的好坏，更合理的方式还是需要用自己的数据来测评，如果只是用一些公开的benchmark，如上面提到的langchain
benchmark，还是CRUD提出的以新闻为数据的benchmark，都有一点不那么让人信服，毕竟你费劲巴拉地搭建一个RAG的chatbot，还是要在自己的具体的业务场景表现好，客户才会买账吧。</p>
<p>但更多情况下，业务场景下往往是缺少金标数据集的，这时候就需要去针对自己的业务场景去生成一些“合成”数据集。我们可能基于的就是一堆的业务文档，这些文档有的是PDF，有的可能是word，也会有PPT，如果根据这些文档去生成自己的评测数据集，这样基于这个评测数据集我们再去“调整”我们RAG中的各个能影响RAG
performance的环节：embedding模型选择哪个，LLM选择哪个？chunking应该如何优化等等？加了rewrite和rerank等techniques之后有没有让RAG的效果变好，这里的变好仅仅是指在我们自己的业务数据上变好，而不是在其他开源的benchmark上，这样才具有一定的说服力。</p>
<p>参考博客<a target="_blank" rel="noopener" href="https://huggingface.co/learn/cookbook/rag_evaluation">RAG
Evaluation</a>, 文章介绍了一种根据documents生成synthetic evaluation
dataset的办法，里面还加了一些tricks：如何用一个critique
agents去筛选QA。不过该篇文章evaluation环节仅仅计算了answer和query的关系（faithfulness），它给出的理由是：</p>
<blockquote>
<p>Out of <a target="_blank" rel="noopener" href="https://docs.ragas.io/en/latest/concepts/metrics/index.html">the
different RAG evaluation metrics</a>, we choose to focus only on
faithfulness since it the best end-to-end metric of our system’s
performance.</p>
</blockquote>
<h1 id="rag-中的painpoints">RAG 中的PainPoints</h1>
<p>参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=EBpT_cscTis">LLamaindex出品的视频</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/12-rag-pain-points-and-proposed-solutions-43709939a28c#cea4">12
RAG Pain Points and Proposed Solutions</a></li>
</ul>
<p>上面视频对应的博客</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.05856">Seven Failure Points When
Engineering a Retrieval Augmented Generation System</a></li>
</ul>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240725140423246-17218874651211.png" alt="image-20240725140423246">
<figcaption aria-hidden="true">image-20240725140423246</figcaption>
</figure>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240725140443402.png" alt="image-20240725140443402">
<figcaption aria-hidden="true">image-20240725140443402</figcaption>
</figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YAN</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Symbols count total: </span>
    <span title="Symbols count total">203k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">3:04</span>
  </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
