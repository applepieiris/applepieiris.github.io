<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=consolas:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.8.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="Be smart! Keep Learning">
<meta property="og:type" content="website">
<meta property="og:title" content="YAN&#39;s Blog">
<meta property="og:url" content="http://example.com/page/3/index.html">
<meta property="og:site_name" content="YAN&#39;s Blog">
<meta property="og:description" content="Be smart! Keep Learning">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="YAN">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/3/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>YAN's Blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">YAN's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="YAN"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">YAN</p>
  <div class="site-description" itemprop="description">Be smart! Keep Learning</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">49</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/applepieiris" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/" class="post-title-link" itemprop="url">构建和评估RAG应用</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-12-05 09:56:05" itemprop="dateCreated datePublished" datetime="2023-12-05T09:56:05+08:00">2023-12-05</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2024-08-30 09:38:27" itemprop="dateModified" datetime="2024-08-30T09:38:27+08:00">2024-08-30</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>8.3k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>8 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>最近吴恩达出了一个小课程，传送门: <a target="_blank" rel="noopener" href="https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/">Building
and Evaluating Advanced RAG Applications</a>。<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1494y1E7H9?p=3&amp;vd_source=f998f640fc8575504e3e97753bf817f4">B站</a>也有人搬运了，有中英文字幕。最近也正好在做RAG相关的项目，看到这个课程里有一些新的东西，权当在这篇博客里总结记录。</p>
<p>另外还推荐阅读一篇综述<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.05876">Trends in Integration of
Knowledge and Large Language Models: A Survey and Taxonomy of Methods,
Benchmarks, and Applications</a>, 该综述的第三章详细介绍了retrieval
augmentation的方法。我这篇博客会首先理顺一些理论，然后再介绍吴恩达课程里的知识（个人认为吴大佬出的关于LLM的一系列shot
course可食用性不够高，比如上面说的这个RAG相关的课怎么看都觉得是在推广LlamaIndex这个框架，对于原理一句话带过，很多细节不清楚）。</p>
<p>3.2节提到的两个工作值得注意：</p>
<ol type="1">
<li>Query2doc</li>
</ol>
<blockquote>
<p>Query2doc prompts the LLMs to generate a pseudo-document by employing
a few-shot prompting paradigm. Subsequently, the original query is
expanded by incorporating the pseudo-document. The retriever module uses
this new query to retrieve a list of relevant documents.</p>
</blockquote>
<ol start="2" type="1">
<li>Rewrite-Retrieve-Read</li>
</ol>
<blockquote>
<p>Different with Query2doc,they adopt a trainable language model to
perform the rewriting step</p>
</blockquote>
<p>在抽取的context的使用上，我们一般的认知是加入到prompt里，告诉LLM根据这个context回答某个query，这篇综述在3.2节还概括介绍了另外两种使用knowledge的方式：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20231205145030752.png" alt="image-20231205145030752">
<figcaption aria-hidden="true">image-20231205145030752</figcaption>
</figure>
<p>我个人认为第二种方式实操性差一点，第三种和第一种应该是大家会普遍采取的方式，第二种需要更多精细的prompt设计。</p>
<hr>
<p>以下为课程相关的 ，传送门: <a target="_blank" rel="noopener" href="https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/">Building
and Evaluating Advanced RAG Applications</a>. 课程笔记参考<a target="_blank" rel="noopener" href="https://medium.com/@LakshmiNarayana_U/frameworks-in-focus-building-and-evaluating-advanced-rag-with-trulens-and-llamaindex-insights-19db95ffcf6e">Frameworks
in Focus: ‘Building and Evaluating Advanced RAG’ with TruLens and
LlamaIndex Insights</a></p>
<h1 id="构建-construction">构建 Construction</h1>
<p>简单的RAG构建的资料太多太多了，最简易的RAG构建可以参考<a target="_blank" rel="noopener" href="https://huggingface.co/learn/cookbook/rag_zephyr_langchain">Simple
RAG for GitHub issues using Hugging Face Zephyr and LangChain</a>.</p>
<p>RAG中两个最核心的模块： Retrieval 和 Generation
(Read)，内部都有很多可以enhance的地方。这里列举一些可以查阅的资料，内整理了一些对于RAG的enhancement的点：</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.19473">Retrieval-Augmented
Generation for AI-Generated Content: A Survey</a> Chapter 3</li>
<li><a target="_blank" rel="noopener" href="https://medium.com/aimonks/retrieval-augmented-generation-rag-enhancement-for-llm-based-prediction-relp-59645a67dcdb">Retrieval
Augmented Generation (RAG) Enhancement for LLM-based Prediction —
RELP</a></li>
<li><a target="_blank" rel="noopener" href="https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6">Advanced
RAG Techniques: an Illustrated Overview</a> 这个博客整理的挺全面的</li>
<li><a target="_blank" rel="noopener" href="https://github.com/huggingface/cookbook/blob/main/notebooks/en/advanced_rag.ipynb">Advanced
RAG on Hugging Face documentation using LangChain</a></li>
</ul>
<figure>
<img src="https://camo.githubusercontent.com/738a616ea3fc69c8c0a0f26deae64b0f88e6e1d430db5c0454f1127b362b2e98/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f68756767696e67666163652f636f6f6b626f6f6b2d696d616765732f7265736f6c76652f6d61696e2f5241475f776f726b666c6f772e706e67" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>上面这张图来自于langchain的cookbook，蓝色部分是作者认为<code>all possibilities for system enhancement</code>。我这里只对一些我关注的技术做整理和探索。</p>
<h2 id="chunking">Chunking</h2>
<p>有一堆文档，如何将这些文档切分成“完美的”chunk。</p>
<p>我比较关注的是对PDF格式的文件的处理，比较有参考价值的资料：<a target="_blank" rel="noopener" href="https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb">5
Levels Of Text
Splitting</a>，内介绍的level1和level2的切分方式都是现在比较常见的。</p>
<p>对于PDF中的图片，也有<a target="_blank" rel="noopener" href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Multi_modal_RAG.ipynb">博客</a>进行了探索</p>
<p><img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/下载-17152333305102.png"></p>
<p>对于PDF中table的处理，一个可行的方式是用Unstuctured这个library抽取出HTML格式的table，然后用LLM将其summary一下，那么对于retriveal的时候，是将summary的vector和query的vector去进行比对的，如果match上了，就会把原生的HTML的表格表示输入给LLM去生成最终的答案。做法详见<a target="_blank" rel="noopener" href="https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb">Semi-structured
RAG</a>。<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/下载%20(1)-17152442960173.png"></p>
<p>对于PDF中图片的处理，也是对image先用LLM总结描述一下。其实<a target="_blank" rel="noopener" href="https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb">5
Levels Of Text
Splitting</a>里面介绍的方法都是可以的，但我觉得实操会有一定的难度。因为PDF中的images是会被单独放到一个文件夹里的，前后夹的文本其实是丢失了，这样不可避免的就会丢失一定的语义信息。表格其实还好，但是很多时候贴了一张图片之后，后面的文字基本上是相关联的。这时候需要把图片的信息和后面的文字结合起来就需要知道每一个图片所在pdf的位置，我目前看到的资料还没有很好的解决这个问题。</p>
<h1 id="评估-evaluation">评估 Evaluation</h1>
<p>该课程建议从三个维度来评测一个RAG Application的好坏：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20231205105234142.png" alt="image-20231205105234142">
<figcaption aria-hidden="true">image-20231205105234142</figcaption>
</figure>
<ul>
<li>问题和回答的相关性</li>
<li>根据问题抽取出来的context和问题的相关性</li>
<li>回答和context的相关性</li>
</ul>
<p>该课程主要目的是宣传自己的框架Trulens(目前该框架在github有1.8k
star，热度不咋高)，如果想了解Evaluation的全景知识建议看一下<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.10997">Retrieval-Augmented Generation
for Large Language Models: A Survey</a></p>
<p>上面的review中很重要的两个总结：</p>
<ol type="1">
<li>上面所说的三个quality
score如何计算？可以看到仍然是我们熟悉的一些metrics</li>
</ol>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240228142327120.png" alt="image-20240228142327120">
<figcaption aria-hidden="true">image-20240228142327120</figcaption>
</figure>
<ol start="2" type="1">
<li>现有的可用评估框架有哪些？</li>
</ol>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240508133011839.png" alt="image-20240508133011839">
<figcaption aria-hidden="true">image-20240508133011839</figcaption>
</figure>
<p>我们上面提到的课程里使用的就是该表格中列出的TruLens.
上面这张表格总结的还不是特别全面，而且没有datasets的整理，24年新出的文章<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.17043">CRUD-RAG: A Comprehensive Chinese
Benchmark for Retrieval-Augmented Generation of Large Language
Models</a> 中对这部分做了整理：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240228152011016.png" alt="image-20240228152011016">
<figcaption aria-hidden="true">image-20240228152011016</figcaption>
</figure>
<blockquote>
<p>这里做一下update，在作者写这篇文章时，综述<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.10997">Retrieval-Augmented Generation
for Large Language Models: A
Survey</a>还未对评估的数据集做整理，但最近一期3月的论文更新中已经有了这部分的内容。主要增添了对于每一个评测任务的数据集的整理。</p>
</blockquote>
<p>其中[7]就是RGB，它数据的生成是利用一系列收集到的news
report，然后利用LLM来基于这些report生成relevant
events,questions和answers。[38]是ARES，利用flan-t5来生成的一系列合成query和answer。其中比较重要的一列是是否有金标准，也就是上图中的倒数第二列。
13,12以及38分别是TruLens-Eval，RAGAS和ARES，这三个是不需要金标准的，不过代价是需要用到Chatgpt来做自动评估呀，这些可都是白花花的银子。使用Trulens-Eval都是需要配置openai的API的。</p>
<h2 id="langchain-benchmark">LangChain Benchmark</h2>
<p>对于想要快速去搭建一个评估RAG的框架的人来说，最好是有现成的可以直接用的评估体系，省去自己搜集数据以及编写各种计算metrics的麻烦。langchain提供了这么一个benchmark包，<a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langchain-benchmarks/notebooks/retrieval/comparing_techniques.html">介绍传送门</a>,截止到24年3月，该库已经包含了三个开源数据集，两个是上面介绍的python文档和pdf的QA问答数据集，还有一个是正在开发中的基于PPT的问答数据集：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240314142718109.png" alt="image-20240314142718109">
<figcaption aria-hidden="true">image-20240314142718109</figcaption>
</figure>
<p>这份langchain官方教程里用了好多新的tool，其中一个就是Smith,
在notebook中clone的所有数据集都可以在这个平台上看到，有点像console。LangChain
Docs Q&amp;A的数据长这样：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;question&quot;</span>: <span class="string">&quot;How can I parallelize calls in LangChain?&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;answer&quot;</span>: <span class="string">&quot;To make parallel calls from a LangChain object, use the &#x27;batch()&#x27; (or asynchronous &#x27;abatch()&#x27;) method. You can also use a `RunnableParallel` object.&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>langchain-benchmark总体而言还处于初期，对于retrival的task也只有三个数据集做支撑，定制化的程度不是特别高。具体可以参考<a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langchain-benchmarks/">langchain-benchmark官方教程</a>。</p>
<p>今天在看huggingface官网文档的时候又看到官方出了新的evaluation的<a target="_blank" rel="noopener" href="https://github.com/huggingface/cookbook/blob/main/notebooks/en/rag_evaluation.ipynb">guidebook</a>，这份代码里写的相当详细，不再是一个普通的RAG评估流程，还介绍了评估数据集的生成方式，最重要的是还做了数据集的filtering，这份教程对于企业内部生成自己的评估数据集是有很大的参考价值的。</p>
<h2 id="crud">CRUD</h2>
<p>现在我们花点篇幅来详细说一下CRUD这个中文评估benchmark。作者的出发点在于评估一个RAG的应用，要区别于评估一个LLM模型，下面这句话是作者从四个维度来评估RAG的出发点：</p>
<blockquote>
<p>Lewis et al. [25] argue that the core of RAG systems is their
interactive way of combining LLMs with external knowledge sources</p>
</blockquote>
<p>RAG和LLM的交互方式，也就是RAG帮助LLM做了哪些东西让LLM能更好的回答问题，作者觉得是这四个方面：Create，Read，Update和Delete.</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240314135617343.png" alt="image-20240314135617343">
<figcaption aria-hidden="true">image-20240314135617343</figcaption>
</figure>
<p>Read很常见，RAG会从知识库中搜集更多的信息来供LLM回答问题，Update主要是为了解决LLM无法回答具有时效性的问题，或者当时训练模型时没有加入的信息，Delete这点其实在我看来有点牵强。Read和Update这两点确实是评估一个RAG很关键的方面。</p>
<p>做RAG的评估，最重要的两点就是：</p>
<ol type="1">
<li>数据集的准备，作者打算从上面四个维度去衡量一个RAG的好坏，那就得准备相应的数据集，这部分的工作是我们平时自己做测评的重点</li>
<li>测评metrics的选择，除了我们熟知的BLEU，ROUGE，还有bert判分。其中还有作者基于QuestEval创造的RAGQuestEval评分。这个metrics还挺有意思的。这里放在这里详细介绍下：</li>
</ol>
<p>首先基于ground truth
sentence生成一系列的问题，生成问题的prompt设计是这样的：</p>
<blockquote>
<p>你是一位新闻编辑，现在，你被提供了一篇新闻，请先从新闻中抽取出你认为重要的所有关键信息（通常为一个关键词，包含文章中的所有实体和名词性短语），然后，根据关键信息设计几个问题，考验大家能否正确回答问题。用json的形式返回答案。以下是个例子。</p>
<p>新闻：2014年，全国新增并网光伏发电容量1060万千瓦，约占全球新增容量的四分之一。其中，全国新增光伏电站855万千瓦，分布式205万千瓦。据统计，2014年中国光伏发电量达到了250亿千瓦时，同比增⻓超过
200%。</p>
<p>{json_response}</p>
<p>现在你需要为这篇新闻设计问题，尽量涵盖大多数关键信息，请尽量让答案可以用两三个词回答，答案不能太长，key_info包含文章中的所有实体和名词性短语，question与key_info一一对应，数量一致，输出用json的格式：</p>
<p>{news}</p>
</blockquote>
<p>注意这里先让LLM抽取文章中的所有实体和名词性短语作为关键信息，question是根据这些关键信息生成的。问题生成完之后分别用reference
sentence和ground truth
sentence作为context，去让LLM回答上面生成的问题。如果遇到无法回答的问题就让LLM答“无法回答”.
最后一步针对回答的结果计算precision 和 recall。</p>
<p>该文章作者在数据的处理方面，选择去爬取网上最新的news，然后用这8000个新闻建立了三个task的数据集：open-domain
multi-document
summarization(考察RAG的delete能力)，text-continuation(考察RAG的Generation能力)，question-answering(read能力)和hallucination
modification(考察RAG的Update能力)。</p>
<p>其实仔细看上面review中的总结，CRUD这篇文章里提到的应该考察RAG的“哪些能力”还是不够全面的，而且我个人认为CRUD里面仅仅是以end-to-end的方式计算generated
anwser和gound
truth之间的差距也是不太可取的，它没有涉及到RAG里面很重要的一个环节：retrieval。更全面的方式应该是计算三种quality
scores（具体参考review的介绍）：</p>
<ul>
<li>context relevance： query 和 context 的关系</li>
<li>faithfulness(groundness)：answer 和 context 的关系</li>
</ul>
<p><strong>This measures the factual consistency of the generated answer
againest the given context</strong></p>
<p>主要用于检测LLM的幻觉。这里<a target="_blank" rel="noopener" href="https://aws.amazon.com/cn/blogs/china/automated-rag-project-assessment-testing-using-trulens/">博客</a>
对trulens的计算方式做了详细介绍，注意它里面的prompt的设计。Ragas框架对于faithfulness的计算查看<a target="_blank" rel="noopener" href="https://docs.ragas.io/en/latest/concepts/metrics/faithfulness.html">Faithfulness</a>，也是用chatgpt来把answer中的statement拆开然后分别去与召回的context做对照，可以查看ragas框架计算faithfulness的<a target="_blank" rel="noopener" href="https://github.com/explodinggradients/ragas/blob/c5eac536000fcbc3d9fb9a741dfe10163cdc3cce/src/ragas/metrics/_faithfulness.py#L108">代码</a>.</p>
<p><em>My spicy comment:
trulens和ragas两者还挺类似的，就是ragas除了计算faithfulness，还多了好几个metrics，如context
precision, context recall, context entity recall。其实就是把context
relevance这个metric拆分地更细了。不仅如此，ragas把answer
relevance也拆的更细了，它包含了answer correctness, answer
relevance和answer similarity.
相比较而言，ragas在笔者写这篇文章的时候，star数是要比trulens多的，前者4.8k，后者1.8k。而且issues明显要多于trulens，直觉上看应该是ragas用的人比较多。</em></p>
<p>在整理这部分metrics的时候，也搜了一下大家都在用什么样的框架来评估自己的RAG，看到reddit上也有人有这样的疑问<a target="_blank" rel="noopener" href="https://www.reddit.com/r/LangChain/comments/1bijg75/why_is_everyone_using_ragas_for_rag_evaluation/">Why
is everyone using RAGAS for RAG evaluation? For me it looks very
unreliable</a>,
我觉得其中一个回答比较贴合当下对于RAG评估的一个现状：</p>
<blockquote>
<p>There is no proper techincal report, paper, or any experiment that
ragas metric is useful and effective to evaluate LLM performance. That's
why I do not choose ragas at my <a target="_blank" rel="noopener" href="https://github.com/Marker-Inc-Korea/AutoRAG">AutoRAG</a> tool. I
use metrics like G-eval or sem score that has proper experiment and
result that shows such metrics are effective. I think evaluating LLM
generation performance is not easy problem and do not have silver
bullet. All we can do is doing lots of experiment and mixing various
metrics for reliable result. In this term, ragas can be a opiton... (If
i am missing ragas experiment or benchmark result, let me know)</p>
<p>https://www.reddit.com/r/LangChain/comments/1bijg75/comment/kvoj1q8/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button</p>
</blockquote>
<ul>
<li>answer relevance： answer 和 query 的关系</li>
</ul>
<p>至于计算出上面这三个方面的数值，有多种方式。有用LLM的，比如Trulens就是用的chatgpt，也可以用claude，参考见<a target="_blank" rel="noopener" href="https://aws.amazon.com/cn/blogs/china/automated-rag-project-assessment-testing-using-trulens/">基于大语言模型知识问答应用落地实践
– 使用 TruLens 做自动化 RAG
项目评估测试</a>。也有直接计算相似度的，比如我们熟悉的bert
score，rouge-L。review在这里也进行了整理：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240508132450374.png" alt="image-20240508132450374">
<figcaption aria-hidden="true">image-20240508132450374</figcaption>
</figure>
<h2 id="customization模式">Customization模式</h2>
<p>其实在具体的业务场景下，如果已经搭建了一套RAG系统，如何来评估这个RAG系统的好坏，更合理的方式还是需要用自己的数据来测评，如果只是用一些公开的benchmark，如上面提到的langchain
benchmark，还是CRUD提出的以新闻为数据的benchmark，都有一点不那么让人信服，毕竟你费劲巴拉地搭建一个RAG的chatbot，还是要在自己的具体的业务场景表现好，客户才会买账吧。</p>
<p>但更多情况下，业务场景下往往是缺少金标数据集的，这时候就需要去针对自己的业务场景去生成一些“合成”数据集。我们可能基于的就是一堆的业务文档，这些文档有的是PDF，有的可能是word，也会有PPT，如果根据这些文档去生成自己的评测数据集，这样基于这个评测数据集我们再去“调整”我们RAG中的各个能影响RAG
performance的环节：embedding模型选择哪个，LLM选择哪个？chunking应该如何优化等等？加了rewrite和rerank等techniques之后有没有让RAG的效果变好，这里的变好仅仅是指在我们自己的业务数据上变好，而不是在其他开源的benchmark上，这样才具有一定的说服力。</p>
<p>参考博客<a target="_blank" rel="noopener" href="https://huggingface.co/learn/cookbook/rag_evaluation">RAG
Evaluation</a>, 文章介绍了一种根据documents生成synthetic evaluation
dataset的办法，里面还加了一些tricks：如何用一个critique
agents去筛选QA。不过该篇文章evaluation环节仅仅计算了answer和query的关系（faithfulness），它给出的理由是：</p>
<blockquote>
<p>Out of <a target="_blank" rel="noopener" href="https://docs.ragas.io/en/latest/concepts/metrics/index.html">the
different RAG evaluation metrics</a>, we choose to focus only on
faithfulness since it the best end-to-end metric of our system’s
performance.</p>
</blockquote>
<h1 id="rag-中的painpoints">RAG 中的PainPoints</h1>
<p>参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=EBpT_cscTis">LLamaindex出品的视频</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/12-rag-pain-points-and-proposed-solutions-43709939a28c#cea4">12
RAG Pain Points and Proposed Solutions</a></li>
</ul>
<p>上面视频对应的博客</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.05856">Seven Failure Points When
Engineering a Retrieval Augmented Generation System</a></li>
</ul>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240725140423246-17218874651211.png" alt="image-20240725140423246">
<figcaption aria-hidden="true">image-20240725140423246</figcaption>
</figure>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20240725140443402.png" alt="image-20240725140443402">
<figcaption aria-hidden="true">image-20240725140443402</figcaption>
</figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/09/19/RLHF%E7%BB%86%E8%8A%82%E8%A7%A3%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/19/RLHF%E7%BB%86%E8%8A%82%E8%A7%A3%E6%9E%90/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-19 09:21:55" itemprop="dateCreated datePublished" datetime="2023-09-19T09:21:55+08:00">2023-09-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2023-10-11 14:37:24" itemprop="dateModified" datetime="2023-10-11T14:37:24+08:00">2023-10-11</time>
      </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>1.5k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>1 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>写这篇博客的初衷是自己一直以来都在关注supervised
finetuning，但对强化学习这一块一直都没有过多的涉猎，一方面是因为它是大模型技术模块里相对成本比较高的过程，还有一方面是我对强化学习没有系统性的学习，觉得有一丢丢的难理解，躺在list里的斯坦福的强化学习课程也一直搁浅，传送门：
<a target="_blank" rel="noopener" href="https://web.stanford.edu/class/cs234/">cs234n</a>,
课程视频和PPT都是可以免费下载的。</p>
<p>我这篇博客主要是受llama2模型的<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09288">paper</a>的启发，觉得这篇文章在RLHF方面写的非常之细致，并且代码也进行了开源，可以对照代码进行学习.
移步这篇文章的3.2节。当然也有很多博客详细介绍了这篇文章的强化学习部分的细节，参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/644697081">【LLM】Meta LLaMA
2中RLHF技术细节</a></p>
<p>首先RLHF包含了两个步骤，第一个就是训练一个reward
modeling来对LM生成的回答进行打分，这个分数是一个数值型的数据；第二部分就是用这个RM去调整我们的LM，使得LM能output更符合人类期望的回答。也有作者将SFT放到了RLHF的第一阶段，比如<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2303.18223">A Survey of Large Language
Models</a> 的5.2.3节将RLHF分为了三阶段：</p>
<figure>
<img src="/2023/09/19/RLHF%E7%BB%86%E8%8A%82%E8%A7%A3%E6%9E%90/image-20231011143645416.png" alt="image-20231011143645416">
<figcaption aria-hidden="true">image-20231011143645416</figcaption>
</figure>
<p>不过我认为SFT还是隔离开讲比较好。</p>
<h1 id="reward-modeling">Reward Modeling</h1>
<h2 id="数据">数据</h2>
<p>prompt好准备，那么打分这个就要靠人来打分了，人打分有一定的主观臆测性，所以就换成了比较哪一种回答比较好，像LLAMA2的做法就是分了四个等级：significantly
better, better, slightly better or negligibly better / unsure。</p>
<h2 id="rm模型">RM模型</h2>
<p>hugging face <a target="_blank" rel="noopener" href="https://huggingface.co/blog/zh/rlhf">blog</a>
中有一段话：</p>
<blockquote>
<p>这个过程中一个有趣的产物是目前成功的 RLHF 系统使用了和生成模型具有
不同 大小的 LM (例如 OpenAI 使用了 175B 的 LM 和 6B 的 RM，Anthropic
使用的 LM 和 RM 从 10B 到 52B 大小不等，DeepMind 使用了 70B 的
Chinchilla 模型分别作为 LM 和 RM)
。一种直觉是，偏好模型和生成模型需要具有类似的能力来理解提供给它们的文本</p>
</blockquote>
<h2 id="rm的训练">RM的训练</h2>
<p>直观上理解我们现在有了prompts，也有了这些prompts的generation在我们的LM上的的generation的评分ranking，那么怎么来用这些数据训练呢？</p>
<p>就拿LLAMA2的做法来说，它用了一个和预训练模型一模一样的模型作为RM的初始模型，唯一不同的是将LM中用作预测下一个token的分类头替换成了另一个可以输出分值的回归头就像下面这样：</p>
<p><img src="/2023/09/19/RLHF%E7%BB%86%E8%8A%82%E8%A7%A3%E6%9E%90/image-20230918142104723.png"></p>
<p>上图出自state of GPT。</p>
<p>loss的计算采用的是ouyang 2022的<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2203.02155">Training language models to
follow instructions with human feedback</a> 提出的计算方式：</p>
<figure>
<img src="/2023/09/19/RLHF%E7%BB%86%E8%8A%82%E8%A7%A3%E6%9E%90/image-20231011143044591.png" alt="image-20231011143044591">
<figcaption aria-hidden="true">image-20231011143044591</figcaption>
</figure>
<p>其中r是RM输出的标量值代表分值。不过llama2的做法在这个loss基础上加了一个margin，刚刚提到它在人工标注这些generation的时候分了四个档次，有的回答会比另一个对手super
better，有的只是稍微好一点，所以这种“好的程度”可以在loss中区分出来，所以作者在loss的的计算里加了一个margin：</p>
<figure>
<img src="/2023/09/19/RLHF%E7%BB%86%E8%8A%82%E8%A7%A3%E6%9E%90/image-20231011143054960.png" alt="image-20231011143054960">
<figcaption aria-hidden="true">image-20231011143054960</figcaption>
</figure>
<p>super better的就用一个比较大的m值。</p>
<h1 id="rl-fine-tuning">RL Fine-tuning</h1>
<h1 id="推荐阅读">推荐阅读</h1>
<ul>
<li>https://github.com/opendilab/awesome-RLHF</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/644697081">LLM Meta LLaMA
2中RLHF技术细节</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/" class="post-title-link" itemprop="url">LLM大模型推理加速</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-29 09:55:47" itemprop="dateCreated datePublished" datetime="2023-08-29T09:55:47+08:00">2023-08-29</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2023-12-28 17:17:13" itemprop="dateModified" datetime="2023-12-28T17:17:13+08:00">2023-12-28</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>7.2k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>7 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>最近框架vLLM在LLaMA-13B以及7B模型上的推理速度相比较Tranformers有了质的提升。之前写过一篇大模型量化技术的文章，量化技术算是大模型出来初期大家使用的普遍比较多的方法之一，这里强调一点，我这里所说的模型加速是指在推理阶段我们让模型运行的更快，而且返回的结果要和原来的大参数模型差不多。这里重点强调的原因是我在看一些资料的时候发现有不少博客分享的是在模型训练阶段或者finetune阶段如何让模型训练的更快，这里就涉及到efficient
finetuning的技术（p-tuning,
prefix-tuning等），我这篇博客只关注模型训练完成之后如何在推理阶段让它更快，在同样时间内处理更多sequence（吞吐量througout），显存占用更低。大模型推理加速技术为什么这么受关注还是因为想在一些消费级别显卡上部署一个大模型为用户所用，而不是仅仅停留在实验室阶段。</p>
<blockquote>
<p>我在看这个topic下的文章的时候，发现往往一些方法提出来有一些是减少了显存占用，有一些是提高了吞吐量（跟减少latency一回事），所以具体在实现时应用哪个办法加速你的模型推理还要根据实际情况去对比分析，或者你每个方法都尝试一下也行。当然又一些方法集成地很好，比如量化模型中的GPQT已经集成进transformers库，用起来很方便。如果碰到一些很复杂的，比如prune“剪枝”就有点难以快速验证。</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://vllm.ai/">vLLM</a>
暂时还没有文章发出来，我在谷歌搜寻有没有review介绍大模型加速文章的时候也没找到很新的文章，不过找到了一篇微软在23年发布的<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2304.04487">LLMA</a>,
我本来觉得思想类似，但是后来仔细看了下文章发现并不是一回事，我觉得文章标题有点误导人，起的太大了，本质上文章其实就是发现了decoding时候生成的句子和之前的句子有一部分的文字重叠，所以作者考虑这部分重叠内容其实不需要让模型再去decoding了，那就想了个办法，在decoding的时候把前面一步的结果保存下来，比较当前步骤和前一步骤token的差距，差距小的就不再进行计算了</p>
<blockquote>
<p>一点不成熟的想法：这个文章思路可取，但创造力有限。</p>
</blockquote>
<p>不过它在introduction章节介绍了四种比较通用的加速方法：quantization,
pruning, compression and distillation，同样的分类也可以在<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.07877.pdf">A Comprehensive Survey on
Model Quantization for Deep Neural Networks</a>文章中找到，
不过两者介绍的有一点点的不同：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230828093646830.png" alt="image-20230828093646830">
<figcaption aria-hidden="true">image-20230828093646830</figcaption>
</figure>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.07877.pdf">Survey</a>将四种技术统一包含在了模型压缩里。我觉得review里这种分类比较合理，因为微软这篇文章compression引用的文章是</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.02925">Bert-of-theseus:
Compressing bert by progressive module replacing</a>,
compression应该是一种统称。后面我看到知乎有一篇<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/642412124">文章</a>更详细的介绍了大模型的推理优化技术，它这个分类也符合我的理解，模型压缩（model
compression）里包含模型量化，pruning，low-rank
approximation和知识蒸馏这些技术。而且知乎这篇文章的分类也符合survey里的介绍：</p>
<blockquote>
<p>In designing accelerators, researchers concentrate on network
compression, parallel processing, and optimizing memory transfers for
processing speed-up.</p>
</blockquote>
<p>我这里做个思维导图总结一下：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230830133030762-16933734335261.png" alt="image-20230830133030762">
<figcaption aria-hidden="true">image-20230830133030762</figcaption>
</figure>
<p>题外话，根据LLMA文章的意思，它提出的这种帮助reduce the serving
cost的方式不属于上述任意一类，它认为以transformer为基础的生成模型，推理阶段主要消耗的时间瓶颈在autoregressive
decoding。这里贴原文便于理解</p>
<blockquote>
<p>While there are general methodologies that help reduce the serving
cost of LLMs such as quantization(Dettmers &amp; Zettlemoyer, 2023),
pruning (Frantar &amp; Alistarh, 2023), compression (Xu et al., 2020)
and distillation (Wang et al., 2020), <strong>the inference efficiency
bottleneck of these transformer-based generative models (e.g., GPT) is
mainly associated with autoregressive decoding: at test time, output
tokens must be decoded (sequentially) one by one, which poses
significant challenges for the LLMs to be deployed at scale.</strong>
这里补充介绍一下AI模型中精度，你会在各种场合下碰到FP32，FP16，int8，int4等名词。</p>
</blockquote>
<p>32-bit：也称全精度(Single precision)，<code>fp32</code>,
采用32位（4字节）来对数据进行编码。能够表达的数据动态区间是 <span class="math display">\[
1.4 * 10^{-45} - 1.7 * 10 ^ {38}
\]</span></p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/Float_example-16933762107103.svg" alt="Float_example">
<figcaption aria-hidden="true">Float_example</figcaption>
</figure>
<p>16-bit：半精度（half precision）,<code>fp16</code>,
能够表达的数据动态区间是 <span class="math display">\[
6 * 10^{-8} - 65504
\]</span></p>
<p><img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/IEEE_754r_Half_Floating_Point_Format-16933761704442.svg"></p>
<p>BF-16： 也称为半精度，可以表达比FP16更大的数，但是精度比fp16差</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230830145435142-16933784764394.png" alt="image-20230830145435142">
<figcaption aria-hidden="true">image-20230830145435142</figcaption>
</figure>
<p>int8,int4顾名思义就是 8bit和4个bit来表示数字，int8的表达数值范围是
<code>-128~127</code> <a target="_blank" rel="noopener" href="https://blog.csdn.net/ordmeng/article/details/99620804">why</a>，无符号范围是<code>0~255</code>，int4的表达数值范围是<code>-8~7</code>。注意这里的计算方式和上面的浮点数可不一样，上面的浮点数中的8bits的exponent是指数表达，所以将指数那一部分的表达加和之后还要取2的指数，见<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36533552/article/details/105885714">具体计算</a>.
再详细一点的介绍见<a target="_blank" rel="noopener" href="https://huggingface.co/blog/zh/hf-bitsandbytes-integration">hugging
face 博客</a></p>
<h1 id="模型压缩-model-compression">模型压缩 model compression</h1>
<h2 id="quatization-量化">Quatization 量化</h2>
<h3 id="参考文献">参考文献</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.07877.pdf">A Comprehensive
Survey on Model Quantization for Deep Neural Networks</a></li>
<li>https://huggingface.co/docs/transformers/main_classes/quantization#autogptq-integration
huggingface的document，其中放在最上面的就是<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2210.17323.pdf">GPTQ</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/627436535">大语言模型的模型量化(INT8/INT4)技术</a>
讲解了<code>LLM.int8()</code></li>
<li><a target="_blank" rel="noopener" href="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/">LLM.int8()
and Emergent Features</a></li>
</ul>
<p>模型的量化可以分为两种方式：</p>
<ol type="1">
<li>post-training quantization
模型训练好之后，将模型的参数们降低精度</li>
<li>quantization-aware Training
在模型训练的过程中使用量化的方式，优点是比前者performance要好，但是需要更多的计算资源去训练</li>
</ol>
<p>量化顾名思义要把原来用高精度表达的值映射到一个低精度的空间，目标呢就是让模型的performance不能有很大的降低。那如何映射和对哪一些值进行映射，这两个方向是现在量化方法的主攻方向。</p>
<p>很典型的LLM.int8()算法，不是大刀阔斧地对所有值一次性量化，也不是把矩阵中所有值一起量化，而是先找出那些离群值，然后对这些离群值再按照居正中行列来进行量化：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230831103106998.png" alt="LLM.int8()">
<figcaption aria-hidden="true">LLM.int8()</figcaption>
</figure>
<p>最重要的是上面那一部分。计算拆解见<a target="_blank" rel="noopener" href="https://huggingface.co/blog/zh/hf-bitsandbytes-integration">大规模
Transformer 模型 8 比特矩阵乘简介</a></p>
<h3 id="gptq">GPTQ</h3>
<p>读者可以自行阅读GPTQ的原文来了解它具体是如何做的，我喜欢找一些其他的文章来看别的作者是如何介绍自己的同行作品的，比如下面的这篇文章<a target="_blank" rel="noopener" href="https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf">SmoothQuant:
Accurate and Efficient Post-Training Quantization for Large Language
Models</a> 的第六章节related
work里这样比较自己的smoothQuant和其他的量化模型方法的：</p>
<blockquote>
<p>GPTQ (Frantar et al., 2022) applies quantization only to weights but
not activations.
GPTQ这种方法只对weights做了量化，并没有对激活值做量化（我个人认为虽然这是事实，但有点硬凹的意思，因为对activations做量化映射并不会加速很多）</p>
<p>LLM.int8() uses mixed int8/fp16 decomposition to address the
activation outliers. However, such implementation leads to large latency
overhead, which can be even slower than FP16 inference.
意思是LLM.int8()这种方法只是减少了显存占用，并没有减少推理延迟，说白了就是慢，runtime没提高</p>
</blockquote>
<h2 id="sparsity">Sparsity</h2>
<h2 id="low-rank-approximation">low-Rank Approximation</h2>
<h3 id="lora"><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2106.09685">Lora</a></h3>
<p>用lora的方式替换全参数微调大模型已经成为好多研究者的选择，一个是它的确有效的降低了训练参数的比例，第二个很大的原因是它的performance还不错，也就是只训练低秩的那些参数矩阵完全可以得到一个高质量的模型.</p>
<blockquote>
<p>We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained
model weights and injects trainable rank decomposition matrices into
each layer of the Transformer architecture, greatly reducing the number
of trainable parameters for downstream tasks</p>
</blockquote>
<p>从文章的介绍可以看出，它主要是应用于transformer架构中的layer中，这个layer包含self-attention，也包含MLP。只要有矩阵乘的地方都可以用lora。</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20231228160733632.png" alt="lora图示">
<figcaption aria-hidden="true">lora图示</figcaption>
</figure>
<p>具体的在transformer中如何使用呢？我们知道transformer架构中涉及矩阵运算的地方：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20231228171113210.png" alt="image-20231228171113210">
<figcaption aria-hidden="true">image-20231228171113210</figcaption>
</figure>
<p>在上图中的self-attention中涉及四个weights矩阵：(Wq, Wk, Wv,
Wo)。在feed-forward(lora原文中叫MLP，我觉得原因在于这里是有两个线性变换的)，具体看attention
is all you need 原文：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20231228171318056.png" alt="image-20231228171318056">
<figcaption aria-hidden="true">image-20231228171318056</figcaption>
</figure>
<p>所以每一个encoder的layer都有6个weights矩阵可以实施lora。lora的作者仅仅对attention
weights做了lora，更简化的只是对其中的Wq和Wv做了lora变换。实现上由于现在有了peft库，只是几句话就能实现对这两个矩阵进行lora：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_peft_config</span>(<span class="params">model</span>):</span></span><br><span class="line">    <span class="keyword">from</span> peft <span class="keyword">import</span> (</span><br><span class="line">        get_peft_model,</span><br><span class="line">        LoraConfig,</span><br><span class="line">        TaskType,</span><br><span class="line">        prepare_model_for_int8_training,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    peft_config = LoraConfig(</span><br><span class="line">        task_type=TaskType.CAUSAL_LM,</span><br><span class="line">        inference_mode=<span class="literal">False</span>,</span><br><span class="line">        r=<span class="number">8</span>,</span><br><span class="line">        lora_alpha=<span class="number">32</span>,</span><br><span class="line">        lora_dropout=<span class="number">0.05</span>,</span><br><span class="line">        target_modules = [<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>] <span class="comment"># 这里仅仅对Q和V的变换矩阵做lora</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prepare int-8 model for training</span></span><br><span class="line">    model = prepare_model_for_int8_training(model)</span><br><span class="line">    model = get_peft_model(model, peft_config)</span><br><span class="line">    model.print_trainable_parameters()</span><br><span class="line">    <span class="keyword">return</span> model, peft_config</span><br><span class="line"></span><br><span class="line"><span class="comment"># create peft config</span></span><br><span class="line">model, lora_config = create_peft_config(model)</span><br></pre></td></tr></table></figure>
<h2 id="knowledge-distillation">Knowledge Distillation</h2>
<p>知识蒸馏出现的比较早，一开始也是在Bert上流行起来的。在LLM这种非常多参数的模型上用的不多。</p>
<p>这里罗列我看的对我理解很有帮助的文章：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/ch08.html#idm45146300040192"><strong>Natural
Language Processing with Transformers, Revised Edition</strong></a>
第八章“Making models smaller via Knowledge Distillation”
很详细的介绍了loss的计算（只是蒸馏的核心点）</li>
<li><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/#distillation">lilian
wen blog</a></li>
</ul>
<h1 id="continuous-batching">Continuous Batching</h1>
<p><strong><em>参考文献：</em></strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.anyscale.com/blog/continuous-batching-llm-inference">How
continuous batching enables 23x throughput in LLM inference while
reducing p50 latency</a></li>
<li></li>
</ul>
<h2 id="vllm">vLLM</h2>
<p>vllm这个包本质上是将显存GPU高效利用了（PagedAttention技术），还有上面提到的LLMA.
不过它们的思路其实大同小异，本质上是为了解决transformer的decoder在文本生成时自回归结构带来的无法并发的开销。推荐阅读<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/_4OxoRLxhOcjGf0Q4Tvp2Q">为什么现在大家都在用
MQA 和 GQA？</a></p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/640.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>那么既然时间开销都在右边这个decoding的阶段，那就想办法解决它。那就是刚刚那篇文章介绍的KV
Cache。作者提到的内存墙的问题也是这个问题的切入点，如何让计算单元更迅速的从存储单元获取数据，Paged
Attention和Flash
Attention都是来解决这个问题的。MQA的本质是减少了数据读取次数，第一次读取进来的K和V给所有的Q用，就放在缓存里。文章里详细讲解了MQA和GQA，这里不再赘述，但有一点值得注意的是，这两种办法再使用的时候可能并不能只是在推理的时候直接改变结构，也许要像作者说的那样：</p>
<blockquote>
<p>如果要用 MQA 和 GQA，可以是从头训练的时候就加上，也可以像 GQA
论文里面一样，用已有的开源模型，挑一些头取个 mean 用来初始化 MQA 或 GQA
继续训练一段时间。</p>
</blockquote>
<h2 id="text-generation-inference">Text Generation Inference</h2>
<h1 id="transformer结构优化">Transformer结构优化</h1>
<p>经典的Transformer架构出来之后，很多工作都在这个架构之上进行了魔改，希望能加快transformer的推理速度，推荐阅读survey
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2009.06732.pdf">Efficient Transformers: A
Survey</a> ,
目前该review已经是第三版本，最新版本是2022年3月份出的，所以内容里没有flash
Attention以及一些更新的技术，希望作者快快更新第四版本的review，技术迭代太快了，亟需大神肝的review总结。</p>
<p>大部分Transformer结构改进的方法的目标都是为了降低GPU的显存占用，也就是提高运算效率，将GPU的运算效率拉满，这就涉及到很底层的对于计算机系统结构的知识。</p>
<h2 id="flash-attention">flash Attention</h2>
<p>Flash Attention （Fast and Memory-Efficient Exact
Attention）详细介绍可以阅读<a target="_blank" rel="noopener" href="https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad">ELI5:
FlashAttention</a>，这是除了了multi-query
attention技术之外用的比较多的加速推理的方式，当然Paged
Attention算是vllm火起来之后的后起之秀。当然也可以阅读<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.14135.pdf">paper</a></p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230829155237303-16932955599782.png" alt="attention on GPT-2">
<figcaption aria-hidden="true">attention on GPT-2</figcaption>
</figure>
<p>在medium的这篇博客里作者首先澄清两个概念，一个是FLOPs（每秒钟浮点运算次数）和IO，前者在GPU的更新换代的情况下获得了高速发展，而GPU的计算单元和显存间的通信却没有获得同样数量级的增长。而从上图可以看出来（原文paper中的），Attention的计算过程中大部分时间都是memory-bound导向的运算，而计算密集型的操作比如矩阵乘法其实只占了耗费时间的一小部分。</p>
<p>传统的attention计算步骤：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230829162652549.png" alt="image-20230829162652549">
<figcaption aria-hidden="true">image-20230829162652549</figcaption>
</figure>
<p>注意这里的HBM是：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230829162732067.png" alt="image-20230829162732067">
<figcaption aria-hidden="true">image-20230829162732067</figcaption>
</figure>
<p>最上面一层是GPU的缓存，中间是高带宽内存，可以理解为GPU的显存，也就是你去买显卡，标注在显卡上的存储，这部分存储会大一点，运算单元需要从这里拿数据到计算单元去计算，可以直接交互，也可以先存储在缓存SRAM内，缓存会比HBM快很多。</p>
<p>从标准的attention计算看到有很多不需要把计算中间结果写回HBM的环节。至于FlashAttention计算推导部分我看了上面的英文博客和<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/638468472">从 FlashAttention 到
PagedAttention, 如何进一步优化 Attention
性能</a>，还是没能理解，感兴趣的小伙伴还是自己去知乎这篇文章里好好看一下。</p>
<h2 id="paged-attention">Paged Attention</h2>
<h2 id="flat-attention">FLAT Attention</h2>
<h1 id="并行-parallel-processing">并行 Parallel Processing</h1>
<h1 id="推荐阅读">推荐阅读</h1>
<ul>
<li><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/#methods-overview">Large
Transformer Model Inference Optimization</a>
lilian的博客，在文首推荐的知乎文章大抵参考了这篇博客，虽然是1月份的文章，但还是推荐阅读食用</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YAN</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Symbols count total: </span>
    <span title="Symbols count total">210k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">3:11</span>
  </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
