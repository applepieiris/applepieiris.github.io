<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=consolas:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.8.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="Be smart! Keep Learning">
<meta property="og:type" content="website">
<meta property="og:title" content="YAN&#39;s Blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="YAN&#39;s Blog">
<meta property="og:description" content="Be smart! Keep Learning">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="YAN">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>YAN's Blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">YAN's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="YAN"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">YAN</p>
  <div class="site-description" itemprop="description">Be smart! Keep Learning</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">52</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/applepieiris" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/" class="post-title-link" itemprop="url">LLM大模型推理加速</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-29 09:55:47" itemprop="dateCreated datePublished" datetime="2023-08-29T09:55:47+08:00">2023-08-29</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2023-08-31 16:14:10" itemprop="dateModified" datetime="2023-08-31T16:14:10+08:00">2023-08-31</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>5.8k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>5 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>最近框架vLLM在LLaMA-13B以及7B模型上的推理速度相比较Tranformers有了质的提升。之前写过一篇大模型量化技术的文章，量化技术算是大模型出来初期大家使用的普遍比较多的方法之一，这里强调一点，我这里所说的模型加速是指在推理阶段我们让模型运行的更快，而且返回的结果要和原来的大参数模型差不多。这里重点强调的原因是我在看一些资料的时候发现有不少博客分享的是在模型训练阶段或者finetune阶段如何让模型训练的更快，这里就涉及到efficient
finetuning的技术（p-tuning,
prefix-tuning等），我这篇博客只关注模型训练完成之后如何在推理阶段让它更快，在同样时间内处理更多sequence（吞吐量througout），显存占用更低。大模型推理加速技术为什么这么受关注还是因为想在一些消费级别显卡上部署一个大模型为用户所用，而不是仅仅停留在实验室阶段。</p>
<blockquote>
<p>我在看这个topic下的文章的时候，发现往往一些方法提出来有一些是减少了显存占用，有一些是提高了吞吐量（跟减少latency一回事），所以具体在实现时应用哪个办法加速你的模型推理还要根据实际情况去对比分析，或者你每个方法都尝试一下也行。当然又一些方法集成地很好，比如量化模型中的GPQT已经集成进transformers库，用起来很方便。如果碰到一些很复杂的，比如prune“剪枝”就有点难以快速验证。</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://vllm.ai/">vLLM</a>
暂时还没有文章发出来，我在谷歌搜寻有没有review介绍大模型加速文章的时候也没找到很新的文章，不过找到了一篇微软在23年发布的<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2304.04487">LLMA</a>,
我本来觉得思想类似，但是后来仔细看了下文章发现并不是一回事，我觉得文章标题有点误导人，起的太大了，本质上文章其实就是发现了decoding时候生成的句子和之前的句子有一部分的文字重叠，所以作者考虑这部分重叠内容其实不需要让模型再去decoding了，那就想了个办法，在decoding的时候把前面一步的结果保存下来，比较当前步骤和前一步骤token的差距，差距小的就不再进行计算了</p>
<blockquote>
<p>一点不成熟的想法：这个文章思路可取，但创造力有限。</p>
</blockquote>
<p>不过它在introduction章节介绍了四种比较通用的加速方法：quantization,
pruning, compression and distillation，同样的分类也可以在<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.07877.pdf">A Comprehensive Survey on
Model Quantization for Deep Neural Networks</a>文章中找到，
不过两者介绍的有一点点的不同：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230828093646830.png" alt="image-20230828093646830">
<figcaption aria-hidden="true">image-20230828093646830</figcaption>
</figure>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.07877.pdf">Survey</a>将四种技术统一包含在了模型压缩里。我觉得review里这种分类比较合理，因为微软这篇文章compression引用的文章是</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.02925">Bert-of-theseus:
Compressing bert by progressive module replacing</a>,
compression应该是一种统称。后面我看到知乎有一篇<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/642412124">文章</a>更详细的介绍了大模型的推理优化技术，它这个分类也符合我的理解，模型压缩（model
compression）里包含模型量化，pruning，low-rank
approximation和知识蒸馏这些技术。而且知乎这篇文章的分类也符合survey里的介绍：</p>
<blockquote>
<p>In designing accelerators, researchers concentrate on network
compression, parallel processing, and optimizing memory transfers for
processing speed-up.</p>
</blockquote>
<p>我这里做个思维导图总结一下：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230830133030762-16933734335261.png" alt="image-20230830133030762">
<figcaption aria-hidden="true">image-20230830133030762</figcaption>
</figure>
<p>题外话，根据LLMA文章的意思，它提出的这种帮助reduce the serving
cost的方式不属于上述任意一类，它认为以transformer为基础的生成模型，推理阶段主要消耗的时间瓶颈在autoregressive
decoding。这里贴原文便于理解</p>
<blockquote>
<p>While there are general methodologies that help reduce the serving
cost of LLMs such as quantization(Dettmers &amp; Zettlemoyer, 2023),
pruning (Frantar &amp; Alistarh, 2023), compression (Xu et al., 2020)
and distillation (Wang et al., 2020), <strong>the inference efficiency
bottleneck of these transformer-based generative models (e.g., GPT) is
mainly associated with autoregressive decoding: at test time, output
tokens must be decoded (sequentially) one by one, which poses
significant challenges for the LLMs to be deployed at scale.</strong>
这里补充介绍一下AI模型中精度，你会在各种场合下碰到FP32，FP16，int8，int4等名词。</p>
</blockquote>
<p>32-bit：也称全精度(Single precision)，<code>fp32</code>,
采用32位（4字节）来对数据进行编码。能够表达的数据动态区间是 <span class="math display">\[
1.4 * 10^{-45} - 1.7 * 10 ^ {38}
\]</span></p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/Float_example-16933762107103.svg" alt="Float_example">
<figcaption aria-hidden="true">Float_example</figcaption>
</figure>
<p>16-bit：半精度（half precision）,<code>fp16</code>,
能够表达的数据动态区间是 <span class="math display">\[
6 * 10^{-8} - 65504
\]</span></p>
<p><img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/IEEE_754r_Half_Floating_Point_Format-16933761704442.svg"></p>
<p>BF-16： 也称为半精度，可以表达比FP16更大的数，但是精度比fp16差</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230830145435142-16933784764394.png" alt="image-20230830145435142">
<figcaption aria-hidden="true">image-20230830145435142</figcaption>
</figure>
<p>int8,int4顾名思义就是 8bit和4个bit来表示数字，int8的表达数值范围是
<code>-128~127</code> <a target="_blank" rel="noopener" href="https://blog.csdn.net/ordmeng/article/details/99620804">why</a>，无符号范围是<code>0~255</code>，int4的表达数值范围是<code>-8~7</code>。注意这里的计算方式和上面的浮点数可不一样，上面的浮点数中的8bits的exponent是指数表达，所以将指数那一部分的表达加和之后还要取2的指数，见<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36533552/article/details/105885714">具体计算</a>.
再详细一点的介绍见<a target="_blank" rel="noopener" href="https://huggingface.co/blog/zh/hf-bitsandbytes-integration">hugging
face 博客</a></p>
<h1 id="模型压缩-model-compression">模型压缩 model compression</h1>
<h2 id="quatization-量化">Quatization 量化</h2>
<h3 id="参考文献">参考文献</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.07877.pdf">A Comprehensive
Survey on Model Quantization for Deep Neural Networks</a></li>
<li>https://huggingface.co/docs/transformers/main_classes/quantization#autogptq-integration
huggingface的document，其中放在最上面的就是<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2210.17323.pdf">GPTQ</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/627436535">大语言模型的模型量化(INT8/INT4)技术</a>
讲解了<code>LLM.int8()</code></li>
<li><a target="_blank" rel="noopener" href="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/">LLM.int8()
and Emergent Features</a></li>
</ul>
<p>模型的量化可以分为两种方式：</p>
<ol type="1">
<li>post-training quantization
模型训练好之后，将模型的参数们降低精度</li>
<li>quantization-aware Training
在模型训练的过程中使用量化的方式，优点是比前者performance要好，但是需要更多的计算资源去训练</li>
</ol>
<p>量化顾名思义要把原来用高精度表达的值映射到一个低精度的空间，目标呢就是让模型的performance不能有很大的降低。那如何映射和对哪一些值进行映射，这两个方向是现在量化方法的主攻方向。</p>
<p>很典型的LLM.int8()算法，不是大刀阔斧地对所有值一次性量化，也不是把矩阵中所有值一起量化，而是先找出那些离群值，然后对这些离群值再按照居正中行列来进行量化：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230831103106998.png" alt="LLM.int8()">
<figcaption aria-hidden="true">LLM.int8()</figcaption>
</figure>
<p>最重要的是上面那一部分。计算拆解见<a target="_blank" rel="noopener" href="https://huggingface.co/blog/zh/hf-bitsandbytes-integration">大规模
Transformer 模型 8 比特矩阵乘简介</a></p>
<h3 id="gptq">GPTQ</h3>
<p>读者可以自行阅读GPTQ的原文来了解它具体是如何做的，我喜欢找一些其他的文章来看别的作者是如何介绍自己的同行作品的，比如下面的这篇文章<a target="_blank" rel="noopener" href="https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf">SmoothQuant:
Accurate and Efficient Post-Training Quantization for Large Language
Models</a> 的第六章节related
work里这样比较自己的smoothQuant和其他的量化模型方法的：</p>
<blockquote>
<p>GPTQ (Frantar et al., 2022) applies quantization only to weights but
not activations.
GPTQ这种方法只对weights做了量化，并没有对激活值做量化（我个人认为虽然这是事实，但有点硬凹的意思，因为对activations做量化映射并不会加速很多）</p>
<p>LLM.int8() uses mixed int8/fp16 decomposition to address the
activation outliers. However, such implementation leads to large latency
overhead, which can be even slower than FP16 inference.
意思是LLM.int8()这种方法只是减少了显存占用，并没有减少推理延迟，说白了就是慢，runtime没提高</p>
</blockquote>
<h2 id="sparsity">Sparsity</h2>
<h2 id="low-rank-approximation">low-Rank Approximation</h2>
<h2 id="knowledge-distillation">Knowledge Distillation</h2>
<p>知识蒸馏出现的比较早，一开始也是在Bert上流行起来的。在LLM这种非常多参数的模型上用的不多。</p>
<p>这里罗列我看的对我理解很有帮助的文章：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/ch08.html#idm45146300040192"><strong>Natural
Language Processing with Transformers, Revised Edition</strong></a>
第八章“Making models smaller via Knowledge Distillation”
很详细的介绍了loss的计算（只是蒸馏的核心点）</li>
<li><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/#distillation">lilian
wen blog</a></li>
</ul>
<h1 id="continuous-batching">Continuous Batching</h1>
<p><strong><em>参考文献：</em></strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.anyscale.com/blog/continuous-batching-llm-inference">How
continuous batching enables 23x throughput in LLM inference while
reducing p50 latency</a></li>
<li></li>
</ul>
<h2 id="vllm">vLLM</h2>
<p>vllm这个包本质上是将显存GPU高效利用了（PagedAttention技术），还有上面提到的LLMA.
不过它们的思路其实大同小异，本质上是为了解决transformer的decoder在文本生成时自回归结构带来的无法并发的开销。推荐阅读<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/_4OxoRLxhOcjGf0Q4Tvp2Q">为什么现在大家都在用
MQA 和 GQA？</a></p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/640.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>那么既然时间开销都在右边这个decoding的阶段，那就想办法解决它。那就是刚刚那篇文章介绍的KV
Cache。作者提到的内存墙的问题也是这个问题的切入点，如何让计算单元更迅速的从存储单元获取数据，Paged
Attention和Flash
Attention都是来解决这个问题的。MQA的本质是减少了数据读取次数，第一次读取进来的K和V给所有的Q用，就放在缓存里。文章里详细讲解了MQA和GQA，这里不再赘述，但有一点值得注意的是，这两种办法再使用的时候可能并不能只是在推理的时候直接改变结构，也许要像作者说的那样：</p>
<blockquote>
<p>如果要用 MQA 和 GQA，可以是从头训练的时候就加上，也可以像 GQA
论文里面一样，用已有的开源模型，挑一些头取个 mean 用来初始化 MQA 或 GQA
继续训练一段时间。</p>
</blockquote>
<h2 id="text-generation-inference">Text Generation Inference</h2>
<h1 id="transformer结构优化">Transformer结构优化</h1>
<p>经典的Transformer架构出来之后，很多工作都在这个架构之上进行了魔改，希望能加快transformer的推理速度，推荐阅读survey
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2009.06732.pdf">Efficient Transformers: A
Survey</a> ,
目前该review已经是第三版本，最新版本是2022年3月份出的，所以内容里没有flash
Attention以及一些更新的技术，希望作者快快更新第四版本的review，技术迭代太快了，亟需大神肝的review总结。</p>
<p>大部分Transformer结构改进的方法的目标都是为了降低GPU的显存占用，也就是提高运算效率，将GPU的运算效率拉满，这就涉及到很底层的对于计算机系统结构的知识。</p>
<h2 id="flash-attention">flash Attention</h2>
<p>Flash Attention （Fast and Memory-Efficient Exact
Attention）详细介绍可以阅读<a target="_blank" rel="noopener" href="https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad">ELI5:
FlashAttention</a>，这是除了了multi-query
attention技术之外用的比较多的加速推理的方式，当然Paged
Attention算是vllm火起来之后的后起之秀。当然也可以阅读<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.14135.pdf">paper</a></p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230829155237303-16932955599782.png" alt="attention on GPT-2">
<figcaption aria-hidden="true">attention on GPT-2</figcaption>
</figure>
<p>在medium的这篇博客里作者首先澄清两个概念，一个是FLOPs（每秒钟浮点运算次数）和IO，前者在GPU的更新换代的情况下获得了高速发展，而GPU的计算单元和显存间的通信却没有获得同样数量级的增长。而从上图可以看出来（原文paper中的），Attention的计算过程中大部分时间都是memory-bound导向的运算，而计算密集型的操作比如矩阵乘法其实只占了耗费时间的一小部分。</p>
<p>传统的attention计算步骤：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230829162652549.png" alt="image-20230829162652549">
<figcaption aria-hidden="true">image-20230829162652549</figcaption>
</figure>
<p>注意这里的HBM是：</p>
<figure>
<img src="/2023/08/29/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/image-20230829162732067.png" alt="image-20230829162732067">
<figcaption aria-hidden="true">image-20230829162732067</figcaption>
</figure>
<p>最上面一层是GPU的缓存，中间是高带宽内存，可以理解为GPU的显存，也就是你去买显卡，标注在显卡上的存储，这部分存储会大一点，运算单元需要从这里拿数据到计算单元去计算，可以直接交互，也可以先存储在缓存SRAM内，缓存会比HBM快很多。</p>
<p>从标准的attention计算看到有很多不需要把计算中间结果写回HBM的环节。至于FlashAttention计算推导部分我看了上面的英文博客和<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/638468472">从 FlashAttention 到
PagedAttention, 如何进一步优化 Attention
性能</a>，还是没能理解，感兴趣的小伙伴还是自己去知乎这篇文章里好好看一下。</p>
<h2 id="paged-attention">Paged Attention</h2>
<h2 id="flat-attention">FLAT Attention</h2>
<h1 id="并行-parallel-processing">并行 Parallel Processing</h1>
<h1 id="推荐阅读">推荐阅读</h1>
<ul>
<li><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/#methods-overview">Large
Transformer Model Inference Optimization</a>
lilian的博客，在文首推荐的知乎文章大抵参考了这篇博客，虽然是1月份的文章，但还是推荐阅读食用</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/16/HuggingFace%20Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/16/HuggingFace%20Transformer/" class="post-title-link" itemprop="url">HuggingFace Transformers</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-16 14:33:33" itemprop="dateCreated datePublished" datetime="2023-08-16T14:33:33+08:00">2023-08-16</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2023-08-29 13:24:11" itemprop="dateModified" datetime="2023-08-29T13:24:11+08:00">2023-08-29</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>8.8k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>8 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>今天新开一篇博客，用于记录自己在学习hugging
face的一些笔记。初衷是自己在研究大语言模型的时候，很多时候并不知道如何finetune预训练好的大语言模型，再加上现在很多框架都封装的蛮厉害的，比如如果想实现lora等parameter-efficient的finetune方式，hugging
face中已经写好了PEFT这个库，只需几行代码即可实现，但想知道里面的具体实现还得一层层翻阅源码。之前对于transformer这个库的掌握都不系统，今天开此专题希望能读完hugging
face官方出的关于transformer这个tutorial。</p>
<p>首先一点hugging
face是承载着一系列模型，这些模型有nlp的也有cv的,除了模型还有数据，模型的checkpoints。另外它不仅支持pytorch，还支持tensorflow，我本来是tensorflow的坚定支持者，但看到现在pytorch越发有超越tensorflow的趋势，不掌握不行了快。</p>
<p>我们知道transformer衍生出来的模型有三类，一类是gpt类型，只有decoder，一类是bert，只有encoder部分，还有一类是encoder-decoder，以bart,
T5为首的。</p>
<figure>
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg" alt="A brief chronology of Transformers models.">
<figcaption aria-hidden="true">A brief chronology of Transformers
models.</figcaption>
</figure>
<p>BERT一类的模型可以称为：Encoder models(auto-encoding models),
pre-train的方式就是通过mask掉一个句子中的一些单词，然后让模型去重建这些单词。这一类模型有BERT,RoBERTa,DistilBERT,ALBERT,ELECTRA。<strong>这一类模型更适合用于解决需要理解整个句子意思的一类task</strong>，比如sentence
classification,NER(word classification), extractive question
answering。</p>
<p>GPT一类的模型可以称为：decoder models(auto-regressive
models),pre-train的方式是预测一个句子中下一个单词是什么，它只能看到前面单词的信息，不像bert类的encoder
models可以看到左右的信息。这一类模型有GPT，GPT-2，CTRL等。<strong>这一类模型更适合用于解决文本生成一类的任务</strong>。</p>
<p>T5一类的模型可以称为：encoder-decoder models(sequence-to-sequence
models),pre-train的方式比较复杂，因模型而异。<strong>这一类模型更适合用于解决给定一个input，生成一个新的句子</strong>，比如summarization,translation,generative
question answering.</p>
<h1 id="tokenizer">tokenizer</h1>
<ul>
<li>建议用AutoTokenizer类来导入预训练模型的tokenizer</li>
<li>可以批量encode</li>
<li>集成的函数是两步骤：tokenize+convert_tokens_to_ids</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;distilbert-base-cased&quot;</span>) <span class="comment"># convenient! Defaults to Fast</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># You can pass multiple strings into the tokenizer and pad them as you need</span></span><br><span class="line">model_inputs = tokenizer([<span class="string">&quot;Hugging Face Transformers is great!&quot;</span>,</span><br><span class="line">                         <span class="string">&quot;The quick brown fox jumps over the lazy dog.&quot;</span> +\</span><br><span class="line">                         <span class="string">&quot;Then the dog got up and ran away because she didn&#x27;t like foxes.&quot;</span>,</span><br><span class="line">                         ],</span><br><span class="line">                         return_tensors=<span class="string">&quot;pt&quot;</span>,</span><br><span class="line">                         padding=<span class="literal">True</span>,</span><br><span class="line">                         truncation=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Pad token: <span class="subst">&#123;tokenizer.pad_token&#125;</span> | Pad token id: <span class="subst">&#123;tokenizer.pad_token_id&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Padding:&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># You can also decode a whole batch at once: 可以批量解码！</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Batch Decode:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.batch_decode(model_inputs.input_ids))</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Batch Decode: (no special characters)&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.batch_decode(model_inputs.input_ids, skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<h1 id="model">model</h1>
<p>一个很重要的点，不同模型可以做不同的下游任务，比如distilBERT，如果我们想用它来做句子分类的任务，在bert的基础架构上还需要有一些参数，这里叫"heads",这些heads要在你的数据集上训练，transformer提供了特定的类，这样你就不用自己写这个头了。比如DistilBert，它就有DistilBertForSequenceClassification这个类，可以加的后缀有：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">*</span><br><span class="line">*ForMaskedLM</span><br><span class="line">*ForSequenceClassification</span><br><span class="line">*ForTokenClassification</span><br><span class="line">*ForQuestionAnswering</span><br><span class="line">*ForMultipleChoice</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>✳️可以是AutoModel或者是某一个特定的预训练模型，比如DistilBert</p>
<blockquote>
<p>官方建议使用Auto的方式来调用checkpoints，这样可以随意更改传入的pretrained的模型是哪个</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForMaskedLM</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;camembert-base&quot;</span>)</span><br><span class="line">model = AutoModelForMaskedLM.from_pretrained(<span class="string">&quot;camembert-base&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="finetune-a-model-with-trainer-api">finetune a model with Trainer
API</h1>
<h2 id="pytorch">pytorch</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, DataCollatorWithPadding</span><br><span class="line"></span><br><span class="line"><span class="comment"># data processing</span></span><br><span class="line">raw_datasets = load_dataset(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_function</span>(<span class="params">example</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(example[<span class="string">&quot;sentence1&quot;</span>], example[<span class="string">&quot;sentence2&quot;</span>], truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tokenized_datasets = raw_datasets.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model training</span></span><br><span class="line">training_args = TrainingArguments(<span class="string">&quot;test-trainer&quot;</span>, evaluation_strategy=<span class="string">&quot;epoch&quot;</span>)</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_metrics</span>(<span class="params">eval_preds</span>):</span></span><br><span class="line">    metric = evaluate.load(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">    logits, labels = eval_preds</span><br><span class="line">    predictions = np.argmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> metric.compute(predictions=predictions, references=labels)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model,</span><br><span class="line">    training_args,</span><br><span class="line">    train_dataset=tokenized_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=tokenized_datasets[<span class="string">&quot;validation&quot;</span>],</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>
<h2 id="tensorflow">tensorflow</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, DataCollatorWithPadding</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">raw_datasets = load_dataset(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_function</span>(<span class="params">example</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(example[<span class="string">&quot;sentence1&quot;</span>], example[<span class="string">&quot;sentence2&quot;</span>], truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tokenized_datasets = raw_datasets.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="string">&quot;tf&quot;</span>)</span><br><span class="line"></span><br><span class="line">tf_train_dataset = tokenized_datasets[<span class="string">&quot;train&quot;</span>].to_tf_dataset(</span><br><span class="line">    columns=[<span class="string">&quot;attention_mask&quot;</span>, <span class="string">&quot;input_ids&quot;</span>, <span class="string">&quot;token_type_ids&quot;</span>],</span><br><span class="line">    label_cols=[<span class="string">&quot;labels&quot;</span>],</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    collate_fn=data_collator,</span><br><span class="line">    batch_size=<span class="number">8</span>,</span><br><span class="line">) <span class="comment"># 这里会和pytorch不一样</span></span><br><span class="line"></span><br><span class="line">tf_validation_dataset = tokenized_datasets[<span class="string">&quot;validation&quot;</span>].to_tf_dataset(</span><br><span class="line">    columns=[<span class="string">&quot;attention_mask&quot;</span>, <span class="string">&quot;input_ids&quot;</span>, <span class="string">&quot;token_type_ids&quot;</span>],</span><br><span class="line">    label_cols=[<span class="string">&quot;labels&quot;</span>],</span><br><span class="line">    shuffle=<span class="literal">False</span>,</span><br><span class="line">    collate_fn=data_collator,</span><br><span class="line">    batch_size=<span class="number">8</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers.schedules <span class="keyword">import</span> PolynomialDecay</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="comment"># The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied</span></span><br><span class="line"><span class="comment"># by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,</span></span><br><span class="line"><span class="comment"># not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.</span></span><br><span class="line">num_train_steps = <span class="built_in">len</span>(tf_train_dataset) * num_epochs</span><br><span class="line">lr_scheduler = PolynomialDecay(</span><br><span class="line">    initial_learning_rate=<span class="number">5e-5</span>, end_learning_rate=<span class="number">0.0</span>, decay_steps=num_train_steps</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line">opt = Adam(learning_rate=lr_scheduler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="number">2</span>)</span><br><span class="line">loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=opt, loss=loss, metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line"></span><br><span class="line">model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">preds = model.predict(tf_validation_dataset)[<span class="string">&quot;logits&quot;</span>]</span><br><span class="line">class_preds = np.argmax(preds, axis=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(preds.shape, class_preds.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line"></span><br><span class="line">metric = evaluate.load(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">metric.compute(predictions=class_preds, references=raw_datasets[<span class="string">&quot;validation&quot;</span>][<span class="string">&quot;label&quot;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="customized-loops">customized loops</h1>
<h2 id="pytorch-1">pytorch</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AdamW, AutoModelForSequenceClassification, get_scheduler</span><br><span class="line"></span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="number">2</span>)</span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">3e-5</span>)</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">num_training_steps = num_epochs * <span class="built_in">len</span>(train_dataloader)</span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">    num_training_steps=num_training_steps,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">progress_bar = tqdm(<span class="built_in">range</span>(num_training_steps))</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line">        loss = outputs.loss</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        progress_bar.update(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h1 id="datasets">Datasets</h1>
<h1 id="导入">导入</h1>
<p>datasets这个库越来越多人使用了，主要在于方便。支持多种数据存储类型，比如json，txt，csv，pockled
dataframes。</p>
<p>对于json类型的数据，最方便的是一行存储一个字典，也就是一个数据sample，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;a&quot;: 1, &quot;b&quot;: 2.0, &quot;c&quot;: &quot;foo&quot;, &quot;d&quot;: false&#125;</span><br><span class="line">&#123;&quot;a&quot;: 4, &quot;b&quot;: -5.5, &quot;c&quot;: null, &quot;d&quot;: true&#125;</span><br></pre></td></tr></table></figure>
<p>导入就用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from datasets import load_dataset</span><br><span class="line">dataset = load_dataset(&quot;json&quot;, data_files=&quot;my_file.json&quot;)</span><br></pre></td></tr></table></figure>
<p>如果json的组织形式是套娃格式的，只需要指定一个field参数就可以了：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;version&quot;: &quot;0.1.0&quot;,</span><br><span class="line"> &quot;data&quot;: [&#123;&quot;a&quot;: 1, &quot;b&quot;: 2.0, &quot;c&quot;: &quot;foo&quot;, &quot;d&quot;: false&#125;,</span><br><span class="line">          &#123;&quot;a&quot;: 4, &quot;b&quot;: -5.5, &quot;c&quot;: null, &quot;d&quot;: true&#125;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">from datasets import load_dataset</span><br><span class="line">dataset = load_dataset(&quot;json&quot;, data_files=&quot;my_file.json&quot;, field=&quot;data&quot;)</span><br></pre></td></tr></table></figure>
<p>炒鸡方便！</p>
<p>不仅如此，它的data_files这个参数很灵活，可以接受一个字符串也就是数据集所在的路径，也可以接受一个list，也可以接受一个字典，字典的key就是你每个数据集的用处比如train，test啥的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_files = &#123;&quot;train&quot;: &quot;SQuAD_it-train.json&quot;, &quot;test&quot;: &quot;SQuAD_it-test.json&quot;&#125;</span><br><span class="line">squad_it_dataset = load_dataset(&quot;json&quot;, data_files=data_files, field=&quot;data&quot;)</span><br><span class="line">squad_it_dataset</span><br></pre></td></tr></table></figure>
<p>还有一个更方便的，可以接受压缩包数据，这样就不用自己手动解压缩了：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_files = &#123;&quot;train&quot;: &quot;SQuAD_it-train.json.gz&quot;, &quot;test&quot;: &quot;SQuAD_it-test.json.gz&quot;&#125;</span><br><span class="line">squad_it_dataset = load_dataset(&quot;json&quot;, data_files=data_files, field=&quot;data&quot;)</span><br></pre></td></tr></table></figure>
<h1 id="process">process</h1>
<p>数据导入进来的，在正式输入进模型训练之前，还需要进一步的进行处理。</p>
<h1 id="推荐阅读">推荐阅读</h1>
<ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/">hugging face NLP
course</a></li>
<li><a target="_blank" rel="noopener" href="https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/ch01.html#idm45146321158640">natural
language processing for transformers</a></li>
<li></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/" class="post-title-link" itemprop="url">LLM评测/Evaluation</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-16 14:33:33" itemprop="dateCreated datePublished" datetime="2023-08-16T14:33:33+08:00">2023-08-16</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2023-08-29 13:24:10" itemprop="dateModified" datetime="2023-08-29T13:24:10+08:00">2023-08-29</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>4.7k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>4 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>最近在关注模型performance评估的问题，打算在这个主题上做一个整理，也是受到很多博客和文章的启发写这篇文章，所以就将所有推荐阅读的文章放在前面，感兴趣的小伙伴可以拓展阅读。</p>
<ol type="1">
<li>老刘说NLP 公众号中8.10发的一篇文章《如何让自己的大模型榜单评分更高》
这篇文章有点借鉴了hugging face的<a target="_blank" rel="noopener" href="https://huggingface.co/blog/zh/evaluating-mmlu-leaderboard">Open
LLM 排行榜近况</a></li>
<li>https://mp.weixin.qq.com/s/WiF3yRU5MZS7ulLTP8FIpw</li>
</ol>
<p>首先说一下这个<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">LLM榜单</a>,
有四个benchmark，其中上面的博客就是重点讲了为什么同样一个模型比如LLaMA在MMLU上评测的结果会不如llama文章中提的效果，trick就在作者使用MMLU这个benchmark的方式有很大不同，这里来看看MMLU这个benchmark。</p>
<h1 id="mmlu-benchmark">MMLU benchmark</h1>
<p>首先看一下这个数据集到底是什么数据集，长什么样子，先给出文章中的定义：</p>
<blockquote>
<p><strong>MMLU</strong> (<strong>Massive Multitask Language
Understanding</strong>) is a new benchmark designed to measure knowledge
acquired during pretraining by evaluating models exclusively in
zero-shot and few-shot settings.</p>
</blockquote>
<p>这个评测集合里包含了57个学科，也就是57个task。原始的数据集长这样，里面的每个问题包含四个可能选项，且每个问题只有一个正确答案。：</p>
<figure>
<img src="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/image-20230816145305589.png" alt="image-20230816145305589">
<figcaption aria-hidden="true">image-20230816145305589</figcaption>
</figure>
<p>可以看到基本上就是question，answer的组织。注意这里看到原始数据的时候我还有点没看明白，作者的readme中也没写，还是对beginner有点不友好，第一列表示question，第二到第四列表示四个选项，最后一列是答案。所以可以看到原作者在<a target="_blank" rel="noopener" href="https://github.com/hendrycks/test/blob/master/evaluate.py">evaluation</a>的代码中这样处理的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">choices = [<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>, <span class="string">&quot;C&quot;</span>, <span class="string">&quot;D&quot;</span>] <span class="comment"># 首先定义选项</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval</span>(<span class="params">args, subject, engine, dev_df, test_df</span>):</span></span><br><span class="line">    cors = []</span><br><span class="line">    all_probs = []</span><br><span class="line">    answers = choices[:test_df.shape[<span class="number">1</span>]-<span class="number">2</span>] <span class="comment"># 对于每一个csv文件读取进来后取answers</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">label = test_df.iloc[i, test_df.shape[<span class="number">1</span>]-<span class="number">1</span>] <span class="comment"># label这里其实是取得最后一列，也就是答案</span></span><br></pre></td></tr></table></figure>
<p>但这个评测数据集在用来评测LLM的过程中衍生出了很多版本，基本是prompt的变化：</p>
<figure>
<img src="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/640.png" alt="MMLU的不同实现">
<figcaption aria-hidden="true">MMLU的不同实现</figcaption>
</figure>
<p>同样的问答对，比如上面的选择题，Harness没有指令，并且衍生的两个版本也就是helm和harness版本还加了Question这个前缀，harness在选线之前还加了Choices。就这么一点差距，就导致同一个llm的出来的分数不一样：</p>
<figure>
<img src="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/LLM-01-bis-01.png" alt="LLM在不同MMLU实现上的评分">
<figcaption aria-hidden="true">LLM在不同MMLU实现上的评分</figcaption>
</figure>
<blockquote>
<p>关于如何使用这个benchmark，参考<a target="_blank" rel="noopener" href="https://github.com/hendrycks/test/blob/master/evaluate.py">MMLU原始实现</a>，作者写的是用chatgpt来产生答案，prompt为：<code>prompt = "The following are multiple choice questions (with answers) about &#123;&#125;.\n\n".format(format_subject(subject))</code></p>
</blockquote>
<p>这三种实现方式不仅prompt的形式不同，也就是上面提到的。并且它在计算F1score的时候的机制也不同。</p>
<ol type="1">
<li>原始实现</li>
</ol>
<p>在原始实现中的评估的代码是这样写的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> ans <span class="keyword">in</span> answers:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        lprobs.append(c[<span class="string">&quot;choices&quot;</span>][<span class="number">0</span>][<span class="string">&quot;logprobs&quot;</span>][<span class="string">&quot;top_logprobs&quot;</span>][-<span class="number">1</span>][<span class="string">&quot; &#123;&#125;&quot;</span>.<span class="built_in">format</span>(ans)]) <span class="comment"># c是chatgpt的回答</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Warning: &#123;&#125; not found. Artificially adding log prob of -100.&quot;</span>.<span class="built_in">format</span>(ans))</span><br><span class="line">    lprobs.append(-<span class="number">100</span>)</span><br><span class="line">    pred = &#123;<span class="number">0</span>: <span class="string">&quot;A&quot;</span>, <span class="number">1</span>: <span class="string">&quot;B&quot;</span>, <span class="number">2</span>: <span class="string">&quot;C&quot;</span>, <span class="number">3</span>: <span class="string">&quot;D&quot;</span>&#125;[np.argmax(lprobs)]</span><br><span class="line">    probs = softmax(np.array(lprobs))</span><br><span class="line"></span><br><span class="line">    cor = pred == label</span><br><span class="line">    cors.append(cor)</span><br><span class="line">    all_probs.append(probs)</span><br></pre></td></tr></table></figure>
<p>该方法在评估的时候，仅仅比较了模型对四个选项字母的预测概率，哪个选项的概率高就选哪个，即便是在极端情况下四个选项的概率值都很低的情况下也会选择某个选项，但其实模型有时候会回答很多不相关的东西（都是很高的概率的token），所以这种方式有点”放水“，整体评估出来的分数会偏高。</p>
<ol start="2" type="1">
<li><a target="_blank" rel="noopener" href="https://github.com/stanford-crfm/helm">HELM实现</a></li>
</ol>
<p>HELM实现是根据模型预测的下一个输出词元的概率来选择输出文本，并将生成的文本与正确答案的文本进行对比。这种方式有效避免了如果模型的答案中出现概率高的token不是选项中的任意一个，那么就会判为错误答案。</p>
<p>看了helm的代码仓库，着实有点丰富。内容很多我都没有找到在哪个文件里做的evaluation的计算，只知道了读取csv的地方。有好心的小伙伴可以私信我告诉我在哪里。</p>
<ol start="3" type="1">
<li>harness实现</li>
</ol>
<p>这是hugging
face的llm榜单所用的实现。它不再是只是统计选项，而是连同选项字母以及后面的答案一起被考虑进来，计算的是整个序列的概率（获取每个词元的概率
(与上面其他实现一样)
并求它们的联合概率），那么很容易一些长文本的联合概率会比短文本的联合概率大，所以作者说可以在联合概率的基础上在做一个归一化，也就是用对数联合概率/
token数。</p>
<figure>
<img src="/2023/08/16/LLM%E8%AF%84%E6%B5%8B-Evaluation/image-20230817100952429.png" alt="MMLU三种实现对于模型输出的总结">
<figcaption aria-hidden="true">MMLU三种实现对于模型输出的总结</figcaption>
</figure>
<p>例如实现如下，基于GPT2计算句子联合概率的一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">data = [</span><br><span class="line">    <span class="string">&quot;A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN)&quot;</span>,</span><br><span class="line">    <span class="string">&quot;The term MLP is used ambiguously, sometimes loosely to any feedforward ANN, sometimes strictly to refer to networks composed of multiple layers of perceptrons (with threshold activation); see § Terminology&quot;</span>,</span><br><span class="line">    <span class="string">&#x27;Multilayer perceptrons are sometimes colloquially referred to as &quot;vanilla&quot; neural networks, especially when they have a single hidden layer.[1]&#x27;</span>,</span><br><span class="line">    <span class="string">&quot;An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function.&quot;</span>,</span><br><span class="line">]</span><br><span class="line">model = transformers.GPT2LMHeadModel.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line">tok = transformers.GPT2Tokenizer.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line">tgs = []</span><br><span class="line"><span class="keyword">for</span> dat <span class="keyword">in</span> data:</span><br><span class="line">    random.seed(dat)</span><br><span class="line">    <span class="comment"># print(model(tok.encode(dat, return_tensors=&quot;pt&quot;))[0][0])</span></span><br><span class="line">    toks = tok.encode(dat, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">    ind = random.randrange(<span class="built_in">len</span>(toks[<span class="number">0</span>]) - <span class="number">1</span>)</span><br><span class="line">    logits = F.log_softmax(model(toks)[<span class="number">0</span>], dim=-<span class="number">1</span>)[:, :-<span class="number">1</span>]  <span class="comment"># [batch, seq, vocab]</span></span><br><span class="line">    res = torch.gather(logits, <span class="number">2</span>, toks[:, <span class="number">1</span>:].unsqueeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    tgs.append(<span class="built_in">float</span>(res[ind:].<span class="built_in">sum</span>()))</span><br></pre></td></tr></table></figure>
<p>在“老刘说NLP”的博客中也提到了一点，就是上面的方式都是开源模型，所以很容易就能得到每一个token的预测概率，所以返回结果可以拆的这么细致来分析。如果是闭源模型只返回response的话，这时候就需要用正则的方式来抽取回答内容里的选项，比如<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.04813">CEVAL</a>的测试方案：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_cot_answer</span>(<span class="params">self, line, gen_ans</span>):</span></span><br><span class="line">    m = re.findall(<span class="string">r&#x27;所以答案是(.+?)。&#x27;</span>, gen_ans, re.M)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(m) &gt; <span class="number">0</span> <span class="keyword">and</span> m[-<span class="number">1</span>] <span class="keyword">in</span> self.choices:</span><br><span class="line">        <span class="keyword">return</span> m[-<span class="number">1</span>], <span class="literal">True</span></span><br><span class="line">    answer_patterns = [</span><br><span class="line">        <span class="string">r&#x27;([ABCD])是正确的&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;选项([ABCD])正确&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;答案为([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;答案是([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;答案([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;选择([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;答案：([ABCD])&#x27;</span>,</span><br><span class="line">        <span class="string">r&#x27;选择答案([ABCD])&#x27;</span></span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># RE extraction</span></span><br><span class="line">    <span class="keyword">for</span> answer_pattern <span class="keyword">in</span> answer_patterns:</span><br><span class="line">        m = re.search(answer_pattern, gen_ans, re.M)</span><br><span class="line">        <span class="keyword">if</span> m:</span><br><span class="line">            answer = m.group(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> answer, <span class="literal">False</span></span><br><span class="line">        <span class="comment"># only containing one choice-character</span></span><br><span class="line">        m = re.findall(<span class="string">r&#x27;[ABCD]&#x27;</span>, gen_ans, re.M)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(m) == <span class="number">1</span>:</span><br><span class="line">            answer = m[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">return</span> answer, <span class="literal">False</span></span><br><span class="line">        answer_word_counter = <span class="number">0</span></span><br><span class="line">        <span class="comment"># only containing one choice-context</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> self.choices:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(line[<span class="string">f&#x27;<span class="subst">&#123;c&#125;</span>&#x27;</span>]) <span class="keyword">in</span> gen_ans:</span><br><span class="line">                answer = c</span><br><span class="line">                answer_word_counter += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> answer_word_counter == <span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">return</span> answer, <span class="literal">False</span></span><br><span class="line">                <span class="keyword">return</span> <span class="string">&#x27;-&#x27;</span>, <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>对CLEVA评测平台感兴趣的可以看原文paper或者参考<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/WiF3yRU5MZS7ulLTP8FIpw">文章</a>。原文说CLEVA是专门为评估中文语言模型而设计的平台。</p>
</blockquote>
<h1 id="section"></h1>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YAN</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Symbols count total: </span>
    <span title="Symbols count total">234k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">3:33</span>
  </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
