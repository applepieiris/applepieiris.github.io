<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=consolas:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.8.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="Be smart! Keep Learning">
<meta property="og:type" content="website">
<meta property="og:title" content="YAN&#39;s Blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="YAN&#39;s Blog">
<meta property="og:description" content="Be smart! Keep Learning">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="YAN">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>YAN's Blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">YAN's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="YAN"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">YAN</p>
  <div class="site-description" itemprop="description">Be smart! Keep Learning</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/applepieiris" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/25/3D-Representations/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/02/25/3D-Representations/" class="post-title-link" itemprop="url">3D Representations</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-02-25 16:04:21" itemprop="dateCreated datePublished" datetime="2025-02-25T16:04:21+08:00">2025-02-25</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2025-02-26 14:49:42" itemprop="dateModified" datetime="2025-02-26T14:49:42+08:00">2025-02-26</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/cv/" itemprop="url" rel="index"><span itemprop="name">cv</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>2.1k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>2 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>3D 视觉中有多个topic，诸如struture form motion, 3D object pose
estimation, 3D
姿态的估计等等，这些topic都建立在能够表示3D数据的基础上，不同的表示方式需要使用不同的处理方式。<a target="_blank" rel="noopener" href="https://cs231n.stanford.edu/schedule.html">cs231n</a>课程中对3D视觉部分只放了一讲:
3D shape
representations。我就按照这个lecture的脉络把相关知识点记录下来，就当自己也把这部分的知识学习一遍。相关的review推荐：<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.06475">3D Representation Methods: A
Survey</a></p>
<h1 id="depth-map">Depth Map</h1>
<p>深度估计。对于一张图片来说，depth
map是一个和静态图片一样长宽的，记录了那一个pixel相对于camera的距离。一个静态有颜色的图片shape是：
3*H*W</p>
<p>, 那么一个depth map的shape就是： 1*H*W。</p>
<p>一个2.5D的 RGB-D Image： <code>RGB image + Depth map</code></p>
<h1 id="voxels-体素">Voxels 体素</h1>
<p>关于体素的解释：</p>
<ul>
<li>https://en.wikipedia.org/wiki/Voxel</li>
</ul>
<blockquote>
<p>Voxel grid representation is a method for modeling 3D objects where
the space is divided into a regular grid of cubes, known as voxels
(volume elements). Each voxel in the grid can store information such as
color, density, or material properties, allowing for a detailed
volumetric representation of the object.</p>
</blockquote>
<p>那么如何从一张静态图片生成一个体素呢？lecture推荐了Choy的一篇文章：3D-R2N2:
A Unified Approach for Single and Multi-view 3D Object
Reconstruction。在3D
Reconstruction这个topic下也有好心的网友整理了一系列的<a target="_blank" rel="noopener" href="https://github.com/bluestyle97/awesome-3d-reconstruction-papers?tab=readme-ov-file">paper</a>，3D-R2N2也在其中,
该仓库作者将其分类为multi-view。</p>
<figure>
<img src="/2025/02/25/3D-Representations/image-20250226100747562.png" alt="image-20250226100747562">
<figcaption aria-hidden="true">image-20250226100747562</figcaption>
</figure>
<p>这篇文章所做的就是：输入一张图片，该网络会生成这张图片中object的3D
voxel。</p>
<figure>
<img src="/2025/02/25/3D-Representations/image-20250226100921435.png" alt="image-20250226100921435">
<figcaption aria-hidden="true">image-20250226100921435</figcaption>
</figure>
<p>其中loss采用的是：per-voxel
cross-entropy，这部分chatgpt的解释为：</p>
<p>文章中使用的 <strong>per-voxel cross-entropy loss</strong>
是一个针对每个体素的交叉熵损失，常用于
<strong>二分类问题</strong>，即每个体素是否被占据。</p>
<figure>
<img src="/2025/02/25/3D-Representations/image-20250226101227750.png" alt="image-20250226101227750">
<figcaption aria-hidden="true">image-20250226101227750</figcaption>
</figure>
<p><a target="_blank" rel="noopener" href="https://github.com/chrischoy/3D-R2N2/blob/master/lib/layers.py#L578">3D-R2N2</a>官方仓库代码对loss的写法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SoftmaxWithLoss3D</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Softmax with loss (n_batch, n_vox, n_label, n_vox, n_vox)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        self.<span class="built_in">input</span> = <span class="built_in">input</span></span><br><span class="line">        self.exp_x = tensor.exp(self.<span class="built_in">input</span>)</span><br><span class="line">        self.sum_exp_x = tensor.<span class="built_in">sum</span>(self.exp_x, axis=<span class="number">2</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prediction</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.exp_x / self.sum_exp_x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">error</span>(<span class="params">self, y, threshold=<span class="number">0.5</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> tensor.mean(tensor.eq(tensor.ge(self.prediction(), threshold), y))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        y must be a tensor that has the same dimensions as the input. For each</span></span><br><span class="line"><span class="string">        channel, only one element is one indicating the ground truth prediction</span></span><br><span class="line"><span class="string">        label.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> tensor.mean(</span><br><span class="line">            tensor.<span class="built_in">sum</span>(-y * self.<span class="built_in">input</span>, axis=<span class="number">2</span>, keepdims=<span class="literal">True</span>) + tensor.log(self.sum_exp_x))</span><br></pre></td></tr></table></figure>
<p>prediction那里看到作者用的是softmax，并没有用sigmoid，毕竟是一个二分类问题，这里用softmax和sigmoid等价。</p>
<h1 id="pointcloud">Pointcloud</h1>
<p>点云很好理解，就是将上面的voxels换成了点。依旧是两个问题：1.
如何生成一个点云的3D图 2. 如何处理点云表示的3D图</p>
<p>我们先解决第一个问题，<a target="_blank" rel="noopener" href="http://arxiv.org/abs/1612.00603">A
Point Set Generation Network for 3D Object Reconstruction from a Single
Image</a> 这篇文章是第一个用deep learning的方式用来生成point set。</p>
<h1 id="mesh">Mesh</h1>
<h1 id="implicit-surface">Implicit Surface</h1>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/01/09/Novel-view-synthesis-NVS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/01/09/Novel-view-synthesis-NVS/" class="post-title-link" itemprop="url">Novel view synthesis(NVS)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-01-09 10:17:18" itemprop="dateCreated datePublished" datetime="2025-01-09T10:17:18+08:00">2025-01-09</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2025-02-25 10:29:13" itemprop="dateModified" datetime="2025-02-25T10:29:13+08:00">2025-02-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/cv/" itemprop="url" rel="index"><span itemprop="name">cv</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>8.5k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>8 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>最近刚接触到这个话题，想开一篇新博客来记录一下自己学习的过程。</p>
<p>首先我是看了两篇review了解了这个topic的主要任务：</p>
<ul>
<li>advancements in radiance field techniques for volumetric video
generation: a technical overview</li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05658">From Capture to Display: A
Survey on Volumetric Video</a></li>
</ul>
<p>另外同步阅读了huggingface的tutorial：https://huggingface.co/learn/computer-vision-course/unit8/3d-vision/nvs。这篇博客将NVS描述为这样一个任务：</p>
<blockquote>
<p>generate views from new camera angles that are plausibly consistent
with a set of images.</p>
</blockquote>
<p>我们在对一个场景进行3D还原时，首先的输入是一系列相机在不同的视角拍摄的静态图片，通过这些图片我们对该场景下的人物以及物体进行3D建模，但相机个数是有限的，如何推算出某个没有相机的角度上的view，这就是NVS这个任务要做的事情。</p>
<p>很多方法在这个topic上提出来，大致可以分成两类：<em>1）generate an
intermediate three-dimensional representation, which is rendered from a
new viewing direction. 比如PixelNeFRF 2）direclty generated new views
without an intermediate 3D representaion</em>， 比如Zero123</p>
<h1 id="nerf">NeRF</h1>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2003.08934">NeRF: Representing Scenes
as Neural Radiance Fields for View Synthesis</a></p>
<p>2020年出的一篇文章，下面这句话就是它这个算法的精华：</p>
<blockquote>
<p>Our algorithm represents a scene using a fully-connected
(nonconvolutional) deep network, whose input is a single continuous 5D
coordinate (spatial location (x, y, z) and viewing direction (θ, φ)) and
whose output is the volume density and view-dependent emitted radiance
at that spatial location</p>
</blockquote>
<h2 id="llff-数据集">LLFF 数据集</h2>
<p>在查看NeRF的<a target="_blank" rel="noopener" href="https://github.com/yenchenlin/nerf-pytorch">github
code(Pytorch)</a>时，也有tensorflow版本，移步<a target="_blank" rel="noopener" href="https://github.com/bmild/nerf">官方repo</a>。发现作者使用了两个数据集，其中一个就是LLFF，本着学习的原则，先把<a target="_blank" rel="noopener" href="https://github.com/Fyusion/LLFF">LLFF数据集</a>搞清楚。</p>
<p>LLFF全称为Local light Field
Fusion，也是提出了一个NVS的算法。LLFF的主旨思想是：</p>
<blockquote>
<p>present a simple and reliable method for view synthesis from a set of
input images captured by a handheld camera in an irregular grid
pattern.</p>
</blockquote>
<p>简单说就是：该方法可以从一系列手持拍摄的静态图片生成一个scene，这个scene可以理解为一个3D的场景，可以用VR眼镜看的那种。</p>
<p>该<a href="(https://github.com/Fyusion/LLFF)">LLFF
repo</a>提供了非常详细的安装教程，令我比较感兴趣的是，它可以基于自己拍摄的一些静态图片生成一个scene。先来看看它的这份代码。</p>
<p>我们的输入是从一系列的images开始的，首先第一步</p>
<ol type="1">
<li>recover cammera poses</li>
</ol>
<p>这一步采用<a target="_blank" rel="noopener" href="https://github.com/colmap/colmap?tab=readme-ov-file">COLMAP</a>
实现了一个 struture from
emotion的pipeline。这一步的输入是一系列的静态图像，输出的是这个场景下的
6-DoF camera poses和 near/far depth bounds。</p>
<hr>
<p>Structure-from-Motion (SfM) <strong>is the process of reconstructing
3D structure from its projections into a series of images. The input is
a set of overlapping images of the same object, taken from different
viewpoints. The output is a 3-D reconstruction of the object, and the
reconstructed intrinsic and extrinsic camera parameters of all
images.</strong></p>
<figure>
<img src="/2025/01/09/Novel-view-synthesis-NVS/incremental-sfm.png" alt="incremental-sfm">
<figcaption aria-hidden="true">incremental-sfm</figcaption>
</figure>
<p>COLMAP使用的方法依赖于<a target="_blank" rel="noopener" href="http://ieeexplore.ieee.org/document/7780814/">Structure-from-Motion
Revisited</a>这篇文章，它将SfM分成三个步骤：</p>
<ul>
<li>feature detection and extraction</li>
</ul>
<p>这一步好理解，特征抽取，利用一个<code>apprearance descriptor f</code></p>
<ul>
<li>feature matching and geometric verification</li>
</ul>
<p><em>feature matching</em>利用前一步的features找出这一系列图片中的same
scene part。原文是这样写的：</p>
<blockquote>
<p>The na ̈ıve approach tests every image pair for scene overlap; it
searches for feature correspondences by finding the most similar feature
in image I(a) for every feature in image I(b), using a similarity metric
comparing the appearance fj of the features</p>
</blockquote>
<p>简单理解就是根据上一步抽取的features去一一比对每一对image
pair，寻找出每一对image pair中的相似feature，从而找出same scene
part。</p>
<p><em>geometric verification</em>
我觉得有点稍微难理解。上一步只是确认了每一张图片中在apperance上相似的scene
part，但有可能不是指代的这个场景下的同一个object（Point）,
所以就需要去verify上一步的match是否准确，怎么verify呢？通过projective
geometry去预估transformation</p>
<blockquote>
<p>Since matching is based solely on appearance, it is not guaranteed
that corresponding features actually map to the same scene point</p>
</blockquote>
<ul>
<li>structure and motion reconstruction</li>
</ul>
<hr>
<p>LLFF中实现上述第一步骤的脚本为<code>imgs2poses.py</code>,
看该源代码就是基于COLMAP来做的。试着运行该脚本，测试数据用repo内的<code>download_data.sh</code>下载的数据。运行完后可以看到如下输出：</p>
<figure>
<img src="/2025/01/09/Novel-view-synthesis-NVS/image-20250114162420847.png" alt="image-20250114162420847">
<figcaption aria-hidden="true">image-20250114162420847</figcaption>
</figure>
<p>其中images是source
images，testscene下除了images这个文件夹，其他都是COLMAP生成的。具体含义参考COLMAP的<a target="_blank" rel="noopener" href="https://colmap.github.io/tutorial.html#data-structure">document</a></p>
<p>logs文件内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Need to run COLMAP</span><br><span class="line">Features extracted</span><br><span class="line">Features matched</span><br><span class="line">Sparse map created</span><br><span class="line">Finished running COLMAP, see data/testscene/colmap_output.txt for logs</span><br><span class="line">Post-colmap</span><br><span class="line">(&#x27;Cameras&#x27;, 5)</span><br><span class="line">(&#x27;Images #&#x27;, 20)</span><br><span class="line">(&#x27;Points&#x27;, (9906, 3), &#x27;Visibility&#x27;, (9906, 20))</span><br><span class="line">(&#x27;Depth stats&#x27;, 13.732739125795911, 118.85217973695897, 30.413495856274356)</span><br><span class="line">Done with imgs2poses</span><br></pre></td></tr></table></figure>
<p>仔细分析一下<code>imgs2poses.py</code>，<strong>一共用COLMAP执行了三条terminal命令</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># extract features</span></span><br><span class="line">feature_extractor_args = [</span><br><span class="line">        <span class="string">&#x27;colmap&#x27;</span>, <span class="string">&#x27;feature_extractor&#x27;</span>, </span><br><span class="line">            <span class="string">&#x27;--database_path&#x27;</span>, os.path.join(basedir, <span class="string">&#x27;database.db&#x27;</span>), </span><br><span class="line">            <span class="string">&#x27;--image_path&#x27;</span>, os.path.join(basedir, <span class="string">&#x27;images&#x27;</span>),</span><br><span class="line">            <span class="string">&#x27;--ImageReader.single_camera&#x27;</span>, <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">            <span class="comment"># &#x27;--SiftExtraction.use_gpu&#x27;, &#x27;0&#x27;,</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># matching</span></span><br><span class="line">exhaustive_matcher_args = [</span><br><span class="line">        <span class="string">&#x27;colmap&#x27;</span>, match_type, </span><br><span class="line">            <span class="string">&#x27;--database_path&#x27;</span>, os.path.join(basedir, <span class="string">&#x27;database.db&#x27;</span>), </span><br><span class="line">    ]</span><br><span class="line"><span class="comment"># Sparse map create</span></span><br><span class="line">mapper_args = [</span><br><span class="line">        <span class="string">&#x27;colmap&#x27;</span>, <span class="string">&#x27;mapper&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;--database_path&#x27;</span>, os.path.join(basedir, <span class="string">&#x27;database.db&#x27;</span>),</span><br><span class="line">            <span class="string">&#x27;--image_path&#x27;</span>, os.path.join(basedir, <span class="string">&#x27;images&#x27;</span>),</span><br><span class="line">            <span class="string">&#x27;--output_path&#x27;</span>, os.path.join(basedir, <span class="string">&#x27;sparse&#x27;</span>), <span class="comment"># --export_path changed to --output_path in colmap 3.6</span></span><br><span class="line">            <span class="string">&#x27;--Mapper.num_threads&#x27;</span>, <span class="string">&#x27;16&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;--Mapper.init_min_tri_angle&#x27;</span>, <span class="string">&#x27;4&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;--Mapper.multiple_models&#x27;</span>, <span class="string">&#x27;0&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;--Mapper.extract_colors&#x27;</span>, <span class="string">&#x27;0&#x27;</span>,</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure>
<p>对照colmap cli的<a target="_blank" rel="noopener" href="https://colmap.github.io/cli.html">guidebook</a>,
作者使用了前三个命令，dense部分没有继续生成。想要知道它output出来的这些.bin文件含义，需要搞明白<code>database.db</code>内有什么东西，它是feature
extraction的产物。db文件可以用vscode的插件打开，它包含7个table：</p>
<figure>
<img src="/2025/01/09/Novel-view-synthesis-NVS/image-20250115133036561.png" alt="image-20250115133036561">
<figcaption aria-hidden="true">image-20250115133036561</figcaption>
</figure>
<p>keypoints表格中，我下图截图的data部分里才是所有feature的信息，内有每一个feature所在的X，Y坐标。</p>
<blockquote>
<p>COLMAP uses the convention that the upper left image corner has
coordinate (0, 0) and the center of the upper left most pixel has
coordinate (0.5, 0.5)</p>
<p>COLMAP在表示图像坐标时，采用了一种特定的坐标系统，其中图像的左上角被定义为坐标原点
(0, 0)。而“the center of the upper left most pixel has coordinate (0.5,
0.5)” 指的是图像中最左上角的像素的中心位置被赋予了坐标 (0.5, 0.5)。</p>
</blockquote>
<figure>
<img src="/2025/01/09/Novel-view-synthesis-NVS/image-20250115133949032.png" alt="image-20250115133949032">
<figcaption aria-hidden="true">image-20250115133949032</figcaption>
</figure>
<p>在这两张表格中，rows表示的数值是number of detected features per
image, 如果rows=0, 那么这个image没有feature</p>
<p>在运行命令<code>colmap exhaustive_matcher --database_path ./data/testscene/database.db</code>后，db文件内的matchs这张表会出现值（之前没有），每一行会表示一张图片和另外一张图片的匹配结果，rows的值表示match上特征点的个数。</p>
<p>第三个命令<code>colmap mapper...</code>实现了sparse
3D的重建，在前面两个命令产生的结果上(feature
extraction&amp;match)。运行完第三条命令后，文件夹内的内容是这样的：</p>
<figure>
<img src="/2025/01/09/Novel-view-synthesis-NVS/image-20250212142310347.png" alt="image-20250212142310347">
<figcaption aria-hidden="true">image-20250212142310347</figcaption>
</figure>
<p>第三条命令产生的结果放在<code>sparse</code>文件夹内。</p>
<p>如果用LLFF
repo内的<code>imgs2poses.py</code>，脚本运行后会发现在testscene文件夹下还会出现一个<code>poses_bounds.npy</code>的文件，查看<code>imgs2poses.py</code>中的gen_poses方法，会发现在执行完colmap的上面三条命令后又再次运行了<code>load_colmap_data()</code>和<code>save_poses()</code>。其中的<code>load_colmap_data()</code>对colmap三条命令的结果，也就是三个bin文件做了读取和处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_colmap_data</span>(<span class="params">realdir</span>):</span></span><br><span class="line">    <span class="comment"># 首先读取cameras.bin，提取内参 height, width, focal</span></span><br><span class="line">    camerasfile = os.path.join(realdir, <span class="string">&#x27;sparse/0/cameras.bin&#x27;</span>)</span><br><span class="line">    camdata = read_model.read_cameras_binary(camerasfile)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># cam = camdata[camdata.keys()[0]]</span></span><br><span class="line">    list_of_keys = <span class="built_in">list</span>(camdata.keys())</span><br><span class="line">    cam = camdata[list_of_keys[<span class="number">0</span>]]</span><br><span class="line">    <span class="built_in">print</span>( <span class="string">&#x27;Cameras&#x27;</span>, <span class="built_in">len</span>(cam))</span><br><span class="line"></span><br><span class="line">    h, w, f = cam.height, cam.width, cam.params[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># w, h, f = factor * w, factor * h, factor * f</span></span><br><span class="line">    hwf = np.array([h,w,f]).reshape([<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    imagesfile = os.path.join(realdir, <span class="string">&#x27;sparse/0/images.bin&#x27;</span>)</span><br><span class="line">    imdata = read_model.read_images_binary(imagesfile)</span><br><span class="line">    </span><br><span class="line">    w2c_mats = []</span><br><span class="line">    bottom = np.array([<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1.</span>]).reshape([<span class="number">1</span>,<span class="number">4</span>])</span><br><span class="line">    </span><br><span class="line">    names = [imdata[k].name <span class="keyword">for</span> k <span class="keyword">in</span> imdata]</span><br><span class="line">    <span class="built_in">print</span>( <span class="string">&#x27;Images #&#x27;</span>, <span class="built_in">len</span>(names))</span><br><span class="line">    perm = np.argsort(names)</span><br><span class="line">    <span class="comment"># 这里 im.qvec2rotmat() 是从四元数计算出 3x3 旋转矩阵，im.tvec 是 3x1 平移向量。这两者合并成 4x4 的 world-to-camera 变换矩阵，并存入 w2c_mats</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> imdata:</span><br><span class="line">        im = imdata[k]</span><br><span class="line">        R = im.qvec2rotmat()</span><br><span class="line">        t = im.tvec.reshape([<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line">        m = np.concatenate([np.concatenate([R, t], <span class="number">1</span>), bottom], <span class="number">0</span>)</span><br><span class="line">        w2c_mats.append(m)</span><br><span class="line">    </span><br><span class="line">    w2c_mats = np.stack(w2c_mats, <span class="number">0</span>)</span><br><span class="line">    c2w_mats = np.linalg.inv(w2c_mats)</span><br><span class="line">    </span><br><span class="line">    poses = c2w_mats[:, :<span class="number">3</span>, :<span class="number">4</span>].transpose([<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>]) <span class="comment"># # 取3x4部分（旋转 + 平移）</span></span><br><span class="line">    poses = np.concatenate([poses, np.tile(hwf[..., np.newaxis], [<span class="number">1</span>,<span class="number">1</span>,poses.shape[-<span class="number">1</span>]])], <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    points3dfile = os.path.join(realdir, <span class="string">&#x27;sparse/0/points3D.bin&#x27;</span>)</span><br><span class="line">    pts3d = read_model.read_points3d_binary(points3dfile)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># must switch to [-u, r, -t] from [r, -u, t], NOT [r, u, -t]</span></span><br><span class="line">    <span class="comment"># 此时 poses.shape = (3,5,N)，但它的坐标轴方向仍然是 COLMAP 的 [right, down, forwards]，我们需要转换成 [down, right, backwards]</span></span><br><span class="line">    poses = np.concatenate([poses[:, <span class="number">1</span>:<span class="number">2</span>, :], poses[:, <span class="number">0</span>:<span class="number">1</span>, :], -poses[:, <span class="number">2</span>:<span class="number">3</span>, :], poses[:, <span class="number">3</span>:<span class="number">4</span>, :], poses[:, <span class="number">4</span>:<span class="number">5</span>, :]], <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> poses, pts3d, perm</span><br></pre></td></tr></table></figure>
<p>这里readme中也对<code>pose_bounds.npy</code>的内容进行了解释。这个文件很重要，可以看到nerf中导入LLFF数据集也是从该文件中导入的数据，导入方式只有三行代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">poses_arr = np.load(os.path.join(basedir, <span class="string">&#x27;poses_bounds.npy&#x27;</span>))</span><br><span class="line">poses = poses_arr[:, :-<span class="number">2</span>].reshape([-<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>]).transpose([<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>])</span><br><span class="line">bds = poses_arr[:, -<span class="number">2</span>:].transpose([<span class="number">1</span>,<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>我本来也是先看的nerf的代码，看到这里导入LLFF数据集的方式没看懂，主要是不知道pose_bounds.npy内存储的内容的具体含义，所以才补充了LLFF的上面的诸多知识空白。LLFF的作者在readme中对这个文件的含义进行了解释：</p>
</blockquote>
<p><strong><em>This file stores a numpy array of size Nx17 (where N is
the number of input images).Each row of length 17 gets reshaped into a
3x5 pose matrix and 2 depth values that bound the closest and farthest
scene content from that point of view.</em></strong></p>
<p>这里有一个比较重要的概念”相机的姿态“(cammera
poses)，可以参考阅读：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://yunyang1994.github.io/2019/12/27/%E4%BB%80%E4%B9%88%E6%98%AF%E7%9B%B8%E6%9C%BA%E7%9A%84%E4%BD%8D%E5%A7%BF/">什么是相机的位姿
？</a></li>
</ul>
<blockquote>
<p>其中 <code>T_&#123;cw&#125;</code>
为该点从世界坐标系变换到相机坐标系的变换矩阵， <code>T_&#123;wc&#125;</code>
为该点从相机坐标系变换到世界坐标系的变换矩阵。<strong>它们二者都可以用来表示相机的位姿，前者称为相机的外参</strong>。</p>
<p>实践当中使用 <code>T_&#123;cw&#125;</code>
来表示相机位姿更加常见。然而在可视化程序中使用 <code>T_&#123;wc&#125;</code>
来表示相机位姿更为直观，因为此时它的平移向量即为相机原点在世界坐标系中的坐标。视觉
Slam 十四讲中的第五讲的 joinMap 使用的就是 <code>T_&#123;wc&#125;</code>
来表示相机位姿进行点云拼接。</p>
</blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.mkari.de/posts/cam-transform/">Camera
Conventions, Transforms, and Conversions</a></li>
</ul>
<p><code>poses = poses_arr[:, :-2].reshape([-1, 3, 5]).transpose([1,2,0])</code>
取前15个数，去掉最后2个near和far，并reshape成（N，3，5）的矩阵：</p>
<p>┌ ┐ │ R11 R12 R13 t1 hwf1 │ │ R21 R22 R23 t2 hwf2 │ │ R31 R32 R33 t3
hwf3 │ └ ┘</p>
<p>其中R部分是旋转矩阵，t是平移向量。</p>
<p><strong>最终
<code>poses.shape = (3, 5, N)</code>，其中</strong>：</p>
<ul>
<li><code>poses[:, :4, :]</code> → <code>3x4</code> 的
<strong>相机外参</strong> (<code>R|t</code>)</li>
<li><code>poses[:, 4:5, :]</code> → <code>3x1</code> 的
<strong>相机内参</strong> <code>[h, w, f]</code></li>
</ul>
<p><code>bds = poses_arr[:, -2:].transpose([1,0])</code>：取出
<strong>最后两列</strong>，即 <code>near</code> 和 <code>far</code>
深度边界，形状是 <code>(N, 2)</code>。</p>
<p><strong><code>.transpose([1,0])</code></strong>：交换轴顺序，使
<code>bds.shape = (2, N)</code>：</p>
<ul>
<li><strong>第一维 (2)</strong>：表示 <strong>最近 (<code>near</code>)
和 最远 (<code>far</code>) 深度</strong></li>
<li><strong>第二维 (N)</strong>：对应 <strong>N 张图像</strong></li>
</ul>
<ol start="2" type="1">
<li>Generate MPIs</li>
</ol>
<p>脚本为<code>imgs2mpi2.py</code>。这一步是采用训练好的tensorflow模型对第一步骤的结果进行MPI的生成。这一步实现的结果如下：</p>
<figure>
<img src="/2025/01/09/Novel-view-synthesis-NVS/image-20250212153512883.png" alt="image-20250212153512883">
<figcaption aria-hidden="true">image-20250212153512883</figcaption>
</figure>
<ol start="3" type="1">
<li>Render novel views</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/08/13/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/08/13/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">生成模型</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-08-13 14:04:23" itemprop="dateCreated datePublished" datetime="2024-08-13T14:04:23+08:00">2024-08-13</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2024-12-04 15:30:33" itemprop="dateModified" datetime="2024-12-04T15:30:33+08:00">2024-12-04</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>2.3k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>2 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>在这个全民皆Difussion
Model的时代，已经有人忘却了曾经的王者GAN。这篇博客是自己记录学习生成模型，这里的生成模型仅局限于生成图片的模型，不是LLM这一类自然语言生成模型。启发点有两个：
-
在斯坦福cs231n这节课里，GAN这一章节详细讲解了从VAE到GAN的发展脉络，并没有延申到Difussion
Model。Difussion Model的内容放到cs236中去讲了。 - Lilian
Blog曾写过一篇<a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What
are Diffusion Models?</a>, 内扩展了两篇介绍: GAN 和 VAE。</p>
<h1 id="self-supervised-learning">Self-supervised Learning</h1>
<p>自监督学习中最出彩的就是对比学习。</p>
<h2 id="contrastive-learning">Contrastive Learning</h2>
<p>参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-05-31-contrastive/">Contrastive
Representation Learning</a></li>
<li><a target="_blank" rel="noopener" href="https://shairozsohail.medium.com/contrastive-representation-learning-a-comprehensive-guide-part-1-foundations-90c1944dbd1e">Contrastive
Representation Learning — A Comprehensive Guide (part 1,
foundations)</a></li>
</ul>
<blockquote>
<p>The goal of contrastive representation learning is to learn such an
embedding space in which similar sample pairs stay close to each other
while dissimilar ones are far apart.</p>
</blockquote>
<p>对比学习的本质思想是：将positive的sample和negtive的sample距离分割的越远。cs231n中的lecture12发展逻辑捋的很好，是因为需要一个更general的pretext
task:</p>
<p><img src="/2024/08/13/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/image-20240904095209899.png" alt="image-20240904095209899">之前的pretext task都是基于"visual common
sense"，例如预测rotations，画面修复inpainting, 颜色填充colorization等，
造成的问题是"learned representations may not be general"。</p>
<p>在contrastive
learning中有一个loss比较重要：<code>infoNCE</code>，是在<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.03748">Representation Learning with
Contrastive Predictive
Coding</a>文章中提到的，这个loss就是为了上面的本质思想量身定制，如何将一个class的sample拉的更近，而不属于一个class的sample拉的更远呢？</p>
<figure>
<img src="/2024/08/13/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/image-20240904133326653-17254280087461.png" alt="image-20240904133326653">
<figcaption aria-hidden="true">image-20240904133326653</figcaption>
</figure>
<p>这里我贴一下<code>chatgpt</code>对于上述公式为什么可以作为损失函数的解释，比我自己组织的语言要好：</p>
<figure>
<img src="/2024/08/13/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/image-20240904133608437-17254281697663.png" alt="image-20240904133551625">
<figcaption aria-hidden="true">image-20240904133551625</figcaption>
</figure>
<p>那么我们有了基本思想和loss函数之后如何训练这个网络呢？<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.05709">A Simple Framework for
Contrastive Learning of Visual Representations</a>
这篇文章给我们介绍了一个<code>SimCLR</code>，可以重点阅读一下。文章内给出了算法伪代码：</p>
<figure>
<img src="/2024/08/13/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/image-20240904134006818.png" alt="image-20240904134006818">
<figcaption aria-hidden="true">image-20240904134006818</figcaption>
</figure>
<p>SimCLR也披露了自己的训练代码：https://github.com/google-research/simclr?tab=readme-ov-file</p>
<p>SimCLR属于<strong>Instance contrastive
learning</strong>中的一种，包括he Kaiming团队所出的MOCO以及MOCO
V2，与之相对应的是另外一种contrastive learning: <strong>Sequence
contrastive learning</strong>, 代表为CPC（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.03748">Representation Learning with
Contrastive Predictive Coding</a>）。其实很好理解，如果接触过image
segmentation这个任务的话，会记得在图像分割这个任务里有两种segmentation，一种是instance
segmentation，一种是semantic segmentation。contrastive
learning中的instance对比学习就是如上面图示，猫的图片它是一个class，我们将猫的图片和狗的图片的距离变大，而让猫和猫的图片的”距离“变小。对于sequence对比学习，顾名思义就是加入了序列的影响：</p>
<figure>
<img src="/2024/08/13/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/image-20240904144210318.png" alt="image-20240904144210318">
<figcaption aria-hidden="true">image-20240904144210318</figcaption>
</figure>
<h1 id="generative-modeling">Generative Modeling</h1>
<p>generative
modeling被认为是自监督学习的一种，但他们俩的目的不一样，前者是希望建立一个模型，我们用这个模型可以生成一些diverse和realistic的图片，后者是希望通过自监督的representation
learning去生成图片更好的features，用这些features去帮助下游任务拥有更好的performance。<img src="/2024/08/13/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/image-20241016103604877.png" alt="image-20241016103604877"></p>
<p>强烈建议阅读<a target="_blank" rel="noopener" href="http://arxiv.org/abs/1701.00160">NIPS 2016
Tutorial: Generative Adversarial Networks</a>,
作者将拟合Pmodel的方式分成两种：</p>
<figure>
<img src="/2024/08/13/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/image-20241016103757996.png" alt="image-20241016103757996">
<figcaption aria-hidden="true">image-20241016103757996</figcaption>
</figure>
<p>其中Explicit Density中可以Tractable
density的PixelRNN是我们所熟知的，我对于Tractable
Density的理解就是可以求导并利用梯度下降去一点点降低loss的函数，那么对于FVBN来说，拟合Pmodel的方式就是：一张图片的概率等于所有这张图片上pixel的联合概率<img src="/2024/08/13/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/image-20241016105016392.png" alt="image-20241016105016392"></p>
<p>而近似估算中的VAE则采用的是一种引入潜在变量z的方式来间接的建模数据，导致密度函数不可解，必须采用变分推断等近似方法。那么为什么要引入潜在变量z呢？</p>
<figure>
<img src="/2024/08/13/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/image-20241016111401857.png" alt="image-20241016111401857">
<figcaption aria-hidden="true">image-20241016111401857</figcaption>
</figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YAN</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Symbols count total: </span>
    <span title="Symbols count total">216k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">3:16</span>
  </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
