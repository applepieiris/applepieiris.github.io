<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto+Sans+SC:300,300italic,400,400italic,700,700italic%7CArial:300,300italic,400,400italic,700,700italic%7Csans-serif:300,300italic,400,400italic,700,700italic%7CPingFang+SC:300,300italic,400,400italic,700,700italic%7CMicrosoft+YaHei:300,300italic,400,400italic,700,700italic%7CSource+Han+Serif:300,300italic,400,400italic,700,700italic%7Cserif:300,300italic,400,400italic,700,700italic%7CFira+Code:300,300italic,400,400italic,700,700italic%7CConsolas:300,300italic,400,400italic,700,700italic%7Cmonospace:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.8.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="这篇博客主要记录博主在探索meta开源的llama大模型，包括它的数据，training代码以及评测方法。之所以想记录下来，主要是因为llama的论文写的极其的细致，完美践行了开源这个词(感谢meta!)，第二个原因是它的文档以及社区都很活跃，使用人群广泛，我们可以借助很多中文社区的复现情况去一探大公司在实现一个大模型时候的考量，以及思考它为什么会这样做。 之前写过一篇关于斯坦福的alpaca的">
<meta property="og:type" content="article">
<meta property="og:title" content="LLaMA系列模型浅析">
<meta property="og:url" content="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/index.html">
<meta property="og:site_name" content="YAN&#39;s Blog">
<meta property="og:description" content="这篇博客主要记录博主在探索meta开源的llama大模型，包括它的数据，training代码以及评测方法。之所以想记录下来，主要是因为llama的论文写的极其的细致，完美践行了开源这个词(感谢meta!)，第二个原因是它的文档以及社区都很活跃，使用人群广泛，我们可以借助很多中文社区的复现情况去一探大公司在实现一个大模型时候的考量，以及思考它为什么会这样做。 之前写过一篇关于斯坦福的alpaca的">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231212161342629.png">
<meta property="og:image" content="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231212162349967.png">
<meta property="og:image" content="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231213162604171.png">
<meta property="og:image" content="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/0_K45DoPRbhC5-dqq1-17025178394161.webp">
<meta property="og:image" content="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214093939577.png">
<meta property="og:image" content="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214094523587.png">
<meta property="og:image" content="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214100917596.png">
<meta property="og:image" content="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214103540564.png">
<meta property="og:image" content="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231215135557606.png">
<meta property="og:image" content="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231215152735803.png">
<meta property="og:image" content="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231215152803294.png">
<meta property="article:published_time" content="2023-12-12T06:56:24.000Z">
<meta property="article:modified_time" content="2024-08-30T01:50:10.828Z">
<meta property="article:author" content="YAN">
<meta property="article:tag" content="nlp">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231212161342629.png">


<link rel="canonical" href="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/","path":"2023/12/12/LLaMA系列模型浅析/","title":"LLaMA系列模型浅析"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LLaMA系列模型浅析 | YAN's Blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">YAN's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#chinese-llama"><span class="nav-number">1.</span> <span class="nav-text">Chinese LLaMA</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#llama"><span class="nav-number">2.</span> <span class="nav-text">LLaMA</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8B%93%E5%B1%95%E8%A1%A5%E5%85%85%E4%BB%8B%E7%BB%8D"><span class="nav-number">2.1.</span> <span class="nav-text">拓展补充介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#rmsnorm"><span class="nav-number">2.1.1.</span> <span class="nav-text">RMSNorm</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#batch-normalization"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">Batch Normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#layer-normalization"><span class="nav-number">2.1.1.2.</span> <span class="nav-text">Layer Normalization</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#swiglu"><span class="nav-number">2.1.2.</span> <span class="nav-text">SwiGLU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rotary-embedding-rope"><span class="nav-number">2.1.3.</span> <span class="nav-text">Rotary Embedding, RoPE</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#attention-is-all-you-need%E4%B8%AD%E7%9A%84position-embedding"><span class="nav-number">2.1.3.1.</span> <span class="nav-text">Attention is
All you need中的position embedding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#roperotary-position-embedding"><span class="nav-number">2.1.3.2.</span> <span class="nav-text">RoPE(rotary Position
Embedding)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#grouped-query-attention-gqa"><span class="nav-number">2.1.4.</span> <span class="nav-text">Grouped-Query Attention (GQA)</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="YAN"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">YAN</p>
  <div class="site-description" itemprop="description">Be smart! Keep Learning</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">49</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/applepieiris" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;applepieiris" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/applepieiris" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          LLaMA系列模型浅析
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-12-12 14:56:24" itemprop="dateCreated datePublished" datetime="2023-12-12T14:56:24+08:00">2023-12-12</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2024-08-30 09:50:10" itemprop="dateModified" datetime="2024-08-30T09:50:10+08:00">2024-08-30</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>10 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>这篇博客主要记录博主在探索meta开源的llama大模型，包括它的数据，training代码以及评测方法。之所以想记录下来，主要是因为llama的论文写的极其的细致，完美践行了开源这个词(感谢meta!)，第二个原因是它的文档以及社区都很活跃，使用人群广泛，我们可以借助很多中文社区的复现情况去一探大公司在实现一个大模型时候的考量，以及思考它为什么会这样做。</p>
<p>之前写过一篇关于斯坦福的alpaca的代码的解析，后来看过很多关于微调大模型(supervised
finetuning)的代码仓库，大家的实现思路基本上都可以追溯到alpaca的这份代码。</p>
<p>首先我会将所有我参考的资料罗列在前面，方便大家查找： - <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama/tree/main">llama代码仓库</a>
这个仓库是介绍如何下载llama模型 - <a href="lll">llama "食谱"</a>
一开始我想在上一个llama仓库中找到相关的train代码，找了半天发现根本没有。后来才发现meta官方将所有finetune(pretrain
from scrach)的代码放在这个仓库，适合developer - <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09288">Llama 2: Open Foundation and
Fine-Tuned Chat Models</a> llama2的research paper。强烈建议食用</p>
<p>中文社区的LLama的工作 - <a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2">Chinese LLaMA
Alpaca2</a></p>
<p>这个仓库同样有配套的文章<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.08177.pdf">Efficient and Effective Text
Encoding for Chinese LLaMA and Alpaca</a></p>
<p>这个仓库的工作主要是两个：</p>
<ol type="1">
<li>扩充了llama原来的token，也就是中文的那部分</li>
<li>用新的中文数据在llama上进行了continue
pretraining，并且发布了在instruction数据上的微调模型</li>
</ol>
<p>研究思路很简单，在别人模型上继续预训练，并参照alpaca对预训练的模型进行instruction
finetune让其具备follow instructions的能力。我们首先从这个Chinese
LLaMA代码仓库看起。</p>
<h1 id="chinese-llama">Chinese LLaMA</h1>
<p>作者自述做这份工作的原因是原生llama模型的词汇表中仅包含1000+个中文字符，所以首要任务是要扩充llama的词表。他们首先训练了一个中文的tokenizer，然后将其与llama的tokenizer进行融合，融合后的tokenizer拥有49953个token,
那么输入的词汇表数就从32000扩充到了49953。作者的实验还发现用新的融合后的tokenizer去tokenize序列要比旧的tokenizer编码后的序列要短。那很自然的就减少了很多计算量。</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231212161342629.png" alt="image-20231212161342629">
<figcaption aria-hidden="true">image-20231212161342629</figcaption>
</figure>
<p>在准备好tokenizer之后就到了训练环节，作者在这里没有采用全参数微调而是采用了Lora这种高效微调的方式。其实我看到这里是有疑问的，当然作者也在issue中做了回答：</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231212162349967.png" alt="image-20231212162349967">
<figcaption aria-hidden="true">image-20231212162349967</figcaption>
</figure>
<blockquote>
<p>我个人认为continue
pretraining是需要全参数微调的，而且还是在扩充了词表的情况下。</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/blob/main/scripts/training/run_clm_pt_with_peft.py">预训练脚本</a>,这个脚本是作者在<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py">transformers库的run_clm.py</a>上修改的，至于中文预训练数据部分，作者采用了20G的纯文本数据，并将他们分成了每个block
512个token。我们来看看代码是怎么写的，源代码在<a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/blob/main/scripts/training/run_clm_pt_with_peft.py">run_clm_pt_with_peft</a>,可以先将<a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/tree/main">Chinese-LLaMA-Alpaca-2</a>拉到本地，在文件姐scripts里可以看到training文件夹里有两个训练代码，一个是pretrain的，一个是sft的。我们先看前面这个pretrain的，它的训练任务很好理解，就是用decoder这种模型架构训练一个输入序列的下一个单词。</p>
<p>作者在这个仓库里没有放训练数据，我们先在该仓库里创建一个<code>./data</code>,里面放一些txt格式的数据用于测试，比如一些小说啥的，训练脚本在处理数据时会自动对他们进行读取并chunk成512长度的序列。作者在paper里提到的他们team训练的tokenizer也一并在scripts的tokenizer文件夹内，要跑通train这个代码需要在run_pt.sh内将这些参数都制定好。</p>
<p>先来看load数据以及处理部分的重点代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">files = [file.name <span class="keyword">for</span> file <span class="keyword">in</span> path.glob(<span class="string">&quot;*.txt&quot;</span>)]</span><br><span class="line"><span class="keyword">for</span> idx, file <span class="keyword">in</span> <span class="built_in">enumerate</span>(files):<span class="comment"># files为./data文件夹内所有的txt</span></span><br><span class="line">    data_file = os.path.join(path, file)</span><br><span class="line">    filename = <span class="string">&#x27;&#x27;</span>.join(file.split(<span class="string">&quot;.&quot;</span>)[:-<span class="number">1</span>])</span><br><span class="line">    cache_path = os.path.join(data_args.data_cache_dir, filename+<span class="string">f&quot;_<span class="subst">&#123;block_size&#125;</span>&quot;</span>)</span><br><span class="line">    os.makedirs(cache_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        processed_dataset = datasets.load_from_disk(cache_path, keep_in_memory=<span class="literal">False</span>) <span class="comment"># 首先使用datasets导入</span></span><br><span class="line">        logger.info(<span class="string">f&#x27;training datasets-<span class="subst">&#123;filename&#125;</span> has been loaded from disk&#x27;</span>)</span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            cache_dir = os.path.join(data_args.data_cache_dir, filename+<span class="string">f&quot;_text_<span class="subst">&#123;block_size&#125;</span>&quot;</span>)</span><br><span class="line">            os.makedirs(cache_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">            raw_dataset = load_dataset(<span class="string">&quot;text&quot;</span>, data_files=data_file, cache_dir=cache_dir, keep_in_memory=<span class="literal">False</span>)</span><br><span class="line">            logger.info(<span class="string">f&quot;<span class="subst">&#123;file&#125;</span> has been loaded&quot;</span>)</span><br><span class="line">            tokenized_dataset = raw_dataset.<span class="built_in">map</span>(</span><br><span class="line">                tokenize_function,</span><br><span class="line">                batched=<span class="literal">True</span>,</span><br><span class="line">                num_proc=data_args.preprocessing_num_workers,</span><br><span class="line">                remove_columns=<span class="string">&quot;text&quot;</span>,</span><br><span class="line">                load_from_cache_file=<span class="literal">True</span>,</span><br><span class="line">                keep_in_memory=<span class="literal">False</span>,</span><br><span class="line">                cache_file_names = &#123;k: os.path.join(cache_dir, <span class="string">&#x27;tokenized.arrow&#x27;</span>) <span class="keyword">for</span> k <span class="keyword">in</span> raw_dataset&#125;,</span><br><span class="line">                desc=<span class="string">&quot;Running tokenizer on dataset&quot;</span>,</span><br><span class="line">            )</span><br><span class="line">            grouped_datasets = tokenized_dataset.<span class="built_in">map</span>(</span><br><span class="line">                group_texts,</span><br><span class="line">                batched=<span class="literal">True</span>,</span><br><span class="line">                num_proc=data_args.preprocessing_num_workers,</span><br><span class="line">                load_from_cache_file=<span class="literal">True</span>,</span><br><span class="line">                keep_in_memory=<span class="literal">False</span>,</span><br><span class="line">                cache_file_names = &#123;k: os.path.join(cache_dir, <span class="string">&#x27;grouped.arrow&#x27;</span>) <span class="keyword">for</span> k <span class="keyword">in</span> tokenized_dataset&#125;,</span><br><span class="line">                desc=<span class="string">f&quot;Grouping texts in chunks of <span class="subst">&#123;block_size&#125;</span>&quot;</span>,</span><br><span class="line">            ) <span class="comment"># </span></span><br><span class="line">            processed_dataset = grouped_datasets</span><br><span class="line">            processed_dataset.save_to_disk(cache_path)</span><br><span class="line">            <span class="keyword">if</span> idx == <span class="number">0</span>: <span class="comment"># </span></span><br><span class="line">                lm_datasets = processed_dataset[<span class="string">&#x27;train&#x27;</span>]</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 如果有多于2个txt,那么将这些数据叠加起来</span></span><br><span class="line">                <span class="keyword">assert</span> lm_datasets.features.<span class="built_in">type</span> == processed_dataset[<span class="string">&quot;train&quot;</span>].features.<span class="built_in">type</span></span><br><span class="line">                lm_datasets = concatenate_datasets([lm_datasets, processed_dataset[<span class="string">&quot;train&quot;</span>]])</span><br></pre></td></tr></table></figure>
<p>内有两个帮助函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_function</span>(<span class="params">examples</span>):</span></span><br><span class="line">        <span class="keyword">with</span> CaptureLogger(tok_logger) <span class="keyword">as</span> cl:</span><br><span class="line">            output = tokenizer(examples[<span class="string">&quot;text&quot;</span>]) <span class="comment"># 仅仅做了tokenize这一个动作,而且会在每一个序列的结尾都加上EOS,由于设置了tokenizer.add_eos_token = True</span></span><br><span class="line">        <span class="comment"># clm input could be much much longer than block_size</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;Token indices sequence length is longer than the&quot;</span> <span class="keyword">in</span> cl.out:</span><br><span class="line">            tok_logger.warning(</span><br><span class="line">                <span class="string">&quot;^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits&quot;</span></span><br><span class="line">                <span class="string">&quot; before being passed to the model.&quot;</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_texts</span>(<span class="params">examples</span>):</span> <span class="comment"># 在这个函数里程序将tokenize之后的input_ids和attention_mask进行chunk，保证每个chunk大小都是block_size的</span></span><br><span class="line">    <span class="comment"># Concatenate all texts.</span></span><br><span class="line">    concatenated_examples = &#123;k: <span class="built_in">list</span>(chain(*examples[k])) <span class="keyword">for</span> k <span class="keyword">in</span> examples.keys()&#125;</span><br><span class="line">    total_length = <span class="built_in">len</span>(concatenated_examples[<span class="built_in">list</span>(examples.keys())[<span class="number">0</span>]])</span><br><span class="line">    <span class="comment"># We drop the small remainder, we could add padding if the model supported it instead of this drop, you can</span></span><br><span class="line">    <span class="comment"># customize this part to your needs.</span></span><br><span class="line">    <span class="keyword">if</span> total_length &gt;= block_size:</span><br><span class="line">        total_length = (total_length // block_size) * block_size</span><br><span class="line">        <span class="comment"># Split by chunks of max_len.</span></span><br><span class="line">        result = &#123;</span><br><span class="line">            k: [t[i : i + block_size] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, total_length, block_size)]</span><br><span class="line">            <span class="keyword">for</span> k, t <span class="keyword">in</span> concatenated_examples.items()</span><br><span class="line">        &#125;</span><br><span class="line">        result[<span class="string">&quot;labels&quot;</span>] = result[<span class="string">&quot;input_ids&quot;</span>].copy() <span class="comment"># 这里labels设置成和input_ids一模一样</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<p>可以看到基本采用transformer的库来实现的数据的导入以及process，总体来说使用datasets还是比较方便的。</p>
<p>再来看如何做的lora train：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, TaskType, get_peft_model, PeftModel, get_peft_model_state_dict</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> training_args.full_finetuning: <span class="comment"># 默认模型是全参数微调</span></span><br><span class="line">        <span class="keyword">if</span> training_args.peft_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            logger.info(<span class="string">&quot;Peft from pre-trained model&quot;</span>)</span><br><span class="line">            model = PeftModel.from_pretrained(model, training_args.peft_path, device_map=device_map)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            logger.info(<span class="string">&quot;Init new peft model&quot;</span>)</span><br><span class="line">            target_modules = training_args.trainable.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">            modules_to_save = training_args.modules_to_save</span><br><span class="line">            <span class="keyword">if</span> modules_to_save <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                modules_to_save = modules_to_save.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">            lora_rank = training_args.lora_rank</span><br><span class="line">            lora_dropout = training_args.lora_dropout</span><br><span class="line">            lora_alpha = training_args.lora_alpha</span><br><span class="line">            logger.info(<span class="string">f&quot;target_modules: <span class="subst">&#123;target_modules&#125;</span>&quot;</span>)</span><br><span class="line">            logger.info(<span class="string">f&quot;lora_rank: <span class="subst">&#123;lora_rank&#125;</span>&quot;</span>)</span><br><span class="line">            peft_config = LoraConfig(</span><br><span class="line">                task_type=TaskType.CAUSAL_LM,</span><br><span class="line">                target_modules=target_modules,</span><br><span class="line">                inference_mode=<span class="literal">False</span>,</span><br><span class="line">                r=lora_rank, lora_alpha=lora_alpha,</span><br><span class="line">                lora_dropout=lora_dropout,</span><br><span class="line">                modules_to_save=modules_to_save) <span class="comment"># LoraConfig是PEFT这个包内的</span></span><br><span class="line">            model = get_peft_model(model, peft_config)</span><br><span class="line">        model.print_trainable_parameters()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>该仓库的instruction
finetune的代码和alpaca的思路一样，很多写法都一模一样。不过因为作者在做pretrain的时候用的是lora的形式，所以在sft的时候也需要在这个基础模型上进行微调。作者在<code>run_clm_sft_with_peft.py</code>中是类似于pt脚本中的写法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> training_args.peft_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    logger.info(<span class="string">&quot;Peft from pre-trained model&quot;</span>)</span><br><span class="line">    model = PeftModel.from_pretrained(model, training_args.peft_path, device_map=device_map)</span><br></pre></td></tr></table></figure>
<p>这里的peft_path是需要在train的时候传入参数的，也就是我们在pretrain时候通过call_back函数保存的lora参数,
模型组装好之后训练。博主认为这时候是所有参数一起调整了，包含lora部分以及llama2基础模型部分。</p>
<blockquote>
<p>一点题外话：在阅读Chinese
LLaMA这份代码的时候发现了其中一个作者崔一鸣的博客，内有一个关于大模型的纵览介绍挺适合初学者熟悉大模型的相关技术，也适合面试的盆友回顾以及对自己还没掌握透的知识进行查漏补缺的。<a target="_blank" rel="noopener" href="https://ymcui.com/talk/20230826_baai_llm_tutorial.pdf"><strong>[Methods
and Practices for Large Pre-trained Language
Models](https://ymcui.com/talk/20230826_baai_llm_tutorial.pdf)</strong></a></p>
<p>建议配合<a target="_blank" rel="noopener" href="https://karpathy.ai/stateofgpt.pdf">stateofgpt</a>食用</p>
</blockquote>
<h1 id="llama">LLaMA</h1>
<h2 id="拓展补充介绍">拓展补充介绍</h2>
<p>LLaMA的1和2版本在模型架构上大多数相似，其中三个关键技术使羊驼模型区别于其他模型，这里摘一下llama2
research paper中的描述：</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231213162604171.png" alt="image-20231213162604171">
<figcaption aria-hidden="true">image-20231213162604171</figcaption>
</figure>
<h3 id="rmsnorm">RMSNorm</h3>
<p>在介绍RMSNorm之前补充一下Batch Normalization以及Layer
Normalization</p>
<p>参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://medium.com/@florian_algo/batchnorm-and-layernorm-2637f46a998b">BatchNorm
and LayerNorm</a></li>
<li><a target="_blank" rel="noopener" href="https://tungmphung.com/deep-learning-normalization-methods/">Deep
Learning normalization methods</a></li>
<li><a target="_blank" rel="noopener" href="https://ai.stackexchange.com/questions/27309/what-are-the-consequences-of-layer-norm-vs-batch-norm">[What
are the consequences of layer norm vs batch
norm?](https://ai.stackexchange.com/questions/27309/what-are-the-consequences-of-layer-norm-vs-batch-norm)</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/70065235/understanding-torch-nn-layernorm-in-nlp">[Understanding
torch.nn.LayerNorm in
nlp](https://stackoverflow.com/questions/70065235/understanding-torch-nn-layernorm-in-nlp)</a></li>
</ul>
<p><img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/0_K45DoPRbhC5-dqq1-17025178394161.webp"></p>
<p>上面图片中，每一行属于一个batch的数据，不用管这个batch内的数据是2维的还是1维的。</p>
<h4 id="batch-normalization">Batch Normalization</h4>
<blockquote>
<p>for each dimension of the input, all data points in the batch are
gathered and normalized with the same mean and standard deviation</p>
<p>BN的所有计算都在一个batch以内，也就是我们用到的数据只是这个batch内的数据，不会涉及到其他batch的数据</p>
</blockquote>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214093939577.png" alt="image-20231214093939577">
<figcaption aria-hidden="true">image-20231214093939577</figcaption>
</figure>
<p>上面的伪代码中的<code>x</code>可以是一个向量，如果是向量的情况下涉及到的x的相加都是向量的运算。值得注意的是在卷积层里，<code>dimension</code>指的是channel维度的</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214094523587.png" alt="image-20231214094523587">
<figcaption aria-hidden="true">image-20231214094523587</figcaption>
</figure>
<p>也就是不同channel计算出的μ和σ是不同的。</p>
<blockquote>
<p>the input data is normalized separately for each channel in a
convolutional layer.</p>
</blockquote>
<p>而在全连接层，<code>dimension</code>就是指feature维度。</p>
<h4 id="layer-normalization">Layer Normalization</h4>
<blockquote>
<p>with LayerNorm, we normalize each data point separately. Moreover,
each data point’s mean and variance are shared over all hidden units
(i.e. neurons) of the layer</p>
</blockquote>
<p>跟batch没关系，在layer层面去计算均值和方差。比如在全连接层，输入是125个神经元的话，就对这些神经元进行归一化。也就是数据中的每一个data
points都是独立进行归一化的，和其他data
points无关。那么对于卷积层来说的话就有两种计算方式：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41978699/article/details/122778085">参考</a></p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214100917596.png" alt="image-20231214100917596">
<figcaption aria-hidden="true">image-20231214100917596</figcaption>
</figure>
<p>看pytorch的<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">doc</a>多采取前一种全部一股脑求平均和方差的方式。</p>
<p>RMSNorm</p>
<p>RMSNorm的research paper写着一部分写的特别清楚，推荐查看原文<a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf">Root
Mean Square Layer Normalization</a></p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214103540564.png" alt="image-20231214103540564">
<figcaption aria-hidden="true">image-20231214103540564</figcaption>
</figure>
<p>RMSNorm去除了LN的求平均数的过程，并且将LN中的除以方差变成了除以<code>root mean square</code>。来看llama中的代码实现：<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama/blob/main/llama/model.py#L63">llama/llama/model.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_norm</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply the RMSNorm normalization to the input tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x (torch.Tensor): The input tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            torch.Tensor: The normalized tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x * torch.rsqrt(x.<span class="built_in">pow</span>(<span class="number">2</span>).mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) + self.eps) <span class="comment"># eps防止除以0</span></span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="swiglu">SwiGLU</h3>
<p>阅读知乎这篇博客<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/650237644">大模型基础｜激活函数｜从ReLU
到SwiGLU</a></p>
<h3 id="rotary-embedding-rope">Rotary Embedding, RoPE</h3>
<h4 id="attention-is-all-you-need中的position-embedding">Attention is
All you need中的position embedding</h4>
<p>首先回顾下在Attention is all you
need原文paper中对于位置编码的公式：</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231215135557606.png" alt="image-20231215135557606">
<figcaption aria-hidden="true">image-20231215135557606</figcaption>
</figure>
<p>我一开始理解这两个公式的时候很困难，后来查了一些资料，发现很多人也在这里由一些困惑，包括tensorflow官方的实现方式<a target="_blank" rel="noopener" href="https://www.tensorflow.org/tutorials/text/transformer?hl=zh-cn#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%88positional_encoding%EF%BC%89">位置编码</a>，tensorflow的官方给出的代码是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_angles</span>(<span class="params">pos, i, d_model</span>):</span></span><br><span class="line">  angle_rates = <span class="number">1</span> / np.power(<span class="number">10000</span>, (<span class="number">2</span> * (i//<span class="number">2</span>)) / np.float32(d_model))</span><br><span class="line">  <span class="keyword">return</span> pos * angle_rates</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span>(<span class="params">position, d_model</span>):</span></span><br><span class="line">  angle_rads = get_angles(np.arange(position)[:, np.newaxis],</span><br><span class="line">                          np.arange(d_model)[np.newaxis, :],</span><br><span class="line">                          d_model)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 将 sin 应用于数组中的偶数索引（indices）；2i</span></span><br><span class="line">  angle_rads[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(angle_rads[:, <span class="number">0</span>::<span class="number">2</span>]) <span class="comment"># 不懂双冒号切片的可参考： https://stackoverflow.com/questions/3453085/what-is-double-colon-in-python-when-subscripting-sequences</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 将 cos 应用于数组中的奇数索引；2i+1</span></span><br><span class="line">  angle_rads[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(angle_rads[:, <span class="number">1</span>::<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">  pos_encoding = angle_rads[np.newaxis, ...]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> tf.cast(pos_encoding, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<p>在<code>get_angles</code>方法里10000的指数系数中tensorflow的实现多加了一个<code>i//2</code>。这里我非常困惑，后来发现stackflow上也有同样的发问：</p>
<ul>
<li>[<a target="_blank" rel="noopener" href="https://datascience.stackexchange.com/questions/79995/explanation-about-i-2-in-positional-encoding-in-tensorflow-tutorial-about-trans/126053#126053">Explanation
about i//2 in positional encoding in tensorflow tutorial about
transformers</a></li>
<li>[<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/69036265/why-does-the-i-need-to-be-divided-by-2-in-caculating-positional-encoding">Why
does the 'i' need to be divided by 2 in caculating positional
encoding?</a></li>
</ul>
<p>推荐阅读一下<a target="_blank" rel="noopener" href="https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/">A
Gentle Introduction to Positional Encoding in Transformer Models, Part
1</a>。该作者的实现方式更符合人类的理解方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPositionEncoding</span>(<span class="params">seq_len, d, n=<span class="number">10000</span></span>):</span></span><br><span class="line">    P = np.zeros((seq_len, d))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(seq_len):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="built_in">int</span>(d/<span class="number">2</span>)): <span class="comment"># 这里只循环d//2次</span></span><br><span class="line">            denominator = np.power(n, <span class="number">2</span>*i/d)</span><br><span class="line">            P[k, <span class="number">2</span>*i] = np.sin(k/denominator)</span><br><span class="line">            P[k, <span class="number">2</span>*i+<span class="number">1</span>] = np.cos(k/denominator)</span><br><span class="line">    <span class="keyword">return</span> P</span><br><span class="line"> </span><br><span class="line">P = getPositionEncoding(seq_len=<span class="number">4</span>, d=<span class="number">4</span>, n=<span class="number">100</span>)</span><br><span class="line"><span class="built_in">print</span>(P)</span><br></pre></td></tr></table></figure>
<p>那么该怎么理解paper中的公式以及tensorflow//2的这个实现呢。就拿某一个sequence中的token来举例子，如果我们想要编码的向量长度是20,也就是d=20。那么tensorflow的做法是首先创建一个长度为20的向量，然后依次求其中的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">该token的position encoding所有应该求值得index</span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>]</span><br><span class="line">angle_rates = <span class="number">1</span> / np.power(<span class="number">10000</span>, (<span class="number">2</span> * (i//<span class="number">2</span>)) / np.float32(d_model)) 这句话</span><br><span class="line"><span class="number">10000</span>的指数(<span class="number">2</span> * (i//<span class="number">2</span>))是</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">18</span>, <span class="number">18</span>]</span><br></pre></td></tr></table></figure>
<p>所以对照paper中的公式表达的意思就是：</p>
<p>在向量的偶数index位置，比如0，2...等，公式里的2i就等于它的index</p>
<p>在向量的奇数index位置，比如1,3...等，公式里的10000的指数也就是2i的位置应该取这个奇数的前一个偶数值。</p>
<p>那么我们来看看tensorflow的这份代码就对上了：</p>
<p>10000的指数部分出现的值为：
<code>[0, 0, 2, 2, 4, 4, 6, 6, 8, 8, 10, 10, 12, 12, 14, 14, 16, 16, 18, 18]</code></p>
<p>所以paper里的这个公式要将2i当作一个整体来看。</p>
<h4 id="roperotary-position-embedding"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.09864v5.pdf">RoPE(rotary Position
Embedding)</a></h4>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231215152735803.png" alt="image-20231215152735803">
<figcaption aria-hidden="true">image-20231215152735803</figcaption>
</figure>
<p>RoPE进一步改进了绝对位置编码，是一种在transformer
attention中的Q和K上添加相对位置信息的方法</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231215152803294.png" alt="image-20231215152803294">
<figcaption aria-hidden="true">image-20231215152803294</figcaption>
</figure>
<p>首先作者将隐藏层的向量每两个维度编成一组，看成2维的向量；然后对于特定位置m的x1,x2，将他们旋转mθ角度，用新的x1,x2值替换老的值加入到query和key中。</p>
<h3 id="grouped-query-attention-gqa">Grouped-Query Attention (GQA)</h3>
<p>GQA是llama2相较于llama1新采用的技术，它是一种提升推理速度的方法，主要针对多头注意力机制进行改进，与KV
Cache搭配使用</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div></div>
  <button>
    Donate
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechat_pay.jpg" alt="YAN WeChat Pay">
        <span>WeChat Pay</span>
      </div>

  </div>
</div>

          <div class="post-tags">
              <a href="/tags/nlp/" rel="tag"># nlp</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/12/06/Sequence%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%AD%E5%A4%84%E7%90%86%E4%B8%8D%E5%AE%9A%E9%95%BF%E6%95%B0%E6%8D%AE/" rel="prev" title="Sequence分类问题中处理不定长数据">
                  <i class="fa fa-chevron-left"></i> Sequence分类问题中处理不定长数据
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/01/02/Transformer%E6%8B%86%E8%A7%A3/" rel="next" title="Transformer拆解">
                  Transformer拆解 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YAN</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Symbols count total: </span>
    <span title="Symbols count total">214k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">3:14</span>
  </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
