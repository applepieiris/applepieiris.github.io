<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=consolas:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.8.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="Be smart! Keep Learning">
<meta property="og:type" content="website">
<meta property="og:title" content="YAN&#39;s Blog">
<meta property="og:url" content="http://example.com/default_index/index.html">
<meta property="og:site_name" content="YAN&#39;s Blog">
<meta property="og:description" content="Be smart! Keep Learning">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="YAN">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/default_index/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"default_index/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>YAN's Blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">YAN's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="YAN"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">YAN</p>
  <div class="site-description" itemprop="description">Be smart! Keep Learning</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">45</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/applepieiris" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/" class="post-title-link" itemprop="url">LLaMA系列模型浅析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-12-12 14:56:24" itemprop="dateCreated datePublished" datetime="2023-12-12T14:56:24+08:00">2023-12-12</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2023-12-27 13:17:53" itemprop="dateModified" datetime="2023-12-27T13:17:53+08:00">2023-12-27</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>10 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>这篇博客主要记录博主在探索meta开源的llama大模型，包括它的数据，training代码以及评测方法。之所以想记录下来，主要是因为llama的论文写的极其的细致，完美践行了开源这个词(感谢meta!)，第二个原因是它的文档以及社区都很活跃，使用人群广泛，我们可以借助很多中文社区的复现情况去一探大公司在实现一个大模型时候的考量，以及思考它为什么会这样做。</p>
<p>之前写过一篇关于斯坦福的alpaca的代码的解析，后来看过很多关于微调大模型(supervised
finetuning)的代码仓库，大家的实现思路基本上都可以追溯到alpaca的这份代码。</p>
<p>首先我会将所有我参考的资料罗列在前面，方便大家查找： - <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama/tree/main">llama代码仓库</a>
这个仓库是介绍如何下载llama模型 - <a href="lll">llama "食谱"</a>
一开始我想在上一个llama仓库中找到相关的train代码，找了半天发现根本没有。后来才发现meta官方将所有finetune(pretrain
from scrach)的代码放在这个仓库，适合developer - <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09288">Llama 2: Open Foundation and
Fine-Tuned Chat Models</a> llama2的research paper。强烈建议食用</p>
<p>中文社区的LLama的工作 - <a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2">Chinese LLaMA
Alpaca2</a></p>
<p>这个仓库同样有配套的文章<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.08177.pdf">Efficient and Effective Text
Encoding for Chinese LLaMA and Alpaca</a></p>
<p>这个仓库的工作主要是两个：</p>
<ol type="1">
<li>扩充了llama原来的token，也就是中文的那部分</li>
<li>用新的中文数据在llama上进行了continue
pretraining，并且发布了在instruction数据上的微调模型</li>
</ol>
<p>研究思路很简单，在别人模型上继续预训练，并参照alpaca对预训练的模型进行instruction
finetune让其具备follow instructions的能力。我们首先从这个Chinese
LLaMA代码仓库看起。</p>
<h1 id="chinese-llama">Chinese LLaMA</h1>
<p>作者自述做这份工作的原因是原生llama模型的词汇表中仅包含1000+个中文字符，所以首要任务是要扩充llama的词表。他们首先训练了一个中文的tokenizer，然后将其与llama的tokenizer进行融合，融合后的tokenizer拥有49953个token,
那么输入的词汇表数就从32000扩充到了49953。作者的实验还发现用新的融合后的tokenizer去tokenize序列要比旧的tokenizer编码后的序列要短。那很自然的就减少了很多计算量。</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231212161342629.png" alt="image-20231212161342629">
<figcaption aria-hidden="true">image-20231212161342629</figcaption>
</figure>
<p>在准备好tokenizer之后就到了训练环节，作者在这里没有采用全参数微调而是采用了Lora这种高效微调的方式。其实我看到这里是有疑问的，当然作者也在issue中做了回答：</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231212162349967.png" alt="image-20231212162349967">
<figcaption aria-hidden="true">image-20231212162349967</figcaption>
</figure>
<blockquote>
<p>我个人认为continue
pretraining是需要全参数微调的，而且还是在扩充了词表的情况下。</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/blob/main/scripts/training/run_clm_pt_with_peft.py">预训练脚本</a>,这个脚本是作者在<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py">transformers库的run_clm.py</a>上修改的，至于中文预训练数据部分，作者采用了20G的纯文本数据，并将他们分成了每个block
512个token。我们来看看代码是怎么写的，源代码在<a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/blob/main/scripts/training/run_clm_pt_with_peft.py">run_clm_pt_with_peft</a>,可以先将<a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/tree/main">Chinese-LLaMA-Alpaca-2</a>拉到本地，在文件姐scripts里可以看到training文件夹里有两个训练代码，一个是pretrain的，一个是sft的。我们先看前面这个pretrain的，它的训练任务很好理解，就是用decoder这种模型架构训练一个输入序列的下一个单词。</p>
<p>作者在这个仓库里没有放训练数据，我们先在该仓库里创建一个<code>./data</code>,里面放一些txt格式的数据用于测试，比如一些小说啥的，训练脚本在处理数据时会自动对他们进行读取并chunk成512长度的序列。作者在paper里提到的他们team训练的tokenizer也一并在scripts的tokenizer文件夹内，要跑通train这个代码需要在run_pt.sh内将这些参数都制定好。</p>
<p>先来看load数据以及处理部分的重点代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">files = [file.name <span class="keyword">for</span> file <span class="keyword">in</span> path.glob(<span class="string">&quot;*.txt&quot;</span>)]</span><br><span class="line"><span class="keyword">for</span> idx, file <span class="keyword">in</span> <span class="built_in">enumerate</span>(files):<span class="comment"># files为./data文件夹内所有的txt</span></span><br><span class="line">    data_file = os.path.join(path, file)</span><br><span class="line">    filename = <span class="string">&#x27;&#x27;</span>.join(file.split(<span class="string">&quot;.&quot;</span>)[:-<span class="number">1</span>])</span><br><span class="line">    cache_path = os.path.join(data_args.data_cache_dir, filename+<span class="string">f&quot;_<span class="subst">&#123;block_size&#125;</span>&quot;</span>)</span><br><span class="line">    os.makedirs(cache_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        processed_dataset = datasets.load_from_disk(cache_path, keep_in_memory=<span class="literal">False</span>) <span class="comment"># 首先使用datasets导入</span></span><br><span class="line">        logger.info(<span class="string">f&#x27;training datasets-<span class="subst">&#123;filename&#125;</span> has been loaded from disk&#x27;</span>)</span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            cache_dir = os.path.join(data_args.data_cache_dir, filename+<span class="string">f&quot;_text_<span class="subst">&#123;block_size&#125;</span>&quot;</span>)</span><br><span class="line">            os.makedirs(cache_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">            raw_dataset = load_dataset(<span class="string">&quot;text&quot;</span>, data_files=data_file, cache_dir=cache_dir, keep_in_memory=<span class="literal">False</span>)</span><br><span class="line">            logger.info(<span class="string">f&quot;<span class="subst">&#123;file&#125;</span> has been loaded&quot;</span>)</span><br><span class="line">            tokenized_dataset = raw_dataset.<span class="built_in">map</span>(</span><br><span class="line">                tokenize_function,</span><br><span class="line">                batched=<span class="literal">True</span>,</span><br><span class="line">                num_proc=data_args.preprocessing_num_workers,</span><br><span class="line">                remove_columns=<span class="string">&quot;text&quot;</span>,</span><br><span class="line">                load_from_cache_file=<span class="literal">True</span>,</span><br><span class="line">                keep_in_memory=<span class="literal">False</span>,</span><br><span class="line">                cache_file_names = &#123;k: os.path.join(cache_dir, <span class="string">&#x27;tokenized.arrow&#x27;</span>) <span class="keyword">for</span> k <span class="keyword">in</span> raw_dataset&#125;,</span><br><span class="line">                desc=<span class="string">&quot;Running tokenizer on dataset&quot;</span>,</span><br><span class="line">            )</span><br><span class="line">            grouped_datasets = tokenized_dataset.<span class="built_in">map</span>(</span><br><span class="line">                group_texts,</span><br><span class="line">                batched=<span class="literal">True</span>,</span><br><span class="line">                num_proc=data_args.preprocessing_num_workers,</span><br><span class="line">                load_from_cache_file=<span class="literal">True</span>,</span><br><span class="line">                keep_in_memory=<span class="literal">False</span>,</span><br><span class="line">                cache_file_names = &#123;k: os.path.join(cache_dir, <span class="string">&#x27;grouped.arrow&#x27;</span>) <span class="keyword">for</span> k <span class="keyword">in</span> tokenized_dataset&#125;,</span><br><span class="line">                desc=<span class="string">f&quot;Grouping texts in chunks of <span class="subst">&#123;block_size&#125;</span>&quot;</span>,</span><br><span class="line">            ) <span class="comment"># </span></span><br><span class="line">            processed_dataset = grouped_datasets</span><br><span class="line">            processed_dataset.save_to_disk(cache_path)</span><br><span class="line">            <span class="keyword">if</span> idx == <span class="number">0</span>: <span class="comment"># </span></span><br><span class="line">                lm_datasets = processed_dataset[<span class="string">&#x27;train&#x27;</span>]</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 如果有多于2个txt,那么将这些数据叠加起来</span></span><br><span class="line">                <span class="keyword">assert</span> lm_datasets.features.<span class="built_in">type</span> == processed_dataset[<span class="string">&quot;train&quot;</span>].features.<span class="built_in">type</span></span><br><span class="line">                lm_datasets = concatenate_datasets([lm_datasets, processed_dataset[<span class="string">&quot;train&quot;</span>]])</span><br></pre></td></tr></table></figure>
<p>内有两个帮助函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_function</span>(<span class="params">examples</span>):</span></span><br><span class="line">        <span class="keyword">with</span> CaptureLogger(tok_logger) <span class="keyword">as</span> cl:</span><br><span class="line">            output = tokenizer(examples[<span class="string">&quot;text&quot;</span>]) <span class="comment"># 仅仅做了tokenize这一个动作,而且会在每一个序列的结尾都加上EOS,由于设置了tokenizer.add_eos_token = True</span></span><br><span class="line">        <span class="comment"># clm input could be much much longer than block_size</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;Token indices sequence length is longer than the&quot;</span> <span class="keyword">in</span> cl.out:</span><br><span class="line">            tok_logger.warning(</span><br><span class="line">                <span class="string">&quot;^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits&quot;</span></span><br><span class="line">                <span class="string">&quot; before being passed to the model.&quot;</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_texts</span>(<span class="params">examples</span>):</span> <span class="comment"># 在这个函数里程序将tokenize之后的input_ids和attention_mask进行chunk，保证每个chunk大小都是block_size的</span></span><br><span class="line">    <span class="comment"># Concatenate all texts.</span></span><br><span class="line">    concatenated_examples = &#123;k: <span class="built_in">list</span>(chain(*examples[k])) <span class="keyword">for</span> k <span class="keyword">in</span> examples.keys()&#125;</span><br><span class="line">    total_length = <span class="built_in">len</span>(concatenated_examples[<span class="built_in">list</span>(examples.keys())[<span class="number">0</span>]])</span><br><span class="line">    <span class="comment"># We drop the small remainder, we could add padding if the model supported it instead of this drop, you can</span></span><br><span class="line">    <span class="comment"># customize this part to your needs.</span></span><br><span class="line">    <span class="keyword">if</span> total_length &gt;= block_size:</span><br><span class="line">        total_length = (total_length // block_size) * block_size</span><br><span class="line">        <span class="comment"># Split by chunks of max_len.</span></span><br><span class="line">        result = &#123;</span><br><span class="line">            k: [t[i : i + block_size] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, total_length, block_size)]</span><br><span class="line">            <span class="keyword">for</span> k, t <span class="keyword">in</span> concatenated_examples.items()</span><br><span class="line">        &#125;</span><br><span class="line">        result[<span class="string">&quot;labels&quot;</span>] = result[<span class="string">&quot;input_ids&quot;</span>].copy() <span class="comment"># 这里labels设置成和input_ids一模一样</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<p>可以看到基本采用transformer的库来实现的数据的导入以及process，总体来说使用datasets还是比较方便的。</p>
<p>再来看如何做的lora train：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, TaskType, get_peft_model, PeftModel, get_peft_model_state_dict</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> training_args.full_finetuning: <span class="comment"># 默认模型是全参数微调</span></span><br><span class="line">        <span class="keyword">if</span> training_args.peft_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            logger.info(<span class="string">&quot;Peft from pre-trained model&quot;</span>)</span><br><span class="line">            model = PeftModel.from_pretrained(model, training_args.peft_path, device_map=device_map)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            logger.info(<span class="string">&quot;Init new peft model&quot;</span>)</span><br><span class="line">            target_modules = training_args.trainable.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">            modules_to_save = training_args.modules_to_save</span><br><span class="line">            <span class="keyword">if</span> modules_to_save <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                modules_to_save = modules_to_save.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">            lora_rank = training_args.lora_rank</span><br><span class="line">            lora_dropout = training_args.lora_dropout</span><br><span class="line">            lora_alpha = training_args.lora_alpha</span><br><span class="line">            logger.info(<span class="string">f&quot;target_modules: <span class="subst">&#123;target_modules&#125;</span>&quot;</span>)</span><br><span class="line">            logger.info(<span class="string">f&quot;lora_rank: <span class="subst">&#123;lora_rank&#125;</span>&quot;</span>)</span><br><span class="line">            peft_config = LoraConfig(</span><br><span class="line">                task_type=TaskType.CAUSAL_LM,</span><br><span class="line">                target_modules=target_modules,</span><br><span class="line">                inference_mode=<span class="literal">False</span>,</span><br><span class="line">                r=lora_rank, lora_alpha=lora_alpha,</span><br><span class="line">                lora_dropout=lora_dropout,</span><br><span class="line">                modules_to_save=modules_to_save) <span class="comment"># LoraConfig是PEFT这个包内的</span></span><br><span class="line">            model = get_peft_model(model, peft_config)</span><br><span class="line">        model.print_trainable_parameters()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>该仓库的instruction
finetune的代码和alpaca的思路一样，很多写法都一模一样。不过因为作者在做pretrain的时候用的是lora的形式，所以在sft的时候也需要在这个基础模型上进行微调。作者在<code>run_clm_sft_with_peft.py</code>中是类似于pt脚本中的写法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> training_args.peft_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    logger.info(<span class="string">&quot;Peft from pre-trained model&quot;</span>)</span><br><span class="line">    model = PeftModel.from_pretrained(model, training_args.peft_path, device_map=device_map)</span><br></pre></td></tr></table></figure>
<p>这里的peft_path是需要在train的时候传入参数的，也就是我们在pretrain时候通过call_back函数保存的lora参数,
模型组装好之后训练。博主认为这时候是所有参数一起调整了，包含lora部分以及llama2基础模型部分。</p>
<blockquote>
<p>一点题外话：在阅读Chinese
LLaMA这份代码的时候发现了其中一个作者崔一鸣的博客，内有一个关于大模型的纵览介绍挺适合初学者熟悉大模型的相关技术，也适合面试的盆友回顾以及对自己还没掌握透的知识进行查漏补缺的。<a target="_blank" rel="noopener" href="https://ymcui.com/talk/20230826_baai_llm_tutorial.pdf"><strong>[Methods
and Practices for Large Pre-trained Language
Models](https://ymcui.com/talk/20230826_baai_llm_tutorial.pdf)</strong></a></p>
<p>建议配合<a target="_blank" rel="noopener" href="https://karpathy.ai/stateofgpt.pdf">stateofgpt</a>食用</p>
</blockquote>
<h1 id="llama">LLaMA</h1>
<h2 id="拓展补充介绍">拓展补充介绍</h2>
<p>LLaMA的1和2版本在模型架构上大多数相似，其中三个关键技术使羊驼模型区别于其他模型，这里摘一下llama2
research paper中的描述：</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231213162604171.png" alt="image-20231213162604171">
<figcaption aria-hidden="true">image-20231213162604171</figcaption>
</figure>
<h3 id="rmsnorm">RMSNorm</h3>
<p>在介绍RMSNorm之前补充一下Batch Normalization以及Layer
Normalization</p>
<p>参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://medium.com/@florian_algo/batchnorm-and-layernorm-2637f46a998b">BatchNorm
and LayerNorm</a></li>
<li><a target="_blank" rel="noopener" href="https://tungmphung.com/deep-learning-normalization-methods/">Deep
Learning normalization methods</a></li>
<li><a target="_blank" rel="noopener" href="https://ai.stackexchange.com/questions/27309/what-are-the-consequences-of-layer-norm-vs-batch-norm">[What
are the consequences of layer norm vs batch
norm?](https://ai.stackexchange.com/questions/27309/what-are-the-consequences-of-layer-norm-vs-batch-norm)</a></li>
</ul>
<p><img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/0_K45DoPRbhC5-dqq1-17025178394161.webp"></p>
<p>上面图片中，每一行属于一个batch的数据，不用管这个batch内的数据是2维的还是1维的。</p>
<h4 id="batch-normalization">Batch Normalization</h4>
<blockquote>
<p>for each dimension of the input, all data points in the batch are
gathered and normalized with the same mean and standard deviation</p>
<p>BN的所有计算都在一个batch以内，也就是我们用到的数据只是这个batch内的数据，不会涉及到其他batch的数据</p>
</blockquote>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214093939577.png" alt="image-20231214093939577">
<figcaption aria-hidden="true">image-20231214093939577</figcaption>
</figure>
<p>上面的伪代码中的<code>x</code>可以是一个向量，如果是向量的情况下涉及到的x的相加都是向量的运算。值得注意的是在卷积层里，<code>dimension</code>指的是channel维度的</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214094523587.png" alt="image-20231214094523587">
<figcaption aria-hidden="true">image-20231214094523587</figcaption>
</figure>
<p>也就是不同channel计算出的μ和σ是不同的。</p>
<blockquote>
<p>the input data is normalized separately for each channel in a
convolutional layer.</p>
</blockquote>
<p>而在全连接层，<code>dimension</code>就是指feature维度。</p>
<h4 id="layer-normalization">Layer Normalization</h4>
<blockquote>
<p>with LayerNorm, we normalize each data point separately. Moreover,
each data point’s mean and variance are shared over all hidden units
(i.e. neurons) of the layer</p>
</blockquote>
<p>跟batch没关系，在layer层面去计算均值和方差。比如在全连接层，输入是125个神经元的话，就对这些神经元进行归一化。也就是数据中的每一个data
points都是独立进行归一化的，和其他data
points无关。那么对于卷积层来说的话就有两种计算方式：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41978699/article/details/122778085">参考</a></p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214100917596.png" alt="image-20231214100917596">
<figcaption aria-hidden="true">image-20231214100917596</figcaption>
</figure>
<p>看pytorch的<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">doc</a>多采取前一种全部一股脑求平均和方差的方式。</p>
<p>RMSNorm</p>
<p>RMSNorm的research paper写着一部分写的特别清楚，推荐查看原文<a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf">Root
Mean Square Layer Normalization</a></p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231214103540564.png" alt="image-20231214103540564">
<figcaption aria-hidden="true">image-20231214103540564</figcaption>
</figure>
<p>RMSNorm去除了LN的求平均数的过程，并且将LN中的除以方差变成了除以<code>root mean square</code>。来看llama中的代码实现：<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama/blob/main/llama/model.py#L63">llama/llama/model.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_norm</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply the RMSNorm normalization to the input tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x (torch.Tensor): The input tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            torch.Tensor: The normalized tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x * torch.rsqrt(x.<span class="built_in">pow</span>(<span class="number">2</span>).mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) + self.eps) <span class="comment"># eps防止除以0</span></span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="swiglu">SwiGLU</h3>
<p>阅读知乎这篇博客<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/650237644">大模型基础｜激活函数｜从ReLU
到SwiGLU</a></p>
<h3 id="rotary-embedding-rope">Rotary Embedding, RoPE</h3>
<h4 id="attention-is-all-you-need中的position-embedding">Attention is
All you need中的position embedding</h4>
<p>首先回顾下在Attention is all you
need原文paper中对于位置编码的公式：</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231215135557606.png" alt="image-20231215135557606">
<figcaption aria-hidden="true">image-20231215135557606</figcaption>
</figure>
<p>我一开始理解这两个公式的时候很困难，后来查了一些资料，发现很多人也在这里由一些困惑，包括tensorflow官方的实现方式<a target="_blank" rel="noopener" href="https://www.tensorflow.org/tutorials/text/transformer?hl=zh-cn#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%88positional_encoding%EF%BC%89">位置编码</a>，tensorflow的官方给出的代码是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_angles</span>(<span class="params">pos, i, d_model</span>):</span></span><br><span class="line">  angle_rates = <span class="number">1</span> / np.power(<span class="number">10000</span>, (<span class="number">2</span> * (i//<span class="number">2</span>)) / np.float32(d_model))</span><br><span class="line">  <span class="keyword">return</span> pos * angle_rates</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span>(<span class="params">position, d_model</span>):</span></span><br><span class="line">  angle_rads = get_angles(np.arange(position)[:, np.newaxis],</span><br><span class="line">                          np.arange(d_model)[np.newaxis, :],</span><br><span class="line">                          d_model)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 将 sin 应用于数组中的偶数索引（indices）；2i</span></span><br><span class="line">  angle_rads[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(angle_rads[:, <span class="number">0</span>::<span class="number">2</span>]) <span class="comment"># 不懂双冒号切片的可参考： https://stackoverflow.com/questions/3453085/what-is-double-colon-in-python-when-subscripting-sequences</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 将 cos 应用于数组中的奇数索引；2i+1</span></span><br><span class="line">  angle_rads[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(angle_rads[:, <span class="number">1</span>::<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">  pos_encoding = angle_rads[np.newaxis, ...]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> tf.cast(pos_encoding, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<p>在<code>get_angles</code>方法里10000的指数系数中tensorflow的实现多加了一个<code>i//2</code>。这里我非常困惑，后来发现stackflow上也有同样的发问：</p>
<ul>
<li>[<a target="_blank" rel="noopener" href="https://datascience.stackexchange.com/questions/79995/explanation-about-i-2-in-positional-encoding-in-tensorflow-tutorial-about-trans/126053#126053">Explanation
about i//2 in positional encoding in tensorflow tutorial about
transformers</a></li>
<li>[<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/69036265/why-does-the-i-need-to-be-divided-by-2-in-caculating-positional-encoding">Why
does the 'i' need to be divided by 2 in caculating positional
encoding?</a></li>
</ul>
<p>推荐阅读一下<a target="_blank" rel="noopener" href="https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/">A
Gentle Introduction to Positional Encoding in Transformer Models, Part
1</a>。该作者的实现方式更符合人类的理解方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPositionEncoding</span>(<span class="params">seq_len, d, n=<span class="number">10000</span></span>):</span></span><br><span class="line">    P = np.zeros((seq_len, d))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(seq_len):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="built_in">int</span>(d/<span class="number">2</span>)): <span class="comment"># 这里只循环d//2次</span></span><br><span class="line">            denominator = np.power(n, <span class="number">2</span>*i/d)</span><br><span class="line">            P[k, <span class="number">2</span>*i] = np.sin(k/denominator)</span><br><span class="line">            P[k, <span class="number">2</span>*i+<span class="number">1</span>] = np.cos(k/denominator)</span><br><span class="line">    <span class="keyword">return</span> P</span><br><span class="line"> </span><br><span class="line">P = getPositionEncoding(seq_len=<span class="number">4</span>, d=<span class="number">4</span>, n=<span class="number">100</span>)</span><br><span class="line"><span class="built_in">print</span>(P)</span><br></pre></td></tr></table></figure>
<p>那么该怎么理解paper中的公式以及tensorflow//2的这个实现呢。就拿某一个sequence中的token来举例子，如果我们想要编码的向量长度是20,也就是d=20。那么tensorflow的做法是首先创建一个长度为20的向量，然后依次求其中的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">该token的position encoding所有应该求值得index</span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>]</span><br><span class="line">angle_rates = <span class="number">1</span> / np.power(<span class="number">10000</span>, (<span class="number">2</span> * (i//<span class="number">2</span>)) / np.float32(d_model)) 这句话</span><br><span class="line"><span class="number">10000</span>的指数(<span class="number">2</span> * (i//<span class="number">2</span>))是</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">18</span>, <span class="number">18</span>]</span><br></pre></td></tr></table></figure>
<p>所以对照paper中的公式表达的意思就是：</p>
<p>在向量的偶数index位置，比如0，2...等，公式里的2i就等于它的index</p>
<p>在向量的奇数index位置，比如1,3...等，公式里的10000的指数也就是2i的位置应该取这个奇数的前一个偶数值。</p>
<p>那么我们来看看tensorflow的这份代码就对上了：</p>
<p>10000的指数部分出现的值为：
<code>[0, 0, 2, 2, 4, 4, 6, 6, 8, 8, 10, 10, 12, 12, 14, 14, 16, 16, 18, 18]</code></p>
<p>所以paper里的这个公式要将2i当作一个整体来看。</p>
<h4 id="roperotary-position-embedding"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.09864v5.pdf">RoPE(rotary Position
Embedding)</a></h4>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231215152735803.png" alt="image-20231215152735803">
<figcaption aria-hidden="true">image-20231215152735803</figcaption>
</figure>
<p>RoPE进一步改进了绝对位置编码，是一种在transformer
attention中的Q和K上添加相对位置信息的方法</p>
<figure>
<img src="/2023/12/12/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%B5%85%E6%9E%90/image-20231215152803294.png" alt="image-20231215152803294">
<figcaption aria-hidden="true">image-20231215152803294</figcaption>
</figure>
<p>首先作者将隐藏层的向量每两个维度编成一组，看成2维的向量；然后对于特定位置m的x1,x2，将他们旋转mθ角度，用新的x1,x2值替换老的值加入到query和key中。</p>
<h3 id="grouped-query-attention-gqa">Grouped-Query Attention (GQA)</h3>
<p>GQA是llama2相较于llama1新采用的技术，它是一种提升推理速度的方法，主要针对多头注意力机制进行改进，与KV
Cache搭配使用</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/06/Sequence%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%AD%E5%A4%84%E7%90%86%E4%B8%8D%E5%AE%9A%E9%95%BF%E6%95%B0%E6%8D%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/06/Sequence%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%AD%E5%A4%84%E7%90%86%E4%B8%8D%E5%AE%9A%E9%95%BF%E6%95%B0%E6%8D%AE/" class="post-title-link" itemprop="url">Sequence分类问题中处理不定长数据</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-12-06 10:55:07 / Modified: 13:20:23" itemprop="dateCreated datePublished" datetime="2023-12-06T10:55:07+08:00">2023-12-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>6.2k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>6 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>之前写过一篇关于stanford
alpaca的代码的分析，最近在kaggle上看到一个检测某段长文本是否是AI生成的任务<a target="_blank" rel="noopener" href="https://www.kaggle.com/c/llm-detect-ai-generated-text">LLM -
Detect AI Generated
Text</a>,自己也在尝试做这个任务的时候，发现斯坦福的这份代码真是常看常新。对于数据的准备部分，有很多选择，比如在创建Dataset的时候就把所有的字符串数据tokenize好，在get_item()的函数返回时就返回input_ids，也可以是像斯坦福的<a target="_blank" rel="noopener" href="https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py">这份代码</a>一样，先把数据读取进来然后再用<code>DataCollator</code>处理（padding）。</p>
<p>我之前没有发现斯坦福这份代码这么写的真正原因，直到我自己来处理这种不定长的序列输入时才发现这样写的绝妙，因为我们都知道矩阵是每一行都需要是同样的size，所以斯坦福的写法在数据处理前期一直在用list，而不是batch。</p>
<p>先来说说transformer的对于序列分类的官方教程的写法<a target="_blank" rel="noopener" href="https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/sequence_classification.ipynb">传送门</a></p>
<p>transformer的这个教程直接使用的是自己的数据集，已经规整为datasets了，首先它对数据集使用map函数做了截断的处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;distilbert-base-uncased&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_function</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">&quot;text&quot;</span>], truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">tokenized_imdb = imdb.<span class="built_in">map</span>(preprocess_function, batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>然后重点来了，注意在上面的<code>preprocess_function</code>中并没有对序列进行padding，只是对过长的序列做了截断。接着作者使用了<code>datacollatorwithpadding</code>，给出的理由是：</p>
<blockquote>
<p>It's more efficient to <em>dynamically pad</em> the sentences to the
longest length in a batch during collation, instead of padding the whole
dataset to the maximum length.</p>
</blockquote>
<p>我们可以用官方的文档中看到对于datacollator的<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main_classes/data_collator">定义</a>：</p>
<blockquote>
<p>Data collators are objects that will form a batch by using a list of
dataset elements as input. These elements are of the same type as the
elements of <code>train_dataset</code> or <code>eval_dataset</code></p>
</blockquote>
<p>也就是data
collators的输入是一个list，list里的每一个元素跟train_dataset中的元素是一样的。在data
collator中你可以做一些processing，如padding，random
masking。我们接下来可以看到斯坦福的羊驼代码就是将padding的步骤放到了data
collator内。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorWithPadding</span><br><span class="line"></span><br><span class="line">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span><br></pre></td></tr></table></figure>
<p>在这里的教程里，作者使用了<code>DataCollatorWithPadding</code>，它会动态地pad
inputs。我看到文档里还有<code>class transformers.DataCollatorForTokenClassification</code>这个类，maybe可以处理不定长输入，留作后续探索。</p>
<p>transformer的这个教程还是过于简单了，在实际的case中情况会复杂一点。接下来我们看斯坦福的羊驼咋处理不定长sequence的。</p>
<p>首先它先把Dataset定义好：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    sources: <span class="type">Sequence</span>[<span class="built_in">str</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">    targets: <span class="type">Sequence</span>[<span class="built_in">str</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">    tokenizer: transformers.PreTrainedTokenizer,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="type">Dict</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Preprocess the data by tokenizing.&quot;&quot;&quot;</span></span><br><span class="line">    examples = [s + t <span class="keyword">for</span> s, t <span class="keyword">in</span> <span class="built_in">zip</span>(sources, targets)]</span><br><span class="line">    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) <span class="keyword">for</span> strings <span class="keyword">in</span> (examples, sources)]</span><br><span class="line">    input_ids = examples_tokenized[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">    labels = copy.deepcopy(input_ids)</span><br><span class="line">    <span class="keyword">for</span> label, source_len <span class="keyword">in</span> <span class="built_in">zip</span>(labels, sources_tokenized[<span class="string">&quot;input_ids_lens&quot;</span>]):</span><br><span class="line">        label[:source_len] = IGNORE_INDEX</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(input_ids=input_ids, labels=labels)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SupervisedDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Dataset for supervised fine-tuning.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data_path: <span class="built_in">str</span>, tokenizer: transformers.PreTrainedTokenizer</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SupervisedDataset, self).__init__()</span><br><span class="line">        logging.warning(<span class="string">&quot;Loading data...&quot;</span>)</span><br><span class="line">        list_data_dict = utils.jload(data_path)</span><br><span class="line"></span><br><span class="line">        logging.warning(<span class="string">&quot;Formatting inputs...&quot;</span>)</span><br><span class="line">        prompt_input, prompt_no_input = PROMPT_DICT[<span class="string">&quot;prompt_input&quot;</span>], PROMPT_DICT[<span class="string">&quot;prompt_no_input&quot;</span>]</span><br><span class="line">        sources = [</span><br><span class="line">            prompt_input.format_map(example) <span class="keyword">if</span> example.get(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;&quot;</span>) != <span class="string">&quot;&quot;</span> <span class="keyword">else</span> prompt_no_input.format_map(example)</span><br><span class="line">            <span class="keyword">for</span> example <span class="keyword">in</span> list_data_dict</span><br><span class="line">        ]</span><br><span class="line">        targets = [<span class="string">f&quot;<span class="subst">&#123;example[<span class="string">&#x27;output&#x27;</span>]&#125;</span><span class="subst">&#123;tokenizer.eos_token&#125;</span>&quot;</span> <span class="keyword">for</span> example <span class="keyword">in</span> list_data_dict] <span class="comment"># 将output之后加上[EOS]</span></span><br><span class="line"></span><br><span class="line">        logging.warning(<span class="string">&quot;Tokenizing inputs... This may take some time...&quot;</span>)</span><br><span class="line">        data_dict = preprocess(sources, targets, tokenizer)</span><br><span class="line"></span><br><span class="line">        self.input_ids = data_dict[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">        self.labels = data_dict[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.input_ids)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, i</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(input_ids=self.input_ids[i], labels=self.labels[i])</span><br></pre></td></tr></table></figure>
<p>玄机在：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_tokenize_fn</span>(<span class="params">strings: <span class="type">Sequence</span>[<span class="built_in">str</span>], tokenizer: transformers.PreTrainedTokenizer</span>) -&gt; <span class="type">Dict</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Tokenize a list of strings.&quot;&quot;&quot;</span></span><br><span class="line">    tokenized_list = [</span><br><span class="line">        tokenizer(</span><br><span class="line">            text,</span><br><span class="line">            return_tensors=<span class="string">&quot;pt&quot;</span>,</span><br><span class="line">            padding=<span class="string">&quot;longest&quot;</span>,</span><br><span class="line">            max_length=tokenizer.model_max_length,</span><br><span class="line">            truncation=<span class="literal">True</span>,</span><br><span class="line">        ) <span class="comment"># 这里做了循环，也就是对strings这个list里的每一个sequence单独做的tokenize，然后把这些不等长的input_ids一同放到一个list里。之所以用list，是因为list里可以存储不等长的list。一直到这一步都没有做padding</span></span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> strings</span><br><span class="line">    ]</span><br><span class="line">    input_ids = labels = [tokenized.input_ids[<span class="number">0</span>] <span class="keyword">for</span> tokenized <span class="keyword">in</span> tokenized_list]</span><br><span class="line">    input_ids_lens = labels_lens = [</span><br><span class="line">        tokenized.input_ids.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>().item() <span class="keyword">for</span> tokenized <span class="keyword">in</span> tokenized_list</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        labels=labels,</span><br><span class="line">        input_ids_lens=input_ids_lens,</span><br><span class="line">        labels_lens=labels_lens,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>当我们调用<code>SupervisedDataset</code>实例化数据后我们来看看数据长什么样子</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">train_dataset中有两个key，一个Input_ids,一个labels</span><br><span class="line">input_ids中的值长这样：</span><br><span class="line">tensor([    2, 45943,    16,    41, 15741,    14,  7448,    10,  3685,     4,</span><br><span class="line">        21062,    10,  1263,    14, 16574, 25830,     5,  2069,     4, 50118,</span><br><span class="line">        50118, 48134, 41241,    35, 50118, 31033,   130,  4965,    13,  4959,</span><br><span class="line">         2245,     4, 50118, 50118, 48134, 19121,    35,   134,     4, 43800,</span><br><span class="line">           10,  9320,  5626,     8,   146,   686,     7,   680,  2710,     9,</span><br><span class="line">        12849,     8,  8942,     4,  1437, 50118,   176,     4, 30450,  4595,</span><br><span class="line">            7,   489,   110,   809,  2171,     8,   670,     4,  1437, 50118,</span><br><span class="line">          246,     4,  2315,   615,  3581,     8,  3014,    10,  4292,  3581,</span><br><span class="line">         3078,     4,     2])</span><br><span class="line">labels中的值长这样：</span><br><span class="line">tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,</span><br><span class="line">         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,</span><br><span class="line">         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,</span><br><span class="line">         -100,  -100,  -100,  -100,  -100,  -100,  -100,   134,     4, 43800,</span><br><span class="line">           10,  9320,  5626,     8,   146,   686,     7,   680,  2710,     9,</span><br><span class="line">        12849,     8,  8942,     4,  1437, 50118,   176,     4, 30450,  4595,</span><br><span class="line">            7,   489,   110,   809,  2171,     8,   670,     4,  1437, 50118,</span><br><span class="line">          246,     4,  2315,   615,  3581,     8,  3014,    10,  4292,  3581,</span><br><span class="line">         3078,     4,     2])</span><br></pre></td></tr></table></figure>
<p>而且input_ids中每一个值的长度都是不同的，这是因为没有做padding的结果，<strong>仅仅</strong>是将所有的过长的sequence截断了。</p>
<p>羊驼的代码将所有的padding细节都放到了collator里：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataCollatorForSupervisedDataset</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Collate examples for supervised fine-tuning.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    tokenizer: transformers.PreTrainedTokenizer</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, instances: <span class="type">Sequence</span>[<span class="type">Dict</span>]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span></span><br><span class="line">        input_ids, labels = <span class="built_in">tuple</span>([instance[key] <span class="keyword">for</span> instance <span class="keyword">in</span> instances] <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">&quot;input_ids&quot;</span>, <span class="string">&quot;labels&quot;</span>))</span><br><span class="line">        input_ids = torch.nn.utils.rnn.pad_sequence(</span><br><span class="line">            input_ids, batch_first=<span class="literal">True</span>, padding_value=self.tokenizer.pad_token_id</span><br><span class="line">        )</span><br><span class="line">        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=<span class="literal">True</span>, padding_value=IGNORE_INDEX)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            labels=labels,</span><br><span class="line">            attention_mask=input_ids.ne(self.tokenizer.pad_token_id), <span class="comment"># see https://pytorch.org/docs/stable/generated/torch.ne.html#torch.ne</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>这里作者写了一个自己的类，继承自object。这里没有继承transformer的DefaultDataCollator，暂时不清楚用意，但我觉得应该也可以。这个类实现了一个<code>__call__</code>方法，接受的是一个Sequence（可迭代对象），对象中是字典（input_ids,
labels），我们上面在创建数据集的时候getitem每次返回一个dict，这个dict里有input_id和label。现在的collator接受的是这个字典的list，也就是有很多个数据（batch_size），我们对这个batch里的数据统一进行padding，这样就实现了在batch内部去pad，避免将所有的字符串都pad成最长的字符长度。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YAN">
      <meta itemprop="description" content="Be smart! Keep Learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YAN's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/" class="post-title-link" itemprop="url">构建和评估RAG应用</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-12-05 09:56:05 / Modified: 17:06:00" itemprop="dateCreated datePublished" datetime="2023-12-05T09:56:05+08:00">2023-12-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>1.6k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>1 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>最近吴恩达出了一个小课程，传送门: <a target="_blank" rel="noopener" href="https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/">Building
and Evaluating Advanced RAG Applications</a>。<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1494y1E7H9?p=3&amp;vd_source=f998f640fc8575504e3e97753bf817f4">B站</a>也有人搬运了，有中英文字幕。最近也正好在做RAG相关的项目，看到这个课程里有一些新的东西，权当在这篇博客里总结记录。</p>
<p>另外还推荐阅读一篇综述<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.05876">Trends in Integration of
Knowledge and Large Language Models: A Survey and Taxonomy of Methods,
Benchmarks, and Applications</a>, 该综述的第三章详细介绍了rettieval
augmentation的方法。我这篇博客会首先理顺一些理论，然后再介绍吴恩达课程里的知识（个人认为吴大佬出的关于LLM的一系列shot
course可食用性不够高，比如上面说的这个RAG相关的课怎么看都觉得是在推广LlamaIndex这个框架，对于原理一句话带过，很多细节不清楚）。</p>
<p>3.2节提到的两个工作值得注意：</p>
<ol type="1">
<li>Query2doc</li>
</ol>
<blockquote>
<p>Query2doc prompts the LLMs to generate a pseudo-document by employing
a few-shot prompting paradigm. Subsequently, the original query is
expanded by incorporating the pseudo-document. The retriever module uses
this new query to retrieve a list of relevant documents.</p>
</blockquote>
<ol start="2" type="1">
<li>Rewrite-Retrieve-Read</li>
</ol>
<blockquote>
<p>Different with Query2doc,they adopt a trainable language model to
perform the rewriting step</p>
</blockquote>
<p>在抽取的context的使用上，我们一般的认知是加入到prompt里，告诉LLM根据这个context回答某个query，这篇综述在3.2节还概括介绍了另外两种使用knowledge的方式：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20231205145030752.png" alt="image-20231205145030752">
<figcaption aria-hidden="true">image-20231205145030752</figcaption>
</figure>
<p>我个人认为第二种方式实操性差一点，第三种和第一种应该是大家会普遍采取的方式，第二种需要更多精细的prompt设计。</p>
<hr>
<p>以下为课程相关的 ，传送门: <a target="_blank" rel="noopener" href="https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/">Building
and Evaluating Advanced RAG Applications</a></p>
<h1 id="构建">构建</h1>
<p>构建一个准确的RAG系统，其中一个很重要的环节就是如何抽取跟query很相关的context。该课程介绍了两种retrival的方法</p>
<h2 id="sentence-window-retrieval">Sentence Window Retrieval</h2>
<p>乍看名字看不出来什么意思，其实本质很简单。在前期处理document时我们将document都处理成了小的chunks，然后对这些chunks进行embedding。我们对这些chunks进行query的时候，抽取出来的结果也是小的chunks，但很多时候抽取出来的那些chunks上下关联的chunk也是语义相关的，所以我们就让retrival不仅要返回抽取的最相似的chunks，也要把这些chunks周围window_size的chunk都一并返回给我作为context。</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20231205111431615.png" alt="image-20231205111431615">
<figcaption aria-hidden="true">image-20231205111431615</figcaption>
</figure>
<p>在sentence-window retrieval pipeline中：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20231205111519611.png" alt="image-20231205111519611">
<figcaption aria-hidden="true">image-20231205111519611</figcaption>
</figure>
<p>最重要的不同在<code>Context around the chunks added to the retrieved ones</code></p>
<h2 id="auto-merging-retrival">auto merging retrival</h2>
<h1 id="评估-evaluation">评估 Evaluation</h1>
<p>该课程建议从三个维度来评测一个RAG Application的好坏：</p>
<figure>
<img src="/2023/12/05/%E6%9E%84%E5%BB%BA%E5%92%8C%E8%AF%84%E4%BC%B0RAG%E5%BA%94%E7%94%A8/image-20231205105234142.png" alt="image-20231205105234142">
<figcaption aria-hidden="true">image-20231205105234142</figcaption>
</figure>
<ul>
<li>问题和回答的相关性</li>
<li>根据问题抽取出来的context和问题的相关性</li>
<li>回答和context的相关性</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/default_index/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/default_index/page/15/">15</a><a class="extend next" rel="next" href="/default_index/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YAN</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Symbols count total: </span>
    <span title="Symbols count total">182k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">2:46</span>
  </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
